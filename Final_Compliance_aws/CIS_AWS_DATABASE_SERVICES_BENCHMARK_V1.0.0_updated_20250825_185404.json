[
  {
    "id": "2.1",
    "title": "Ensure Amazon VPC (Virtual Private Cloud) has been created",
    "assessment": "Manual",
    "description": "Amazon VPCs allow you to launch AWS resources into a defined virtual network, providing network isolation and controlling inbound and outbound traffic. Here\u2019s a step- by-step guide on how to create an Amazon Virtual Private Cloud (VPC):",
    "rationale": "Impact: User would be required to have an AWS account to access AWS resources.",
    "audit": "1. Sign in to the AWS Management Console Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon VPC Console Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/vpc/. 3. Create a VPC In the Amazon VPC console, click Your VPCs in the left-hand navigation menu. \u2022 Click on the Create VPC button. \u2022 Provide a name and CIDR block for your VPC. The CIDR block defines the IP address range for your VPC. \u2022 Configure any additional settings, such as enabling DNS hostnames or DNS resolution. \u2022 Click on the Create button to create the VPC. 4. Create Subnets In the Amazon VPC console \u2022 Click on Subnets in the navigation menu. \u2022 Click on the Create subnet button. \u2022 Select the VPC you created in the previous step. \u2022 Provide a name and CIDR block for the subnet. The CIDR block should be a subset of the VPC's CIDR block. \u2022 Specify the availability zone where you want the subnet to be located. \u2022 Click on the Create button to create the subnet.  \u2022 Repeat these steps to create additional subnets if needed. 5. Create Security Groups In the Amazon VPC console, click Security Groups in the navigation menu. \u2022 Click on the Create security group button. \u2022 Provide a name and description for the security group. \u2022 Select the VPC you created earlier. \u2022 Configure inbound and outbound rules to allow the necessary network traffic for your Aurora database. \u2022 Click on the Create button to create the security group.",
    "remediation": "Follow the AWS documentation and create an AWS account to create a VPC (Virtual Private Cloud). References: 1. https://aws.amazon.com/products/databases/ 2. https://console.aws.amazon.com/vpc/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "User would be required to have an AWS account to access AWS resources.",
    "references": "1. https://aws.amazon.com/products/databases/ 2. https://console.aws.amazon.com/vpc/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon VPCs allow you to launch AWS resources into a defined virtual network, providing network isolation and controlling inbound and outbound traffic. Here\u2019s a step- by-step guide on how to create an Amazon Virtual Private Cloud (VPC):",
      "audit_steps": "1. Sign in to the AWS Management Console Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon VPC Console Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/vpc/. 3. Create a VPC In the Amazon VPC console, click Your VPCs in the left-hand navigation menu. \u2022 Click on the Create VPC button. \u2022 Provide a name and CIDR block for your VPC. The CIDR block defines the IP address range for your VPC. \u2022 Configure any additional settings, such as enabling DNS hostnames or DNS resolution. \u2022 Click on the Create button to create the VPC. 4. Create Subnets In the Amazon VPC console \u2022 Click on Subnets in the navigation menu. \u2022 Click on the Create subnet button. \u2022 Select the VPC you created in the previous step. \u2022 Provide a name and CIDR block for the subnet. The CIDR block should be a subset of the VPC's CIDR block. \u2022 Specify the availability zone where you want the subnet to be located. \u2022 Click on the Create button to create the subnet.  \u2022 Repeat these steps to create additional subnets if needed. 5. Create Security Groups In the Amazon VPC console, click Security Groups in the navigation menu. \u2022 Click on the Create security group button. \u2022 Provide a name and description for the security group. \u2022 Select the VPC you created earlier. \u2022 Configure inbound and outbound rules to allow the necessary network traffic for your Aurora database. \u2022 Click on the Create button to create the security group.",
      "remediation_steps": "Follow the AWS documentation and create an AWS account to create a VPC (Virtual Private Cloud). References: 1. https://aws.amazon.com/products/databases/ 2. https://console.aws.amazon.com/vpc/",
      "rationale": "Impact: User would be required to have an AWS account to access AWS resources.",
      "impact": "User would be required to have an AWS account to access AWS resources."
    },
    "function_name": "rds_vpc_created_check",
    "coverage": 90,
    "rule_id": "aws.rds.vpc.created.vpc_created"
  },
  {
    "id": "2.2",
    "title": "Ensure the Use of Security Groups",
    "assessment": "Manual",
    "description": "Security groups act as a firewall for associated Amazon RDS DB instances, controlling both inbound and outbound traffic. Here is a step-by-step guide on how to create and use Security Groups for an Amazon Aurora instance:",
    "rationale": "Creating your severity group either inbound or outbound rules. Inbound rules allow an individual to create a rule that permits the traffic to go to a specific port depending on which source it\u2019s coming from. Outbound rules enable your instances to connect with one another allow them to connect to the internet. If needed, you can limit the outgoing traffic.",
    "audit": "1. Sign in to AWS Management Console If you do not already have an AWS account, you'll need to create one at https://aws.amazon.com. 2. Navigate to Amazon EC2 Dashboard Once you have logged in to the AWS Management Console, navigate to the EC2 service. You can find this under the Compute category. 3. Create a New Security Group \u2022 In the EC2 Dashboard, find the Network & Security section on the left-side navigation pane, then click Security Groups. \u2022 Click on the Create Security Group button. 4. Configure the New Security Group \u2022 In the Create Security Group panel, give your new security group a name and a description. \u2022 Select the VPC in which your Amazon Aurora instance will be deployed. \u2022 Then click Create. 5. Add Rules to the Security Group After creating the Security Group, you can add inbound and outbound rules. For Inbound Rules: \u2022 Click on the Inbound rules tab, then click Edit inbound rules.  \u2022 Click Add Rule. For the type, select MYSQL/Aurora. For the source, you can specify the IP addresses allowed to access your Amazon Aurora instance. For Outbound Rules: \u2022 Click on the Outbound rules tab, then click Edit outbound rules. Outbound rules allow your instances to communicate with other instances or access the internet. You can restrict outbound traffic if necessary. In most cases, you can leave the default setting, which allows all outbound traffic. 6. Assign the Security Group to Amazon Aurora \u2022 When launching a new Amazon Aurora instance (in the Amazon RDS dashboard), you can select your new security group in the Configure advanced settings step. \u2022 If your Aurora instance has already been launched, you can modify it to use the new security group by selecting the instance. \u2022 Click Modify, and then select the new security group.",
    "remediation": "Once created a security group identify the rules you would like to create either inbound or outbound. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check",
      "ec2_default_security_group_no_inbound_rules"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check",
        "ec2_default_security_group_no_inbound_rules"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Security groups act as a firewall for associated Amazon RDS DB instances, controlling both inbound and outbound traffic. Here is a step-by-step guide on how to create and use Security Groups for an Amazon Aurora instance:",
      "audit_steps": "1. Sign in to AWS Management Console If you do not already have an AWS account, you'll need to create one at https://aws.amazon.com. 2. Navigate to Amazon EC2 Dashboard Once you have logged in to the AWS Management Console, navigate to the EC2 service. You can find this under the Compute category. 3. Create a New Security Group \u2022 In the EC2 Dashboard, find the Network & Security section on the left-side navigation pane, then click Security Groups. \u2022 Click on the Create Security Group button. 4. Configure the New Security Group \u2022 In the Create Security Group panel, give your new security group a name and a description. \u2022 Select the VPC in which your Amazon Aurora instance will be deployed. \u2022 Then click Create. 5. Add Rules to the Security Group After creating the Security Group, you can add inbound and outbound rules. For Inbound Rules: \u2022 Click on the Inbound rules tab, then click Edit inbound rules.  \u2022 Click Add Rule. For the type, select MYSQL/Aurora. For the source, you can specify the IP addresses allowed to access your Amazon Aurora instance. For Outbound Rules: \u2022 Click on the Outbound rules tab, then click Edit outbound rules. Outbound rules allow your instances to communicate with other instances or access the internet. You can restrict outbound traffic if necessary. In most cases, you can leave the default setting, which allows all outbound traffic. 6. Assign the Security Group to Amazon Aurora \u2022 When launching a new Amazon Aurora instance (in the Amazon RDS dashboard), you can select your new security group in the Configure advanced settings step. \u2022 If your Aurora instance has already been launched, you can modify it to use the new security group by selecting the instance. \u2022 Click Modify, and then select the new security group.",
      "remediation_steps": "Once created a security group identify the rules you would like to create either inbound or outbound. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Creating your severity group either inbound or outbound rules. Inbound rules allow an individual to create a rule that permits the traffic to go to a specific port depending on which source it\u2019s coming from. Outbound rules enable your instances to connect with one another allow them to connect to the internet. If needed, you can limit the outgoing traffic.",
      "impact": ""
    },
    "function_name": "rds_security_groups_configured_check",
    "coverage": 90,
    "rule_id": "aws.rds.security_group.configured.security_groups_configured"
  },
  {
    "id": "2.3",
    "title": "Ensure Data at Rest is Encrypted",
    "assessment": "Manual",
    "description": "Amazon Aurora allows you to encrypt your databases using keys you manage through AWS Key Management Service (KMS). Here is a step-by-step guide on how to encrypt data at rest for an Amazon Aurora instance:",
    "rationale": "Once you are in your AWS account you can either create or modify your existing Aurora DB. A master key would be needed by the authorized user to enable encryption. Enabling encryption would keep the user\u2019s data private and stored securely, which would only allow them to access it with their key. Impact: Unauthorized users will not be able to access the account because a key would be needed that only authorized users have access to.",
    "audit": "1. Sign in to AWS Management Console If you do not already have an AWS account, you'll need to create one at https://aws.amazon.com 2. Navigate to Amazon RDS Dashboard Navigate to the RDS service once logged in to the AWS Management Console. You can find this under the Database category. 3. Create or Modify an Amazon Aurora DB Instance \u2022 If creating a new Aurora DB instance, select Create Database and choose Amazon Aurora as your engine option. \u2022 If you are modifying an existing Aurora DB instance, select the instance from the RDS Dashboard and click Modify. 4. Enable Encryption \u2022 In the Settings or DB Instance Settings section, you will see an option labeled Enable encryption. Check this box to enable encryption for data at rest. \u2022 You will also need to select a master key to use for encryption. You can choose the default AWS managed key for RDS or a custom AWS Key Management Service (KMS) key you have created.   Note : \u2022 If you are creating a new DB instance, the Enable encryption option is found under the Settings section. \u2022 If you are modifying an existing DB instance, the Enable encryption option is found under the DB Instance Settings section. However, to encrypt an existing Aurora instance that was not initially created with encryption enabled, you will need to create a snapshot of the instance, make a copy of the snapshot with encryption enabled, and then restore the DB instance from the copied snapshot. 5. Launch the DB Instance \u2022 After you have selected the appropriate encryption settings, click Create database or Continue (if modifying an existing instance). \u2022 Review your settings on the following page, and if everything looks correct, click Launch DB Instance or Modify DB Instance.",
    "remediation": "Allows authorized user to access the data. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Unauthorized users will not be able to access the account because a key would be needed that only authorized users have access to.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check",
      "aurora_database_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check",
        "aurora_database_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon Aurora allows you to encrypt your databases using keys you manage through AWS Key Management Service (KMS). Here is a step-by-step guide on how to encrypt data at rest for an Amazon Aurora instance:",
      "audit_steps": "1. Sign in to AWS Management Console If you do not already have an AWS account, you'll need to create one at https://aws.amazon.com 2. Navigate to Amazon RDS Dashboard Navigate to the RDS service once logged in to the AWS Management Console. You can find this under the Database category. 3. Create or Modify an Amazon Aurora DB Instance \u2022 If creating a new Aurora DB instance, select Create Database and choose Amazon Aurora as your engine option. \u2022 If you are modifying an existing Aurora DB instance, select the instance from the RDS Dashboard and click Modify. 4. Enable Encryption \u2022 In the Settings or DB Instance Settings section, you will see an option labeled Enable encryption. Check this box to enable encryption for data at rest. \u2022 You will also need to select a master key to use for encryption. You can choose the default AWS managed key for RDS or a custom AWS Key Management Service (KMS) key you have created.   Note : \u2022 If you are creating a new DB instance, the Enable encryption option is found under the Settings section. \u2022 If you are modifying an existing DB instance, the Enable encryption option is found under the DB Instance Settings section. However, to encrypt an existing Aurora instance that was not initially created with encryption enabled, you will need to create a snapshot of the instance, make a copy of the snapshot with encryption enabled, and then restore the DB instance from the copied snapshot. 5. Launch the DB Instance \u2022 After you have selected the appropriate encryption settings, click Create database or Continue (if modifying an existing instance). \u2022 Review your settings on the following page, and if everything looks correct, click Launch DB Instance or Modify DB Instance.",
      "remediation_steps": "Allows authorized user to access the data. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Once you are in your AWS account you can either create or modify your existing Aurora DB. A master key would be needed by the authorized user to enable encryption. Enabling encryption would keep the user\u2019s data private and stored securely, which would only allow them to access it with their key. Impact: Unauthorized users will not be able to access the account because a key would be needed that only authorized users have access to.",
      "impact": "Unauthorized users will not be able to access the account because a key would be needed that only authorized users have access to."
    },
    "function_name": "rds_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.instance.encryption.encryption_at_rest_enabled"
  },
  {
    "id": "2.4",
    "title": "Ensure Data in Transit is Encrypted",
    "assessment": "Manual",
    "description": "Use SSL (Secure Sockets Layer) to secure data in transit. Aurora supports SSL- encrypted connections between your application and your DB instance. Here is a step- by-step guide on how to encrypt data in transit for an Amazon Aurora instance:",
    "rationale": "Aurora supports SSL-encrypted application for the individual DB. To secure your data in transit the individual should identify their client application and what is supported by SSL/TLS in order to configure it correctly. Impact: If the configuration is not properly implemented the data can be compromised by malicious actors, they could cause ransomware attack or possibly steal data.",
    "audit": "1. Sign in to AWS Management Console If you do not already have an AWS account, you'll need to create one at https://aws.amazon.com. 2. Navigate to Amazon RDS Dashboard \u2022 Navigate to the RDS service once logged in to the AWS Management Console. \u2022 You can find this under the Database category. 3. Create or Modify an Amazon Aurora DB Instance \u2022 If creating a new Aurora DB instance, select Create Database and choose Amazon Aurora as your engine option. \u2022 If you are modifying an existing Aurora DB instance, select the instance from the RDS Dashboard and click Modify. 4. Enable Encryption \u2022 By default, Aurora uses Secure Socket Layer (SSL) or Transport Layer Security (TLS) to encrypt data in transit. However, you must ensure that your client application supports SSL/TLS and is correctly configured to use it. \u2022 For MySQL-compatible Aurora, Amazon provides an SSL certificate that you can download from their documentation.  \u2022 PostgreSQL-compatible Aurora uses the default PostgreSQL SSL certificate. Once you have the appropriate certificate, you must configure your client application to use SSL/TLS. For example, in MySQL, you might use a command like this: mysql -h <myinstance.123456789012.us-east-1.rds.amazonaws.com> --ssl- ca=</path_to_certificate/rds-combined-ca-bundle.pem> --ssl- mode=VERIFY_IDENTITY For PostgreSQL, you might use a command like this: psql \"host=<myinstance.123456789012.us-east-1.rds.amazonaws.com> sslmode=verify-ca sslrootcert=</path_to_certificate/rds-combined-ca- bundle.pem>\" Replace <myinstance.123456789012.us-east-1.rds.amazonaws.com> with the endpoint for your DB instance, and replace </path_to_certificate/rds-combined-ca- bundle.pem> with the path to the SSL certificate on your local machine. 5. Verify Encryption After configuring your client to use SSL/TLS, you should verify that encryption in transit is working correctly. You can do this by checking the status of the SSL connection from within the database itself. For example, in MySQL, you can run the following command: SHOW STATUS LIKE 'Ssl_cipher'; In PostgreSQL, you can run the following command: SHOW ssl; In both cases, if SSL is enabled, you should see a non-empty cipher suite or on as a result.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the configuration is not properly implemented the data can be compromised by malicious actors, they could cause ransomware attack or possibly steal data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_in_transit_encryption_enabled",
      "rds_instance_ssl_encryption_enabled"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_in_transit_encryption_enabled",
        "rds_instance_ssl_encryption_enabled"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Use SSL (Secure Sockets Layer) to secure data in transit. Aurora supports SSL- encrypted connections between your application and your DB instance. Here is a step- by-step guide on how to encrypt data in transit for an Amazon Aurora instance:",
      "audit_steps": "1. Sign in to AWS Management Console If you do not already have an AWS account, you'll need to create one at https://aws.amazon.com. 2. Navigate to Amazon RDS Dashboard \u2022 Navigate to the RDS service once logged in to the AWS Management Console. \u2022 You can find this under the Database category. 3. Create or Modify an Amazon Aurora DB Instance \u2022 If creating a new Aurora DB instance, select Create Database and choose Amazon Aurora as your engine option. \u2022 If you are modifying an existing Aurora DB instance, select the instance from the RDS Dashboard and click Modify. 4. Enable Encryption \u2022 By default, Aurora uses Secure Socket Layer (SSL) or Transport Layer Security (TLS) to encrypt data in transit. However, you must ensure that your client application supports SSL/TLS and is correctly configured to use it. \u2022 For MySQL-compatible Aurora, Amazon provides an SSL certificate that you can download from their documentation.  \u2022 PostgreSQL-compatible Aurora uses the default PostgreSQL SSL certificate. Once you have the appropriate certificate, you must configure your client application to use SSL/TLS. For example, in MySQL, you might use a command like this: mysql -h <myinstance.123456789012.us-east-1.rds.amazonaws.com> --ssl- ca=</path_to_certificate/rds-combined-ca-bundle.pem> --ssl- mode=VERIFY_IDENTITY For PostgreSQL, you might use a command like this: psql \"host=<myinstance.123456789012.us-east-1.rds.amazonaws.com> sslmode=verify-ca sslrootcert=</path_to_certificate/rds-combined-ca- bundle.pem>\" Replace <myinstance.123456789012.us-east-1.rds.amazonaws.com> with the endpoint for your DB instance, and replace </path_to_certificate/rds-combined-ca- bundle.pem> with the path to the SSL certificate on your local machine. 5. Verify Encryption After configuring your client to use SSL/TLS, you should verify that encryption in transit is working correctly. You can do this by checking the status of the SSL connection from within the database itself. For example, in MySQL, you can run the following command: SHOW STATUS LIKE 'Ssl_cipher'; In PostgreSQL, you can run the following command: SHOW ssl; In both cases, if SSL is enabled, you should see a non-empty cipher suite or on as a result.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Aurora supports SSL-encrypted application for the individual DB. To secure your data in transit the individual should identify their client application and what is supported by SSL/TLS in order to configure it correctly. Impact: If the configuration is not properly implemented the data can be compromised by malicious actors, they could cause ransomware attack or possibly steal data.",
      "impact": "If the configuration is not properly implemented the data can be compromised by malicious actors, they could cause ransomware attack or possibly steal data."
    },
    "function_name": "rds_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.instance.encryption.encryption_in_transit_enabled"
  },
  {
    "id": "2.5",
    "title": "Ensure IAM Roles and Policies are Created",
    "assessment": "Manual",
    "description": "AWS Identity and Access Management (IAM) helps manage access to AWS resources. While you cannot directly associate IAM roles with Amazon Aurora instances, you can use IAM roles and policies to define which AWS IAM users and groups have management permissions for Amazon RDS resources and what actions they can perform. Here is a guide:",
    "rationale": "Individual creates IAM roles and polices that define specific permission given to that role. This determines what the identity or instance can and cannot do. Impact: If an IAM Role is not created, then it would be challenging to access AWS resources.",
    "audit": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to IAM Dashboard \u2022 Navigate to the IAM service once logged in to the AWS Management Console. \u2022 This is under the Security, Identity, & Compliance category. 3. Create a New IAM Role \u2022 In the IAM Dashboard, find the Roles section on the left-side navigation pane and click on it. Then, click on the Create Role button. 4. Select the Service that will Use the Role \u2022 Choose RDS as the AWS service that will use this new role, then click Next: Permissions. 5. Attach Policy  \u2022 In the next screen, you can attach policies defining this role\u2019s permissions. You can use the filter to find existing policies like AmazonRDSFullAccess or AmazonRDSReadOnlyAccess. \u2022 Select the appropriate policy and then click Next: Tags. 6. Add Tags (Optional) \u2022 You can add metadata to the role by attaching tags as key-value pairs. This is optional, and you can proceed to the next step if you do not wish to add tags. 7. Review \u2022 Provide a name and a description for the role. Review the role and then click Create Role. 8. Creating IAM Policy (Optional) \u2022 You can create a custom IAM policy if the predefined policies do not meet your requirements. \u2022 Navigate to Policies in the IAM dashboard and click Create Policy. \u2022 Use the visual editor or JSON editor to define the permissions. \u2022 Once done, click Review Policy, give it a name and a description, and click Create Policy. \u2022 You can then attach this custom policy to the IAM role. 9. Assign the IAM Role to an IAM User or Group To assign the newly created role to an IAM User or Group. \u2022 Navigate to the user or group in the IAM dashboard. \u2022 Click Add permissions. \u2022 Then Attach existing policies directly. \u2022 Use the filter to find your new role and select it. \u2022 Click Next: Review and then Add permissions.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If an IAM Role is not created, then it would be challenging to access AWS resources.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_iam_authentication_enabled",
      "rds_instance_iam_authentication_enabled"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_iam_authentication_enabled",
        "rds_instance_iam_authentication_enabled"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS Identity and Access Management (IAM) helps manage access to AWS resources. While you cannot directly associate IAM roles with Amazon Aurora instances, you can use IAM roles and policies to define which AWS IAM users and groups have management permissions for Amazon RDS resources and what actions they can perform. Here is a guide:",
      "audit_steps": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to IAM Dashboard \u2022 Navigate to the IAM service once logged in to the AWS Management Console. \u2022 This is under the Security, Identity, & Compliance category. 3. Create a New IAM Role \u2022 In the IAM Dashboard, find the Roles section on the left-side navigation pane and click on it. Then, click on the Create Role button. 4. Select the Service that will Use the Role \u2022 Choose RDS as the AWS service that will use this new role, then click Next: Permissions. 5. Attach Policy  \u2022 In the next screen, you can attach policies defining this role\u2019s permissions. You can use the filter to find existing policies like AmazonRDSFullAccess or AmazonRDSReadOnlyAccess. \u2022 Select the appropriate policy and then click Next: Tags. 6. Add Tags (Optional) \u2022 You can add metadata to the role by attaching tags as key-value pairs. This is optional, and you can proceed to the next step if you do not wish to add tags. 7. Review \u2022 Provide a name and a description for the role. Review the role and then click Create Role. 8. Creating IAM Policy (Optional) \u2022 You can create a custom IAM policy if the predefined policies do not meet your requirements. \u2022 Navigate to Policies in the IAM dashboard and click Create Policy. \u2022 Use the visual editor or JSON editor to define the permissions. \u2022 Once done, click Review Policy, give it a name and a description, and click Create Policy. \u2022 You can then attach this custom policy to the IAM role. 9. Assign the IAM Role to an IAM User or Group To assign the newly created role to an IAM User or Group. \u2022 Navigate to the user or group in the IAM dashboard. \u2022 Click Add permissions. \u2022 Then Attach existing policies directly. \u2022 Use the filter to find your new role and select it. \u2022 Click Next: Review and then Add permissions.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Individual creates IAM roles and polices that define specific permission given to that role. This determines what the identity or instance can and cannot do. Impact: If an IAM Role is not created, then it would be challenging to access AWS resources.",
      "impact": "If an IAM Role is not created, then it would be challenging to access AWS resources."
    },
    "function_name": "rds_iam_roles_policies_created_check",
    "coverage": 90,
    "rule_id": "aws.rds.iam.roles.iam_roles_policies_created"
  },
  {
    "id": "2.6",
    "title": "Ensure Database Audit Logging is Enabled",
    "assessment": "Manual",
    "description": "Amazon Aurora provides advanced auditing capabilities through AWS CloudTrail and Amazon RDS Database Activity Streams. Here is a step-by-step guide on how to enable and use these features:",
    "rationale": "Allows individuals to access and retrieve their old logs, log their new events, and store their log.",
    "audit": "Below are the instructions for enabling logging through AWS CloudTrail: 1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to CloudTrail Dashboard \u2022 Navigate to the CloudTrail service. \u2022 You can find this under the Management & Governance category. 3. Create a new trail \u2022 In the CloudTrail Dashboard, click on Create trail. \u2022 Provide a name for the trail, and specify the S3 bucket where you want the logs to be stored. 4. Configure trail settings \u2022 Choose the settings that meet your requirements. For instance, you can log events for all regions, or you can log management events, data events, or both. 5. Create the trail \u2022 After specifying the trail settings, click Create. Below are the instructions for enabling logging through Amazon Database Activity Streams:  1. Navigate to Amazon RDS Dashboard \u2022 In the AWS Management Console, navigate to the RDS service. \u2022 You can find this under the Database category. 2. Choose your Aurora DB instance \u2022 In the RDS Dashboard, click on Databases, and then click on the name of your Aurora DB instance. 3. Enable Database Activity Streams \u2022 In the Connectivity & Security tab, find the Database Activity Streams section. Click Create stream. \u2022 In the Create Stream panel, choose the settings that meet your requirements and click Create. Note : Enabling Database Activity Streams can impact the performance of your DB instance, so you should test this feature in a non-production environment before enabling it in production. 4. View the Database Activity Stream \u2022 You can view the Database Activity Stream using Amazon Kinesis Data Streams. \u2022 In the Kinesis Data Streams dashboard, click on the stream\u2019s name and then click View data.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_integration_cloudwatch_logs",
      "aurora_audit_logging_and_access_control_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_integration_cloudwatch_logs",
        "aurora_audit_logging_and_access_control_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon Aurora provides advanced auditing capabilities through AWS CloudTrail and Amazon RDS Database Activity Streams. Here is a step-by-step guide on how to enable and use these features:",
      "audit_steps": "Below are the instructions for enabling logging through AWS CloudTrail: 1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to CloudTrail Dashboard \u2022 Navigate to the CloudTrail service. \u2022 You can find this under the Management & Governance category. 3. Create a new trail \u2022 In the CloudTrail Dashboard, click on Create trail. \u2022 Provide a name for the trail, and specify the S3 bucket where you want the logs to be stored. 4. Configure trail settings \u2022 Choose the settings that meet your requirements. For instance, you can log events for all regions, or you can log management events, data events, or both. 5. Create the trail \u2022 After specifying the trail settings, click Create. Below are the instructions for enabling logging through Amazon Database Activity Streams:  1. Navigate to Amazon RDS Dashboard \u2022 In the AWS Management Console, navigate to the RDS service. \u2022 You can find this under the Database category. 2. Choose your Aurora DB instance \u2022 In the RDS Dashboard, click on Databases, and then click on the name of your Aurora DB instance. 3. Enable Database Activity Streams \u2022 In the Connectivity & Security tab, find the Database Activity Streams section. Click Create stream. \u2022 In the Create Stream panel, choose the settings that meet your requirements and click Create. Note : Enabling Database Activity Streams can impact the performance of your DB instance, so you should test this feature in a non-production environment before enabling it in production. 4. View the Database Activity Stream \u2022 You can view the Database Activity Stream using Amazon Kinesis Data Streams. \u2022 In the Kinesis Data Streams dashboard, click on the stream\u2019s name and then click View data.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Allows individuals to access and retrieve their old logs, log their new events, and store their log.",
      "impact": ""
    },
    "function_name": "rds_audit_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.instance.logging.audit_logging_enabled"
  },
  {
    "id": "2.7",
    "title": "Ensure Passwords are Regularly Rotated",
    "assessment": "Manual",
    "description": "Regularly rotating your Aurora passwords is critical to access management, contributing to maintaining system security. The database password can be rotated in Amazon Aurora, but the access keys refer to the rotation of AWS IAM User access keys.",
    "rationale": "Updating your password is critical to access AWS resources. This also ensures that your account is being kept safe from a potential threat. Impact: Having the passwords updated frequently allows only the authorized individual to access the AWS resources.",
    "audit": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to Amazon RDS Dashboard \u2022 Navigate to the RDS service once logged in to the AWS Management Console. You can find this under the Database category. 3. Choose your Aurora DB instance \u2022 In the RDS Dashboard, click on Databases, and then click on the name of your Aurora DB instance. 4. Modify the instance \u2022 Click Modify. \u2022 In the Settings section, enter a new password in the Master password and Confirm password fields. 5. Apply the changes  \u2022 Scroll to the bottom and choose when to apply the changes. You can apply them immediately or schedule them for the next maintenance window. \u2022 Then, click Continue and Modify DB Instance. Note : Changing the master password will reboot the DB instance if you apply the change immediately.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Having the passwords updated frequently allows only the authorized individual to access the AWS resources.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "aurora_password_rotation_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "aurora_password_rotation_check"
      ],
      "automation_level": "MANUAL_WITH_FUNCTIONS",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Regularly rotating your Aurora passwords is critical to access management, contributing to maintaining system security. The database password can be rotated in Amazon Aurora, but the access keys refer to the rotation of AWS IAM User access keys.",
      "audit_steps": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to Amazon RDS Dashboard \u2022 Navigate to the RDS service once logged in to the AWS Management Console. You can find this under the Database category. 3. Choose your Aurora DB instance \u2022 In the RDS Dashboard, click on Databases, and then click on the name of your Aurora DB instance. 4. Modify the instance \u2022 Click Modify. \u2022 In the Settings section, enter a new password in the Master password and Confirm password fields. 5. Apply the changes  \u2022 Scroll to the bottom and choose when to apply the changes. You can apply them immediately or schedule them for the next maintenance window. \u2022 Then, click Continue and Modify DB Instance. Note : Changing the master password will reboot the DB instance if you apply the change immediately.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Updating your password is critical to access AWS resources. This also ensures that your account is being kept safe from a potential threat. Impact: Having the passwords updated frequently allows only the authorized individual to access the AWS resources.",
      "impact": "Having the passwords updated frequently allows only the authorized individual to access the AWS resources."
    },
    "function_name": "rds_passwords_rotation_check",
    "coverage": 90,
    "rule_id": "aws.rds.credentials.passwords.password_rotation_enabled"
  },
  {
    "id": "2.8",
    "title": "Ensure Access Keys are Regularly Rotated",
    "assessment": "Manual",
    "description": "Regularly rotating your Aurora Access Keys is critical to access management, contributing to maintaining system security.",
    "rationale": "Rotating AWS IAM user access keys ensures security and any potential risk of the business that may be compromised due to the active key since it changes quite often. Impact: Only authorized personnel would need to login with their key, which restricts unauthorized users access to the database.",
    "audit": "1. Navigate to IAM Dashboard \u2022 In the AWS Management Console, navigate to the IAM service. \u2022 You can find this under the Security, Identity, & Compliance category. 2. Choose the IAM User \u2022 In the IAM Dashboard, click on Users. \u2022 Then click on the name of the user whose access keys you want to rotate. 3. Create a New Access Key \u2022 In the Security credentials tab, click Create access key. \u2022 Store the new access key ID and secret access key in a secure location. 4. Replace the Old Access Key \u2022 Update all your applications to use the new access key. \u2022 Verify that the applications are working correctly. 5. Delete the Old Access Key \u2022 In the Security credentials tab, find the old access key. \u2022 Click Delete, and then confirm the deletion.  Note : To avoid disrupting your applications, keep the old access key until you have replaced it in all your applications and confirmed that they are working correctly.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Only authorized personnel would need to login with their key, which restricts unauthorized users access to the database.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "iam",
      "functions": [],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use iam boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Regularly rotating your Aurora Access Keys is critical to access management, contributing to maintaining system security.",
      "audit_steps": "1. Navigate to IAM Dashboard \u2022 In the AWS Management Console, navigate to the IAM service. \u2022 You can find this under the Security, Identity, & Compliance category. 2. Choose the IAM User \u2022 In the IAM Dashboard, click on Users. \u2022 Then click on the name of the user whose access keys you want to rotate. 3. Create a New Access Key \u2022 In the Security credentials tab, click Create access key. \u2022 Store the new access key ID and secret access key in a secure location. 4. Replace the Old Access Key \u2022 Update all your applications to use the new access key. \u2022 Verify that the applications are working correctly. 5. Delete the Old Access Key \u2022 In the Security credentials tab, find the old access key. \u2022 Click Delete, and then confirm the deletion.  Note : To avoid disrupting your applications, keep the old access key until you have replaced it in all your applications and confirmed that they are working correctly.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Rotating AWS IAM user access keys ensures security and any potential risk of the business that may be compromised due to the active key since it changes quite often. Impact: Only authorized personnel would need to login with their key, which restricts unauthorized users access to the database.",
      "impact": "Only authorized personnel would need to login with their key, which restricts unauthorized users access to the database."
    },
    "function_name": "rds_access_keys_rotation_check",
    "coverage": 90,
    "rule_id": "aws.rds.credentials.access_keys.access_key_rotation_enabled"
  },
  {
    "id": "2.9",
    "title": "Ensure Least Privilege Access",
    "assessment": "Manual",
    "description": "Use the principle of least privilege when granting access to your Amazon Aurora resources. This principle of least privilege (POLP) is a computer security concept where users are given the minimum access levels necessary to complete their job functions. In Amazon Aurora, this can be implemented at various levels, including AWS IAM for managing AWS resources and within the database for managing database users and roles. Here is a step-by-step guide for each:",
    "rationale": "POLP limits the user interaction on the database, and it only gives the database permission to complete the necessary or mandatory task. AWS IAM gives permission for what the entity can and cannot do. Incorporating both POLP and AWS IAM in a database gives limited permission to the user to complete the tasks. Impact: Users would need to create a IAM role to implement POLP into their database.",
    "audit": "Implementing POLP with AWS IAM 1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to IAM Dashboard \u2022 Navigate to the IAM service once logged in to the AWS Management Console. \u2022 You can find this under the Security, Identity, & Compliance category. 3. Create a new IAM role or user \u2022 If creating a new IAM role or user, click Roles or Users. \u2022 Then Create role or Create user.   4. Attach minimum necessary permissions \u2022 When attaching policies, give only the permissions necessary to perform the intended tasks. \u2022 AWS provides many predefined policies designed following the POLP. You can create a custom policy with precise - permissions if none suits your needs. Implementing POLP within Amazon Aurora 1. Log into your Aurora Database Depending on your Aurora database engine, you can log in through the terminal using a MySQL or PostgreSQL client. You'll need your host endpoint, username, and password to log in. 2. Create a new user You can create a new user with the CREATE USER command in SQL. For example, CREATE USER '<username>'@'<localhost>' IDENTIFIED BY 'password'; 3. Grant minimal necessary privileges After creating the user, you can grant privileges using the GRANT command. The privileges should be as limited as possible for the user to perform their necessary functions. For example, GRANT SELECT, INSERT ON <mydb.mytbl> TO '<username>'@'<localhost>'; 4. Regularly review permissions It is essential to regularly review and update permissions to make sure they adhere to the principle of least privilege. You can view a user's permissions with the SHOW GRANTS command; for example, SHOW GRANTS FOR '<username>'@'<localhost>';",
    "remediation": "This is important because it reduces and secures any possible threat that an unauthorized user can gain by hacking into the system. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Users would need to create a IAM role to implement POLP into their database.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "aurora_user_role_and_iam_policy_check",
      "database_authentication_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "aurora_user_role_and_iam_policy_check",
        "database_authentication_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Use the principle of least privilege when granting access to your Amazon Aurora resources. This principle of least privilege (POLP) is a computer security concept where users are given the minimum access levels necessary to complete their job functions. In Amazon Aurora, this can be implemented at various levels, including AWS IAM for managing AWS resources and within the database for managing database users and roles. Here is a step-by-step guide for each:",
      "audit_steps": "Implementing POLP with AWS IAM 1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to IAM Dashboard \u2022 Navigate to the IAM service once logged in to the AWS Management Console. \u2022 You can find this under the Security, Identity, & Compliance category. 3. Create a new IAM role or user \u2022 If creating a new IAM role or user, click Roles or Users. \u2022 Then Create role or Create user.   4. Attach minimum necessary permissions \u2022 When attaching policies, give only the permissions necessary to perform the intended tasks. \u2022 AWS provides many predefined policies designed following the POLP. You can create a custom policy with precise - permissions if none suits your needs. Implementing POLP within Amazon Aurora 1. Log into your Aurora Database Depending on your Aurora database engine, you can log in through the terminal using a MySQL or PostgreSQL client. You'll need your host endpoint, username, and password to log in. 2. Create a new user You can create a new user with the CREATE USER command in SQL. For example, CREATE USER '<username>'@'<localhost>' IDENTIFIED BY 'password'; 3. Grant minimal necessary privileges After creating the user, you can grant privileges using the GRANT command. The privileges should be as limited as possible for the user to perform their necessary functions. For example, GRANT SELECT, INSERT ON <mydb.mytbl> TO '<username>'@'<localhost>'; 4. Regularly review permissions It is essential to regularly review and update permissions to make sure they adhere to the principle of least privilege. You can view a user's permissions with the SHOW GRANTS command; for example, SHOW GRANTS FOR '<username>'@'<localhost>';",
      "remediation_steps": "This is important because it reduces and secures any possible threat that an unauthorized user can gain by hacking into the system. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "POLP limits the user interaction on the database, and it only gives the database permission to complete the necessary or mandatory task. AWS IAM gives permission for what the entity can and cannot do. Incorporating both POLP and AWS IAM in a database gives limited permission to the user to complete the tasks. Impact: Users would need to create a IAM role to implement POLP into their database.",
      "impact": "Users would need to create a IAM role to implement POLP into their database."
    },
    "function_name": "rds_least_privilege_access_check",
    "coverage": 90,
    "rule_id": "aws.rds.access.privileges.least_privilege_access_enforced"
  },
  {
    "id": "2.10",
    "title": "Ensure Automatic Backups and Retention Policies are configured",
    "assessment": "Manual",
    "description": "Backups help protect your data from accidental loss or database failure. With Amazon Aurora, you can turn on automatic backups and specify a retention period. The backups include a daily snapshot of the entire DB instance and transaction logs.",
    "rationale": "The individual logs into their account and chooses their database once selected they can modify the backup settings. To have the database being backed up automatically the individual is encouraged to select from 1 to 35 days. This ensures that the file is being saved automatically and can prevent it from accidental loss. This ensures that the individual can restore their files quickly in the event of a data loss. Impact: It would result in having the files protected and being able to retrieve those files in the event of an accidental loss.",
    "audit": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to Amazon RDS Dashboard \u2022 Navigate to the RDS service once logged in to the AWS Management Console. \u2022 You can find this under the Database category. 3. Choose your Aurora DB instance \u2022 In the RDS Dashboard, click on Databases. \u2022 Then click on the name of your Aurora DB instance. 4. Check or modify the backup settings \u2022 In the Details section, find the Backup section. Here, you can see if automatic backups are enabled (the Backup retention period is more than 0 days) and when the backup window is.  o To modify these settings, click Modify. o In the Backup section of the Modify DB instance screen, you can change the Backup retention period and the Backup window. o The retention period can be between 1 and 35 days. To disable automatic backups, set the retention period to 0 days. 5. Apply the changes \u2022 Scroll to the bottom and choose when to apply the changes. You can apply them immediately or schedule them for the next maintenance window. \u2022 Then, click Continue and Modify DB Instance.",
    "remediation": "This is important because it would allow the user to automatically save their files and instantly have access to their files when needed. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "It would result in having the files protected and being able to retrieve those files in the event of an accidental loss.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_backup_enabled",
      "rds_cluster_protected_by_backup_plan",
      "rds_instance_backup_retention_period"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_backup_enabled",
        "rds_cluster_protected_by_backup_plan",
        "rds_instance_backup_retention_period"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Backups help protect your data from accidental loss or database failure. With Amazon Aurora, you can turn on automatic backups and specify a retention period. The backups include a daily snapshot of the entire DB instance and transaction logs.",
      "audit_steps": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to Amazon RDS Dashboard \u2022 Navigate to the RDS service once logged in to the AWS Management Console. \u2022 You can find this under the Database category. 3. Choose your Aurora DB instance \u2022 In the RDS Dashboard, click on Databases. \u2022 Then click on the name of your Aurora DB instance. 4. Check or modify the backup settings \u2022 In the Details section, find the Backup section. Here, you can see if automatic backups are enabled (the Backup retention period is more than 0 days) and when the backup window is.  o To modify these settings, click Modify. o In the Backup section of the Modify DB instance screen, you can change the Backup retention period and the Backup window. o The retention period can be between 1 and 35 days. To disable automatic backups, set the retention period to 0 days. 5. Apply the changes \u2022 Scroll to the bottom and choose when to apply the changes. You can apply them immediately or schedule them for the next maintenance window. \u2022 Then, click Continue and Modify DB Instance.",
      "remediation_steps": "This is important because it would allow the user to automatically save their files and instantly have access to their files when needed. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "The individual logs into their account and chooses their database once selected they can modify the backup settings. To have the database being backed up automatically the individual is encouraged to select from 1 to 35 days. This ensures that the file is being saved automatically and can prevent it from accidental loss. This ensures that the individual can restore their files quickly in the event of a data loss. Impact: It would result in having the files protected and being able to retrieve those files in the event of an accidental loss.",
      "impact": "It would result in having the files protected and being able to retrieve those files in the event of an accidental loss."
    },
    "function_name": "rds_backup_retention_policies_check",
    "coverage": 90,
    "rule_id": "aws.rds.backup.retention.backup_retention_policies_configured"
  },
  {
    "id": "2.11",
    "title": "Ensure Multi-Factor Authentication (MFA) is in use",
    "assessment": "Manual",
    "description": "MFA adds an extra layer of protection to your AWS resources. MFA can be used to secure AWS Management Console and CLI access which indirectly affects Aurora and other AWS services.",
    "rationale": "Multi-Factor Authentication (MFA) requires an individual to select a second step of verification process to access the platform. The individual has a choice of either selecting a virtual MFA device or a hardware MFA device to complete the process. MFA must also be used when performing specific actions to modify their database. Impact: The user is required to complete the second step which is the multi-factor authentication before any access is granted to them.",
    "audit": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to IAM Dashboard \u2022 Navigate to the IAM service once logged in to the AWS Management Console. \u2022 You can find this under the Security, Identity, & Compliance category. 3. Select the User \u2022 In the IAM Dashboard, click on Users. \u2022 Click on the name of the user for whom you want to enable MFA. 4. Manage MFA Device \u2022 In the User details page, click the Security credentials tab. \u2022 In the Multi-factor authentication (MFA) section. \u2022 Click on Manage.   5. Choose MFA Device Type \u2022 You can choose a virtual MFA device (such as an app on a smartphone) or a hardware MFA device. \u2022 Choose the device type that suits your requirements. 6. Follow the MFA Device Setup Wizard \u2022 The setup wizard will guide you through setting up your MFA device. \u2022 This will typically involve scanning a QR code or entering a serial number in your MFA device and then entering two consecutive MFA codes from your device. 7. Enable MFA Protected API Access \u2022 By writing an IAM policy, you can enforce MFA authentication for AWS CLI or SDK operations. This policy specifies that MFA must be used to perform specific actions, such as calling the Amazon RDS APIs to modify a DB instance.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "The user is required to complete the second step which is the multi-factor authentication before any access is granted to them.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_multi_az",
      "rds_instance_multi_az"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_multi_az",
        "rds_instance_multi_az"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "MFA adds an extra layer of protection to your AWS resources. MFA can be used to secure AWS Management Console and CLI access which indirectly affects Aurora and other AWS services.",
      "audit_steps": "1. Sign in to AWS Management Console \u2022 If you do not already have an AWS account, you will need to create one at https://aws.amazon.com. 2. Navigate to IAM Dashboard \u2022 Navigate to the IAM service once logged in to the AWS Management Console. \u2022 You can find this under the Security, Identity, & Compliance category. 3. Select the User \u2022 In the IAM Dashboard, click on Users. \u2022 Click on the name of the user for whom you want to enable MFA. 4. Manage MFA Device \u2022 In the User details page, click the Security credentials tab. \u2022 In the Multi-factor authentication (MFA) section. \u2022 Click on Manage.   5. Choose MFA Device Type \u2022 You can choose a virtual MFA device (such as an app on a smartphone) or a hardware MFA device. \u2022 Choose the device type that suits your requirements. 6. Follow the MFA Device Setup Wizard \u2022 The setup wizard will guide you through setting up your MFA device. \u2022 This will typically involve scanning a QR code or entering a serial number in your MFA device and then entering two consecutive MFA codes from your device. 7. Enable MFA Protected API Access \u2022 By writing an IAM policy, you can enforce MFA authentication for AWS CLI or SDK operations. This policy specifies that MFA must be used to perform specific actions, such as calling the Amazon RDS APIs to modify a DB instance.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Multi-Factor Authentication (MFA) requires an individual to select a second step of verification process to access the platform. The individual has a choice of either selecting a virtual MFA device or a hardware MFA device to complete the process. MFA must also be used when performing specific actions to modify their database. Impact: The user is required to complete the second step which is the multi-factor authentication before any access is granted to them.",
      "impact": "The user is required to complete the second step which is the multi-factor authentication before any access is granted to them."
    },
    "function_name": "rds_mfa_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.authentication.mfa.mfa_enabled"
  },
  {
    "id": "3.1",
    "title": "Ensure to Choose the Appropriate Database Engine",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "1. Evaluate Your Requirements \u2022 Understand your application's specific requirements, such as performance, scalability, data volume, and compatibility with existing systems. \u2022 Consider factors like data structure, workload type (OLTP or OLAP), and specific features required by your application. 2. Research Available Database Engines \u2022 Familiarize yourself with the available database engine options supported by Amazon RDS. \u2022 Research each database engine's capabilities, features, performance characteristics, and licensing models. 3. Compare Features and Compatibility \u2022 Compare the features and capabilities of each database engine with your application's requirements. \u2022 Evaluate data types, indexing options, query optimization, high availability, replication, and backup and restore capabilities. \u2022 Consider compatibility with your existing applications, frameworks, and tools. 4. Evaluate Performance and Scalability \u2022 Consider the performance characteristics of each database engine, including throughput, latency, and concurrency capabilities. \u2022 Evaluate scalability options, such as horizontal scaling or vertical scaling. \u2022 Analyze benchmarks, customer reviews, and case studies to gain insights into the performance of each database engine. 5. Consider Managed Database Services \u2022 Assess the benefits of Amazon RDS managed database services, such as Amazon Aurora, which offers high performance, scalability, and built-in fault tolerance.  \u2022 Evaluate the additional features and optimizations Amazon Aurora provides compared to traditional database engines. 6. Evaluate Licensing and Costs \u2022 Consider the licensing models and costs associated with each database engine, including license fees and support costs. \u2022 Evaluate the pricing structure of the database engines in terms of instance types, storage, data transfer, and other factors. 7. Determine Vendor Support \u2022 Evaluate the level of support the database engine vendors provide, including documentation, forums, community support, and enterprise support options. \u2022 Consider the vendor's reputation, track record, and commitment to security and compliance. 8. Make an Informed Decision \u2022 Select the database engine that best aligns with your application requirements, performance needs, scalability goals, compatibility, and budget based on your evaluation and analysis. \u2022 Consider long-term considerations such as potential future growth, flexibility, and ease of migration to other database engines if needed.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Evaluate Your Requirements \u2022 Understand your application's specific requirements, such as performance, scalability, data volume, and compatibility with existing systems. \u2022 Consider factors like data structure, workload type (OLTP or OLAP), and specific features required by your application. 2. Research Available Database Engines \u2022 Familiarize yourself with the available database engine options supported by Amazon RDS. \u2022 Research each database engine's capabilities, features, performance characteristics, and licensing models. 3. Compare Features and Compatibility \u2022 Compare the features and capabilities of each database engine with your application's requirements. \u2022 Evaluate data types, indexing options, query optimization, high availability, replication, and backup and restore capabilities. \u2022 Consider compatibility with your existing applications, frameworks, and tools. 4. Evaluate Performance and Scalability \u2022 Consider the performance characteristics of each database engine, including throughput, latency, and concurrency capabilities. \u2022 Evaluate scalability options, such as horizontal scaling or vertical scaling. \u2022 Analyze benchmarks, customer reviews, and case studies to gain insights into the performance of each database engine. 5. Consider Managed Database Services \u2022 Assess the benefits of Amazon RDS managed database services, such as Amazon Aurora, which offers high performance, scalability, and built-in fault tolerance.  \u2022 Evaluate the additional features and optimizations Amazon Aurora provides compared to traditional database engines. 6. Evaluate Licensing and Costs \u2022 Consider the licensing models and costs associated with each database engine, including license fees and support costs. \u2022 Evaluate the pricing structure of the database engines in terms of instance types, storage, data transfer, and other factors. 7. Determine Vendor Support \u2022 Evaluate the level of support the database engine vendors provide, including documentation, forums, community support, and enterprise support options. \u2022 Consider the vendor's reputation, track record, and commitment to security and compliance. 8. Make an Informed Decision \u2022 Select the database engine that best aligns with your application requirements, performance needs, scalability goals, compatibility, and budget based on your evaluation and analysis. \u2022 Consider long-term considerations such as potential future growth, flexibility, and ease of migration to other database engines if needed.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "manual_check_required",
    "coverage": 0
  },
  {
    "id": "3.2",
    "title": "Ensure to Create The Appropriate Deployment Configuration",
    "assessment": "Manual",
    "description": "This control is important and helps businesses to choose from two deployment options, either single or multi-AZ deployment. Depending on the business factor and their security needs the organization is then encouraged to make a decision that would benefit them.",
    "rationale": "",
    "audit": "1. Evaluate High Availability Requirements \u2022 Assess the high availability needs of your application. Consider factors such as uptime requirements, business continuity, and disaster recovery. \u2022 Determine if your application requires automatic failover, data durability, and minimal downtime during maintenance or outages. 2. Understand RDS Deployment Options \u2022 Familiarize yourself with the deployment options available on Amazon RDS. These include single-AZ (Availability Zone) and multi-AZ deployments. \u2022 Understand the differences between these options regarding availability, durability, and cost. 3. Single-AZ Deployment \u2022 Consider a single-AZ deployment if high availability is not a critical requirement for your application. \u2022 In a single-AZ deployment, your database runs in a single Availability Zone, providing basic durability and availability. 4. Multi-AZ Deployment \u2022 Choose a multi-AZ deployment if high availability and automatic failover are crucial for your application. \u2022 In a multi-AZ deployment, your database is replicated synchronously to a standby replica in a different Availability Zone, providing automatic failover in the event of a primary database failure.  \u2022 Multi-AZ deployments provide enhanced availability and durability, ensuring minimal downtime during maintenance or outages. 5. Evaluate Cost Implications \u2022 Consider the cost implications of your deployment choice. \u2022 Multi-AZ deployments incur additional costs than single-AZ deployments due to the replication and standby infrastructure. 6. Make a Deployment Decision \u2022 Based on your evaluation of high availability requirements, consider the trade- offs between single-AZ and multi-AZ deployments. \u2022 Choose the appropriate deployment configuration that meets your application's availability, durability, and cost requirements. 7. Configure RDS Deployment \u2022 Once you have determined the deployment configuration, go to the Amazon RDS console. \u2022 Create a new database instance or modify an existing one to match your chosen deployment configuration. \u2022 Follow the prompts and configure the deployment options, selecting the desired AZs and replication settings. \u2022 Adjust other configuration settings, such as instance type, storage, and backup options, based on your application's needs. 8. Test and Monitor \u2022 After the deployment is set up, thoroughly test your application's functionality and performance. \u2022 Monitor the RDS instance and replication status using the Amazon RDS console or CloudWatch metrics. \u2022 Ensure that the database failover and automatic maintenance operations work as expected.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This control is important and helps businesses to choose from two deployment options, either single or multi-AZ deployment. Depending on the business factor and their security needs the organization is then encouraged to make a decision that would benefit them.",
      "audit_steps": "1. Evaluate High Availability Requirements \u2022 Assess the high availability needs of your application. Consider factors such as uptime requirements, business continuity, and disaster recovery. \u2022 Determine if your application requires automatic failover, data durability, and minimal downtime during maintenance or outages. 2. Understand RDS Deployment Options \u2022 Familiarize yourself with the deployment options available on Amazon RDS. These include single-AZ (Availability Zone) and multi-AZ deployments. \u2022 Understand the differences between these options regarding availability, durability, and cost. 3. Single-AZ Deployment \u2022 Consider a single-AZ deployment if high availability is not a critical requirement for your application. \u2022 In a single-AZ deployment, your database runs in a single Availability Zone, providing basic durability and availability. 4. Multi-AZ Deployment \u2022 Choose a multi-AZ deployment if high availability and automatic failover are crucial for your application. \u2022 In a multi-AZ deployment, your database is replicated synchronously to a standby replica in a different Availability Zone, providing automatic failover in the event of a primary database failure.  \u2022 Multi-AZ deployments provide enhanced availability and durability, ensuring minimal downtime during maintenance or outages. 5. Evaluate Cost Implications \u2022 Consider the cost implications of your deployment choice. \u2022 Multi-AZ deployments incur additional costs than single-AZ deployments due to the replication and standby infrastructure. 6. Make a Deployment Decision \u2022 Based on your evaluation of high availability requirements, consider the trade- offs between single-AZ and multi-AZ deployments. \u2022 Choose the appropriate deployment configuration that meets your application's availability, durability, and cost requirements. 7. Configure RDS Deployment \u2022 Once you have determined the deployment configuration, go to the Amazon RDS console. \u2022 Create a new database instance or modify an existing one to match your chosen deployment configuration. \u2022 Follow the prompts and configure the deployment options, selecting the desired AZs and replication settings. \u2022 Adjust other configuration settings, such as instance type, storage, and backup options, based on your application's needs. 8. Test and Monitor \u2022 After the deployment is set up, thoroughly test your application's functionality and performance. \u2022 Monitor the RDS instance and replication status using the Amazon RDS console or CloudWatch metrics. \u2022 Ensure that the database failover and automatic maintenance operations work as expected.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "rds_deployment_configuration_check",
    "coverage": 90,
    "rule_id": "aws.rds.deployment.configuration.deployment_configuration_appropriate"
  },
  {
    "id": "3.3",
    "title": "Ensure to Create a Virtual Private Cloud (VPC)",
    "assessment": "Manual",
    "description": "Setting up a Virtual Private Cloud (VPC) protects the private network that has been established from any external networks from interfering. It allows internal networks to communicate with one another with the network that has been established.",
    "rationale": "Impact: Builds a strong connection between internal networks and the internet, and it secures your data from getting into the hand of an unauthorized party.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon VPC Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/vpc/. 3. Create a VPC \u2022 In the Amazon VPC console, click Your VPCs in the left-side menu. \u2022 Click on Create VPC to begin creating a new VPC. \u2022 Provide a name and the desired IPv4 CIDR block for your VPC. \u2022 Configure additional settings, such as IPv6 CIDR block, tenancy, and DNS resolution. \u2022 Click Create to create the VPC. 4. Create Subnets \u2022 In the Amazon VPC console, click Subnets in the left-side menu. \u2022 Click on Create subnet to create a subnet within the VPC. \u2022 Select the VPC you created in the previous step. \u2022 Provide a name, choose an availability zone, and specify the IPv4 CIDR block for the subnet. \u2022 Configure additional settings, such as IPv6 CIDR block and availability zone.  \u2022 Click Create to create the subnet. 5. Configure Route Tables \u2022 In the Amazon VPC console, click on Route Tables in the left-side menu. \u2022 Click on Create route table to create a new route table. \u2022 Provide a name for the route table and select the VPC you created earlier. \u2022 Click Create to create the route table. \u2022 Associate the route table with the desired subnets by selecting the route table and clicking on the Subnet associations tab. \u2022 Click Edit subnet associations and select the desired subnets to associate them with the route table. 6. Configure Security Groups \u2022 In the Amazon VPC console, click Security Groups in the left-side menu. \u2022 Click on Create security group to create a new security group. \u2022 Provide a name and description for the security group. \u2022 Select the VPC you created earlier. \u2022 Configure inbound and outbound rules to control network traffic to and from your RDS instances. \u2022 Click Create to create the security group. 7. Configure Network Access Control Lists (ACLs) \u2022 In the Amazon VPC console, click on Network ACLs in the left-side menu. \u2022 Click on Create network ACL to create a new network ACL. \u2022 Provide a name for the network ACL and select the VPC you created earlier. \u2022 Configure inbound and outbound rules to allow or deny specific types of traffic. \u2022 Associate the network ACL with the desired subnets by selecting the network ACL and clicking on the Subnet associations tab. \u2022 Click Edit subnet associations and select the desired subnets to associate them with the network ACL. 8. Use the VPC with Amazon RDS \u2022 Select the appropriate VPC, subnets, and security groups when creating an RDS instance. \u2022 Configure the database instance with the desired network and security settings within the chosen VPC.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Builds a strong connection between internal networks and the internet, and it secures your data from getting into the hand of an unauthorized party.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Setting up a Virtual Private Cloud (VPC) protects the private network that has been established from any external networks from interfering. It allows internal networks to communicate with one another with the network that has been established.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon VPC Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/vpc/. 3. Create a VPC \u2022 In the Amazon VPC console, click Your VPCs in the left-side menu. \u2022 Click on Create VPC to begin creating a new VPC. \u2022 Provide a name and the desired IPv4 CIDR block for your VPC. \u2022 Configure additional settings, such as IPv6 CIDR block, tenancy, and DNS resolution. \u2022 Click Create to create the VPC. 4. Create Subnets \u2022 In the Amazon VPC console, click Subnets in the left-side menu. \u2022 Click on Create subnet to create a subnet within the VPC. \u2022 Select the VPC you created in the previous step. \u2022 Provide a name, choose an availability zone, and specify the IPv4 CIDR block for the subnet. \u2022 Configure additional settings, such as IPv6 CIDR block and availability zone.  \u2022 Click Create to create the subnet. 5. Configure Route Tables \u2022 In the Amazon VPC console, click on Route Tables in the left-side menu. \u2022 Click on Create route table to create a new route table. \u2022 Provide a name for the route table and select the VPC you created earlier. \u2022 Click Create to create the route table. \u2022 Associate the route table with the desired subnets by selecting the route table and clicking on the Subnet associations tab. \u2022 Click Edit subnet associations and select the desired subnets to associate them with the route table. 6. Configure Security Groups \u2022 In the Amazon VPC console, click Security Groups in the left-side menu. \u2022 Click on Create security group to create a new security group. \u2022 Provide a name and description for the security group. \u2022 Select the VPC you created earlier. \u2022 Configure inbound and outbound rules to control network traffic to and from your RDS instances. \u2022 Click Create to create the security group. 7. Configure Network Access Control Lists (ACLs) \u2022 In the Amazon VPC console, click on Network ACLs in the left-side menu. \u2022 Click on Create network ACL to create a new network ACL. \u2022 Provide a name for the network ACL and select the VPC you created earlier. \u2022 Configure inbound and outbound rules to allow or deny specific types of traffic. \u2022 Associate the network ACL with the desired subnets by selecting the network ACL and clicking on the Subnet associations tab. \u2022 Click Edit subnet associations and select the desired subnets to associate them with the network ACL. 8. Use the VPC with Amazon RDS \u2022 Select the appropriate VPC, subnets, and security groups when creating an RDS instance. \u2022 Configure the database instance with the desired network and security settings within the chosen VPC.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Builds a strong connection between internal networks and the internet, and it secures your data from getting into the hand of an unauthorized party.",
      "impact": "Builds a strong connection between internal networks and the internet, and it secures your data from getting into the hand of an unauthorized party."
    },
    "function_name": "rds_vpc_created_check",
    "coverage": 90,
    "rule_id": "aws.rds.vpc.created.vpc_created"
  },
  {
    "id": "3.4",
    "title": "Ensure to Configure Security Groups",
    "assessment": "Manual",
    "description": "Configuring security groups benefits the user because it helps manage networks within the database and gives only certain permission for traffic that leaves and enters the database.",
    "rationale": "Impact: Allows certain users to access the instance and it only allows them to work within that network.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance for which you want to configure security groups. Click on the instance name to access its details page. 4. Navigate to the Connectivity & Security Section \u2022 In the instance details page, navigate to the Connectivity & Security or \"Security\" section. 5. View and Modify Existing Security Groups \u2022 Under the Security section, you will see the existing security groups associated with the RDS instance. \u2022 Take note of the existing security groups and their inbound and outbound rules.   6. Create a New Security Group \u2022 If you need to create a new security group for the RDS instance \u2022 Click the Create New Security Group button. \u2022 Provide a name and description for the new security group. \u2022 Configure the inbound and outbound rules to control network traffic to and from the RDS instance. \u2022 Click \"Create\" to create the new security group. 7. Modify Security Group Rules \u2022 To modify the rules of an existing security group, click on the security group name or the Modify button next to it. \u2022 You can add, edit, or delete inbound and outbound rules on the security group details page. \u2022 Specify each rule's source IP addresses, port ranges, and protocols. \u2022 Click Save or Apply Changes to update the security group rules. 8. Associate Security Groups \u2022 To associate a security group with the RDS instance, navigate to the Connectivity & Security or Security section of the instance details page. \u2022 Click Modify next to the VPC security groups option. \u2022 Select the desired security groups from the list. \u2022 Click Save or Apply Changes to associate them with the RDS instance. 9. Verify and Test Security Group Configuration \u2022 Review the security group settings to match your network access requirements. \u2022 Test the connectivity to the RDS instance by attempting to access it from authorized IP addresses or applications. 10. Monitor and Update Security Groups \u2022 Regularly monitor the network traffic and access patterns to your RDS instance. \u2022 Update the security group rules as needed to reflect changes in your network access requirements.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Allows certain users to access the instance and it only allows them to work within that network.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Configuring security groups benefits the user because it helps manage networks within the database and gives only certain permission for traffic that leaves and enters the database.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance for which you want to configure security groups. Click on the instance name to access its details page. 4. Navigate to the Connectivity & Security Section \u2022 In the instance details page, navigate to the Connectivity & Security or \"Security\" section. 5. View and Modify Existing Security Groups \u2022 Under the Security section, you will see the existing security groups associated with the RDS instance. \u2022 Take note of the existing security groups and their inbound and outbound rules.   6. Create a New Security Group \u2022 If you need to create a new security group for the RDS instance \u2022 Click the Create New Security Group button. \u2022 Provide a name and description for the new security group. \u2022 Configure the inbound and outbound rules to control network traffic to and from the RDS instance. \u2022 Click \"Create\" to create the new security group. 7. Modify Security Group Rules \u2022 To modify the rules of an existing security group, click on the security group name or the Modify button next to it. \u2022 You can add, edit, or delete inbound and outbound rules on the security group details page. \u2022 Specify each rule's source IP addresses, port ranges, and protocols. \u2022 Click Save or Apply Changes to update the security group rules. 8. Associate Security Groups \u2022 To associate a security group with the RDS instance, navigate to the Connectivity & Security or Security section of the instance details page. \u2022 Click Modify next to the VPC security groups option. \u2022 Select the desired security groups from the list. \u2022 Click Save or Apply Changes to associate them with the RDS instance. 9. Verify and Test Security Group Configuration \u2022 Review the security group settings to match your network access requirements. \u2022 Test the connectivity to the RDS instance by attempting to access it from authorized IP addresses or applications. 10. Monitor and Update Security Groups \u2022 Regularly monitor the network traffic and access patterns to your RDS instance. \u2022 Update the security group rules as needed to reflect changes in your network access requirements.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Allows certain users to access the instance and it only allows them to work within that network.",
      "impact": "Allows certain users to access the instance and it only allows them to work within that network."
    },
    "function_name": "rds_security_groups_configured_check",
    "coverage": 90,
    "rule_id": "aws.rds.security_group.configured.security_groups_configured"
  },
  {
    "id": "3.5",
    "title": "Enable Encryption at Rest",
    "assessment": "Manual",
    "description": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
    "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to enable encryption at rest. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Encryption & Security section. 4. Enable Encryption at Rest \u2022 Under the Encryption or Encryption at Rest section \u2022 Click on the Modify button or the Enable option to enable encryption at rest. \u2022 Choose the desired encryption option, either AWS managed keys (default) or Customer managed keys using AWS Key Management Service (KMS). \u2022 If selecting AWS managed keys, you do not need to perform additional configuration steps. \u2022 If selecting Customer managed keys you will need to specify the KMS key you want to use for encryption.  \u2022 Select the appropriate KMS key or create a new KMS key if necessary. \u2022 Click Continue or Save to apply the changes. 5. Monitor the Encryption Status \u2022 After enabling encryption at rest, monitor the encryption status of your RDS instance. \u2022 In the RDS console, check the Encryption or Encryption at Rest section to ensure that encryption is enabled, and the status is In Progress or Enabled. 6. Verify Encryption at Rest \u2022 Validate that data at rest is encrypted by accessing the RDS instance and examining the database files. \u2022 Confirm that the data is stored in an encrypted format.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to enable encryption at rest. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Encryption & Security section. 4. Enable Encryption at Rest \u2022 Under the Encryption or Encryption at Rest section \u2022 Click on the Modify button or the Enable option to enable encryption at rest. \u2022 Choose the desired encryption option, either AWS managed keys (default) or Customer managed keys using AWS Key Management Service (KMS). \u2022 If selecting AWS managed keys, you do not need to perform additional configuration steps. \u2022 If selecting Customer managed keys you will need to specify the KMS key you want to use for encryption.  \u2022 Select the appropriate KMS key or create a new KMS key if necessary. \u2022 Click Continue or Save to apply the changes. 5. Monitor the Encryption Status \u2022 After enabling encryption at rest, monitor the encryption status of your RDS instance. \u2022 In the RDS console, check the Encryption or Encryption at Rest section to ensure that encryption is enabled, and the status is In Progress or Enabled. 6. Verify Encryption at Rest \u2022 Validate that data at rest is encrypted by accessing the RDS instance and examining the database files. \u2022 Confirm that the data is stored in an encrypted format.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
      "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext."
    },
    "function_name": "rds_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.instance.encryption.encryption_at_rest_enabled"
  },
  {
    "id": "3.6",
    "title": "Enable Encryption in Transit",
    "assessment": "Manual",
    "description": "Amazon Relational Database uses SSL/TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by SSL/TLS to configure it correctly.",
    "rationale": "",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to implement encryption in transit. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Encryption & Security section. 4. Enable SSL/TLS \u2022 Under the Connectivity or Encryption in Transit section \u2022 Click the Modify or Edit option to enable SSL/TLS encryption. \u2022 Select the option to enable SSL/TLS encryption. \u2022 Choose the SSL/TLS certificate authority (CA) certificate option that best suits your needs: o If you have an existing certificate, select Use a certificate from ACM (AWS Certificate Manager) or Use a certificate from AWS Secrets Manager. o If you do not have a certificate, select Generate a new certificate. Click Continue or Save to apply the changes.   5. Verify SSL/TLS Encryption \u2022 After enabling SSL/TLS encryption, monitor the encryption status of your RDS instance. \u2022 In the RDS console, check the Connectivity or \"Encryption in Transit\" section to ensure that SSL/TLS encryption is enabled, and the status is \"In Progress\" or \"Enabled.\" 6. Test SSL/TLS Encryption \u2022 Connect to your RDS instance using a database client or application that supports SSL/TLS encryption. \u2022 Configure the client or application to use SSL/TLS encryption by specifying the SSL/TLS certificate details. \u2022 Verify that the connection is established successfully with SSL/TLS encryption. 7. Monitor and Manage SSL/TLS Certificates \u2022 Regularly monitor the SSL/TLS certificates associated with your RDS instances. \u2022 Manage certificate expiration and renewal to ensure uninterrupted SSL/TLS encryption.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon Relational Database uses SSL/TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by SSL/TLS to configure it correctly.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to implement encryption in transit. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Encryption & Security section. 4. Enable SSL/TLS \u2022 Under the Connectivity or Encryption in Transit section \u2022 Click the Modify or Edit option to enable SSL/TLS encryption. \u2022 Select the option to enable SSL/TLS encryption. \u2022 Choose the SSL/TLS certificate authority (CA) certificate option that best suits your needs: o If you have an existing certificate, select Use a certificate from ACM (AWS Certificate Manager) or Use a certificate from AWS Secrets Manager. o If you do not have a certificate, select Generate a new certificate. Click Continue or Save to apply the changes.   5. Verify SSL/TLS Encryption \u2022 After enabling SSL/TLS encryption, monitor the encryption status of your RDS instance. \u2022 In the RDS console, check the Connectivity or \"Encryption in Transit\" section to ensure that SSL/TLS encryption is enabled, and the status is \"In Progress\" or \"Enabled.\" 6. Test SSL/TLS Encryption \u2022 Connect to your RDS instance using a database client or application that supports SSL/TLS encryption. \u2022 Configure the client or application to use SSL/TLS encryption by specifying the SSL/TLS certificate details. \u2022 Verify that the connection is established successfully with SSL/TLS encryption. 7. Monitor and Manage SSL/TLS Certificates \u2022 Regularly monitor the SSL/TLS certificates associated with your RDS instances. \u2022 Manage certificate expiration and renewal to ensure uninterrupted SSL/TLS encryption.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "rds_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.instance.encryption.encryption_in_transit_enabled"
  },
  {
    "id": "3.7",
    "title": "Ensure to Implement Access Control and Authentication",
    "assessment": "Manual",
    "description": "Users should select whether they like to enable authentication. If they want to authenticate a password would be required, which would only allow the authorized person to access the database. Defining access control allows specific workers in a business access to the database.",
    "rationale": "",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to implement access control and authentication. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Connectivity & Security section. 4. Enable IAM Database Authentication \u2022 Under the Connectivity or Connectivity & Security section. \u2022 Click the Modify or Edit option to enable IAM Database Authentication. \u2022 Select the option to enable IAM Database Authentication. \u2022 Click Continue or Save to apply the changes. 5. Create and Configure IAM Database Users \u2022 Click Users in the left-side menu in the Amazon RDS console. \u2022 Click Create database user to create a new IAM database user.  \u2022 Provide a username and select the IAM role or IAM user that will be associated with the database user. \u2022 Configure the authentication type, either Password-based or IAM authentication. \u2022 Set the desired password or leave it blank for IAM authentication. \u2022 Configure the database user's privileges and permissions based on your application's requirements. \u2022 Click Create to create the IAM database user. 6. Configure Database User Privileges \u2022 Click Users in the left-side menu in the Amazon RDS console. \u2022 Select the database user you created in the previous step. \u2022 Click on Modify to modify the user's settings and permissions. \u2022 Configure the user's access privileges, including database access, object permissions, and privileges. \u2022 Click Save or Apply Changes to update the user's privileges. 7. Test Access and Authentication \u2022 Test the access and authentication by connecting to the RDS instance using the IAM database user's credentials or IAM role. \u2022 Verify that the authentication and access control mechanisms are functioning correctly. 8. Monitor and Manage IAM Database Users \u2022 Regularly monitor and review the IAM database users and their access privileges. \u2022 Adjust user privileges as needed based on changes in your application requirements. \u2022 Remove or disable database users when they are no longer needed.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Users should select whether they like to enable authentication. If they want to authenticate a password would be required, which would only allow the authorized person to access the database. Defining access control allows specific workers in a business access to the database.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to implement access control and authentication. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Connectivity & Security section. 4. Enable IAM Database Authentication \u2022 Under the Connectivity or Connectivity & Security section. \u2022 Click the Modify or Edit option to enable IAM Database Authentication. \u2022 Select the option to enable IAM Database Authentication. \u2022 Click Continue or Save to apply the changes. 5. Create and Configure IAM Database Users \u2022 Click Users in the left-side menu in the Amazon RDS console. \u2022 Click Create database user to create a new IAM database user.  \u2022 Provide a username and select the IAM role or IAM user that will be associated with the database user. \u2022 Configure the authentication type, either Password-based or IAM authentication. \u2022 Set the desired password or leave it blank for IAM authentication. \u2022 Configure the database user's privileges and permissions based on your application's requirements. \u2022 Click Create to create the IAM database user. 6. Configure Database User Privileges \u2022 Click Users in the left-side menu in the Amazon RDS console. \u2022 Select the database user you created in the previous step. \u2022 Click on Modify to modify the user's settings and permissions. \u2022 Configure the user's access privileges, including database access, object permissions, and privileges. \u2022 Click Save or Apply Changes to update the user's privileges. 7. Test Access and Authentication \u2022 Test the access and authentication by connecting to the RDS instance using the IAM database user's credentials or IAM role. \u2022 Verify that the authentication and access control mechanisms are functioning correctly. 8. Monitor and Manage IAM Database Users \u2022 Regularly monitor and review the IAM database users and their access privileges. \u2022 Adjust user privileges as needed based on changes in your application requirements. \u2022 Remove or disable database users when they are no longer needed.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "rds_access_control_authentication_check",
    "coverage": 90,
    "rule_id": "aws.rds.authentication.access_control.access_control_authentication_implemented"
  },
  {
    "id": "3.8",
    "title": "Ensure to Regularly Patch Systems",
    "assessment": "Manual",
    "description": "",
    "rationale": "Impact: Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack.",
    "audit": "1. Stay Informed about Database Engine Updates \u2022 Stay up-to-date with the latest information regarding database engine updates and patches provided by the respective database engine vendors (e.g., MySQL, PostgreSQL, Oracle, SQL Server). \u2022 Subscribe to release announcements, security bulletins, and updates from the database engine vendor or AWS. 2. Review the Database Engine Documentation \u2022 Refer to the documentation provided by the database engine vendor to understand the recommended patching and update processes specific to the database engine you use on Amazon RDS. \u2022 Review the vendor's guidelines and best practices for applying updates and patches. 3. Plan for Maintenance Windows \u2022 Determine regular maintenance windows during which you can schedule updates and patches for your RDS instances. \u2022 Coordinate with your team to ensure minimal disruption to your applications and users during the maintenance window. 4. Enable Automated Minor Version Upgrades \u2022 In the Amazon RDS console, select the RDS instance you want to enable automated upgrades. \u2022 Under the Maintenance & backups or Maintenance section. \u2022 Enable the Auto minor version upgrade option.  \u2022 This allows Amazon RDS to automatically apply eligible minor version upgrades to your RDS instances during the maintenance window. 5. Monitor Available Updates \u2022 Regularly monitor the Pending Maintenance section in the Amazon RDS console for any updates or patches for your RDS instances. \u2022 Pay attention to notifications and alerts from AWS about pending updates. 6. Schedule Updates and Patches \u2022 Review the available updates and patches and their associated release notes and security advisories. \u2022 Please select the appropriate updates based on their impact, criticality, and compatibility with your applications. \u2022 Schedule the updates and patches to be applied during the designated maintenance window. 7. Apply Updates and Patches \u2022 During the scheduled maintenance window, Amazon RDS automatically applies the eligible updates and patches to your RDS instances. \u2022 Monitor the progress of the updates and patches through the Amazon RDS console. 8. Test and Validate \u2022 After the updates and patches are applied, thoroughly test your applications to ensure they function as expected. \u2022 Validate the database performance, data integrity, and application functionality. 9. Monitor for Issues \u2022 Monitor the performance and behavior of your RDS instances after the updates and patches are applied. \u2022 Keep an eye out for any issues or anomalies and address them promptly. 10. Review and Document \u2022 Review the release notes and documentation of the applied updates and patches to understand the changes and improvements they bring. \u2022 Document the update and patching process, including the applied versions, dates, and any issues encountered.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Stay Informed about Database Engine Updates \u2022 Stay up-to-date with the latest information regarding database engine updates and patches provided by the respective database engine vendors (e.g., MySQL, PostgreSQL, Oracle, SQL Server). \u2022 Subscribe to release announcements, security bulletins, and updates from the database engine vendor or AWS. 2. Review the Database Engine Documentation \u2022 Refer to the documentation provided by the database engine vendor to understand the recommended patching and update processes specific to the database engine you use on Amazon RDS. \u2022 Review the vendor's guidelines and best practices for applying updates and patches. 3. Plan for Maintenance Windows \u2022 Determine regular maintenance windows during which you can schedule updates and patches for your RDS instances. \u2022 Coordinate with your team to ensure minimal disruption to your applications and users during the maintenance window. 4. Enable Automated Minor Version Upgrades \u2022 In the Amazon RDS console, select the RDS instance you want to enable automated upgrades. \u2022 Under the Maintenance & backups or Maintenance section. \u2022 Enable the Auto minor version upgrade option.  \u2022 This allows Amazon RDS to automatically apply eligible minor version upgrades to your RDS instances during the maintenance window. 5. Monitor Available Updates \u2022 Regularly monitor the Pending Maintenance section in the Amazon RDS console for any updates or patches for your RDS instances. \u2022 Pay attention to notifications and alerts from AWS about pending updates. 6. Schedule Updates and Patches \u2022 Review the available updates and patches and their associated release notes and security advisories. \u2022 Please select the appropriate updates based on their impact, criticality, and compatibility with your applications. \u2022 Schedule the updates and patches to be applied during the designated maintenance window. 7. Apply Updates and Patches \u2022 During the scheduled maintenance window, Amazon RDS automatically applies the eligible updates and patches to your RDS instances. \u2022 Monitor the progress of the updates and patches through the Amazon RDS console. 8. Test and Validate \u2022 After the updates and patches are applied, thoroughly test your applications to ensure they function as expected. \u2022 Validate the database performance, data integrity, and application functionality. 9. Monitor for Issues \u2022 Monitor the performance and behavior of your RDS instances after the updates and patches are applied. \u2022 Keep an eye out for any issues or anomalies and address them promptly. 10. Review and Document \u2022 Review the release notes and documentation of the applied updates and patches to understand the changes and improvements they bring. \u2022 Document the update and patching process, including the applied versions, dates, and any issues encountered.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack.",
      "impact": "Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack."
    },
    "function_name": "rds_patching_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.maintenance.patching.regular_patching_enabled"
  },
  {
    "id": "3.9",
    "title": "Ensure Monitoring and Logging is Enabled",
    "assessment": "Manual",
    "description": "",
    "rationale": "Impact: If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to enable monitoring and logging. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Monitoring & Logs section. 4. Enable Enhanced Monitoring \u2022 Under the Monitoring section. \u2022 Click on the Modify button or Edit option to enable enhanced monitoring. \u2022 Choose the desired monitoring granularity (1-minute or 5-minute intervals) and the retention period for the monitoring data. \u2022 Click Continue or Save to apply the changes. 5. Enable Enhanced Logging \u2022 Under the Logs or Monitoring & Logs section. \u2022 Click on the Modify button or Edit option to enable enhanced logging.  \u2022 Choose the desired log types to enable, such as general, error, slow query, or audit logs. \u2022 Configure the log file retention period based on your needs. \u2022 Select the destination for the logs, such as Amazon CloudWatch Logs or an Amazon S3 bucket. \u2022 Configure the log format and other settings if applicable. \u2022 Click Continue or Save to apply the changes. 6. Configure CloudWatch Alarms (Optional) \u2022 Click Alarms in the Amazon RDS console menu. \u2022 Click Create alarm to create a CloudWatch alarm to monitor specific metrics or log events. \u2022 Configure the alarm threshold, actions to take when the threshold is breached, and notification settings. \u2022 Click Create to create the CloudWatch alarm. 7. Monitor and Analyze the Metrics and Logs \u2022 Monitor the metrics and logs in the Amazon RDS console or by accessing CloudWatch or the configured log destination. \u2022 Use the metrics and logs to gain insights into your RDS instance's performance, behavior, and issues. \u2022 Analyze the metrics and logs to identify areas for optimization, troubleshoot problems, or detect anomalies. 8. Set Up Automated Actions (Optional) \u2022 In the Amazon RDS console, click on Event subscriptions in the left-side menu. \u2022 Click Create event subscription to set up automated actions based on specific events or log entries. \u2022 Configure the event pattern, target actions, and notification settings. \u2022 Click Create to create the event subscription. 9. Monitor and Respond to Alerts \u2022 Monitor the CloudWatch alarms and event notifications for any alerts or triggers based on the configured thresholds. \u2022 Respond to alerts promptly by investigating and resolving the underlying issues or taking appropriate actions.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to enable monitoring and logging. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Configuration or Monitoring & Logs section. 4. Enable Enhanced Monitoring \u2022 Under the Monitoring section. \u2022 Click on the Modify button or Edit option to enable enhanced monitoring. \u2022 Choose the desired monitoring granularity (1-minute or 5-minute intervals) and the retention period for the monitoring data. \u2022 Click Continue or Save to apply the changes. 5. Enable Enhanced Logging \u2022 Under the Logs or Monitoring & Logs section. \u2022 Click on the Modify button or Edit option to enable enhanced logging.  \u2022 Choose the desired log types to enable, such as general, error, slow query, or audit logs. \u2022 Configure the log file retention period based on your needs. \u2022 Select the destination for the logs, such as Amazon CloudWatch Logs or an Amazon S3 bucket. \u2022 Configure the log format and other settings if applicable. \u2022 Click Continue or Save to apply the changes. 6. Configure CloudWatch Alarms (Optional) \u2022 Click Alarms in the Amazon RDS console menu. \u2022 Click Create alarm to create a CloudWatch alarm to monitor specific metrics or log events. \u2022 Configure the alarm threshold, actions to take when the threshold is breached, and notification settings. \u2022 Click Create to create the CloudWatch alarm. 7. Monitor and Analyze the Metrics and Logs \u2022 Monitor the metrics and logs in the Amazon RDS console or by accessing CloudWatch or the configured log destination. \u2022 Use the metrics and logs to gain insights into your RDS instance's performance, behavior, and issues. \u2022 Analyze the metrics and logs to identify areas for optimization, troubleshoot problems, or detect anomalies. 8. Set Up Automated Actions (Optional) \u2022 In the Amazon RDS console, click on Event subscriptions in the left-side menu. \u2022 Click Create event subscription to set up automated actions based on specific events or log entries. \u2022 Configure the event pattern, target actions, and notification settings. \u2022 Click Create to create the event subscription. 9. Monitor and Respond to Alerts \u2022 Monitor the CloudWatch alarms and event notifications for any alerts or triggers based on the configured thresholds. \u2022 Respond to alerts promptly by investigating and resolving the underlying issues or taking appropriate actions.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data.",
      "impact": "If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data."
    },
    "function_name": "rds_monitoring_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.monitoring.logging.monitoring_logging_enabled"
  },
  {
    "id": "3.10",
    "title": "Ensure to Enable Backup and Recovery",
    "assessment": "Manual",
    "description": "The individual logs into their AWS account and chooses their Amazon relational database that they want to backup. To have the database being backed up automatically the individual is encouraged to enable backup. This ensures that the file is being saved automatically and can prevent it from accidental loss. This ensures that the individual can restore their files quickly in the event of a data loss.",
    "rationale": "Impact: It would result in having the files protected and being able to retrieve those files in the event of an accidental loss.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to implement backup and recovery. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Backup & Restore or Backup section. 4. Configure Automated Backups \u2022 Under the Backup section. \u2022 Click the Modify or Edit option to configure automated backups. \u2022 Enable automated backups by selecting the desired backup retention period. \u2022 Specify the preferred backup window during which automated backups can occur. \u2022 Choose whether to enable Multi-AZ backups for enhanced durability and availability.  \u2022 Click Continue or Save to apply the changes. 5. Restore from Backups \u2022 In the Amazon RDS console, click on Snapshots or Instances in the left-side menu. \u2022 Select the snapshot or instance from which you want to perform a restore. \u2022 Click Restore snapshot or Restore to point in time to initiate restoration. \u2022 Configure the parameters for the restored instance, such as instance identifier, instance class, storage type, and VPC settings. \u2022 Specify the desired option for creating a new DB instance or restoring to an existing DB instance. \u2022 Configure additional settings, such as enabling Multi-AZ deployment or enabling encryption. \u2022 Click \"Restore\" or \"Create\" to initiate the restore process. 6. Test and Validate the Restored Instance \u2022 After completing the restore process, test the restored RDS instance to ensure it functions as expected. \u2022 Verify the data, configuration, and connectivity of the restored instance. 7. Monitor and Manage Backups \u2022 Regularly monitor the status and health of your automated backups and manual snapshots. \u2022 Review the backup retention policy and adjust it to align with your business requirements. \u2022 Manage and delete older backups or snapshots to free up storage and reduce costs. 8. Perform Point-in-Time Recovery (Optional) \u2022 In the Amazon RDS console, click on \"Snapshots\" or Instances in the left-side menu. \u2022 Select the instance for which you want to perform point-in-time recovery. \u2022 Click on Restore to point in time to initiate the point-in-time recovery process. \u2022 Specify the desired timestamp or time range to restore to. \u2022 Configure the parameters for the restored instance, similar to the restore from the backup process. \u2022 Click Restore or \"Create\" to initiate the point-in-time recovery process.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "It would result in having the files protected and being able to retrieve those files in the event of an accidental loss.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "The individual logs into their AWS account and chooses their Amazon relational database that they want to backup. To have the database being backed up automatically the individual is encouraged to enable backup. This ensures that the file is being saved automatically and can prevent it from accidental loss. This ensures that the individual can restore their files quickly in the event of a data loss.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to implement backup and recovery. \u2022 Click on the instance name to access its details page. \u2022 In the instance details page, navigate to the Backup & Restore or Backup section. 4. Configure Automated Backups \u2022 Under the Backup section. \u2022 Click the Modify or Edit option to configure automated backups. \u2022 Enable automated backups by selecting the desired backup retention period. \u2022 Specify the preferred backup window during which automated backups can occur. \u2022 Choose whether to enable Multi-AZ backups for enhanced durability and availability.  \u2022 Click Continue or Save to apply the changes. 5. Restore from Backups \u2022 In the Amazon RDS console, click on Snapshots or Instances in the left-side menu. \u2022 Select the snapshot or instance from which you want to perform a restore. \u2022 Click Restore snapshot or Restore to point in time to initiate restoration. \u2022 Configure the parameters for the restored instance, such as instance identifier, instance class, storage type, and VPC settings. \u2022 Specify the desired option for creating a new DB instance or restoring to an existing DB instance. \u2022 Configure additional settings, such as enabling Multi-AZ deployment or enabling encryption. \u2022 Click \"Restore\" or \"Create\" to initiate the restore process. 6. Test and Validate the Restored Instance \u2022 After completing the restore process, test the restored RDS instance to ensure it functions as expected. \u2022 Verify the data, configuration, and connectivity of the restored instance. 7. Monitor and Manage Backups \u2022 Regularly monitor the status and health of your automated backups and manual snapshots. \u2022 Review the backup retention policy and adjust it to align with your business requirements. \u2022 Manage and delete older backups or snapshots to free up storage and reduce costs. 8. Perform Point-in-Time Recovery (Optional) \u2022 In the Amazon RDS console, click on \"Snapshots\" or Instances in the left-side menu. \u2022 Select the instance for which you want to perform point-in-time recovery. \u2022 Click on Restore to point in time to initiate the point-in-time recovery process. \u2022 Specify the desired timestamp or time range to restore to. \u2022 Configure the parameters for the restored instance, similar to the restore from the backup process. \u2022 Click Restore or \"Create\" to initiate the point-in-time recovery process.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: It would result in having the files protected and being able to retrieve those files in the event of an accidental loss.",
      "impact": "It would result in having the files protected and being able to retrieve those files in the event of an accidental loss."
    },
    "function_name": "rds_backup_recovery_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.backup.recovery.backup_recovery_enabled"
  },
  {
    "id": "3.11",
    "title": "Ensure to Regularly Review Security Configuration",
    "assessment": "Manual",
    "description": "This helps by reviewing the database factors from database engine, review instance details, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
    "rationale": "Impact: Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to review the security configuration. \u2022 Click on the instance name to access its details page. 4. Review the Database Engine Documentation \u2022 Refer to the documentation provided by the database engine vendor (e.g., MySQL, PostgreSQL, Oracle, SQL Server) to understand the security best practices and configuration options specific to the database engine you use on Amazon RDS. \u2022 Review the vendor's guidelines for securing the database engine and associated components.   5. Review the Instance Details \u2022 In the instance details page, review the configuration settings related to security. \u2022 Security group associations: Ensure the appropriate security groups are assigned to the RDS instance to control inbound and outbound traffic. \u2022 IAM database authentication: Verify if IAM database authentication is enabled for enhanced security. \u2022 Encryption at rest: Confirm if encryption at rest is enabled using either AWS- managed keys or customer-managed keys. \u2022 Encryption in transit: Check if SSL/TLS encryption is enabled for secure data transmission. Backup and retention: Review the automated backup settings and retention period to ensure data recovery capability. 6. Review Database User Privileges \u2022 Click Users in the Amazon RDS console menu. \u2022 Review the privileges assigned to database users. \u2022 Ensure that the least privileged access is implemented, granting only necessary privileges to each user or role. 7. Review Audit and Logging Configuration \u2022 In the Amazon RDS console, navigate to the Configuration or Monitoring & Logs section. \u2022 Review the settings related to database audit logging and logging. \u2022 Ensure appropriate logs are enabled and configured to capture necessary information for security analysis and monitoring. 8. Review Network Security \u2022 In the Amazon RDS console, navigate to the Connectivity & Security or Security section. \u2022 Review the network security settings, including the associated security groups and their rules. \u2022 Verify that only necessary ports are open, and access is restricted to trusted sources. 9. Review and Address Security Recommendations \u2022 Periodically review the security recommendations provided by AWS through the Amazon RDS console or the AWS Trusted Advisor service. \u2022 Address any security recommendations promptly to ensure a secure configuration.   10. Document and Update \u2022 Document the security configuration settings and any changes made during the review process. \u2022 Maintain an up-to-date inventory of the security controls and configurations implemented for your RDS instances.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps by reviewing the database factors from database engine, review instance details, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon RDS Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/rds/. 3. Select the RDS Instance \u2022 Choose the Amazon RDS instance you want to review the security configuration. \u2022 Click on the instance name to access its details page. 4. Review the Database Engine Documentation \u2022 Refer to the documentation provided by the database engine vendor (e.g., MySQL, PostgreSQL, Oracle, SQL Server) to understand the security best practices and configuration options specific to the database engine you use on Amazon RDS. \u2022 Review the vendor's guidelines for securing the database engine and associated components.   5. Review the Instance Details \u2022 In the instance details page, review the configuration settings related to security. \u2022 Security group associations: Ensure the appropriate security groups are assigned to the RDS instance to control inbound and outbound traffic. \u2022 IAM database authentication: Verify if IAM database authentication is enabled for enhanced security. \u2022 Encryption at rest: Confirm if encryption at rest is enabled using either AWS- managed keys or customer-managed keys. \u2022 Encryption in transit: Check if SSL/TLS encryption is enabled for secure data transmission. Backup and retention: Review the automated backup settings and retention period to ensure data recovery capability. 6. Review Database User Privileges \u2022 Click Users in the Amazon RDS console menu. \u2022 Review the privileges assigned to database users. \u2022 Ensure that the least privileged access is implemented, granting only necessary privileges to each user or role. 7. Review Audit and Logging Configuration \u2022 In the Amazon RDS console, navigate to the Configuration or Monitoring & Logs section. \u2022 Review the settings related to database audit logging and logging. \u2022 Ensure appropriate logs are enabled and configured to capture necessary information for security analysis and monitoring. 8. Review Network Security \u2022 In the Amazon RDS console, navigate to the Connectivity & Security or Security section. \u2022 Review the network security settings, including the associated security groups and their rules. \u2022 Verify that only necessary ports are open, and access is restricted to trusted sources. 9. Review and Address Security Recommendations \u2022 Periodically review the security recommendations provided by AWS through the Amazon RDS console or the AWS Trusted Advisor service. \u2022 Address any security recommendations promptly to ensure a secure configuration.   10. Document and Update \u2022 Document the security configuration settings and any changes made during the review process. \u2022 Maintain an up-to-date inventory of the security controls and configurations implemented for your RDS instances.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack.",
      "impact": "Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack."
    },
    "function_name": "rds_security_configuration_review_check",
    "coverage": 90,
    "rule_id": "aws.rds.security.configuration.security_configuration_reviewed"
  },
  {
    "id": "4.1",
    "title": "Ensure AWS Identity and Access Management (IAM) is in use",
    "assessment": "Manual",
    "description": "AWS Identity and Access Management (IAM) lets you securely control your users' access to AWS services and resources. To manage access control for Amazon DynamoDB, you can create IAM policies that control access to tables and data.",
    "rationale": "IAM policies help you control and maintain access to Amazon DynamoDB as needed.",
    "audit": "1. Open IAM Console \u2022 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. Navigate to Policies \u2022 In the IAM console, in the navigation pane, choose Policies. 3. Create Policy \u2022 Choose Create policy. \u2022 You will be taken to the Create policy page. 4. Choose Service \u2022 Click on Choose a service. \u2022 Type DynamoDB in the search box and select it. 5. Configure Actions \u2022 Under the Actions section, select the actions you want to allow the user to perform. \u2022 For instance, you can select Read to allow read actions like GetItem, Scan, Query, etc.   6. Set Resources \u2022 Under the Resources section, you can specify which tables this policy applies to. \u2022 You can choose \"All resources\" or specify the ARN (Amazon Resource Name) of specific tables. 7. Review Policy \u2022 Click on Review policy. \u2022 Give your policy a name and description. \u2022 Then click Create policy. \u2022 Now, you have an IAM policy. 8. Attach Policy \u2022 Navigate to the Users, Groups, or Roles section in the IAM console. \u2022 Choose an existing user, group, or role, or create a new one. \u2022 Once you've selected a user, group, or role, click Add permissions. \u2022 Choose Attach existing policies directly. \u2022 Search for your created policy, select it, and click Attach policy. \u2022 With these steps, you have attached an IAM policy that controls access to DynamoDB resources.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS Identity and Access Management (IAM) lets you securely control your users' access to AWS services and resources. To manage access control for Amazon DynamoDB, you can create IAM policies that control access to tables and data.",
      "audit_steps": "1. Open IAM Console \u2022 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. 2. Navigate to Policies \u2022 In the IAM console, in the navigation pane, choose Policies. 3. Create Policy \u2022 Choose Create policy. \u2022 You will be taken to the Create policy page. 4. Choose Service \u2022 Click on Choose a service. \u2022 Type DynamoDB in the search box and select it. 5. Configure Actions \u2022 Under the Actions section, select the actions you want to allow the user to perform. \u2022 For instance, you can select Read to allow read actions like GetItem, Scan, Query, etc.   6. Set Resources \u2022 Under the Resources section, you can specify which tables this policy applies to. \u2022 You can choose \"All resources\" or specify the ARN (Amazon Resource Name) of specific tables. 7. Review Policy \u2022 Click on Review policy. \u2022 Give your policy a name and description. \u2022 Then click Create policy. \u2022 Now, you have an IAM policy. 8. Attach Policy \u2022 Navigate to the Users, Groups, or Roles section in the IAM console. \u2022 Choose an existing user, group, or role, or create a new one. \u2022 Once you've selected a user, group, or role, click Add permissions. \u2022 Choose Attach existing policies directly. \u2022 Search for your created policy, select it, and click Attach policy. \u2022 With these steps, you have attached an IAM policy that controls access to DynamoDB resources.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "IAM policies help you control and maintain access to Amazon DynamoDB as needed.",
      "impact": ""
    },
    "function_name": "dynamodb_iam_implemented_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.iam.implementation.iam_implemented"
  },
  {
    "id": "4.2",
    "title": "Ensure Fine-Grained Access Control is implemented",
    "assessment": "Manual",
    "description": "Fine-Grained Access Control (FGAC) on Amazon DynamoDB allows you to control access to data at the row level. Using IAM policies, you can restrict access based on the content within the request. Here is how you can implement FGAC:",
    "rationale": "Fine-Grained access control helps users to create and allow specific permission within that DB.",
    "audit": "1. Create an IAM Role \u2022 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. \u2022 In the navigation pane, choose Roles and select Create role. \u2022 Choose AWS service as the type of trusted entity. \u2022 Choose DynamoDB as the service that will use this role, then click Next: Permissions. \u2022 On the Attach permissions policies page, choose Next: Tags. You do not need to attach a policy to this role yet. \u2022 On the Add tags page, choose Next: Review. \u2022 On the Review page, for Role name, enter a name for your role, such as DynamoDBFineGrainedAccessRole. \u2022 Choose Create role. 2. Create an IAM Policy for Fine-Grained Access Control \u2022 In the navigation pane, choose Policies and select Create policy. \u2022 Choose the JSON tab. \u2022 Paste the following policy into the policy document field, replacing us-west-2 , 123456789012 , myddbtable , HK , and RANGEK with your own values:  { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:GetItem\", \"dynamodb:BatchGetItem\", \"dynamodb:Query\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\" ], \"Resource\": \"arn:aws:dynamodb:<us-west- 2:123456789012:table/myddbtable>\", \"Condition\": { \"ForAllValues:StringEquals\": { \"dynamodb:LeadingKeys\": [\"${www.amazon.com:user_id}\"], \"dynamodb:Attributes\": [ \"<HK>\", \"<RANGEK>\" ] }, \"StringEqualsIfExists\": { \"dynamodb:Select\": \"SPECIFIC_ATTRIBUTES\" } } } ] } In this policy: \u2022 dynamodb:LeadingKeys restrict access to only the items where the hash key value is the same as the user's ID. \u2022 dynamodb:Attributes restrict access to only the \"HK\" and \"RANGEK\" attributes of the items. \u2022 dynamodb:Select only allows the SPECIFIC_ATTRIBUTES operator. \u2022 Choose Next: Tags, add any tags if needed, and then choose Next: Review. \u2022 For Name, enter a name for your policy, such as DynamoDBFineGrainedAccessPolicy. \u2022 Choose Create policy. 3. Attach the Policy to the Role \u2022 In the navigation pane, choose Roles. \u2022 Choose the role that you created in the previous step. \u2022 On the Permissions tab, choose Attach policies. \u2022 In the Filter policies search box, enter the policy name you created before. \u2022 Select the check box for your policy, then choose Attach policy.  Note : Fine-grained access control is a powerful feature but can be complex to configure. Be sure to test your setup to ensure it works as expected thoroughly.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Fine-Grained Access Control (FGAC) on Amazon DynamoDB allows you to control access to data at the row level. Using IAM policies, you can restrict access based on the content within the request. Here is how you can implement FGAC:",
      "audit_steps": "1. Create an IAM Role \u2022 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. \u2022 In the navigation pane, choose Roles and select Create role. \u2022 Choose AWS service as the type of trusted entity. \u2022 Choose DynamoDB as the service that will use this role, then click Next: Permissions. \u2022 On the Attach permissions policies page, choose Next: Tags. You do not need to attach a policy to this role yet. \u2022 On the Add tags page, choose Next: Review. \u2022 On the Review page, for Role name, enter a name for your role, such as DynamoDBFineGrainedAccessRole. \u2022 Choose Create role. 2. Create an IAM Policy for Fine-Grained Access Control \u2022 In the navigation pane, choose Policies and select Create policy. \u2022 Choose the JSON tab. \u2022 Paste the following policy into the policy document field, replacing us-west-2 , 123456789012 , myddbtable , HK , and RANGEK with your own values:  { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"dynamodb:GetItem\", \"dynamodb:BatchGetItem\", \"dynamodb:Query\", \"dynamodb:PutItem\", \"dynamodb:UpdateItem\", \"dynamodb:DeleteItem\" ], \"Resource\": \"arn:aws:dynamodb:<us-west- 2:123456789012:table/myddbtable>\", \"Condition\": { \"ForAllValues:StringEquals\": { \"dynamodb:LeadingKeys\": [\"${www.amazon.com:user_id}\"], \"dynamodb:Attributes\": [ \"<HK>\", \"<RANGEK>\" ] }, \"StringEqualsIfExists\": { \"dynamodb:Select\": \"SPECIFIC_ATTRIBUTES\" } } } ] } In this policy: \u2022 dynamodb:LeadingKeys restrict access to only the items where the hash key value is the same as the user's ID. \u2022 dynamodb:Attributes restrict access to only the \"HK\" and \"RANGEK\" attributes of the items. \u2022 dynamodb:Select only allows the SPECIFIC_ATTRIBUTES operator. \u2022 Choose Next: Tags, add any tags if needed, and then choose Next: Review. \u2022 For Name, enter a name for your policy, such as DynamoDBFineGrainedAccessPolicy. \u2022 Choose Create policy. 3. Attach the Policy to the Role \u2022 In the navigation pane, choose Roles. \u2022 Choose the role that you created in the previous step. \u2022 On the Permissions tab, choose Attach policies. \u2022 In the Filter policies search box, enter the policy name you created before. \u2022 Select the check box for your policy, then choose Attach policy.  Note : Fine-grained access control is a powerful feature but can be complex to configure. Be sure to test your setup to ensure it works as expected thoroughly.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Fine-Grained access control helps users to create and allow specific permission within that DB.",
      "impact": ""
    },
    "function_name": "dynamodb_fine_grained_access_control_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.access.control.fine_grained_access_control_implemented"
  },
  {
    "id": "4.3",
    "title": "Ensure DynamoDB Encryption at Rest",
    "assessment": "Manual",
    "description": "Encryption at rest in Amazon DynamoDB enhances the security of your data by encrypting it using AWS Key Management Service (AWS KMS) keys. Here is how to enable encryption at rest while creating a DynamoDB table.",
    "rationale": "Once the user is in their AWS account, they should open the DynamoDB to create the table and enable encryption. A key would be required to be created to enable encryption. Only the authorized user would always have access to this key. Enabling encryption would keep the user\u2019s data private and stored securely, which would only allow them to access it with their key. Impact: Add an additional layer of security by preventing any unauthorized personnel from accessing the data since both IAM access to the data and access to the encryption key would be required.",
    "audit": "1. Open DynamoDB Console \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. 2. Create DynamoDB Table \u2022 Click Create table. This will bring you to the Create DynamoDB table page. 3. Specify Table Details \u2022 Enter a Table name and Primary key. \u2022 The primary key consists of a partition key and, optionally, a sort key. \u2022 Fill in these details according to your requirements. 4. Enable Encryption \u2022 Under the Settings section, check the Enable encryption at rest. \u2022 By default, DynamoDB uses an AWS-owned CMK to encrypt your data.  \u2022 To use an AWS-managed CMK or a customer-managed CMK instead, select AWS-managed CMK or Customer-managed CMK from the dropdown menu, then choose the desired CMK. 5. Create a Table \u2022 Click Create. \u2022 This will create your DynamoDB table with encryption at rest enabled. Note : 1. The setting for encryption at rest applies to all DynamoDB data associated with the table, including primary key data and indexes. 2. If you need to apply encryption at rest to an existing table, you can modify the table settings. However, modifying settings on large tables could take time and impact performance during the transition. 3. Ensure you have the necessary permissions in AWS KMS when choosing an AWS-managed CMK or a customer-managed CMK.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Add an additional layer of security by preventing any unauthorized personnel from accessing the data since both IAM access to the data and access to the encryption key would be required.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Encryption at rest in Amazon DynamoDB enhances the security of your data by encrypting it using AWS Key Management Service (AWS KMS) keys. Here is how to enable encryption at rest while creating a DynamoDB table.",
      "audit_steps": "1. Open DynamoDB Console \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. 2. Create DynamoDB Table \u2022 Click Create table. This will bring you to the Create DynamoDB table page. 3. Specify Table Details \u2022 Enter a Table name and Primary key. \u2022 The primary key consists of a partition key and, optionally, a sort key. \u2022 Fill in these details according to your requirements. 4. Enable Encryption \u2022 Under the Settings section, check the Enable encryption at rest. \u2022 By default, DynamoDB uses an AWS-owned CMK to encrypt your data.  \u2022 To use an AWS-managed CMK or a customer-managed CMK instead, select AWS-managed CMK or Customer-managed CMK from the dropdown menu, then choose the desired CMK. 5. Create a Table \u2022 Click Create. \u2022 This will create your DynamoDB table with encryption at rest enabled. Note : 1. The setting for encryption at rest applies to all DynamoDB data associated with the table, including primary key data and indexes. 2. If you need to apply encryption at rest to an existing table, you can modify the table settings. However, modifying settings on large tables could take time and impact performance during the transition. 3. Ensure you have the necessary permissions in AWS KMS when choosing an AWS-managed CMK or a customer-managed CMK.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Once the user is in their AWS account, they should open the DynamoDB to create the table and enable encryption. A key would be required to be created to enable encryption. Only the authorized user would always have access to this key. Enabling encryption would keep the user\u2019s data private and stored securely, which would only allow them to access it with their key. Impact: Add an additional layer of security by preventing any unauthorized personnel from accessing the data since both IAM access to the data and access to the encryption key would be required.",
      "impact": "Add an additional layer of security by preventing any unauthorized personnel from accessing the data since both IAM access to the data and access to the encryption key would be required."
    },
    "function_name": "dynamodb_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.table.encryption.encryption_at_rest_enabled"
  },
  {
    "id": "4.4",
    "title": "Ensure DynamoDB Encryption in Transit",
    "assessment": "Manual",
    "description": "Use the SSL/TLS protocol to encrypt data in transit between your applications and DynamoDB. Amazon DynamoDB encrypts data in transit by default using Transport Layer Security (TLS) encryption. Here is a step-by-step guide on how to ensure encryption in transit for your DynamoDB:",
    "rationale": "Amazon DynamoDB uses TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by TLS to configure it correctly. Impact: If the user does not have the code configured correctly it would not be able to connect to the DynamoDB.",
    "audit": "1. Access the DynamoDB Console \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. 2. Create or Select a DynamoDB Table \u2022 You can create a new DynamoDB table or select an existing one to configure encryption in transit. 3. Verify Encryption Settings \u2022 By default, DynamoDB encrypts data in transit using TLS. To ensure that encryption in transit is enabled: \u2022 In the DynamoDB console, select your table. \u2022 In the table details, navigate to the Overview tab. \u2022 Under the Encryption section, verify that \"Encryption at rest\" is enabled. This indicates that data is encrypted at rest. \u2022 Confirm that Encryption in transit is enabled. It should be enabled by default.   4. Use SSL/TLS Endpoints for API Calls \u2022 To ensure that your API calls to DynamoDB are encrypted in transit, use SSL/TLS endpoints: \u2022 Use the appropriate SDK or AWS CLI in your application or code that interacts with DynamoDB. \u2022 By default, the SDKs and AWS CLI use the SSL/TLS endpoints provided by DynamoDB. \u2022 Verify that your code is configured to connect to DynamoDB using the appropriate SSL/TLS endpoint.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the user does not have the code configured correctly it would not be able to connect to the DynamoDB.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Use the SSL/TLS protocol to encrypt data in transit between your applications and DynamoDB. Amazon DynamoDB encrypts data in transit by default using Transport Layer Security (TLS) encryption. Here is a step-by-step guide on how to ensure encryption in transit for your DynamoDB:",
      "audit_steps": "1. Access the DynamoDB Console \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. 2. Create or Select a DynamoDB Table \u2022 You can create a new DynamoDB table or select an existing one to configure encryption in transit. 3. Verify Encryption Settings \u2022 By default, DynamoDB encrypts data in transit using TLS. To ensure that encryption in transit is enabled: \u2022 In the DynamoDB console, select your table. \u2022 In the table details, navigate to the Overview tab. \u2022 Under the Encryption section, verify that \"Encryption at rest\" is enabled. This indicates that data is encrypted at rest. \u2022 Confirm that Encryption in transit is enabled. It should be enabled by default.   4. Use SSL/TLS Endpoints for API Calls \u2022 To ensure that your API calls to DynamoDB are encrypted in transit, use SSL/TLS endpoints: \u2022 Use the appropriate SDK or AWS CLI in your application or code that interacts with DynamoDB. \u2022 By default, the SDKs and AWS CLI use the SSL/TLS endpoints provided by DynamoDB. \u2022 Verify that your code is configured to connect to DynamoDB using the appropriate SSL/TLS endpoint.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Amazon DynamoDB uses TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by TLS to configure it correctly. Impact: If the user does not have the code configured correctly it would not be able to connect to the DynamoDB.",
      "impact": "If the user does not have the code configured correctly it would not be able to connect to the DynamoDB."
    },
    "function_name": "dynamodb_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.table.encryption.encryption_in_transit_enabled"
  },
  {
    "id": "4.5",
    "title": "Ensure VPC Endpoints are configured",
    "assessment": "Manual",
    "description": "Using VPC endpoints with Amazon DynamoDB allows you to securely access DynamoDB resources within your Amazon Virtual Private Cloud (VPC). This keeps your traffic off the public internet.",
    "rationale": "Using VPC endpoint in the DynamoDB helps ensure that the data is secured and that no external networks would have access to the network. It is a private network where the user has access to their desired availability zones and subnets.",
    "audit": "1. Open Amazon VPC Console \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. 2. Create a VPC Endpoint \u2022 In the Amazon VPC console, navigate to the Endpoints section in the left-side menu. \u2022 Click Create Endpoint. \u2022 Select your desired VPC in the VPC dropdown menu. \u2022 In the Service category section, choose AWS services. \u2022 In the Filter Services search box, enter DynamoDB and select DynamoDB from the results. \u2022 Choose your desired availability zone(s) and subnet(s). \u2022 Leave the default settings for other options or customize them according to your requirements. \u2022 Click Create endpoint. 3. Update Route Tables \u2022 In the Amazon VPC console, navigate to the Route Tables section in the left-side menu. \u2022 Find the route table associated with your VPC or subnet from which you want to access DynamoDB. 1. Edit the route table and add a route for the DynamoDB VPC endpoint.  o Destination: Enter the CIDR block of the DynamoDB VPC endpoint, typically in the form of vpce-xxxxxx-xxxxxxx-xxxxxxx- xxxxxxx.vpce.amazonaws.com/32. o Target: Select the VPC endpoint ID from the dropdown menu. 2. Save the changes to update the route table. 3. Verify Connectivity To ensure that your VPC endpoint for DynamoDB is functioning correctly: \u2022 Launch an Amazon EC2 instance within your VPC or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 From the EC2 instance, try to access DynamoDB using the SDK or CLI. \u2022 Ensure that the access to DynamoDB is successful and that data can be retrieved or modified.",
    "remediation": "Additional Information: Amazon DynamoDB uses Gateway VPC Endpoints, unlike other services that may offer Interface VPC Endpoints. There are some differences such as Gateway VPC Endpoints do not permit cross-region communication. See AWS's Documentation for more information. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Using VPC endpoints with Amazon DynamoDB allows you to securely access DynamoDB resources within your Amazon Virtual Private Cloud (VPC). This keeps your traffic off the public internet.",
      "audit_steps": "1. Open Amazon VPC Console \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. 2. Create a VPC Endpoint \u2022 In the Amazon VPC console, navigate to the Endpoints section in the left-side menu. \u2022 Click Create Endpoint. \u2022 Select your desired VPC in the VPC dropdown menu. \u2022 In the Service category section, choose AWS services. \u2022 In the Filter Services search box, enter DynamoDB and select DynamoDB from the results. \u2022 Choose your desired availability zone(s) and subnet(s). \u2022 Leave the default settings for other options or customize them according to your requirements. \u2022 Click Create endpoint. 3. Update Route Tables \u2022 In the Amazon VPC console, navigate to the Route Tables section in the left-side menu. \u2022 Find the route table associated with your VPC or subnet from which you want to access DynamoDB. 1. Edit the route table and add a route for the DynamoDB VPC endpoint.  o Destination: Enter the CIDR block of the DynamoDB VPC endpoint, typically in the form of vpce-xxxxxx-xxxxxxx-xxxxxxx- xxxxxxx.vpce.amazonaws.com/32. o Target: Select the VPC endpoint ID from the dropdown menu. 2. Save the changes to update the route table. 3. Verify Connectivity To ensure that your VPC endpoint for DynamoDB is functioning correctly: \u2022 Launch an Amazon EC2 instance within your VPC or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 From the EC2 instance, try to access DynamoDB using the SDK or CLI. \u2022 Ensure that the access to DynamoDB is successful and that data can be retrieved or modified.",
      "remediation_steps": "Additional Information: Amazon DynamoDB uses Gateway VPC Endpoints, unlike other services that may offer Interface VPC Endpoints. There are some differences such as Gateway VPC Endpoints do not permit cross-region communication. See AWS's Documentation for more information. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Using VPC endpoint in the DynamoDB helps ensure that the data is secured and that no external networks would have access to the network. It is a private network where the user has access to their desired availability zones and subnets.",
      "impact": ""
    },
    "function_name": "dynamodb_vpc_endpoints_configured_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.network.vpc_endpoints.vpc_endpoints_configured"
  },
  {
    "id": "4.6",
    "title": "Ensure DynamoDB Streams and AWS Lambda for Automated Compliance Checking is Enabled",
    "assessment": "Manual",
    "description": "Enabling DynamoDB Streams and integrating AWS Lambda allows you to automate compliance checking and perform actions based on changes made to your DynamoDB data.",
    "rationale": "Enabling the DynamoDB with AWS Lambda allows the individual to either use an existing or create a new execution role that allows Lambda to access DynamoDB and write logs.",
    "audit": "1. Open DynamoDB Console \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. 2. Create or Select a DynamoDB Table \u2022 You can create a new DynamoDB table or select an existing one to enable DynamoDB Streams. 3. Enable DynamoDB Streams \u2022 In the DynamoDB console, select your table. \u2022 Click on the Overview tab. \u2022 Under the DynamoDB Streams section, click on Manage stream. \u2022 In the Manage stream dialog, choose Enable and select the desired view type (e.g., New and old images). \u2022 Click Enable. 4. Create an AWS Lambda Function \u2022 Open the AWS Management Console and navigate to the Lambda service at https://console.aws.amazon.com/lambda/. \u2022 Click Create function to create a new Lambda function. \u2022 Choose a function name, runtime (e.g., Node.js, Python), and other basic settings.  \u2022 Under Permissions, choose an existing or create a new execution role that allows Lambda to access DynamoDB and write logs. \u2022 Click Create function to create the Lambda function. 5. Configure AWS Lambda with DynamoDB Stream \u2022 Scroll down to the Designer section in the Lambda function editor. \u2022 Click on Add trigger. \u2022 Select DynamoDB from the trigger list. \u2022 In the Configure triggers dialog, choose the DynamoDB table and the stream that you enabled in the previous step. \u2022 Define the batch size and starting position, if applicable. \u2022 Click \"Add\". 6. Write Lambda Function Code for Compliance Checking \u2022 In the Lambda function editor, scroll up to the code editor section. \u2022 Write your compliance-checking logic in the selected runtime language (e.g., Node.js, Python). \u2022 The code should handle the incoming DynamoDB stream records and perform the necessary compliance checks. \u2022 If needed, you can use the AWS SDKs or other libraries to interact with DynamoDB or other AWS services. 7. Configure Lambda Function Settings \u2022 Scroll down to the Function overview section. \u2022 Configure the memory, timeout, and other settings as per your requirements. \u2022 Click Save to save the Lambda function. 8. Test the Compliance Checking \u2022 You can test the compliance checking by changing the DynamoDB table and observing the Lambda function's behavior through the CloudWatch logs or other desired actions performed by the function.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enabling DynamoDB Streams and integrating AWS Lambda allows you to automate compliance checking and perform actions based on changes made to your DynamoDB data.",
      "audit_steps": "1. Open DynamoDB Console \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. 2. Create or Select a DynamoDB Table \u2022 You can create a new DynamoDB table or select an existing one to enable DynamoDB Streams. 3. Enable DynamoDB Streams \u2022 In the DynamoDB console, select your table. \u2022 Click on the Overview tab. \u2022 Under the DynamoDB Streams section, click on Manage stream. \u2022 In the Manage stream dialog, choose Enable and select the desired view type (e.g., New and old images). \u2022 Click Enable. 4. Create an AWS Lambda Function \u2022 Open the AWS Management Console and navigate to the Lambda service at https://console.aws.amazon.com/lambda/. \u2022 Click Create function to create a new Lambda function. \u2022 Choose a function name, runtime (e.g., Node.js, Python), and other basic settings.  \u2022 Under Permissions, choose an existing or create a new execution role that allows Lambda to access DynamoDB and write logs. \u2022 Click Create function to create the Lambda function. 5. Configure AWS Lambda with DynamoDB Stream \u2022 Scroll down to the Designer section in the Lambda function editor. \u2022 Click on Add trigger. \u2022 Select DynamoDB from the trigger list. \u2022 In the Configure triggers dialog, choose the DynamoDB table and the stream that you enabled in the previous step. \u2022 Define the batch size and starting position, if applicable. \u2022 Click \"Add\". 6. Write Lambda Function Code for Compliance Checking \u2022 In the Lambda function editor, scroll up to the code editor section. \u2022 Write your compliance-checking logic in the selected runtime language (e.g., Node.js, Python). \u2022 The code should handle the incoming DynamoDB stream records and perform the necessary compliance checks. \u2022 If needed, you can use the AWS SDKs or other libraries to interact with DynamoDB or other AWS services. 7. Configure Lambda Function Settings \u2022 Scroll down to the Function overview section. \u2022 Configure the memory, timeout, and other settings as per your requirements. \u2022 Click Save to save the Lambda function. 8. Test the Compliance Checking \u2022 You can test the compliance checking by changing the DynamoDB table and observing the Lambda function's behavior through the CloudWatch logs or other desired actions performed by the function.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Enabling the DynamoDB with AWS Lambda allows the individual to either use an existing or create a new execution role that allows Lambda to access DynamoDB and write logs.",
      "impact": ""
    },
    "function_name": "dynamodb_streams_lambda_automation_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.streams.lambda.dynamodb_streams_lambda_automation_enabled"
  },
  {
    "id": "4.7",
    "title": "Ensure Monitor and Audit Activity is enabled",
    "assessment": "Manual",
    "description": "Regular monitoring and auditing of activity in Amazon DynamoDB help ensure your database's security, performance, and compliance.",
    "rationale": "This keeps track and ensures who has recently modified a document and monitors all activity within the database. This information allows the individual to use the details provided for auditing purposes and to address any compliance requirements.",
    "audit": "1. Enable CloudTrail Logging for DynamoDB \u2022 Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/. \u2022 Choose Trails from the left-side menu. \u2022 Click Create trail or select an existing trail. \u2022 Specify a trail name, choose an S3 bucket for storing logs, and configure other trail settings. \u2022 Under Data events, select the checkbox for DynamoDB to enable logging of DynamoDB data events. \u2022 Click Create trail or Save changes to save the CloudTrail configuration. 2. Enable DynamoDB Streams \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. \u2022 Select the DynamoDB table you want to monitor. \u2022 Click on the Overview tab. \u2022 Under the DynamoDB Streams section, click Manage stream. \u2022 Enable DynamoDB Streams with the desired view type (e.g., New and old images). \u2022 Click Enable. 3. Configure Amazon CloudWatch Alarms \u2022 Sign in to the AWS Management Console and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/. \u2022 In the left-side menu, click on Alarms.  \u2022 Click Create alarm. \u2022 Select a DynamoDB metric to monitor (e.g., Read or Write capacity units). \u2022 Configure the threshold, conditions, and actions for the alarm. \u2022 Choose the actions to take when the alarm state is triggered (e.g., send notifications, auto-scaling actions, etc.). \u2022 Click Create alarm to save the configuration. 4. Analyze and Review Logs and Metrics \u2022 Sign in to the AWS Management Console and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/. \u2022 In the left-side menu, click Logs to access CloudWatch Logs. \u2022 Select the appropriate log group for DynamoDB (e.g., /aws/dynamodb/TableName). \u2022 Review the logs to monitor activities, errors, and any unusual behavior. \u2022 Navigate to the CloudWatch console and click Metrics in the left-side menu. \u2022 Select the DynamoDB namespace and the desired metrics (e.g., ConsumedReadCapacityUnits, ConsumedWriteCapacityUnits). \u2022 Analyze the metrics to identify trends, capacity needs, and potential issues. 5. Enable AWS Config for DynamoDB \u2022 Sign in to the AWS Management Console and open the AWS Config console at https://console.aws.amazon.com/config/. \u2022 Click on Rules in the left-side menu. \u2022 Click Add rule. \u2022 Configure a rule for DynamoDB compliance checks, such as checking for unencrypted tables or insecure IAM policies. \u2022 Customize the rule settings and scope based on your requirements. \u2022 Click Save to create the AWS Config rule.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Regular monitoring and auditing of activity in Amazon DynamoDB help ensure your database's security, performance, and compliance.",
      "audit_steps": "1. Enable CloudTrail Logging for DynamoDB \u2022 Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/. \u2022 Choose Trails from the left-side menu. \u2022 Click Create trail or select an existing trail. \u2022 Specify a trail name, choose an S3 bucket for storing logs, and configure other trail settings. \u2022 Under Data events, select the checkbox for DynamoDB to enable logging of DynamoDB data events. \u2022 Click Create trail or Save changes to save the CloudTrail configuration. 2. Enable DynamoDB Streams \u2022 Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/. \u2022 Select the DynamoDB table you want to monitor. \u2022 Click on the Overview tab. \u2022 Under the DynamoDB Streams section, click Manage stream. \u2022 Enable DynamoDB Streams with the desired view type (e.g., New and old images). \u2022 Click Enable. 3. Configure Amazon CloudWatch Alarms \u2022 Sign in to the AWS Management Console and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/. \u2022 In the left-side menu, click on Alarms.  \u2022 Click Create alarm. \u2022 Select a DynamoDB metric to monitor (e.g., Read or Write capacity units). \u2022 Configure the threshold, conditions, and actions for the alarm. \u2022 Choose the actions to take when the alarm state is triggered (e.g., send notifications, auto-scaling actions, etc.). \u2022 Click Create alarm to save the configuration. 4. Analyze and Review Logs and Metrics \u2022 Sign in to the AWS Management Console and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/. \u2022 In the left-side menu, click Logs to access CloudWatch Logs. \u2022 Select the appropriate log group for DynamoDB (e.g., /aws/dynamodb/TableName). \u2022 Review the logs to monitor activities, errors, and any unusual behavior. \u2022 Navigate to the CloudWatch console and click Metrics in the left-side menu. \u2022 Select the DynamoDB namespace and the desired metrics (e.g., ConsumedReadCapacityUnits, ConsumedWriteCapacityUnits). \u2022 Analyze the metrics to identify trends, capacity needs, and potential issues. 5. Enable AWS Config for DynamoDB \u2022 Sign in to the AWS Management Console and open the AWS Config console at https://console.aws.amazon.com/config/. \u2022 Click on Rules in the left-side menu. \u2022 Click Add rule. \u2022 Configure a rule for DynamoDB compliance checks, such as checking for unencrypted tables or insecure IAM policies. \u2022 Customize the rule settings and scope based on your requirements. \u2022 Click Save to create the AWS Config rule.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This keeps track and ensures who has recently modified a document and monitors all activity within the database. This information allows the individual to use the details provided for auditing purposes and to address any compliance requirements.",
      "impact": ""
    },
    "function_name": "dynamodb_monitor_audit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.dynamodb.monitoring.audit.monitor_audit_activity_enabled"
  },
  {
    "id": "5.1",
    "title": "Ensure Secure Access to ElastiCache",
    "assessment": "Manual",
    "description": "Securing access to Amazon ElastiCache involves implementing appropriate authentication and authorization mechanisms.",
    "rationale": "",
    "audit": "1. Use AWS Identity and Access Management (IAM) \u2022 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. \u2022 Create IAM users or roles for individuals or applications needing ElastiCache access. \u2022 Define fine-grained permissions using IAM policies to allow only necessary actions on ElastiCache resources. \u2022 Assign IAM policies to the IAM users or roles to grant access. 2. Implement Secure Network Access \u2022 Place your ElastiCache cluster within a Virtual Private Cloud (VPC) to control network access. \u2022 Create and configure security groups to allow access only from trusted networks or specific IP ranges. \u2022 Ensure your VPC's network ACLs (Access Control Lists) are properly configured to restrict inbound and outbound traffic. 3. Enable Encryption in Transit \u2022 Configure your ElastiCache cluster to use SSL/TLS encryption for client connections. \u2022 Use the --transit-encryption-enabled parameter when creating or modifying the cluster to enable encryption in transit. \u2022 Update your client applications to connect to the ElastiCache cluster using SSL/TLS. 4. Protect ElastiCache Credentials \u2022 Avoid sharing access keys, secret keys, or IAM user credentials between individuals.  \u2022 Use IAM roles for Amazon EC2 instances or other AWS services to securely access ElastiCache without needing credentials. \u2022 Rotate your access keys regularly and disable or remove unnecessary IAM users or roles. 5. Enable Event Logging and Monitoring \u2022 Enable CloudWatch Logs for your ElastiCache clusters to capture logs and monitor activities. \u2022 Configure CloudWatch Alarms to be notified of any unusual or suspicious behavior. \u2022 Set up CloudTrail to log API calls made to ElastiCache for auditing and compliance purposes. 6. Regularly Review and Update Access Controls \u2022 Perform regular reviews of IAM policies, security groups, and network ACLs to ensure they align with your security requirements. \u2022 Remove any unnecessary or excessive privileges from IAM policies. \u2022 Stay updated with AWS security best practices and recommendations to improve access controls.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Securing access to Amazon ElastiCache involves implementing appropriate authentication and authorization mechanisms.",
      "audit_steps": "1. Use AWS Identity and Access Management (IAM) \u2022 Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. \u2022 Create IAM users or roles for individuals or applications needing ElastiCache access. \u2022 Define fine-grained permissions using IAM policies to allow only necessary actions on ElastiCache resources. \u2022 Assign IAM policies to the IAM users or roles to grant access. 2. Implement Secure Network Access \u2022 Place your ElastiCache cluster within a Virtual Private Cloud (VPC) to control network access. \u2022 Create and configure security groups to allow access only from trusted networks or specific IP ranges. \u2022 Ensure your VPC's network ACLs (Access Control Lists) are properly configured to restrict inbound and outbound traffic. 3. Enable Encryption in Transit \u2022 Configure your ElastiCache cluster to use SSL/TLS encryption for client connections. \u2022 Use the --transit-encryption-enabled parameter when creating or modifying the cluster to enable encryption in transit. \u2022 Update your client applications to connect to the ElastiCache cluster using SSL/TLS. 4. Protect ElastiCache Credentials \u2022 Avoid sharing access keys, secret keys, or IAM user credentials between individuals.  \u2022 Use IAM roles for Amazon EC2 instances or other AWS services to securely access ElastiCache without needing credentials. \u2022 Rotate your access keys regularly and disable or remove unnecessary IAM users or roles. 5. Enable Event Logging and Monitoring \u2022 Enable CloudWatch Logs for your ElastiCache clusters to capture logs and monitor activities. \u2022 Configure CloudWatch Alarms to be notified of any unusual or suspicious behavior. \u2022 Set up CloudTrail to log API calls made to ElastiCache for auditing and compliance purposes. 6. Regularly Review and Update Access Controls \u2022 Perform regular reviews of IAM policies, security groups, and network ACLs to ensure they align with your security requirements. \u2022 Remove any unnecessary or excessive privileges from IAM policies. \u2022 Stay updated with AWS security best practices and recommendations to improve access controls.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "elasticache_secure_access_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.access.secure.secure_access_enabled"
  },
  {
    "id": "5.2",
    "title": "Ensure Network Security is Enabled",
    "assessment": "Manual",
    "description": "Implementing network security for Amazon ElastiCache involves configuring your Virtual Private Cloud (VPC), security groups, and network access controls to control access to your ElastiCache clusters.",
    "rationale": "This helps ensure that the data is safe and protected from any threats and or misconfigurations within the network. This helps to keep a potential hacker getting into the system and compromising the data.",
    "audit": "1. Create or Select a VPC \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC where you want to deploy your ElastiCache cluster. 2. Create Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the desired subnets within your VPC where you want to deploy your ElastiCache cluster. 3. Configure Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group. Or select an existing one to configure the security settings for your ElastiCache cluster. \u2022 Define inbound and outbound rules to control the traffic flow to and from your ElastiCache cluster. o Allow inbound traffic from trusted sources (e.g., specific IP ranges or security groups) on the necessary ports used by your ElastiCache cluster. o Define outbound rules based on your requirements, such as allowing outbound traffic to specific destinations or ports. \u2022 Associate the security group with the ElastiCache cluster when creating or modifying it.  4. Set up Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu. \u2022 Create or select the appropriate network ACL associated with the subnets used by your ElastiCache cluster. \u2022 Configure inbound and outbound rules in the network ACL to allow or deny traffic to and from your ElastiCache cluster. o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. \u2022 Associate the network ACL with the subnets used by your ElastiCache cluster. 5. Configure Route Tables \u2022 In the VPC console, navigate to Route Tables in the left-side menu. \u2022 Create or select the route table associated with the subnets used by your ElastiCache cluster. \u2022 Add or modify routes to ensure traffic to and from your ElastiCache cluster flows correctly. o Ensure that the route table has an appropriate route to the internet gateway or virtual private gateway if external connectivity is required. \u2022 Associate the route table with the subnets used by your ElastiCache cluster. 6. Verify Connectivity and Test \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your ElastiCache cluster or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the connectivity to your ElastiCache cluster by trying to connect to it using the appropriate client or utility. \u2022 Verify that the network security settings allow the necessary traffic and deny unauthorized access.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Implementing network security for Amazon ElastiCache involves configuring your Virtual Private Cloud (VPC), security groups, and network access controls to control access to your ElastiCache clusters.",
      "audit_steps": "1. Create or Select a VPC \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC where you want to deploy your ElastiCache cluster. 2. Create Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the desired subnets within your VPC where you want to deploy your ElastiCache cluster. 3. Configure Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group. Or select an existing one to configure the security settings for your ElastiCache cluster. \u2022 Define inbound and outbound rules to control the traffic flow to and from your ElastiCache cluster. o Allow inbound traffic from trusted sources (e.g., specific IP ranges or security groups) on the necessary ports used by your ElastiCache cluster. o Define outbound rules based on your requirements, such as allowing outbound traffic to specific destinations or ports. \u2022 Associate the security group with the ElastiCache cluster when creating or modifying it.  4. Set up Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu. \u2022 Create or select the appropriate network ACL associated with the subnets used by your ElastiCache cluster. \u2022 Configure inbound and outbound rules in the network ACL to allow or deny traffic to and from your ElastiCache cluster. o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. \u2022 Associate the network ACL with the subnets used by your ElastiCache cluster. 5. Configure Route Tables \u2022 In the VPC console, navigate to Route Tables in the left-side menu. \u2022 Create or select the route table associated with the subnets used by your ElastiCache cluster. \u2022 Add or modify routes to ensure traffic to and from your ElastiCache cluster flows correctly. o Ensure that the route table has an appropriate route to the internet gateway or virtual private gateway if external connectivity is required. \u2022 Associate the route table with the subnets used by your ElastiCache cluster. 6. Verify Connectivity and Test \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your ElastiCache cluster or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the connectivity to your ElastiCache cluster by trying to connect to it using the appropriate client or utility. \u2022 Verify that the network security settings allow the necessary traffic and deny unauthorized access.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps ensure that the data is safe and protected from any threats and or misconfigurations within the network. This helps to keep a potential hacker getting into the system and compromising the data.",
      "impact": ""
    },
    "function_name": "elasticache_network_security_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.network.security.network_security_enabled"
  },
  {
    "id": "5.3",
    "title": "Ensure Encryption at Rest and in Transit is configured",
    "assessment": "Manual",
    "description": "Enabling encryption at rest and in transit for Amazon ElastiCache helps protect your data when it is stored and transmitted.",
    "rationale": "Enabling encryption at rest secured the users data where it is stored. Enabling encryption in transit helps that the data is protected when it is moving from one location to another. Impact: If the user didn\u2019t enable encryption and rest and during transit, there is a possibility of the data being vulnerable to a ransomware attack.",
    "audit": "1. Enable Encryption at Rest \u2022 Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/. \u2022 Create a new ElastiCache cluster or select an existing cluster. \u2022 On the cluster details page, click the Encryption tab. \u2022 Select the option to enable encryption Under the Encryption at Rest section. \u2022 Choose the desired encryption type: o list text hereDefault Encryption: Select this option to use the default AWS- managed key for encryption. o list text hereCustomer Managed Key (CMK): Select this option to use your own AWS Key Management Service (KMS) customer-managed key for encryption. \u2022 If you selected Customer Managed Key (CMK), choose the appropriate KMS key from the dropdown menu. \u2022 Click \"Save changes\" to enable encryption at rest for the ElastiCache cluster. 2. Enable Encryption in Transit \u2022 On the ElastiCache cluster details page, click the Encryption tab. \u2022 Select the option to enable encryption Under the \"Encryption in Transit\" section. \u2022 Choose the desired encryption type:  o list text hereTransit encryption enabled with SSL/TLS: Select this option to enable encryption in transit using SSL/TLS encryption. o list text hereTransit encryption disabled: Select this option if you do not require encryption in transit. \u2022 Click Save changes to enable encryption in transit for the ElastiCache cluster. 3. Verify the Encryption Status \u2022 Wait a few minutes for the changes to propagate and the encryption to take effect. \u2022 Refresh the ElastiCache console and navigate to the cluster details page. \u2022 Verify that the encryption status is now enabled for both encryptions at rest and in transit.",
    "remediation": "The user has two options when it comes to encryption at rest and in transit to choose from. Depending on what actions the user selects from it determines how their data is going to be protected. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the user didn\u2019t enable encryption and rest and during transit, there is a possibility of the data being vulnerable to a ransomware attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enabling encryption at rest and in transit for Amazon ElastiCache helps protect your data when it is stored and transmitted.",
      "audit_steps": "1. Enable Encryption at Rest \u2022 Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/. \u2022 Create a new ElastiCache cluster or select an existing cluster. \u2022 On the cluster details page, click the Encryption tab. \u2022 Select the option to enable encryption Under the Encryption at Rest section. \u2022 Choose the desired encryption type: o list text hereDefault Encryption: Select this option to use the default AWS- managed key for encryption. o list text hereCustomer Managed Key (CMK): Select this option to use your own AWS Key Management Service (KMS) customer-managed key for encryption. \u2022 If you selected Customer Managed Key (CMK), choose the appropriate KMS key from the dropdown menu. \u2022 Click \"Save changes\" to enable encryption at rest for the ElastiCache cluster. 2. Enable Encryption in Transit \u2022 On the ElastiCache cluster details page, click the Encryption tab. \u2022 Select the option to enable encryption Under the \"Encryption in Transit\" section. \u2022 Choose the desired encryption type:  o list text hereTransit encryption enabled with SSL/TLS: Select this option to enable encryption in transit using SSL/TLS encryption. o list text hereTransit encryption disabled: Select this option if you do not require encryption in transit. \u2022 Click Save changes to enable encryption in transit for the ElastiCache cluster. 3. Verify the Encryption Status \u2022 Wait a few minutes for the changes to propagate and the encryption to take effect. \u2022 Refresh the ElastiCache console and navigate to the cluster details page. \u2022 Verify that the encryption status is now enabled for both encryptions at rest and in transit.",
      "remediation_steps": "The user has two options when it comes to encryption at rest and in transit to choose from. Depending on what actions the user selects from it determines how their data is going to be protected. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Enabling encryption at rest secured the users data where it is stored. Enabling encryption in transit helps that the data is protected when it is moving from one location to another. Impact: If the user didn\u2019t enable encryption and rest and during transit, there is a possibility of the data being vulnerable to a ransomware attack.",
      "impact": "If the user didn\u2019t enable encryption and rest and during transit, there is a possibility of the data being vulnerable to a ransomware attack."
    },
    "function_name": "elasticache_encryption_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.encryption.enabled.encryption_at_rest_transit_enabled"
  },
  {
    "id": "5.4",
    "title": "Ensure Automatic Updates and Patching are Enabled",
    "assessment": "Manual",
    "description": "Enabling automatic updates and patching for Amazon ElastiCache ensures that your ElastiCache clusters run the latest software versions with important security fixes and enhancements.",
    "rationale": "Automatic updates help the software be updated and address any vulnerabilities within the software that can help business with any potential exists that can impact the business and prevent any unauthorized access.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the ElastiCache Console \u2022 Open the Amazon ElastiCache console by navigating to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/elasticache/. 3. Select the ElastiCache Cluster \u2022 Choose the ElastiCache cluster you want to enable automatic updates and patching. \u2022 Click on the cluster name to access its details page. 4. Enable Automatic Updates \u2022 Click on the Configuration tab on the cluster details page. \u2022 Scroll down to the Cluster details section. \u2022 Under Cluster maintenance and updates, click Modify. \u2022 In the Maintenance and updates dialog, find the Auto minor version upgrade option and select Enable. \u2022 Leave other settings unchanged or adjust them according to your requirements.  \u2022 Click Save to apply the changes. 5. Verify Automatic Updates Status \u2022 Wait for a few moments for the changes to take effect. \u2022 Refresh the cluster details page to see the updated configuration. \u2022 Verify that the \"Auto minor version upgrade\" setting is now enabled for the ElastiCache cluster.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enabling automatic updates and patching for Amazon ElastiCache ensures that your ElastiCache clusters run the latest software versions with important security fixes and enhancements.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the ElastiCache Console \u2022 Open the Amazon ElastiCache console by navigating to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/elasticache/. 3. Select the ElastiCache Cluster \u2022 Choose the ElastiCache cluster you want to enable automatic updates and patching. \u2022 Click on the cluster name to access its details page. 4. Enable Automatic Updates \u2022 Click on the Configuration tab on the cluster details page. \u2022 Scroll down to the Cluster details section. \u2022 Under Cluster maintenance and updates, click Modify. \u2022 In the Maintenance and updates dialog, find the Auto minor version upgrade option and select Enable. \u2022 Leave other settings unchanged or adjust them according to your requirements.  \u2022 Click Save to apply the changes. 5. Verify Automatic Updates Status \u2022 Wait for a few moments for the changes to take effect. \u2022 Refresh the cluster details page to see the updated configuration. \u2022 Verify that the \"Auto minor version upgrade\" setting is now enabled for the ElastiCache cluster.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Automatic updates help the software be updated and address any vulnerabilities within the software that can help business with any potential exists that can impact the business and prevent any unauthorized access.",
      "impact": ""
    },
    "function_name": "elasticache_automatic_updates_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.maintenance.updates.automatic_updates_patching_enabled"
  },
  {
    "id": "5.5",
    "title": "Ensure Virtual Private Cloud (VPC) is Enabled",
    "assessment": "Manual",
    "description": "Implementing VPC security best practices for Amazon ElastiCache involves configuring your Virtual Private Cloud (VPC) and associated resources to enhance the security of your ElastiCache clusters.",
    "rationale": "This ensures that only authorized users can access their platforms and prevents any mistakes that can lead to a data breach due to the level of security.",
    "audit": "1. Create or Select a VPC \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC to host your ElastiCache clusters. 2. Configure Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the subnets within your VPC where you want to deploy your ElastiCache clusters. \u2022 Ensure you have private subnets for your ElastiCache clusters to avoid exposing them to the public internet. 3. Define Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group or select an existing one for your ElastiCache clusters. \u2022 Configure inbound and outbound rules in the security group to control traffic access. o Allow inbound access only from trusted sources or specific IP ranges required for your applications. o Restrict outbound access to necessary destinations and protocols. \u2022 Associate the security group with your ElastiCache clusters. 4. Configure Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu.  \u2022 Create or select the network ACLs associated with the subnets used by your ElastiCache clusters. \u2022 Configure inbound and outbound rules in the network ACLs to control traffic access. o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. o Deny unnecessary or unwanted traffic. \u2022 Associate the network ACLs with the subnets used by your ElastiCache clusters. 5. Configure Routing \u2022 In the VPC console, navigate to Route Tables in the left-side menu. \u2022 Create or select the route table associated with the subnets used by your ElastiCache clusters. \u2022 Add or modify routes to ensure traffic flows correctly to and from your ElastiCache clusters. \u2022 Ensure that the route table has appropriate routes to the internet gateway or virtual private gateway if external connectivity is required. \u2022 Associate the route table with the subnets used by your ElastiCache clusters. 6. Review and Update Network Security Settings \u2022 Regularly review and update your VPC security configurations, including security groups, network ACLs, and routing, to align with your security requirements. \u2022 Remove any unnecessary or excessive permissions from security groups and tighten inbound and outbound access as needed. \u2022 Stay informed about AWS security best practices and recommendations to enhance your network security.",
    "remediation": "The individual is required to create a subnet and configure their inbound and outbound access. Individuals are supposed to configure their ACL and routing ensuring the traffic is flowing smoothly without any interference. This control is important because it only allows authorized user to access their resources as they prefer. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Implementing VPC security best practices for Amazon ElastiCache involves configuring your Virtual Private Cloud (VPC) and associated resources to enhance the security of your ElastiCache clusters.",
      "audit_steps": "1. Create or Select a VPC \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC to host your ElastiCache clusters. 2. Configure Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the subnets within your VPC where you want to deploy your ElastiCache clusters. \u2022 Ensure you have private subnets for your ElastiCache clusters to avoid exposing them to the public internet. 3. Define Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group or select an existing one for your ElastiCache clusters. \u2022 Configure inbound and outbound rules in the security group to control traffic access. o Allow inbound access only from trusted sources or specific IP ranges required for your applications. o Restrict outbound access to necessary destinations and protocols. \u2022 Associate the security group with your ElastiCache clusters. 4. Configure Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu.  \u2022 Create or select the network ACLs associated with the subnets used by your ElastiCache clusters. \u2022 Configure inbound and outbound rules in the network ACLs to control traffic access. o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. o Deny unnecessary or unwanted traffic. \u2022 Associate the network ACLs with the subnets used by your ElastiCache clusters. 5. Configure Routing \u2022 In the VPC console, navigate to Route Tables in the left-side menu. \u2022 Create or select the route table associated with the subnets used by your ElastiCache clusters. \u2022 Add or modify routes to ensure traffic flows correctly to and from your ElastiCache clusters. \u2022 Ensure that the route table has appropriate routes to the internet gateway or virtual private gateway if external connectivity is required. \u2022 Associate the route table with the subnets used by your ElastiCache clusters. 6. Review and Update Network Security Settings \u2022 Regularly review and update your VPC security configurations, including security groups, network ACLs, and routing, to align with your security requirements. \u2022 Remove any unnecessary or excessive permissions from security groups and tighten inbound and outbound access as needed. \u2022 Stay informed about AWS security best practices and recommendations to enhance your network security.",
      "remediation_steps": "The individual is required to create a subnet and configure their inbound and outbound access. Individuals are supposed to configure their ACL and routing ensuring the traffic is flowing smoothly without any interference. This control is important because it only allows authorized user to access their resources as they prefer. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This ensures that only authorized users can access their platforms and prevents any mistakes that can lead to a data breach due to the level of security.",
      "impact": ""
    },
    "function_name": "elasticache_vpc_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.network.vpc.vpc_enabled"
  },
  {
    "id": "5.6",
    "title": "Ensure Monitoring and Logging is Enabled",
    "assessment": "Manual",
    "description": "Implementing monitoring and logging for Amazon ElastiCache allows you to gain visibility into the performance, health, and behavior of your ElastiCache clusters.",
    "rationale": "This helps the individual know what is being logged within the activity and determine what next step they should take to address any suspicious activity. Impact: If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the ElastiCache Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/elasticache/. 3. Select the ElastiCache Cluster \u2022 Choose the ElastiCache cluster for which you want to implement monitoring and logging. \u2022 Click on the cluster name to access its details page. 4. Enable Enhanced Monitoring \u2022 Click on the Monitoring tab on the cluster details page. \u2022 Under the Monitoring section, click on the Enable Enhanced Monitoring button. \u2022 Select the desired monitoring granularity (1 minute, 5 minutes, or 60 minutes) to capture detailed metrics. \u2022 Choose the desired CloudWatch namespace to store the metrics. \u2022 Click Save changes to enable enhanced monitoring for the ElastiCache cluster.  5. Set Up CloudWatch Alarms \u2022 In the CloudWatch console, navigate to Alarms in the left-side menu. \u2022 Click Create alarm to create a new alarm. \u2022 Select the appropriate ElastiCache metrics from the available options. \u2022 Configure the threshold, conditions, and actions for the alarm. \u2022 Choose the actions to take when the alarm state is triggered (e.g., send notifications, auto-scaling actions, etc.). \u2022 Click Create alarm to save the alarm configuration. 6. Configure CloudWatch Logs \u2022 In the CloudWatch console, navigate to Logs in the left-side menu. \u2022 Click Create log group to create a new one. \u2022 Provide a unique name for the log group and optionally specify a retention period for log data. \u2022 Click Create log group to create the log group. \u2022 On the ElastiCache cluster details page, click the Logging tab. \u2022 Enable the CloudWatch Logs option and select the desired log group from the dropdown menu. \u2022 Click Save changes to enable CloudWatch Logs for the ElastiCache cluster. 7. Verify Monitoring and Logging \u2022 Wait a few minutes for the monitoring and logging configurations to take effect. \u2022 Refresh the cluster details page for the updated monitoring and logging status. \u2022 Navigate to the CloudWatch console to view metrics, alarms, and logs related to your ElastiCache cluster.",
    "remediation": "The individual can understand the health, performance, and behavior of their clusters which allows them to address any unusual activity that takes place. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Implementing monitoring and logging for Amazon ElastiCache allows you to gain visibility into the performance, health, and behavior of your ElastiCache clusters.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the ElastiCache Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/elasticache/. 3. Select the ElastiCache Cluster \u2022 Choose the ElastiCache cluster for which you want to implement monitoring and logging. \u2022 Click on the cluster name to access its details page. 4. Enable Enhanced Monitoring \u2022 Click on the Monitoring tab on the cluster details page. \u2022 Under the Monitoring section, click on the Enable Enhanced Monitoring button. \u2022 Select the desired monitoring granularity (1 minute, 5 minutes, or 60 minutes) to capture detailed metrics. \u2022 Choose the desired CloudWatch namespace to store the metrics. \u2022 Click Save changes to enable enhanced monitoring for the ElastiCache cluster.  5. Set Up CloudWatch Alarms \u2022 In the CloudWatch console, navigate to Alarms in the left-side menu. \u2022 Click Create alarm to create a new alarm. \u2022 Select the appropriate ElastiCache metrics from the available options. \u2022 Configure the threshold, conditions, and actions for the alarm. \u2022 Choose the actions to take when the alarm state is triggered (e.g., send notifications, auto-scaling actions, etc.). \u2022 Click Create alarm to save the alarm configuration. 6. Configure CloudWatch Logs \u2022 In the CloudWatch console, navigate to Logs in the left-side menu. \u2022 Click Create log group to create a new one. \u2022 Provide a unique name for the log group and optionally specify a retention period for log data. \u2022 Click Create log group to create the log group. \u2022 On the ElastiCache cluster details page, click the Logging tab. \u2022 Enable the CloudWatch Logs option and select the desired log group from the dropdown menu. \u2022 Click Save changes to enable CloudWatch Logs for the ElastiCache cluster. 7. Verify Monitoring and Logging \u2022 Wait a few minutes for the monitoring and logging configurations to take effect. \u2022 Refresh the cluster details page for the updated monitoring and logging status. \u2022 Navigate to the CloudWatch console to view metrics, alarms, and logs related to your ElastiCache cluster.",
      "remediation_steps": "The individual can understand the health, performance, and behavior of their clusters which allows them to address any unusual activity that takes place. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps the individual know what is being logged within the activity and determine what next step they should take to address any suspicious activity. Impact: If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data.",
      "impact": "If the individual is not monitoring and logging their activity it allows the attacker to attack the system and extract or destroy data."
    },
    "function_name": "elasticache_monitoring_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.monitoring.logging.monitoring_logging_enabled"
  },
  {
    "id": "5.7",
    "title": "Ensure Security Configurations are Reviewed Regularly",
    "assessment": "Manual",
    "description": "Regularly updating and reviewing the security configuration of your Amazon ElastiCache clusters helps ensure that your clusters are protected against potential vulnerabilities and aligned with your security requirements.",
    "rationale": "This ensures that the clusters are being regularly updated and protected from any potential vulnerabilities as well as meeting the security requirements. Impact: Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the ElastiCache Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/elasticache/. 3. Select the ElastiCache Cluster \u2022 Choose the ElastiCache cluster you want to update and review the security configuration. Click on the cluster name to access its details page. 4. Review IAM Policies \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the IAM Access tab. \u2022 Review the IAM policies associated with the ElastiCache cluster and its resources. \u2022 Ensure that the IAM policies provide the least privileged access, granting only the necessary permissions to users and roles.  \u2022 Update the IAM policies as required based on changes in access requirements or security best practices. 5. Review Security Groups \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Security Groups tab. \u2022 Review the security groups associated with the ElastiCache cluster. \u2022 Ensure that the inbound and outbound rules of the security groups are configured correctly and restrict access to necessary ports and IP ranges. \u2022 Update the security group rules as needed to align with your security requirements. 6. Review Encryption Settings \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Encryption at Rest tab. \u2022 Verify the encryption settings for the ElastiCache cluster. \u2022 Ensure that encryption at rest is enabled and using the appropriate encryption type (default AWS-managed key or customer-managed key). \u2022 Update the encryption settings if necessary to comply with your security policies. 7. Review Network Security \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Network & Security tab. \u2022 Review the VPC, subnets, security groups, and network ACLs associated with the ElastiCache cluster. \u2022 Ensure that the VPC and subnet configurations align with your security requirements. \u2022 Update the network security settings as needed to maintain a secure network architecture. 8. Review Access Control \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Security tab. \u2022 Review the authentication and access control settings for the ElastiCache cluster. \u2022 Ensure that the authentication method (no password, transit encryption, or encryption in transit) meets your security standards. \u2022 Update the access control settings as required to align with your security policies. 9. Regularly Monitor Security Bulletins \u2022 Stay updated with AWS security bulletins, advisories, and best practices.  \u2022 Regularly review security-related announcements from AWS. \u2022 Take necessary actions based on security recommendations, such as applying patches or configuration changes.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Regularly updating and reviewing the security configuration of your Amazon ElastiCache clusters helps ensure that your clusters are protected against potential vulnerabilities and aligned with your security requirements.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the ElastiCache Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/elasticache/. 3. Select the ElastiCache Cluster \u2022 Choose the ElastiCache cluster you want to update and review the security configuration. Click on the cluster name to access its details page. 4. Review IAM Policies \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the IAM Access tab. \u2022 Review the IAM policies associated with the ElastiCache cluster and its resources. \u2022 Ensure that the IAM policies provide the least privileged access, granting only the necessary permissions to users and roles.  \u2022 Update the IAM policies as required based on changes in access requirements or security best practices. 5. Review Security Groups \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Security Groups tab. \u2022 Review the security groups associated with the ElastiCache cluster. \u2022 Ensure that the inbound and outbound rules of the security groups are configured correctly and restrict access to necessary ports and IP ranges. \u2022 Update the security group rules as needed to align with your security requirements. 6. Review Encryption Settings \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Encryption at Rest tab. \u2022 Verify the encryption settings for the ElastiCache cluster. \u2022 Ensure that encryption at rest is enabled and using the appropriate encryption type (default AWS-managed key or customer-managed key). \u2022 Update the encryption settings if necessary to comply with your security policies. 7. Review Network Security \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Network & Security tab. \u2022 Review the VPC, subnets, security groups, and network ACLs associated with the ElastiCache cluster. \u2022 Ensure that the VPC and subnet configurations align with your security requirements. \u2022 Update the network security settings as needed to maintain a secure network architecture. 8. Review Access Control \u2022 Navigate to the Configuration tab on the cluster details page. \u2022 Click on the Security tab. \u2022 Review the authentication and access control settings for the ElastiCache cluster. \u2022 Ensure that the authentication method (no password, transit encryption, or encryption in transit) meets your security standards. \u2022 Update the access control settings as required to align with your security policies. 9. Regularly Monitor Security Bulletins \u2022 Stay updated with AWS security bulletins, advisories, and best practices.  \u2022 Regularly review security-related announcements from AWS. \u2022 Take necessary actions based on security recommendations, such as applying patches or configuration changes.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This ensures that the clusters are being regularly updated and protected from any potential vulnerabilities as well as meeting the security requirements. Impact: Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack.",
      "impact": "Updating the system and being updated with security configurations keeps everything secure and prevents it from an attack."
    },
    "function_name": "elasticache_security_configuration_review_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.security.configuration.security_configuration_reviewed"
  },
  {
    "id": "5.8",
    "title": "Ensure Authentication and Access Control is Enabled",
    "assessment": "Manual",
    "description": "Individual creates IAM roles that would give specific permission to what the user can and cannot do within that database. The Access Control List (ACLs) allows only specific individuals to access the resources.",
    "rationale": "Impact: Use specific client\u2019s applications or tools that allow the authorized personnel to connect to the database.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to implement authentication and access control. \u2022 Click on the Keyspace name to access its details page. 4. Enable IAM for Cassandra \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, locate the \"IAM for Cassandra\" option. \u2022 Click on Edit. \u2022 Select the Enable option to enable IAM for Cassandra authentication and authorization. \u2022 Choose the IAM role(s) that can access the Keyspace  \u2022 Click Save to enable IAM for Cassandra. 5. Define IAM Roles and Permissions \u2022 Open the IAM console by navigating to Identity and Access Management (IAM) in the AWS Management Console. \u2022 Create IAM roles with appropriate policies defining the desired access level to your Amazon Keyspaces resources. \u2022 You may create different roles for different user groups or applications. \u2022 Ensure that the IAM policies associated with these roles allow the necessary permissions for interacting with Keyspaces. \u2022 Attach the IAM roles to the appropriate AWS identities, such as IAM users or AWS Identity and Access Management roles. 6. Review and Update Access Control \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, click on Access Control Lists (ACLs). \u2022 Review the ACLs to define fine-grained access control at the table and row level. o Define rules that allow or deny access based on specific conditions, such as IP addresses or IAM roles. o Ensure that the ACL rules align with your security requirements and restrict access to sensitive data if necessary. \u2022 Update the ACLs as needed to accommodate changes. 7. Verify Authentication and Access Control \u2022 Test the authentication and access control mechanisms using client applications or tools that connect to your Amazon Keyspaces resources. \u2022 Verify that only authorized users or applications can access the Keyspaces resources based on the defined IAM roles and ACL rules. \u2022 Monitor the access logs and perform periodic reviews to ensure the authentication and access control measures function as intended.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Use specific client\u2019s applications or tools that allow the authorized personnel to connect to the database.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Individual creates IAM roles that would give specific permission to what the user can and cannot do within that database. The Access Control List (ACLs) allows only specific individuals to access the resources.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to implement authentication and access control. \u2022 Click on the Keyspace name to access its details page. 4. Enable IAM for Cassandra \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, locate the \"IAM for Cassandra\" option. \u2022 Click on Edit. \u2022 Select the Enable option to enable IAM for Cassandra authentication and authorization. \u2022 Choose the IAM role(s) that can access the Keyspace  \u2022 Click Save to enable IAM for Cassandra. 5. Define IAM Roles and Permissions \u2022 Open the IAM console by navigating to Identity and Access Management (IAM) in the AWS Management Console. \u2022 Create IAM roles with appropriate policies defining the desired access level to your Amazon Keyspaces resources. \u2022 You may create different roles for different user groups or applications. \u2022 Ensure that the IAM policies associated with these roles allow the necessary permissions for interacting with Keyspaces. \u2022 Attach the IAM roles to the appropriate AWS identities, such as IAM users or AWS Identity and Access Management roles. 6. Review and Update Access Control \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, click on Access Control Lists (ACLs). \u2022 Review the ACLs to define fine-grained access control at the table and row level. o Define rules that allow or deny access based on specific conditions, such as IP addresses or IAM roles. o Ensure that the ACL rules align with your security requirements and restrict access to sensitive data if necessary. \u2022 Update the ACLs as needed to accommodate changes. 7. Verify Authentication and Access Control \u2022 Test the authentication and access control mechanisms using client applications or tools that connect to your Amazon Keyspaces resources. \u2022 Verify that only authorized users or applications can access the Keyspaces resources based on the defined IAM roles and ACL rules. \u2022 Monitor the access logs and perform periodic reviews to ensure the authentication and access control measures function as intended.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Use specific client\u2019s applications or tools that allow the authorized personnel to connect to the database.",
      "impact": "Use specific client\u2019s applications or tools that allow the authorized personnel to connect to the database."
    },
    "function_name": "elasticache_authentication_access_control_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.authentication.access_control.authentication_access_control_enabled"
  },
  {
    "id": "5.9",
    "title": "Ensure Audit Logging is Enabled",
    "assessment": "Manual",
    "description": "To manage your enterprise caching solution, it is important that you know how your clusters are performing and the resources they are consuming. It is also important that you know the events that are being generated and the costs of your deployment. Amazon CloudWatch provides metrics for monitoring your cache performance. In addition, cost allocation tags help you monitor and manage costs.",
    "rationale": "Impact: Reduce the risk of any fraud or inconsistency within the database because only authorized user has access to it.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to enable audit logging. \u2022 Click on the Keyspace name to access its details page. 4. Enable Amazon CloudWatch Logs \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Logging section, locate the CloudWatch Logs option. \u2022 Click on Edit. \u2022 Select the Enable option to enable logging for the Keyspace. \u2022 Choose an existing CloudWatch Logs log group or create a new one to store the logs generated by the Keyspace activities. \u2022 Click Save to enable CloudWatch Logs for the Keyspace.  5. Configure CloudWatch Logs \u2022 Open the CloudWatch console by navigating to CloudWatch in the AWS Management Console. \u2022 In the left-side menu, click on Logs. \u2022 Create a new log group or select an existing log group that will store the Keyspaces logs. \u2022 Configure log retention settings based on your retention requirements. Logs can be stored for a specific number of days or indefinitely. \u2022 Define any necessary log group permissions to control access to the logs. \u2022 Optionally, set up log exports or alarms for specific log events or patterns if needed. 6. Verify the Logging Status \u2022 Wait a few minutes for the changes to propagate and the logging configuration to take effect. \u2022 Refresh the Keyspace details page to see the updated logging status. \u2022 Verify that CloudWatch Logs is enabled for the Keyspace. 7. Monitor and Analyze Logs \u2022 Navigate to the CloudWatch console and select the log group that stores the Keyspaces logs. \u2022 Monitor the logs to gain insights into the activities and operations performed on your Keyspace. \u2022 Use CloudWatch Logs features, such as log searching, filtering, and visualization, to analyze the logs and identify any security or operational issues. \u2022 Establish appropriate log monitoring and alerting mechanisms to proactively identify and respond to potential security incidents or operational anomalies.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Reduce the risk of any fraud or inconsistency within the database because only authorized user has access to it.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "To manage your enterprise caching solution, it is important that you know how your clusters are performing and the resources they are consuming. It is also important that you know the events that are being generated and the costs of your deployment. Amazon CloudWatch provides metrics for monitoring your cache performance. In addition, cost allocation tags help you monitor and manage costs.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to enable audit logging. \u2022 Click on the Keyspace name to access its details page. 4. Enable Amazon CloudWatch Logs \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Logging section, locate the CloudWatch Logs option. \u2022 Click on Edit. \u2022 Select the Enable option to enable logging for the Keyspace. \u2022 Choose an existing CloudWatch Logs log group or create a new one to store the logs generated by the Keyspace activities. \u2022 Click Save to enable CloudWatch Logs for the Keyspace.  5. Configure CloudWatch Logs \u2022 Open the CloudWatch console by navigating to CloudWatch in the AWS Management Console. \u2022 In the left-side menu, click on Logs. \u2022 Create a new log group or select an existing log group that will store the Keyspaces logs. \u2022 Configure log retention settings based on your retention requirements. Logs can be stored for a specific number of days or indefinitely. \u2022 Define any necessary log group permissions to control access to the logs. \u2022 Optionally, set up log exports or alarms for specific log events or patterns if needed. 6. Verify the Logging Status \u2022 Wait a few minutes for the changes to propagate and the logging configuration to take effect. \u2022 Refresh the Keyspace details page to see the updated logging status. \u2022 Verify that CloudWatch Logs is enabled for the Keyspace. 7. Monitor and Analyze Logs \u2022 Navigate to the CloudWatch console and select the log group that stores the Keyspaces logs. \u2022 Monitor the logs to gain insights into the activities and operations performed on your Keyspace. \u2022 Use CloudWatch Logs features, such as log searching, filtering, and visualization, to analyze the logs and identify any security or operational issues. \u2022 Establish appropriate log monitoring and alerting mechanisms to proactively identify and respond to potential security incidents or operational anomalies.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Reduce the risk of any fraud or inconsistency within the database because only authorized user has access to it.",
      "impact": "Reduce the risk of any fraud or inconsistency within the database because only authorized user has access to it."
    },
    "function_name": "elasticache_audit_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.logging.audit.audit_logging_enabled"
  },
  {
    "id": "5.10",
    "title": "Ensure Security Configurations are Reviewed Regularly",
    "assessment": "Manual",
    "description": "Regularly updating and reviewing the security configuration of your Amazon Keyspaces environment helps ensure that your database is protected against potential vulnerabilities and aligned with your security requirements.",
    "rationale": "Impact: If you are not updating these regularly, your database would most likely become susceptible to a vulnerable attack. Not updating your IAM permission, network, and encryption setting, and controlling audit logging, would lead to the attacker getting into the system which would result in data loss.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to update and review the security configuration. \u2022 Click on the Keyspace name to access its details page. 4. Review IAM Roles and Permissions \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, review the IAM roles and permissions associated with the Keyspace. \u2022 Ensure that the IAM roles have appropriate permissions and follow the principle of least privilege.  \u2022 Review the IAM policies and make any necessary updates to align with your security requirements. 5. Review Network Security \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Network & Security section, review the VPC, subnets, security groups, and network ACLs associated with the Keyspace. \u2022 Ensure that the VPC and subnet configurations align with your security requirements. \u2022 Review the security group rules and network ACL rules to ensure they restrict access to necessary ports, IP ranges, and protocols. \u2022 Make any necessary updates to tighten the network security settings. 6. Review Encryption Settings \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Encryption section, review the encryption settings for the Keyspace. \u2022 Ensure that encryption at rest and in transit are enabled and the appropriate encryption options are chosen. \u2022 Review any customer-managed keys used for encryption and verify their configurations. 7. Review Access Control \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, review the Access Control Lists (ACLs) for the Keyspace. \u2022 Ensure the ACLs define appropriate access permissions at the table and row levels. \u2022 Review the ACL rules and make any necessary updates to align with your security policies and access requirements. 8. Review Audit Logging \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Review the Keyspace's logging configuration under the Logging section. \u2022 Ensure the logs are captured and stored in CloudWatch Logs as expected. \u2022 Please review the log retention settings and y that they comply with your retention policies. 9. Regularly Monitor Security Bulletins \u2022 Stay updated with AWS security bulletins, advisories, and best practices. \u2022 Monitor AWS security announcements and subscribe to relevant security notifications.  \u2022 Regularly review and apply security patches, updates, and recommended configuration changes for Amazon Keyspaces.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If you are not updating these regularly, your database would most likely become susceptible to a vulnerable attack. Not updating your IAM permission, network, and encryption setting, and controlling audit logging, would lead to the attacker getting into the system which would result in data loss.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Regularly updating and reviewing the security configuration of your Amazon Keyspaces environment helps ensure that your database is protected against potential vulnerabilities and aligned with your security requirements.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to update and review the security configuration. \u2022 Click on the Keyspace name to access its details page. 4. Review IAM Roles and Permissions \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, review the IAM roles and permissions associated with the Keyspace. \u2022 Ensure that the IAM roles have appropriate permissions and follow the principle of least privilege.  \u2022 Review the IAM policies and make any necessary updates to align with your security requirements. 5. Review Network Security \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Network & Security section, review the VPC, subnets, security groups, and network ACLs associated with the Keyspace. \u2022 Ensure that the VPC and subnet configurations align with your security requirements. \u2022 Review the security group rules and network ACL rules to ensure they restrict access to necessary ports, IP ranges, and protocols. \u2022 Make any necessary updates to tighten the network security settings. 6. Review Encryption Settings \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Encryption section, review the encryption settings for the Keyspace. \u2022 Ensure that encryption at rest and in transit are enabled and the appropriate encryption options are chosen. \u2022 Review any customer-managed keys used for encryption and verify their configurations. 7. Review Access Control \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Authentication and access control section, review the Access Control Lists (ACLs) for the Keyspace. \u2022 Ensure the ACLs define appropriate access permissions at the table and row levels. \u2022 Review the ACL rules and make any necessary updates to align with your security policies and access requirements. 8. Review Audit Logging \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Review the Keyspace's logging configuration under the Logging section. \u2022 Ensure the logs are captured and stored in CloudWatch Logs as expected. \u2022 Please review the log retention settings and y that they comply with your retention policies. 9. Regularly Monitor Security Bulletins \u2022 Stay updated with AWS security bulletins, advisories, and best practices. \u2022 Monitor AWS security announcements and subscribe to relevant security notifications.  \u2022 Regularly review and apply security patches, updates, and recommended configuration changes for Amazon Keyspaces.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If you are not updating these regularly, your database would most likely become susceptible to a vulnerable attack. Not updating your IAM permission, network, and encryption setting, and controlling audit logging, would lead to the attacker getting into the system which would result in data loss.",
      "impact": "If you are not updating these regularly, your database would most likely become susceptible to a vulnerable attack. Not updating your IAM permission, network, and encryption setting, and controlling audit logging, would lead to the attacker getting into the system which would result in data loss."
    },
    "function_name": "elasticache_security_configuration_review_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.security.configuration.security_configuration_reviewed"
  },
  {
    "id": "6.1",
    "title": "Ensure Network Security is Enabled",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "1. Create or Select a Virtual Private Cloud (VPC) \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC where you want to deploy your Amazon MemoryDB clusters. 2. Configure Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the subnets within your VPC where you want to deploy your Amazon MemoryDB clusters. \u2022 Ensure you have private subnets to isolate your MemoryDB clusters from the public internet. 3. Define Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group or select an existing one for your Amazon MemoryDB clusters. \u2022 Configure inbound and outbound rules in the security group to control traffic access. o Allow inbound access only from trusted sources, such as specific IP ranges or security groups, on the necessary ports used by MemoryDB. o Define outbound rules based on your requirements, allowing outbound traffic to necessary destinations or ports. \u2022 Associate the security group with your Amazon MemoryDB clusters. 4. Configure Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu. \u2022 Create or select the network ACLs associated with the subnets used by your Amazon MemoryDB clusters. \u2022 Configure inbound and outbound rules in the network ACLs to control traffic access.  o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. o Deny unnecessary or unwanted traffic. \u2022 Associate the network ACLs with the subnets used by your Amazon MemoryDB clusters. 5. Configure VPC Endpoints \u2022 In the VPC console, navigate to Endpoints in the left-side menu. \u2022 Create or select the VPC endpoints required for Amazon MemoryDB. o If you need to access MemoryDB from within your VPC, create a VPC endpoint for Amazon MemoryDB to connect your applications securely. o If you need to access MemoryDB from another VPC or on-premises network, set up VPC peering or a transit gateway to establish a secure connection. 6. Verify Connectivity and Test \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your Amazon MemoryDB clusters or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the connectivity to your Amazon MemoryDB clusters by trying to connect to them using the appropriate client or utility. \u2022 Verify that the network security settings allow the necessary traffic and deny unauthorized access.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Create or Select a Virtual Private Cloud (VPC) \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC where you want to deploy your Amazon MemoryDB clusters. 2. Configure Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the subnets within your VPC where you want to deploy your Amazon MemoryDB clusters. \u2022 Ensure you have private subnets to isolate your MemoryDB clusters from the public internet. 3. Define Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group or select an existing one for your Amazon MemoryDB clusters. \u2022 Configure inbound and outbound rules in the security group to control traffic access. o Allow inbound access only from trusted sources, such as specific IP ranges or security groups, on the necessary ports used by MemoryDB. o Define outbound rules based on your requirements, allowing outbound traffic to necessary destinations or ports. \u2022 Associate the security group with your Amazon MemoryDB clusters. 4. Configure Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu. \u2022 Create or select the network ACLs associated with the subnets used by your Amazon MemoryDB clusters. \u2022 Configure inbound and outbound rules in the network ACLs to control traffic access.  o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. o Deny unnecessary or unwanted traffic. \u2022 Associate the network ACLs with the subnets used by your Amazon MemoryDB clusters. 5. Configure VPC Endpoints \u2022 In the VPC console, navigate to Endpoints in the left-side menu. \u2022 Create or select the VPC endpoints required for Amazon MemoryDB. o If you need to access MemoryDB from within your VPC, create a VPC endpoint for Amazon MemoryDB to connect your applications securely. o If you need to access MemoryDB from another VPC or on-premises network, set up VPC peering or a transit gateway to establish a secure connection. 6. Verify Connectivity and Test \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your Amazon MemoryDB clusters or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the connectivity to your Amazon MemoryDB clusters by trying to connect to them using the appropriate client or utility. \u2022 Verify that the network security settings allow the necessary traffic and deny unauthorized access.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "elasticache_redis_network_security_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.redis.network.security.network_security_enabled"
  },
  {
    "id": "6.2",
    "title": "Ensure Data at Rest and in Transit is Encrypted",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the MemoryDB cluster for which you want to enable encryption at rest and in transit. \u2022 Click on the cluster name to access its details page. 4. Enable Encryption at Rest \u2022 In the cluster details page, navigate to the Encryption at Rest section. \u2022 Click on Modify to edit the encryption settings. \u2022 Select the desired encryption option: o AWS Managed Key (Default): Choose this option to use the default AWS managed key for encryption at rest. Amazon MemoryDB automatically encrypts your data using this key. o Customer Managed Key (CMK): Choose this option if you want to use your own AWS Key Management Service (KMS) customer-managed key for encryption. Select the appropriate CMK from the dropdown menu. \u2022 Click \"Apply Changes\" to enable encryption at rest for the MemoryDB cluster. 5. Enable Encryption in Transit \u2022 In the cluster details page, navigate to the Encryption in Transit section. \u2022 Click on Modify to edit the encryption settings. \u2022 Select the desired encryption option:  o Encryption in Transit Enabled: Choose this option to enable encryption in transit for data transmitted between your client applications and MemoryDB. MemoryDB uses SSL/TLS encryption to secure the communication channel. o Encryption in Transit Disabled: Choose this option if you do not require encryption in transit. \u2022 Click Apply Changes to enable encryption in transit for the MemoryDB cluster. 6. Verify Encryption Status \u2022 Wait a few minutes for the changes to propagate and the encryption settings to take effect. \u2022 Refresh the cluster details page to see the updated encryption status. \u2022 Verify that encryption at rest and in transit are enabled for the MemoryDB cluster.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the MemoryDB cluster for which you want to enable encryption at rest and in transit. \u2022 Click on the cluster name to access its details page. 4. Enable Encryption at Rest \u2022 In the cluster details page, navigate to the Encryption at Rest section. \u2022 Click on Modify to edit the encryption settings. \u2022 Select the desired encryption option: o AWS Managed Key (Default): Choose this option to use the default AWS managed key for encryption at rest. Amazon MemoryDB automatically encrypts your data using this key. o Customer Managed Key (CMK): Choose this option if you want to use your own AWS Key Management Service (KMS) customer-managed key for encryption. Select the appropriate CMK from the dropdown menu. \u2022 Click \"Apply Changes\" to enable encryption at rest for the MemoryDB cluster. 5. Enable Encryption in Transit \u2022 In the cluster details page, navigate to the Encryption in Transit section. \u2022 Click on Modify to edit the encryption settings. \u2022 Select the desired encryption option:  o Encryption in Transit Enabled: Choose this option to enable encryption in transit for data transmitted between your client applications and MemoryDB. MemoryDB uses SSL/TLS encryption to secure the communication channel. o Encryption in Transit Disabled: Choose this option if you do not require encryption in transit. \u2022 Click Apply Changes to enable encryption in transit for the MemoryDB cluster. 6. Verify Encryption Status \u2022 Wait a few minutes for the changes to propagate and the encryption settings to take effect. \u2022 Refresh the cluster details page to see the updated encryption status. \u2022 Verify that encryption at rest and in transit are enabled for the MemoryDB cluster.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "elasticache_redis_encryption_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.redis.encryption.encryption_enabled"
  },
  {
    "id": "6.3",
    "title": "Ensure Authentication and Access Control is Enabled",
    "assessment": "Manual",
    "description": "",
    "rationale": "Users should select whether they like to enable authentication. If they want to authenticate a password would be required, which would only allow the authorized person to access the cluster. Defining access control allows specific workers in a business access to the database. Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the Amazon MemoryDB cluster on which you want to implement authentication and access control. \u2022 Click on the cluster name to access its details page. 4. Enable Authentication \u2022 In the cluster details page, navigate to the Authentication section. \u2022 Click on Modify to edit the authentication settings. \u2022 Select the desired authentication option: o No Authentication: This option allows unauthenticated access to your MemoryDB cluster.  o Password Authentication: Choose this option to enable password-based authentication. Enter the desired password for the cluster. \u2022 Click Apply Changes to enable authentication for the MemoryDB cluster. 5. Define Access Control Policies \u2022 In the cluster details page, navigate to the \"Access Control\" section. \u2022 Click on Modify to edit the access control settings. \u2022 Define the access control policies based on your requirements: o For Redis-based clusters, you can use Redis Access Control Lists (ACLs) to control access at the Redis command level. o Use the Redis commands to create, modify, or delete ACL rules as needed. o You can define rules based on IP addresses, users, or patterns to allow or deny specific commands or operations. \u2022 Click Apply Changes to save the access control policies for the MemoryDB cluster. 6. Test Authentication and Access Control \u2022 Use a Redis client or utility to connect to your Amazon MemoryDB cluster. \u2022 Provide the necessary authentication credentials, such as the password, if password-based authentication is enabled. \u2022 Test the connection and verify that you can access the MemoryDB cluster based on the defined access control policies. 7. Regularly Review and Update Access Control \u2022 Periodically review the access control policies to ensure they align with your security requirements. \u2022 Update the ACL rules, passwords, or other authentication mechanisms to adapt to changing access requirements or security policies.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the Amazon MemoryDB cluster on which you want to implement authentication and access control. \u2022 Click on the cluster name to access its details page. 4. Enable Authentication \u2022 In the cluster details page, navigate to the Authentication section. \u2022 Click on Modify to edit the authentication settings. \u2022 Select the desired authentication option: o No Authentication: This option allows unauthenticated access to your MemoryDB cluster.  o Password Authentication: Choose this option to enable password-based authentication. Enter the desired password for the cluster. \u2022 Click Apply Changes to enable authentication for the MemoryDB cluster. 5. Define Access Control Policies \u2022 In the cluster details page, navigate to the \"Access Control\" section. \u2022 Click on Modify to edit the access control settings. \u2022 Define the access control policies based on your requirements: o For Redis-based clusters, you can use Redis Access Control Lists (ACLs) to control access at the Redis command level. o Use the Redis commands to create, modify, or delete ACL rules as needed. o You can define rules based on IP addresses, users, or patterns to allow or deny specific commands or operations. \u2022 Click Apply Changes to save the access control policies for the MemoryDB cluster. 6. Test Authentication and Access Control \u2022 Use a Redis client or utility to connect to your Amazon MemoryDB cluster. \u2022 Provide the necessary authentication credentials, such as the password, if password-based authentication is enabled. \u2022 Test the connection and verify that you can access the MemoryDB cluster based on the defined access control policies. 7. Regularly Review and Update Access Control \u2022 Periodically review the access control policies to ensure they align with your security requirements. \u2022 Update the ACL rules, passwords, or other authentication mechanisms to adapt to changing access requirements or security policies.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Users should select whether they like to enable authentication. If they want to authenticate a password would be required, which would only allow the authorized person to access the cluster. Defining access control allows specific workers in a business access to the database. Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
      "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data."
    },
    "function_name": "elasticache_redis_authentication_access_control_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.redis.authentication.access_control.authentication_access_control_enabled"
  },
  {
    "id": "6.4",
    "title": "Ensure Audit Logging is Enabled",
    "assessment": "Manual",
    "description": "Enabling audit logging on Amazon MemoryDB allows you to capture and store logs of activities performed on your clusters.",
    "rationale": "It captures and saves logs of activities that took place in the cluster. Impact: Reduces risks of any fraud since worker activity is being monitored and tracked.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the MemoryDB cluster for which you want to enable audit logging. Click on the cluster name to access its details page. 4. Enable Amazon CloudWatch Logs \u2022 In the cluster details page, navigate to the Logging section. \u2022 Click on Modify to edit the logging settings. \u2022 Select the option to enable CloudWatch Logs. \u2022 Choose an existing CloudWatch log group or create a new one to store the logs generated by the MemoryDB cluster activities. \u2022 Optionally, you can specify a log retention period to define how long the logs will be stored. \u2022 Click Apply Changes to enable CloudWatch Logs for the MemoryDB cluster.   5. Configure CloudWatch Logs \u2022 Open the CloudWatch console by navigating to CloudWatch in the AWS Management Console. \u2022 In the left-side menu, click on Logs. \u2022 Create a new log group or select an existing log group that will store the MemoryDB logs. \u2022 Configure log retention settings based on your retention requirements. Logs can be stored for a specific number of days or indefinitely. \u2022 Define any necessary log group permissions to control access to the logs. \u2022 Optionally, set up log exports or alarms for specific log events or patterns if needed. 6. Verify Logging Status \u2022 Wait a few minutes for the changes to propagate and the logging configuration to take effect. \u2022 Refresh the cluster details page to see the updated logging status. \u2022 Verify that CloudWatch Logs is enabled for the MemoryDB cluster. 7. Monitor and Analyze Logs \u2022 Navigate to the CloudWatch console and select the log group that stores the MemoryDB logs. \u2022 Monitor the logs to gain insights into the activities and operations performed on your MemoryDB cluster. \u2022 Use CloudWatch Logs features, such as log searching, filtering, and visualization, to analyze the logs and identify any security or operational issues. \u2022 Establish appropriate log monitoring and alerting mechanisms to proactively identify and respond to potential security incidents or operational anomalies.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Reduces risks of any fraud since worker activity is being monitored and tracked.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enabling audit logging on Amazon MemoryDB allows you to capture and store logs of activities performed on your clusters.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the MemoryDB cluster for which you want to enable audit logging. Click on the cluster name to access its details page. 4. Enable Amazon CloudWatch Logs \u2022 In the cluster details page, navigate to the Logging section. \u2022 Click on Modify to edit the logging settings. \u2022 Select the option to enable CloudWatch Logs. \u2022 Choose an existing CloudWatch log group or create a new one to store the logs generated by the MemoryDB cluster activities. \u2022 Optionally, you can specify a log retention period to define how long the logs will be stored. \u2022 Click Apply Changes to enable CloudWatch Logs for the MemoryDB cluster.   5. Configure CloudWatch Logs \u2022 Open the CloudWatch console by navigating to CloudWatch in the AWS Management Console. \u2022 In the left-side menu, click on Logs. \u2022 Create a new log group or select an existing log group that will store the MemoryDB logs. \u2022 Configure log retention settings based on your retention requirements. Logs can be stored for a specific number of days or indefinitely. \u2022 Define any necessary log group permissions to control access to the logs. \u2022 Optionally, set up log exports or alarms for specific log events or patterns if needed. 6. Verify Logging Status \u2022 Wait a few minutes for the changes to propagate and the logging configuration to take effect. \u2022 Refresh the cluster details page to see the updated logging status. \u2022 Verify that CloudWatch Logs is enabled for the MemoryDB cluster. 7. Monitor and Analyze Logs \u2022 Navigate to the CloudWatch console and select the log group that stores the MemoryDB logs. \u2022 Monitor the logs to gain insights into the activities and operations performed on your MemoryDB cluster. \u2022 Use CloudWatch Logs features, such as log searching, filtering, and visualization, to analyze the logs and identify any security or operational issues. \u2022 Establish appropriate log monitoring and alerting mechanisms to proactively identify and respond to potential security incidents or operational anomalies.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "It captures and saves logs of activities that took place in the cluster. Impact: Reduces risks of any fraud since worker activity is being monitored and tracked.",
      "impact": "Reduces risks of any fraud since worker activity is being monitored and tracked."
    },
    "function_name": "elasticache_redis_audit_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.redis.logging.audit.audit_logging_enabled"
  },
  {
    "id": "6.5",
    "title": "Ensure Security Configurations are Reviewed Regularly",
    "assessment": "Manual",
    "description": "This helps by removing or updating any IAM roles, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
    "rationale": "Impact: By regularly checking these settings in the database the user is preventing the database from a cyber threat.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the Amazon MemoryDB cluster for which you want to update and review the security configuration. \u2022 Click on the cluster name to access its details page. 4. Review IAM Roles and Permissions \u2022 In the cluster details page, navigate to the Security or Access Control section. \u2022 Review the IAM roles and permissions associated with the cluster. \u2022 Ensure that the IAM roles have appropriate permissions and follow the principle of least privilege. \u2022 Review the IAM policies and make any necessary updates to align with your security requirements.  5. Review Network Security \u2022 In the cluster details page, navigate to the Security or Network & Security section. \u2022 Review the Virtual Private Cloud (VPC), subnets, security groups, and network ACLs associated with the cluster. \u2022 Ensure that the VPC and subnet configurations align with your security requirements. \u2022 Review the security group rules and network ACL rules to ensure they restrict access to necessary ports, IP ranges, and protocols. \u2022 Make any necessary updates to tighten the network security settings. 6. Review Encryption Settings \u2022 In the cluster details page, navigate to the Security or Encryption section. \u2022 Review the encryption settings for the cluster. \u2022 Ensure that encryption at rest and in transit are enabled and the appropriate encryption options are chosen. \u2022 Review any customer-managed keys used for encryption and verify their configurations. 7. Review Authentication and Access Control \u2022 In the cluster details page, navigate to the Security or Access Control section. \u2022 Review the authentication options and access control policies in place for the cluster. \u2022 Ensure that the authentication mechanisms and access control policies align with your security requirements. \u2022 Make any necessary updates to adapt to changing access requirements or security policies. 8. Review Audit Logging \u2022 In the cluster details page, navigate to the Monitoring or Logging section. \u2022 Review the logging configuration for the cluster. \u2022 Ensure that the logs are captured and stored as expected. \u2022 Please review the log retention settings and verify that they comply with your retention policies. 9. Regularly Monitor Security Bulletins \u2022 Stay updated with AWS security bulletins, advisories, and best practices. \u2022 Monitor AWS security announcements and subscribe to relevant security notifications. \u2022 Regularly review and apply security patches, updates, and recommended configuration changes for Amazon MemoryDB.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "By regularly checking these settings in the database the user is preventing the database from a cyber threat.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps by removing or updating any IAM roles, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the Amazon MemoryDB cluster for which you want to update and review the security configuration. \u2022 Click on the cluster name to access its details page. 4. Review IAM Roles and Permissions \u2022 In the cluster details page, navigate to the Security or Access Control section. \u2022 Review the IAM roles and permissions associated with the cluster. \u2022 Ensure that the IAM roles have appropriate permissions and follow the principle of least privilege. \u2022 Review the IAM policies and make any necessary updates to align with your security requirements.  5. Review Network Security \u2022 In the cluster details page, navigate to the Security or Network & Security section. \u2022 Review the Virtual Private Cloud (VPC), subnets, security groups, and network ACLs associated with the cluster. \u2022 Ensure that the VPC and subnet configurations align with your security requirements. \u2022 Review the security group rules and network ACL rules to ensure they restrict access to necessary ports, IP ranges, and protocols. \u2022 Make any necessary updates to tighten the network security settings. 6. Review Encryption Settings \u2022 In the cluster details page, navigate to the Security or Encryption section. \u2022 Review the encryption settings for the cluster. \u2022 Ensure that encryption at rest and in transit are enabled and the appropriate encryption options are chosen. \u2022 Review any customer-managed keys used for encryption and verify their configurations. 7. Review Authentication and Access Control \u2022 In the cluster details page, navigate to the Security or Access Control section. \u2022 Review the authentication options and access control policies in place for the cluster. \u2022 Ensure that the authentication mechanisms and access control policies align with your security requirements. \u2022 Make any necessary updates to adapt to changing access requirements or security policies. 8. Review Audit Logging \u2022 In the cluster details page, navigate to the Monitoring or Logging section. \u2022 Review the logging configuration for the cluster. \u2022 Ensure that the logs are captured and stored as expected. \u2022 Please review the log retention settings and verify that they comply with your retention policies. 9. Regularly Monitor Security Bulletins \u2022 Stay updated with AWS security bulletins, advisories, and best practices. \u2022 Monitor AWS security announcements and subscribe to relevant security notifications. \u2022 Regularly review and apply security patches, updates, and recommended configuration changes for Amazon MemoryDB.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: By regularly checking these settings in the database the user is preventing the database from a cyber threat.",
      "impact": "By regularly checking these settings in the database the user is preventing the database from a cyber threat."
    },
    "function_name": "elasticache_redis_security_configuration_review_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.redis.security.configuration.security_configuration_reviewed"
  },
  {
    "id": "6.6",
    "title": "Ensure Monitoring and Alerting is Enabled",
    "assessment": "Manual",
    "description": "Implementing monitoring and alerting on Amazon MemoryDB allows you to proactively detect and respond to any performance issues, security events, or operational anomalies.",
    "rationale": "This helps in ensuring that everything in the system is secure and if there is an unusual activity that takes place it addresses the issues quickly and efficiently. Impact: Enabling monitoring and alerting has a positive impact in the business operations when the issue is identified and addressed accordingly.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the Amazon MemoryDB cluster for which you want to implement monitoring and alerting. Click on the cluster name to access its details page. 4. Enable Amazon CloudWatch \u2022 In the cluster details page, navigate to the Monitoring or CloudWatch section. \u2022 Click on Enable to enable CloudWatch monitoring for the cluster. \u2022 Select the appropriate CloudWatch metric categories to monitor, such as CPU utilization, memory utilization, network traffic, and storage capacity. \u2022 Configure the desired granularity and period for metric collection. \u2022 Click Enable or Save to enable CloudWatch monitoring for the cluster.  5. Set Up CloudWatch Alarms \u2022 In the CloudWatch console, navigate to Alarms in the left-side menu. \u2022 Click on Create Alarm to set up a new alarm. \u2022 Select the CloudWatch metric related to the aspect you want to monitor, such as CPU utilization or memory utilization. \u2022 Configure the alarm threshold based on your desired criteria, such as setting CPU utilization above a certain percentage. \u2022 Define the actions to be taken when the alarm is triggered. \u2022 Click Create Alarm to create the CloudWatch alarm. 6. Configure Amazon EventBridge Rules (Optional) \u2022 In the Amazon EventBridge console, navigate to Rules in the left-side menu. \u2022 Click on Create rule to set up a new rule. \u2022 Define the event pattern or source that should trigger the rule, such as specific MemoryDB events or errors. \u2022 Configure the target actions, such as sending notifications, executing AWS Lambda functions, or invoking AWS Step Functions. \u2022 Click Create to create the EventBridge rule. 7. Configure Auto Scaling (Optional) \u2022 In the MemoryDB cluster details page, navigate to the Auto Scaling section. \u2022 Configure auto-scaling settings based on your workload and performance requirements. \u2022 Define the scaling policies, such as increasing or decreasing the number of replica nodes based on CPU utilization or other metrics. \u2022 Set the desired minimum and maximum number of replica nodes for the cluster. \u2022 Click Save or Apply Changes to apply the auto-scaling configuration. 8. Regularly Review and Adjust Monitoring and Alarms \u2022 Periodically review the CloudWatch metrics and alarms to ensure they align with your monitoring needs and performance expectations. \u2022 Adjust the thresholds and actions based on changing workload patterns or performance requirements. \u2022 Stay informed about new CloudWatch features and best practices to optimize your monitoring setup.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Enabling monitoring and alerting has a positive impact in the business operations when the issue is identified and addressed accordingly.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Implementing monitoring and alerting on Amazon MemoryDB allows you to proactively detect and respond to any performance issues, security events, or operational anomalies.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon MemoryDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/memorydb/. 3. Select the Cluster \u2022 Choose the Amazon MemoryDB cluster for which you want to implement monitoring and alerting. Click on the cluster name to access its details page. 4. Enable Amazon CloudWatch \u2022 In the cluster details page, navigate to the Monitoring or CloudWatch section. \u2022 Click on Enable to enable CloudWatch monitoring for the cluster. \u2022 Select the appropriate CloudWatch metric categories to monitor, such as CPU utilization, memory utilization, network traffic, and storage capacity. \u2022 Configure the desired granularity and period for metric collection. \u2022 Click Enable or Save to enable CloudWatch monitoring for the cluster.  5. Set Up CloudWatch Alarms \u2022 In the CloudWatch console, navigate to Alarms in the left-side menu. \u2022 Click on Create Alarm to set up a new alarm. \u2022 Select the CloudWatch metric related to the aspect you want to monitor, such as CPU utilization or memory utilization. \u2022 Configure the alarm threshold based on your desired criteria, such as setting CPU utilization above a certain percentage. \u2022 Define the actions to be taken when the alarm is triggered. \u2022 Click Create Alarm to create the CloudWatch alarm. 6. Configure Amazon EventBridge Rules (Optional) \u2022 In the Amazon EventBridge console, navigate to Rules in the left-side menu. \u2022 Click on Create rule to set up a new rule. \u2022 Define the event pattern or source that should trigger the rule, such as specific MemoryDB events or errors. \u2022 Configure the target actions, such as sending notifications, executing AWS Lambda functions, or invoking AWS Step Functions. \u2022 Click Create to create the EventBridge rule. 7. Configure Auto Scaling (Optional) \u2022 In the MemoryDB cluster details page, navigate to the Auto Scaling section. \u2022 Configure auto-scaling settings based on your workload and performance requirements. \u2022 Define the scaling policies, such as increasing or decreasing the number of replica nodes based on CPU utilization or other metrics. \u2022 Set the desired minimum and maximum number of replica nodes for the cluster. \u2022 Click Save or Apply Changes to apply the auto-scaling configuration. 8. Regularly Review and Adjust Monitoring and Alarms \u2022 Periodically review the CloudWatch metrics and alarms to ensure they align with your monitoring needs and performance expectations. \u2022 Adjust the thresholds and actions based on changing workload patterns or performance requirements. \u2022 Stay informed about new CloudWatch features and best practices to optimize your monitoring setup.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps in ensuring that everything in the system is secure and if there is an unusual activity that takes place it addresses the issues quickly and efficiently. Impact: Enabling monitoring and alerting has a positive impact in the business operations when the issue is identified and addressed accordingly.",
      "impact": "Enabling monitoring and alerting has a positive impact in the business operations when the issue is identified and addressed accordingly."
    },
    "function_name": "elasticache_redis_monitoring_alerting_enabled_check",
    "coverage": 90,
    "rule_id": "aws.elasticache.redis.monitoring.alerting.monitoring_alerting_enabled"
  },
  {
    "id": "7.1",
    "title": "Ensure Network Architecture Planning",
    "assessment": "Manual",
    "description": "Plan the network architecture to isolate your DocumentDB instances within a secure Virtual Private Cloud (VPC). Configure appropriate security groups and network access control lists (ACLs) to control inbound and outbound traffic to your DocumentDB instances.",
    "rationale": "Depending on how the network is established between devices, which then helps secure data when transferring it from one server to another. Impact: The way the users design their network sets the performance for the system and how it would interact with servers.",
    "audit": "1. Understand Amazon VPC Basics \u2022 Familiarize yourself with Amazon Virtual Private Cloud (VPC) and its concepts. \u2022 Learn about VPC components, including subnets, route tables, and security groups. 2. Determine VPC Requirements for DocumentDB \u2022 Identify the specific networking requirements for your Amazon DocumentDB deployment. \u2022 Consider factors such as network availability, fault tolerance, and security requirements. 3. Create a New VPC or Use an Existing VPC \u2022 Decide whether to create a new VPC dedicated to Amazon DocumentDB or use an existing VPC. \u2022 If creating a new VPC, follow the steps to create a VPC in the AWS Management Console. 4. Configure Subnets  \u2022 Determine the number and size of subnets needed for your DocumentDB deployment. \u2022 Create the required subnets within your VPC, ensuring proper availability zone distribution. 5. Set Up Routing \u2022 Configure the route tables associated with your subnets. \u2022 Ensure that the route tables have the necessary routes for proper network connectivity. 6. Configure Security Groups \u2022 Create or configure security groups to control inbound and outbound traffic to your DocumentDB instances. \u2022 Define the necessary inbound rules to allow access from authorized sources. 7. Plan Connectivity Options \u2022 Decide how your DocumentDB instances will connect to your VPC and other resources. \u2022 Determine if you need to set up VPC peering, VPN connections, or AWS Direct Connect for connectivity. 8. Consider High Availability and Fault Tolerance \u2022 Evaluate your requirements for high availability and fault tolerance. \u2022 Design your network architecture to ensure that DocumentDB instances are deployed across multiple availability zones for resilience. 9. Implement Network Access Control \u2022 Consider using network access control lists (ACLs) to provide an additional layer of security. \u2022 Configure the ACLs to allow only necessary traffic and block unauthorized access. 10. Test and Validate the Network Architecture \u2022 Ensure that your network architecture is correctly configured and meets your requirements. \u2022 Test connectivity and verify that DocumentDB instances can be accessed securely.",
    "remediation": "To establish connection, the users would need to factor in their virtual private cloud (VPC), create subnet, configure routing, and implement ACLs. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "The way the users design their network sets the performance for the system and how it would interact with servers.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Plan the network architecture to isolate your DocumentDB instances within a secure Virtual Private Cloud (VPC). Configure appropriate security groups and network access control lists (ACLs) to control inbound and outbound traffic to your DocumentDB instances.",
      "audit_steps": "1. Understand Amazon VPC Basics \u2022 Familiarize yourself with Amazon Virtual Private Cloud (VPC) and its concepts. \u2022 Learn about VPC components, including subnets, route tables, and security groups. 2. Determine VPC Requirements for DocumentDB \u2022 Identify the specific networking requirements for your Amazon DocumentDB deployment. \u2022 Consider factors such as network availability, fault tolerance, and security requirements. 3. Create a New VPC or Use an Existing VPC \u2022 Decide whether to create a new VPC dedicated to Amazon DocumentDB or use an existing VPC. \u2022 If creating a new VPC, follow the steps to create a VPC in the AWS Management Console. 4. Configure Subnets  \u2022 Determine the number and size of subnets needed for your DocumentDB deployment. \u2022 Create the required subnets within your VPC, ensuring proper availability zone distribution. 5. Set Up Routing \u2022 Configure the route tables associated with your subnets. \u2022 Ensure that the route tables have the necessary routes for proper network connectivity. 6. Configure Security Groups \u2022 Create or configure security groups to control inbound and outbound traffic to your DocumentDB instances. \u2022 Define the necessary inbound rules to allow access from authorized sources. 7. Plan Connectivity Options \u2022 Decide how your DocumentDB instances will connect to your VPC and other resources. \u2022 Determine if you need to set up VPC peering, VPN connections, or AWS Direct Connect for connectivity. 8. Consider High Availability and Fault Tolerance \u2022 Evaluate your requirements for high availability and fault tolerance. \u2022 Design your network architecture to ensure that DocumentDB instances are deployed across multiple availability zones for resilience. 9. Implement Network Access Control \u2022 Consider using network access control lists (ACLs) to provide an additional layer of security. \u2022 Configure the ACLs to allow only necessary traffic and block unauthorized access. 10. Test and Validate the Network Architecture \u2022 Ensure that your network architecture is correctly configured and meets your requirements. \u2022 Test connectivity and verify that DocumentDB instances can be accessed securely.",
      "remediation_steps": "To establish connection, the users would need to factor in their virtual private cloud (VPC), create subnet, configure routing, and implement ACLs. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Depending on how the network is established between devices, which then helps secure data when transferring it from one server to another. Impact: The way the users design their network sets the performance for the system and how it would interact with servers.",
      "impact": "The way the users design their network sets the performance for the system and how it would interact with servers."
    },
    "function_name": "rds_aurora_network_architecture_planning_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.network.architecture.network_architecture_planning_enabled"
  },
  {
    "id": "7.2",
    "title": "Ensure VPC Security is Configured",
    "assessment": "Manual",
    "description": "Creating a VPC, configuring subnets, and creating security groups help isolate your DocumentDB instances within your virtual network and control inbound and outbound traffic.",
    "rationale": "Setting up a Virtual Private Cloud (VPC) protects the private network that has been established from any external networks from interfering. It allows internal networks to communicate with one another with the network that has been established. Impact: Builds a strong connection between internal networks, has a strong connection with the internet, and it secures your data from getting into the hands of an unauthorized party.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon VPC Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/vpc/. 3. Create a VPC (Virtual Private Cloud) \u2022 Click on the Create VPC button to create a new VPC. \u2022 Provide the necessary details, such as VPC name, CIDR block, and additional configuration options. \u2022 Click on Create to create the VPC. 4. Configure VPC Subnets \u2022 Once the VPC is created, navigate to the Subnets section in the VPC console. \u2022 Click on the Create subnet button to create a new subnet. \u2022 Provide the necessary details, such as subnet name, VPC selection, and subnet CIDR block.  \u2022 Repeat this step to create multiple subnets within your VPC, if required. 5. Create Security Groups \u2022 Navigate to the Security Groups section in the VPC console. \u2022 Click the Create security group button to create a new security group. \u2022 Provide a name and description for the security group. \u2022 Configure inbound and outbound rules to allow the necessary traffic to and from the DocumentDB instances. \u2022 Repeat this step to create additional security groups if needed. 6. Launch Amazon DocumentDB Cluster in VPC \u2022 Navigate to the service using the \"Find Services\" search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. \u2022 Click on Create database to create a new DocumentDB cluster. \u2022 Configure the necessary parameters, such as cluster name, instance specifications, and storage options. \u2022 In the Network & Security section, select the VPC and subnets you created earlier. \u2022 Choose the appropriate security group(s) to apply to the DocumentDB cluster. \u2022 Click Create to launch the DocumentDB cluster in the configured VPC. 7. Test Connectivity \u2022 Once the DocumentDB cluster is launched, validate that you can connect to it from authorized sources. \u2022 Use the appropriate database client or tools to establish a connection and verify connectivity. 8. Monitor and Update Security Groups \u2022 Regularly monitor and update the security groups associated with the DocumentDB cluster. \u2022 Modify the inbound and outbound rules to ensure appropriate access control and security.",
    "remediation": "The individual is required to create a subnet and configure their inbound and outbound access. Individuals are supposed to configure and route, ensuring the traffic is flowing smoothly without any interference. This control is important because it only allows authorized users to access their resources as they prefer. References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Builds a strong connection between internal networks, has a strong connection with the internet, and it secures your data from getting into the hands of an unauthorized party.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Creating a VPC, configuring subnets, and creating security groups help isolate your DocumentDB instances within your virtual network and control inbound and outbound traffic.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon VPC Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/vpc/. 3. Create a VPC (Virtual Private Cloud) \u2022 Click on the Create VPC button to create a new VPC. \u2022 Provide the necessary details, such as VPC name, CIDR block, and additional configuration options. \u2022 Click on Create to create the VPC. 4. Configure VPC Subnets \u2022 Once the VPC is created, navigate to the Subnets section in the VPC console. \u2022 Click on the Create subnet button to create a new subnet. \u2022 Provide the necessary details, such as subnet name, VPC selection, and subnet CIDR block.  \u2022 Repeat this step to create multiple subnets within your VPC, if required. 5. Create Security Groups \u2022 Navigate to the Security Groups section in the VPC console. \u2022 Click the Create security group button to create a new security group. \u2022 Provide a name and description for the security group. \u2022 Configure inbound and outbound rules to allow the necessary traffic to and from the DocumentDB instances. \u2022 Repeat this step to create additional security groups if needed. 6. Launch Amazon DocumentDB Cluster in VPC \u2022 Navigate to the service using the \"Find Services\" search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. \u2022 Click on Create database to create a new DocumentDB cluster. \u2022 Configure the necessary parameters, such as cluster name, instance specifications, and storage options. \u2022 In the Network & Security section, select the VPC and subnets you created earlier. \u2022 Choose the appropriate security group(s) to apply to the DocumentDB cluster. \u2022 Click Create to launch the DocumentDB cluster in the configured VPC. 7. Test Connectivity \u2022 Once the DocumentDB cluster is launched, validate that you can connect to it from authorized sources. \u2022 Use the appropriate database client or tools to establish a connection and verify connectivity. 8. Monitor and Update Security Groups \u2022 Regularly monitor and update the security groups associated with the DocumentDB cluster. \u2022 Modify the inbound and outbound rules to ensure appropriate access control and security.",
      "remediation_steps": "The individual is required to create a subnet and configure their inbound and outbound access. Individuals are supposed to configure and route, ensuring the traffic is flowing smoothly without any interference. This control is important because it only allows authorized users to access their resources as they prefer. References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Setting up a Virtual Private Cloud (VPC) protects the private network that has been established from any external networks from interfering. It allows internal networks to communicate with one another with the network that has been established. Impact: Builds a strong connection between internal networks, has a strong connection with the internet, and it secures your data from getting into the hands of an unauthorized party.",
      "impact": "Builds a strong connection between internal networks, has a strong connection with the internet, and it secures your data from getting into the hands of an unauthorized party."
    },
    "function_name": "rds_aurora_vpc_security_configured_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.network.vpc.vpc_security_configured"
  },
  {
    "id": "7.3",
    "title": "Ensure Encryption at Rest is Enabled",
    "assessment": "Manual",
    "description": "",
    "rationale": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest. Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to enable encryption at rest. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Encryption at Rest \u2022 Under the Storage section. \u2022 Click on the \"Edit\" button or \"Modify\" option to configure the encryption settings. \u2022 Choose the option to enable encryption at rest for the cluster. 5. Choose the Encryption Key \u2022 Select the AWS Key Management Service (KMS) key that you want to use for encrypting your DocumentDB data.  \u2022 You can choose an existing KMS key or create a new one. \u2022 Ensure that the KMS key you select has appropriate permissions for DocumentDB to use it. 6. Save the Configuration \u2022 Click the Save button to apply the encryption at rest configuration. \u2022 DocumentDB will start the process of encrypting the existing data and all new data written to the cluster. 7. Verify Encryption Status \u2022 Monitor the cluster status to ensure that the encryption process is completed successfully. \u2022 Once the encryption is enabled, the cluster status will reflect the updated encryption status. 8. Test Connectivity \u2022 Validate that you can still connect to the DocumentDB cluster after enabling encryption at rest. \u2022 Ensure that your applications and authorized users can access the encrypted data. 9. Monitor and Manage Encryption \u2022 Regularly monitor the encryption status of your DocumentDB cluster. \u2022 Ensure that the encryption remains enabled and that no unauthorized modifications are made.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to enable encryption at rest. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Encryption at Rest \u2022 Under the Storage section. \u2022 Click on the \"Edit\" button or \"Modify\" option to configure the encryption settings. \u2022 Choose the option to enable encryption at rest for the cluster. 5. Choose the Encryption Key \u2022 Select the AWS Key Management Service (KMS) key that you want to use for encrypting your DocumentDB data.  \u2022 You can choose an existing KMS key or create a new one. \u2022 Ensure that the KMS key you select has appropriate permissions for DocumentDB to use it. 6. Save the Configuration \u2022 Click the Save button to apply the encryption at rest configuration. \u2022 DocumentDB will start the process of encrypting the existing data and all new data written to the cluster. 7. Verify Encryption Status \u2022 Monitor the cluster status to ensure that the encryption process is completed successfully. \u2022 Once the encryption is enabled, the cluster status will reflect the updated encryption status. 8. Test Connectivity \u2022 Validate that you can still connect to the DocumentDB cluster after enabling encryption at rest. \u2022 Ensure that your applications and authorized users can access the encrypted data. 9. Monitor and Manage Encryption \u2022 Regularly monitor the encryption status of your DocumentDB cluster. \u2022 Ensure that the encryption remains enabled and that no unauthorized modifications are made.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest. Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
      "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext."
    },
    "function_name": "rds_aurora_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.encryption.at_rest.encryption_at_rest_enabled"
  },
  {
    "id": "7.4",
    "title": "Ensure Encryption in Transit is Enabled",
    "assessment": "Manual",
    "description": "",
    "rationale": "Amazon Database DB uses SSL/TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by TLS to configure it correctly.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to enable encryption in transit. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Encryption in Transit \u2022 Under the Network & Security section. \u2022 Click on the Edit button or Modify option to configure the encryption settings. \u2022 Enable the option for encryption in transit by choosing the appropriate setting. \u2022 Note that encryption in transit uses SSL/TLS to secure communications between your applications and the DocumentDB cluster. 5. Save the Configuration \u2022 Click on the \"Save\" button to apply the encryption in transit configuration. \u2022 DocumentDB will automatically handle the SSL/TLS encryption for network traffic between clients and the cluster.  6. Validate Encryption in Transit \u2022 Test the connectivity to your DocumentDB cluster from your applications or clients. \u2022 Ensure that the communication is established securely using SSL/TLS encryption. 7. Monitor and Maintain Encryption in Transit \u2022 Regularly monitor the encryption in transit configuration for your DocumentDB cluster. \u2022 Stay informed about updates or changes in SSL/TLS protocols and encryption standards. \u2022 Keep your client applications current to ensure they support the latest encryption protocols.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to enable encryption in transit. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Encryption in Transit \u2022 Under the Network & Security section. \u2022 Click on the Edit button or Modify option to configure the encryption settings. \u2022 Enable the option for encryption in transit by choosing the appropriate setting. \u2022 Note that encryption in transit uses SSL/TLS to secure communications between your applications and the DocumentDB cluster. 5. Save the Configuration \u2022 Click on the \"Save\" button to apply the encryption in transit configuration. \u2022 DocumentDB will automatically handle the SSL/TLS encryption for network traffic between clients and the cluster.  6. Validate Encryption in Transit \u2022 Test the connectivity to your DocumentDB cluster from your applications or clients. \u2022 Ensure that the communication is established securely using SSL/TLS encryption. 7. Monitor and Maintain Encryption in Transit \u2022 Regularly monitor the encryption in transit configuration for your DocumentDB cluster. \u2022 Stay informed about updates or changes in SSL/TLS protocols and encryption standards. \u2022 Keep your client applications current to ensure they support the latest encryption protocols.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Amazon Database DB uses SSL/TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by TLS to configure it correctly.",
      "impact": ""
    },
    "function_name": "rds_aurora_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.encryption.in_transit.encryption_in_transit_enabled"
  },
  {
    "id": "7.5",
    "title": "Ensure to Implement Access Control and Authentication",
    "assessment": "Manual",
    "description": "Configure authentication mechanisms for your DocumentDB instances, such as using AWS Identity and Access Management (IAM) users or database users. Define appropriate user roles and permissions to control access to the DocumentDB instances and databases.",
    "rationale": "",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to implement access control and authentication. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Authentication \u2022 Under the Network & Security section. \u2022 Click on the Edit button or Modify option to configure the authentication settings. \u2022 Enable the option for authentication by choosing the appropriate setting. \u2022 DocumentDB supports authentication through username and password or through AWS Identity and Access Management (IAM) roles. 5. Configure Database Users \u2022 In the cluster details page, navigate to the Users or Database users section. \u2022 Click the Add user button to create a new database user.  \u2022 Enter the username and password for the database user. \u2022 Assign appropriate permissions to the user, such as read-only or read-write access to specific databases or collections. 6. Save the Configuration \u2022 Click on the Save button to apply the authentication and access control configuration. \u2022 DocumentDB will enforce authentication for connections to the cluster. 7. Test Authentication \u2022 Validate that your client applications or tools can connect to the DocumentDB cluster using the configured authentication credentials. \u2022 Ensure that the authentication process is successfully completed. 8. Monitor and Manage Access Control \u2022 Regularly monitor and manage the access control configuration for your DocumentDB cluster. \u2022 Review and update the permissions assigned to database users as needed. \u2022 Remove any unnecessary or unused database users to minimize security risks. 9. Consider IAM Authentication (Optional) \u2022 If desired, you can also configure IAM authentication for your DocumentDB cluster. \u2022 Follow the AWS documentation to set up IAM authentication for DocumentDB, if applicable.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Configure authentication mechanisms for your DocumentDB instances, such as using AWS Identity and Access Management (IAM) users or database users. Define appropriate user roles and permissions to control access to the DocumentDB instances and databases.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to implement access control and authentication. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Authentication \u2022 Under the Network & Security section. \u2022 Click on the Edit button or Modify option to configure the authentication settings. \u2022 Enable the option for authentication by choosing the appropriate setting. \u2022 DocumentDB supports authentication through username and password or through AWS Identity and Access Management (IAM) roles. 5. Configure Database Users \u2022 In the cluster details page, navigate to the Users or Database users section. \u2022 Click the Add user button to create a new database user.  \u2022 Enter the username and password for the database user. \u2022 Assign appropriate permissions to the user, such as read-only or read-write access to specific databases or collections. 6. Save the Configuration \u2022 Click on the Save button to apply the authentication and access control configuration. \u2022 DocumentDB will enforce authentication for connections to the cluster. 7. Test Authentication \u2022 Validate that your client applications or tools can connect to the DocumentDB cluster using the configured authentication credentials. \u2022 Ensure that the authentication process is successfully completed. 8. Monitor and Manage Access Control \u2022 Regularly monitor and manage the access control configuration for your DocumentDB cluster. \u2022 Review and update the permissions assigned to database users as needed. \u2022 Remove any unnecessary or unused database users to minimize security risks. 9. Consider IAM Authentication (Optional) \u2022 If desired, you can also configure IAM authentication for your DocumentDB cluster. \u2022 Follow the AWS documentation to set up IAM authentication for DocumentDB, if applicable.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "rds_aurora_access_control_authentication_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.authentication.access_control.access_control_authentication_implemented"
  },
  {
    "id": "7.6",
    "title": "Ensure Audit Logging is Enabled",
    "assessment": "Manual",
    "description": "Enable audit logging to capture database activities, including login attempts, queries, and modifications. Send the logs to Amazon CloudWatch or a centralized log management system for analysis and monitoring.",
    "rationale": "It captures and saves logs of activities that took place in the cluster, by recording login attempts, queries, and any changes within the database.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to enable audit logging. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Audit Logging \u2022 Under the Database options or Database features section. \u2022 Click on the Edit button or Modify option to configure the audit logging settings. \u2022 Enable the option for audit logging by choosing the appropriate setting. \u2022 Specify the destination for the audit logs, which can be an Amazon CloudWatch Logs group or an Amazon S3 bucket. 5. Configure Audit Log Destination \u2022 If you choose to send audit logs to an Amazon CloudWatch Logs group, select the existing group or create a new one.  \u2022 If you choose to send audit logs to an Amazon S3 bucket, select the existing bucket or create a new one. Provide the necessary permissions for DocumentDB to write logs to the bucket. 6. Set Audit Log Retention Period \u2022 Specify the retention period for the audit logs, indicating how long the logs should be retained in the selected destination. \u2022 Consider your compliance and regulatory requirements when determining the retention period. 7. Save the Configuration Click on the Save button to apply the audit logging configuration. DocumentDB will start recording audit logs according to the configured settings. 8. Validate Audit Logging \u2022 Perform operations on your DocumentDB cluster to generate audit log events. \u2022 Verify that the audit logs are recorded and sent to the specified destination. \u2022 Review the logs to ensure they contain the expected information and events. 9. Monitor and Analyze Audit Logs \u2022 Use Amazon CloudWatch Logs or other log analysis tools to monitor and analyze the audit logs generated by DocumentDB. \u2022 Set up log metrics, alarms, and notifications to detect unusual activities or security incidents. \u2022 Review audit logs regularly to identify potential security threats, compliance violations, or unauthorized access attempts.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enable audit logging to capture database activities, including login attempts, queries, and modifications. Send the logs to Amazon CloudWatch or a centralized log management system for analysis and monitoring.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to enable audit logging. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Configuration\" section. 4. Enable Audit Logging \u2022 Under the Database options or Database features section. \u2022 Click on the Edit button or Modify option to configure the audit logging settings. \u2022 Enable the option for audit logging by choosing the appropriate setting. \u2022 Specify the destination for the audit logs, which can be an Amazon CloudWatch Logs group or an Amazon S3 bucket. 5. Configure Audit Log Destination \u2022 If you choose to send audit logs to an Amazon CloudWatch Logs group, select the existing group or create a new one.  \u2022 If you choose to send audit logs to an Amazon S3 bucket, select the existing bucket or create a new one. Provide the necessary permissions for DocumentDB to write logs to the bucket. 6. Set Audit Log Retention Period \u2022 Specify the retention period for the audit logs, indicating how long the logs should be retained in the selected destination. \u2022 Consider your compliance and regulatory requirements when determining the retention period. 7. Save the Configuration Click on the Save button to apply the audit logging configuration. DocumentDB will start recording audit logs according to the configured settings. 8. Validate Audit Logging \u2022 Perform operations on your DocumentDB cluster to generate audit log events. \u2022 Verify that the audit logs are recorded and sent to the specified destination. \u2022 Review the logs to ensure they contain the expected information and events. 9. Monitor and Analyze Audit Logs \u2022 Use Amazon CloudWatch Logs or other log analysis tools to monitor and analyze the audit logs generated by DocumentDB. \u2022 Set up log metrics, alarms, and notifications to detect unusual activities or security incidents. \u2022 Review audit logs regularly to identify potential security threats, compliance violations, or unauthorized access attempts.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "It captures and saves logs of activities that took place in the cluster, by recording login attempts, queries, and any changes within the database.",
      "impact": ""
    },
    "function_name": "rds_aurora_audit_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.logging.audit.audit_logging_enabled"
  },
  {
    "id": "7.7",
    "title": "Ensure Regular Updates and Patches",
    "assessment": "Manual",
    "description": "Stay informed about the latest security updates and patches released by Amazon for DocumentDB. Regularly apply updates and patches to your DocumentDB instances to protect against known vulnerabilities.",
    "rationale": "Impact: Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up vulnerabilities that could have led to potential cyber-attack.",
    "audit": "1. Stay Informed \u2022 Stay updated with Amazon DocumentDB announcements, release notes, and security bulletins. \u2022 Subscribe to AWS newsletters, forums, and notifications to receive timely updates regarding updates and patches. 2. Plan for Maintenance Windows \u2022 Determine a suitable maintenance window to apply updates and patches to your DocumentDB cluster. \u2022 Consider the impact on your applications and users when scheduling the maintenance window. 3. Monitor the AWS Management Console \u2022 Regularly check the AWS Management Console for notifications related to available updates and patches for your DocumentDB cluster. \u2022 The console will provide information on new versions and available patches. 4. Review the Release Notes and Changelog \u2022 Before applying any updates or patches, review the release notes and changelog for the new version or patch. \u2022 Pay attention to any compatibility or breaking changes that may require application adjustments.  5. Create a Test Environment (Optional) \u2022 If feasible, create a separate test environment that closely resembles your production environment. \u2022 Deploy a copy of your DocumentDB cluster in the test environment to test the updates and patches before applying them to production. 6. Apply Updates and Patches \u2022 During the scheduled maintenance window, initiate the process to apply updates and patches to your DocumentDB cluster. \u2022 Follow the recommended procedure provided by AWS, which may involve a few simple clicks in the AWS Management Console. \u2022 Ensure that you select the appropriate version or patch to apply. 7. Monitor the Update Process \u2022 Monitor the progress of the update or patch application for your DocumentDB cluster. \u2022 AWS will provide status updates during the process to keep you informed. 8. Verify Post-Update Functionality \u2022 After the update or patch is applied, test the functionality of your applications that rely on the DocumentDB cluster. \u2022 Verify that your applications are working as expected and that any integration or dependencies are intact. 9. Review and Update Documentation \u2022 Update your documentation, including standard operating procedures (SOPs), to reflect the new version or patch applied to the DocumentDB cluster. \u2022 Document any changes or considerations specific to the update or patch. 10. Monitor for New Updates \u2022 Continuously monitor for new updates and patches released by AWS for DocumentDB. \u2022 Repeat the update process regularly to ensure your DocumentDB cluster remains up to date with the latest security enhancements and bug fixes.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up vulnerabilities that could have led to potential cyber-attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Stay informed about the latest security updates and patches released by Amazon for DocumentDB. Regularly apply updates and patches to your DocumentDB instances to protect against known vulnerabilities.",
      "audit_steps": "1. Stay Informed \u2022 Stay updated with Amazon DocumentDB announcements, release notes, and security bulletins. \u2022 Subscribe to AWS newsletters, forums, and notifications to receive timely updates regarding updates and patches. 2. Plan for Maintenance Windows \u2022 Determine a suitable maintenance window to apply updates and patches to your DocumentDB cluster. \u2022 Consider the impact on your applications and users when scheduling the maintenance window. 3. Monitor the AWS Management Console \u2022 Regularly check the AWS Management Console for notifications related to available updates and patches for your DocumentDB cluster. \u2022 The console will provide information on new versions and available patches. 4. Review the Release Notes and Changelog \u2022 Before applying any updates or patches, review the release notes and changelog for the new version or patch. \u2022 Pay attention to any compatibility or breaking changes that may require application adjustments.  5. Create a Test Environment (Optional) \u2022 If feasible, create a separate test environment that closely resembles your production environment. \u2022 Deploy a copy of your DocumentDB cluster in the test environment to test the updates and patches before applying them to production. 6. Apply Updates and Patches \u2022 During the scheduled maintenance window, initiate the process to apply updates and patches to your DocumentDB cluster. \u2022 Follow the recommended procedure provided by AWS, which may involve a few simple clicks in the AWS Management Console. \u2022 Ensure that you select the appropriate version or patch to apply. 7. Monitor the Update Process \u2022 Monitor the progress of the update or patch application for your DocumentDB cluster. \u2022 AWS will provide status updates during the process to keep you informed. 8. Verify Post-Update Functionality \u2022 After the update or patch is applied, test the functionality of your applications that rely on the DocumentDB cluster. \u2022 Verify that your applications are working as expected and that any integration or dependencies are intact. 9. Review and Update Documentation \u2022 Update your documentation, including standard operating procedures (SOPs), to reflect the new version or patch applied to the DocumentDB cluster. \u2022 Document any changes or considerations specific to the update or patch. 10. Monitor for New Updates \u2022 Continuously monitor for new updates and patches released by AWS for DocumentDB. \u2022 Repeat the update process regularly to ensure your DocumentDB cluster remains up to date with the latest security enhancements and bug fixes.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up vulnerabilities that could have led to potential cyber-attack.",
      "impact": "Helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up vulnerabilities that could have led to potential cyber-attack."
    },
    "function_name": "rds_aurora_regular_updates_patches_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.maintenance.updates.regular_updates_patches_enabled"
  },
  {
    "id": "7.8",
    "title": "Ensure to Implement Monitoring and Alerting",
    "assessment": "Manual",
    "description": "This helps by alerting the system if any unusual event has occurred or if a particular threshold has been achieved because the user is able to set a desired interval or the cluster. This then allows system administrators to swiftly correct the situation and avoid subsequent complications if something unusual is happening.",
    "rationale": "Impact: Has a positive impact in the business operations when the issue is identified and addressed accordingly.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to implement monitoring and alerting. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Monitoring\" section. 4. Enable Enhanced Monitoring \u2022 Under the Enhanced Monitoring section. \u2022 Click on the Edit button or Modify option to configure enhanced monitoring settings. \u2022 Enable the desired metrics and set the desired monitoring interval for the cluster. \u2022 Enhanced monitoring provides additional insights into the performance and health of your DocumentDB cluster.  5. Set Up CloudWatch Alarms \u2022 Scroll down to the CloudWatch Alarms section. \u2022 Click on the Create alarm button. \u2022 Configure the CloudWatch alarm based on the metrics you want to monitor and the thresholds you want to set. \u2022 Specify the actions to be taken when the alarm is triggered, such as sending notifications or executing automated actions. 6. Customize Metrics and Dashboards (Optional) \u2022 If desired, you can customize the metrics and dashboards in Amazon CloudWatch to suit your specific monitoring requirements. \u2022 Create custom metrics, build personalized dashboards, and set up alarms based on your application's unique needs. 7. Test Monitoring and Alerting \u2022 Perform operations on your DocumentDB cluster to generate metric data and trigger the configured alarms. \u2022 Verify that CloudWatch is capturing the metrics and triggering the appropriate actions based on your alarm settings. 8. Regularly Review and Fine-Tune \u2022 Regularly review the monitoring metrics, CloudWatch alarms, and any event- driven actions triggered by DocumentDB events. \u2022 Fine-tune the monitoring settings, alarms, and notifications based on the observed patterns and requirements of your application.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Has a positive impact in the business operations when the issue is identified and addressed accordingly.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps by alerting the system if any unusual event has occurred or if a particular threshold has been achieved because the user is able to set a desired interval or the cluster. This then allows system administrators to swiftly correct the situation and avoid subsequent complications if something unusual is happening.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to implement monitoring and alerting. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Monitoring\" section. 4. Enable Enhanced Monitoring \u2022 Under the Enhanced Monitoring section. \u2022 Click on the Edit button or Modify option to configure enhanced monitoring settings. \u2022 Enable the desired metrics and set the desired monitoring interval for the cluster. \u2022 Enhanced monitoring provides additional insights into the performance and health of your DocumentDB cluster.  5. Set Up CloudWatch Alarms \u2022 Scroll down to the CloudWatch Alarms section. \u2022 Click on the Create alarm button. \u2022 Configure the CloudWatch alarm based on the metrics you want to monitor and the thresholds you want to set. \u2022 Specify the actions to be taken when the alarm is triggered, such as sending notifications or executing automated actions. 6. Customize Metrics and Dashboards (Optional) \u2022 If desired, you can customize the metrics and dashboards in Amazon CloudWatch to suit your specific monitoring requirements. \u2022 Create custom metrics, build personalized dashboards, and set up alarms based on your application's unique needs. 7. Test Monitoring and Alerting \u2022 Perform operations on your DocumentDB cluster to generate metric data and trigger the configured alarms. \u2022 Verify that CloudWatch is capturing the metrics and triggering the appropriate actions based on your alarm settings. 8. Regularly Review and Fine-Tune \u2022 Regularly review the monitoring metrics, CloudWatch alarms, and any event- driven actions triggered by DocumentDB events. \u2022 Fine-tune the monitoring settings, alarms, and notifications based on the observed patterns and requirements of your application.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Has a positive impact in the business operations when the issue is identified and addressed accordingly.",
      "impact": "Has a positive impact in the business operations when the issue is identified and addressed accordingly."
    },
    "function_name": "rds_aurora_monitoring_alerting_enabled_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.monitoring.alerting.monitoring_alerting_enabled"
  },
  {
    "id": "7.9",
    "title": "Ensure to Implement Backup and Disaster Recovery",
    "assessment": "Manual",
    "description": "Set up automated backups for your DocumentDB instances to ensure data durability and recoverability. Consider implementing a disaster recovery plan that includes data replication across different availability zones or regions.",
    "rationale": "Having the data backed up ensures that all the crucial information is stored securely it defends against any human errors and system errors that resulted in data loss. An organization that has a disaster recovery plan is prepared for any disruption that would impact business operations. Impact: If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, data loss that cannot be restored, and loss of revenue.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to implement backup and disaster recovery. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Backup\" section. 4. Enable Automated Backups \u2022 Under the Automated backups section.  \u2022 Click on the Edit button or Modify option to configure automated backup settings. \u2022 Enable automated backups by choosing the desired backup retention period. \u2022 Specify the number of days for which automated backups should be retained.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, data loss that cannot be restored, and loss of revenue.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_backup_enabled",
      "rds_cluster_protected_by_backup_plan"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_backup_enabled",
        "rds_cluster_protected_by_backup_plan"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Set up automated backups for your DocumentDB instances to ensure data durability and recoverability. Consider implementing a disaster recovery plan that includes data replication across different availability zones or regions.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon DocumentDB Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/docdb/. 3. Select the DocumentDB Cluster \u2022 Choose the Amazon DocumentDB cluster for which you want to implement backup and disaster recovery. \u2022 Click on the cluster name to access its details page. \u2022 In the cluster details page, navigate to the \"Backup\" section. 4. Enable Automated Backups \u2022 Under the Automated backups section.  \u2022 Click on the Edit button or Modify option to configure automated backup settings. \u2022 Enable automated backups by choosing the desired backup retention period. \u2022 Specify the number of days for which automated backups should be retained.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Having the data backed up ensures that all the crucial information is stored securely it defends against any human errors and system errors that resulted in data loss. An organization that has a disaster recovery plan is prepared for any disruption that would impact business operations. Impact: If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, data loss that cannot be restored, and loss of revenue.",
      "impact": "If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, data loss that cannot be restored, and loss of revenue."
    },
    "function_name": "rds_aurora_backup_disaster_recovery_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.backup.disaster_recovery.backup_disaster_recovery_implemented"
  },
  {
    "id": "7.10",
    "title": "Ensure to Configure Backup Window",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "1. Perform Manual Backups (Optional) \u2022 If desired, you can also create manual backups of your DocumentDB cluster. \u2022 In the cluster details page, navigate to the Backup section. \u2022 Click on the Create backup button. \u2022 Provide a name for the backup and confirm the action. 2. Restore from Backups (Optional) \u2022 If a disaster occurs or you need to restore your DocumentDB cluster to a previous state, you can restore it from the available backups. \u2022 In the cluster details page, navigate to the Backup section. \u2022 Choose the backup from which you want to restore. \u2022 Follow the prompts and provide the necessary information to initiate the restore process. 3. Test Backup and Restore Procedures \u2022 Periodically test the backup and restore procedures to ensure they work as expected. \u2022 Perform test restores on non-production environments to validate the integrity and completeness of the backup data. 4. Regularly Monitor and Validate Backups \u2022 Regularly monitor the backup status and validate that the backups are completed successfully. \u2022 Monitor backup storage usage to ensure it is within the desired limits and plan for additional storage as needed.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_backup_enabled",
      "rds_cluster_protected_by_backup_plan"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_backup_enabled",
        "rds_cluster_protected_by_backup_plan"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Perform Manual Backups (Optional) \u2022 If desired, you can also create manual backups of your DocumentDB cluster. \u2022 In the cluster details page, navigate to the Backup section. \u2022 Click on the Create backup button. \u2022 Provide a name for the backup and confirm the action. 2. Restore from Backups (Optional) \u2022 If a disaster occurs or you need to restore your DocumentDB cluster to a previous state, you can restore it from the available backups. \u2022 In the cluster details page, navigate to the Backup section. \u2022 Choose the backup from which you want to restore. \u2022 Follow the prompts and provide the necessary information to initiate the restore process. 3. Test Backup and Restore Procedures \u2022 Periodically test the backup and restore procedures to ensure they work as expected. \u2022 Perform test restores on non-production environments to validate the integrity and completeness of the backup data. 4. Regularly Monitor and Validate Backups \u2022 Regularly monitor the backup status and validate that the backups are completed successfully. \u2022 Monitor backup storage usage to ensure it is within the desired limits and plan for additional storage as needed.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "rds_aurora_backup_window_configured_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.backup.window.backup_window_configured"
  },
  {
    "id": "7.11",
    "title": "Ensure to Conduct Security Assessments",
    "assessment": "Manual",
    "description": "Periodically perform security assessments, including vulnerability assessments and penetration testing, to identify and address any security weaknesses. Review your security configuration against best practices and industry standards.",
    "rationale": "This helps ensure that any vulnerabilities that might lie dormant be addressed promptly, which would reduce the risk of a malicious attack. Reviewing and making sure the security policies are authentic ensures the safety of the organization data.",
    "audit": "1. Define the Scope of the Security Assessment \u2022 Clearly define the scope of the security assessment for your Amazon DocumentDB cluster. \u2022 Determine the objectives, areas of focus, and any specific compliance or security standards you must adhere to. 2. Review Security Documentation \u2022 Familiarize yourself with the AWS security best practices and documentation related to Amazon DocumentDB. \u2022 Review the AWS Shared Responsibility Model and understand the security controls provided by AWS. 3. Assess Network Security \u2022 Review the network security configuration of your Amazon DocumentDB cluster. \u2022 Ensure it is deployed within a secure Virtual Private Cloud (VPC) with appropriate security groups and network access control lists (ACLs). \u2022 Validate that the network traffic to and from the cluster is appropriately restricted based on your security requirements. 4. Evaluate Encryption Configuration \u2022 Assess the encryption settings for your Amazon DocumentDB cluster. \u2022 Verify that encryption at rest is enabled and that the data stored in the cluster is encrypted.  \u2022 Validate that encryption in transit is enforced, ensuring that all client connections to the cluster are encrypted using SSL/TLS. 5. Review Access Control Mechanisms \u2022 Evaluate the access control mechanisms implemented for your Amazon DocumentDB cluster. \u2022 Ensure that appropriate Identity and Access Management (IAM) policies and roles are in place to control access to the cluster. \u2022 Review user accounts and their privileges, and validate that multi-factor authentication (MFA) is enforced for administrative access. 6. Examine Audit Logging and Monitoring \u2022 Review the audit logging and monitoring configuration for your Amazon DocumentDB cluster. \u2022 Verify that audit logging is enabled, capturing relevant database activities and events. \u2022 Evaluate the monitoring setup using Amazon CloudWatch or other tools to detect unusual or suspicious activities. 7. Assess Backup and Disaster Recovery \u2022 Evaluate the backup and disaster recovery mechanisms in place for your Amazon DocumentDB cluster. \u2022 Verify that automated backups are enabled and configured with an appropriate retention period. \u2022 Validate that manual backups can be performed and restored successfully. 8. Perform Vulnerability Scanning and Penetration Testing (If Applicable) \u2022 If allowed and within the terms of service, perform vulnerability scanning and penetration testing on your Amazon DocumentDB cluster. \u2022 Conduct security assessments to identify any vulnerabilities or weaknesses that could be exploited. 9. Document Findings and Remediation Plan \u2022 Document the findings of your security assessment, including any identified vulnerabilities or areas of improvement. \u2022 Develop a remediation plan that addresses the identified issues and outlines the necessary actions to enhance the security posture of your DocumentDB cluster. 10. Implement Remediation Measures \u2022 Implement the necessary remediation measures based on your remediation plan.  \u2022 Apply security patches, adjust configuration settings, and strengthen access controls as required. 11. Regularly Repeat the Security Assessment \u2022 Conduct regular security assessments on your Amazon DocumentDB cluster to ensure ongoing compliance and identify new risks or vulnerabilities. \u2022 Stay updated with security best practices and apply any relevant updates or patches to your cluster.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Periodically perform security assessments, including vulnerability assessments and penetration testing, to identify and address any security weaknesses. Review your security configuration against best practices and industry standards.",
      "audit_steps": "1. Define the Scope of the Security Assessment \u2022 Clearly define the scope of the security assessment for your Amazon DocumentDB cluster. \u2022 Determine the objectives, areas of focus, and any specific compliance or security standards you must adhere to. 2. Review Security Documentation \u2022 Familiarize yourself with the AWS security best practices and documentation related to Amazon DocumentDB. \u2022 Review the AWS Shared Responsibility Model and understand the security controls provided by AWS. 3. Assess Network Security \u2022 Review the network security configuration of your Amazon DocumentDB cluster. \u2022 Ensure it is deployed within a secure Virtual Private Cloud (VPC) with appropriate security groups and network access control lists (ACLs). \u2022 Validate that the network traffic to and from the cluster is appropriately restricted based on your security requirements. 4. Evaluate Encryption Configuration \u2022 Assess the encryption settings for your Amazon DocumentDB cluster. \u2022 Verify that encryption at rest is enabled and that the data stored in the cluster is encrypted.  \u2022 Validate that encryption in transit is enforced, ensuring that all client connections to the cluster are encrypted using SSL/TLS. 5. Review Access Control Mechanisms \u2022 Evaluate the access control mechanisms implemented for your Amazon DocumentDB cluster. \u2022 Ensure that appropriate Identity and Access Management (IAM) policies and roles are in place to control access to the cluster. \u2022 Review user accounts and their privileges, and validate that multi-factor authentication (MFA) is enforced for administrative access. 6. Examine Audit Logging and Monitoring \u2022 Review the audit logging and monitoring configuration for your Amazon DocumentDB cluster. \u2022 Verify that audit logging is enabled, capturing relevant database activities and events. \u2022 Evaluate the monitoring setup using Amazon CloudWatch or other tools to detect unusual or suspicious activities. 7. Assess Backup and Disaster Recovery \u2022 Evaluate the backup and disaster recovery mechanisms in place for your Amazon DocumentDB cluster. \u2022 Verify that automated backups are enabled and configured with an appropriate retention period. \u2022 Validate that manual backups can be performed and restored successfully. 8. Perform Vulnerability Scanning and Penetration Testing (If Applicable) \u2022 If allowed and within the terms of service, perform vulnerability scanning and penetration testing on your Amazon DocumentDB cluster. \u2022 Conduct security assessments to identify any vulnerabilities or weaknesses that could be exploited. 9. Document Findings and Remediation Plan \u2022 Document the findings of your security assessment, including any identified vulnerabilities or areas of improvement. \u2022 Develop a remediation plan that addresses the identified issues and outlines the necessary actions to enhance the security posture of your DocumentDB cluster. 10. Implement Remediation Measures \u2022 Implement the necessary remediation measures based on your remediation plan.  \u2022 Apply security patches, adjust configuration settings, and strengthen access controls as required. 11. Regularly Repeat the Security Assessment \u2022 Conduct regular security assessments on your Amazon DocumentDB cluster to ensure ongoing compliance and identify new risks or vulnerabilities. \u2022 Stay updated with security best practices and apply any relevant updates or patches to your cluster.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps ensure that any vulnerabilities that might lie dormant be addressed promptly, which would reduce the risk of a malicious attack. Reviewing and making sure the security policies are authentic ensures the safety of the organization data.",
      "impact": ""
    },
    "function_name": "rds_aurora_security_assessments_check",
    "coverage": 90,
    "rule_id": "aws.rds.aurora.security.assessments.security_assessments_conducted"
  },
  {
    "id": "8.1",
    "title": "Ensure Keyspace Security is Configured",
    "assessment": "Manual",
    "description": "To access Amazon Keyspaces, the user would be required to log in with their AWS credentials. Once logged in the user can access the AWS resources and can explore the resources that Amazon Keyspaces offers. Amazon Keyspaces offers a lot of security that can mitigate a potential attack.",
    "rationale": "",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Explore Amazon Keyspaces Security Features \u2022 In the Amazon Keyspaces console, navigate to the Features or Security section to explore the available security features. \u2022 Take note of the following critical security features: o Encryption at Rest: Understand how Amazon Keyspaces provides encryption at rest for your data. It uses server-side encryption by default, ensuring that data stored in Keyspaces is encrypted. o Encryption in Transit: Learn how to configure encryption in transit for data transmitted between your client applications and Amazon Keyspaces. Amazon Keyspaces supports Transport Layer Security (TLS) encryption to secure the communication channel. o Virtual Private Cloud (VPC) Support: Explore the VPC support options Amazon Keyspaces provides. It allows you to deploy your Keyspaces resources within your VPC for enhanced network isolation and control. o Authentication Options: Understand the authentication mechanisms available in Amazon Keyspaces. IAM for Cassandra allows you to use AWS Identity and Access Management (IAM) to authenticate and authorize client connections to Keyspaces.  o Access Control: Learn about access control options in Amazon Keyspaces. It supports fine-grained access control using Access Control Lists (ACLs) at the table and row level to manage access permissions for different users or roles. o Audit Logging: Explore how to enable audit logging for Amazon Keyspaces. Amazon CloudWatch Logs can capture and store logs from your Keyspaces resources, providing visibility into activities for security and compliance purposes. 4. Documentation and Resources \u2022 Access the official Amazon Keyspaces documentation by navigating to the Documentation or Learn section in the Amazon Keyspaces console. \u2022 Review the comprehensive documentation to gain in-depth knowledge about each security feature, including best practices, configuration options, and implementation details. \u2022 Utilize other AWS resources such as whitepapers, blogs, and security-related documentation further to enhance your understanding of Amazon Keyspaces security features.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "To access Amazon Keyspaces, the user would be required to log in with their AWS credentials. Once logged in the user can access the AWS resources and can explore the resources that Amazon Keyspaces offers. Amazon Keyspaces offers a lot of security that can mitigate a potential attack.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Explore Amazon Keyspaces Security Features \u2022 In the Amazon Keyspaces console, navigate to the Features or Security section to explore the available security features. \u2022 Take note of the following critical security features: o Encryption at Rest: Understand how Amazon Keyspaces provides encryption at rest for your data. It uses server-side encryption by default, ensuring that data stored in Keyspaces is encrypted. o Encryption in Transit: Learn how to configure encryption in transit for data transmitted between your client applications and Amazon Keyspaces. Amazon Keyspaces supports Transport Layer Security (TLS) encryption to secure the communication channel. o Virtual Private Cloud (VPC) Support: Explore the VPC support options Amazon Keyspaces provides. It allows you to deploy your Keyspaces resources within your VPC for enhanced network isolation and control. o Authentication Options: Understand the authentication mechanisms available in Amazon Keyspaces. IAM for Cassandra allows you to use AWS Identity and Access Management (IAM) to authenticate and authorize client connections to Keyspaces.  o Access Control: Learn about access control options in Amazon Keyspaces. It supports fine-grained access control using Access Control Lists (ACLs) at the table and row level to manage access permissions for different users or roles. o Audit Logging: Explore how to enable audit logging for Amazon Keyspaces. Amazon CloudWatch Logs can capture and store logs from your Keyspaces resources, providing visibility into activities for security and compliance purposes. 4. Documentation and Resources \u2022 Access the official Amazon Keyspaces documentation by navigating to the Documentation or Learn section in the Amazon Keyspaces console. \u2022 Review the comprehensive documentation to gain in-depth knowledge about each security feature, including best practices, configuration options, and implementation details. \u2022 Utilize other AWS resources such as whitepapers, blogs, and security-related documentation further to enhance your understanding of Amazon Keyspaces security features.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "keyspaces_keyspace_security_configured_check",
    "coverage": 90,
    "rule_id": "aws.keyspaces.keyspace.security.keyspace_security_configured"
  },
  {
    "id": "8.2",
    "title": "Ensure Network Security is Enabled",
    "assessment": "Manual",
    "description": "In order to access Amazon Keyspaces the user is required to set specific networking parameters and security measurements without these extra steps they will not be able to access it. Users are required to create or select a virtual private cloud (VPC) and define their inbound and outbound rules accordingly.",
    "rationale": "Impact: Only authorized users have access to the database which limits and controls any risk of an attack. This ensures better performance of the system to a private network and better security.",
    "audit": "1. Create or Select a Virtual Private Cloud (VPC) \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC where you want to deploy your Amazon Keyspaces resources. 2. Configure Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the subnets within your VPC where you want to deploy your Amazon Keyspaces resources. \u2022 Ensure you have private subnets to isolate your Keyspaces resources from the public internet. 3. Define Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group or select an existing one for your Amazon Keyspaces resources. \u2022 Configure inbound and outbound rules in the security group to control traffic access. o Allow inbound access only from trusted sources, such as specific IP ranges or security groups, on the necessary ports used by Amazon Keyspaces.  o Define outbound rules based on your requirements, allowing outbound traffic to necessary destinations or ports. \u2022 Associate the security group with your Amazon Keyspaces resources. 4. Configure Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu. \u2022 Create or select the network ACLs associated with the subnets used by your Amazon Keyspaces resources. \u2022 Configure inbound and outbound rules in the network ACLs to control traffic access. o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. o list text hereDeny unnecessary or unwanted traffic. \u2022 Associate the network ACLs with the subnets used by your Amazon Keyspaces resources. 5. Configure VPC Endpoints \u2022 In the VPC console, navigate to Endpoints in the left-side menu. \u2022 Create or select the VPC endpoints required for Amazon Keyspaces. \u2022 If you need to access Keyspaces from within your VPC, create a VPC endpoint for Amazon Keyspaces to connect your applications securely. \u2022 If you need to access Keyspaces from another VPC or on-premises network, set up VPC peering or a transit gateway to establish a secure connection. 6. Verify Connectivity and Test \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your Amazon Keyspaces resources or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the connectivity to your Amazon Keyspaces resources by trying to connect to them using the appropriate client or utility. \u2022 Verify that the network security settings allow the necessary traffic and deny unauthorized access.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Only authorized users have access to the database which limits and controls any risk of an attack. This ensures better performance of the system to a private network and better security.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "In order to access Amazon Keyspaces the user is required to set specific networking parameters and security measurements without these extra steps they will not be able to access it. Users are required to create or select a virtual private cloud (VPC) and define their inbound and outbound rules accordingly.",
      "audit_steps": "1. Create or Select a Virtual Private Cloud (VPC) \u2022 Sign in to the AWS Management Console and open the Amazon VPC console at https://console.aws.amazon.com/vpc/. \u2022 Create a new VPC or select an existing VPC where you want to deploy your Amazon Keyspaces resources. 2. Configure Subnets \u2022 In the VPC console, navigate to Subnets in the left-side menu. \u2022 Create or select the subnets within your VPC where you want to deploy your Amazon Keyspaces resources. \u2022 Ensure you have private subnets to isolate your Keyspaces resources from the public internet. 3. Define Security Groups \u2022 In the VPC console, navigate to Security Groups in the left-side menu. \u2022 Create a new security group or select an existing one for your Amazon Keyspaces resources. \u2022 Configure inbound and outbound rules in the security group to control traffic access. o Allow inbound access only from trusted sources, such as specific IP ranges or security groups, on the necessary ports used by Amazon Keyspaces.  o Define outbound rules based on your requirements, allowing outbound traffic to necessary destinations or ports. \u2022 Associate the security group with your Amazon Keyspaces resources. 4. Configure Network Access Control Lists (ACLs) \u2022 In the VPC console, navigate to Network ACLs in the left-side menu. \u2022 Create or select the network ACLs associated with the subnets used by your Amazon Keyspaces resources. \u2022 Configure inbound and outbound rules in the network ACLs to control traffic access. o Define rules based on your security requirements, allowing only necessary protocols, ports, and IP ranges. o list text hereDeny unnecessary or unwanted traffic. \u2022 Associate the network ACLs with the subnets used by your Amazon Keyspaces resources. 5. Configure VPC Endpoints \u2022 In the VPC console, navigate to Endpoints in the left-side menu. \u2022 Create or select the VPC endpoints required for Amazon Keyspaces. \u2022 If you need to access Keyspaces from within your VPC, create a VPC endpoint for Amazon Keyspaces to connect your applications securely. \u2022 If you need to access Keyspaces from another VPC or on-premises network, set up VPC peering or a transit gateway to establish a secure connection. 6. Verify Connectivity and Test \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your Amazon Keyspaces resources or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the connectivity to your Amazon Keyspaces resources by trying to connect to them using the appropriate client or utility. \u2022 Verify that the network security settings allow the necessary traffic and deny unauthorized access.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Only authorized users have access to the database which limits and controls any risk of an attack. This ensures better performance of the system to a private network and better security.",
      "impact": "Only authorized users have access to the database which limits and controls any risk of an attack. This ensures better performance of the system to a private network and better security."
    },
    "function_name": "keyspaces_network_security_enabled_check",
    "coverage": 90,
    "rule_id": "aws.keyspaces.network.security.network_security_enabled"
  },
  {
    "id": "8.3",
    "title": "Ensure Data at Rest and in Transit is Encrypted",
    "assessment": "Manual",
    "description": "Once a user is logged in to their AWS account and has access to their Amazon Keyspaces they are encouraged to choose from the following two options to encrypt their data. Depending on which key they select for encryption at rest would store the data according to their preference. For encryption in transit the user is also encouraged to choose from two options depending on if the data needs to be encrypted during transit.",
    "rationale": "Impact: Prevents any unauthorized user from accessing the database and provides security when transferring the data from one location to another.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to enable encryption at rest and in transit. \u2022 Click on the Keyspace name to access its details page. 4. Enable Encryption at Rest \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Encryption section, locate the \"Encryption at Rest\" option. \u2022 Click on Edit. \u2022 Select the desired encryption setting:  o Default Encryption: Choose this option to use the default AWS-managed key for encryption at rest. Amazon Keyspaces automatically encrypts your data using this default key. o Customer Managed Key (CMK): Choose this option if you want to use your own AWS Key Management Service (KMS) customer-managed key for encryption. Select the appropriate CMK from the dropdown menu. \u2022 Click \"Save\" to enable encryption at rest for the Keyspace. 5. Enable Encryption in Transit \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Encryption section, locate the \"Encryption in Transit\" option. \u2022 Click on Edit. \u2022 Select the desired encryption setting: o Encryption in Transit Enabled: Choose this option to enable encryption in transit for data transmitted between your client applications and Amazon Keyspaces. Keyspaces support Transport Layer Security (TLS) encryption for secure communication. o Encryption in Transit Disabled: Choose this option if you do not require encryption in transit. \u2022 Click \"Save\" to enable encryption in transit for the Keyspace. 6. Verify Encryption Status \u2022 Wait a few minutes for the changes to propagate and the encryption settings to take effect. \u2022 Refresh the Keyspace details page to see the updated encryption status. \u2022 Verify that encryption at rest and in transit are enabled for the Keyspace.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Prevents any unauthorized user from accessing the database and provides security when transferring the data from one location to another.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Once a user is logged in to their AWS account and has access to their Amazon Keyspaces they are encouraged to choose from the following two options to encrypt their data. Depending on which key they select for encryption at rest would store the data according to their preference. For encryption in transit the user is also encouraged to choose from two options depending on if the data needs to be encrypted during transit.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Keyspaces Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/keyspaces/. 3. Select the Keyspace \u2022 Choose the Keyspace (database) for which you want to enable encryption at rest and in transit. \u2022 Click on the Keyspace name to access its details page. 4. Enable Encryption at Rest \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Encryption section, locate the \"Encryption at Rest\" option. \u2022 Click on Edit. \u2022 Select the desired encryption setting:  o Default Encryption: Choose this option to use the default AWS-managed key for encryption at rest. Amazon Keyspaces automatically encrypts your data using this default key. o Customer Managed Key (CMK): Choose this option if you want to use your own AWS Key Management Service (KMS) customer-managed key for encryption. Select the appropriate CMK from the dropdown menu. \u2022 Click \"Save\" to enable encryption at rest for the Keyspace. 5. Enable Encryption in Transit \u2022 In the Keyspace details page, click on the Configuration tab. \u2022 Under the Encryption section, locate the \"Encryption in Transit\" option. \u2022 Click on Edit. \u2022 Select the desired encryption setting: o Encryption in Transit Enabled: Choose this option to enable encryption in transit for data transmitted between your client applications and Amazon Keyspaces. Keyspaces support Transport Layer Security (TLS) encryption for secure communication. o Encryption in Transit Disabled: Choose this option if you do not require encryption in transit. \u2022 Click \"Save\" to enable encryption in transit for the Keyspace. 6. Verify Encryption Status \u2022 Wait a few minutes for the changes to propagate and the encryption settings to take effect. \u2022 Refresh the Keyspace details page to see the updated encryption status. \u2022 Verify that encryption at rest and in transit are enabled for the Keyspace.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Prevents any unauthorized user from accessing the database and provides security when transferring the data from one location to another.",
      "impact": "Prevents any unauthorized user from accessing the database and provides security when transferring the data from one location to another."
    },
    "function_name": "keyspaces_encryption_enabled_check",
    "coverage": 90,
    "rule_id": "aws.keyspaces.encryption.enabled.encryption_at_rest_transit_enabled"
  },
  {
    "id": "9.1",
    "title": "Ensure Network Security is Enabled",
    "assessment": "Manual",
    "description": "This helps ensure that all the necessary security measurements are taken to prevent a cyber-attack. Such as utilizing VPC, creating certain inbound and outbound rules, and ACLs.",
    "rationale": "Impact: Provides privacy and lets the user customize their security preferences. Prevents private network from interfering with public networks.",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster for which you want to configure network security. \u2022 Click on the cluster name to access its details page. 4. Configure Security Groups \u2022 In the cluster details page, navigate to the Connectivity & Security or Network & Security section. \u2022 Under Security Groups, click on Manage security groups. \u2022 Click on Create new security group or select an existing security group associated with your Neptune cluster. \u2022 Configure inbound and outbound rules within the security group to control network traffic. o For inbound rules, specify the allowed source IP addresses or security groups and the necessary ports for accessing the Neptune cluster.  o For outbound rules, define the allowed destination IP addresses or security groups and the required ports for outbound connections from the Neptune cluster. \u2022 Save the security group settings. 5. Configure Network Access Control Lists (ACLs) \u2022 In the cluster details page, navigate to the Connectivity & Security or Network & Security section. \u2022 Under Network Access Control Lists (ACLs), click on Manage network ACLs. \u2022 Create a new network ACL or select an existing one associated with your Amazon Neptune cluster. \u2022 Configure inbound and outbound rules within the network ACL to control network traffic at the subnet level. \u2022 Define rules based on IP address ranges, protocols, and ports to allow or deny specific traffic. \u2022 Consider security best practices and compliance requirements when configuring the network ACL rules. \u2022 Save the network ACL settings. 6. Verify Network Security Configuration \u2022 Review the security group and network ACL settings to ensure they align with your security requirements. \u2022 Confirm that the inbound and outbound rules only allow necessary traffic and deny unauthorized access. \u2022 Verify that your Neptune cluster's security groups and network ACLs are correctly configured. 7. Test Network Connectivity \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your Neptune cluster, or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the network connectivity to your Neptune cluster by attempting to connect to it using the appropriate client or utility. \u2022 Ensure that the network security settings allow the necessary traffic and deny unauthorized access.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Provides privacy and lets the user customize their security preferences. Prevents private network from interfering with public networks.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_vpc_exists_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_vpc_exists_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps ensure that all the necessary security measurements are taken to prevent a cyber-attack. Such as utilizing VPC, creating certain inbound and outbound rules, and ACLs.",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster for which you want to configure network security. \u2022 Click on the cluster name to access its details page. 4. Configure Security Groups \u2022 In the cluster details page, navigate to the Connectivity & Security or Network & Security section. \u2022 Under Security Groups, click on Manage security groups. \u2022 Click on Create new security group or select an existing security group associated with your Neptune cluster. \u2022 Configure inbound and outbound rules within the security group to control network traffic. o For inbound rules, specify the allowed source IP addresses or security groups and the necessary ports for accessing the Neptune cluster.  o For outbound rules, define the allowed destination IP addresses or security groups and the required ports for outbound connections from the Neptune cluster. \u2022 Save the security group settings. 5. Configure Network Access Control Lists (ACLs) \u2022 In the cluster details page, navigate to the Connectivity & Security or Network & Security section. \u2022 Under Network Access Control Lists (ACLs), click on Manage network ACLs. \u2022 Create a new network ACL or select an existing one associated with your Amazon Neptune cluster. \u2022 Configure inbound and outbound rules within the network ACL to control network traffic at the subnet level. \u2022 Define rules based on IP address ranges, protocols, and ports to allow or deny specific traffic. \u2022 Consider security best practices and compliance requirements when configuring the network ACL rules. \u2022 Save the network ACL settings. 6. Verify Network Security Configuration \u2022 Review the security group and network ACL settings to ensure they align with your security requirements. \u2022 Confirm that the inbound and outbound rules only allow necessary traffic and deny unauthorized access. \u2022 Verify that your Neptune cluster's security groups and network ACLs are correctly configured. 7. Test Network Connectivity \u2022 Launch an Amazon EC2 instance within the same VPC and subnet as your Neptune cluster, or use an existing one. \u2022 Connect to the EC2 instance using SSH or other remote access methods. \u2022 Test the network connectivity to your Neptune cluster by attempting to connect to it using the appropriate client or utility. \u2022 Ensure that the network security settings allow the necessary traffic and deny unauthorized access.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Provides privacy and lets the user customize their security preferences. Prevents private network from interfering with public networks.",
      "impact": "Provides privacy and lets the user customize their security preferences. Prevents private network from interfering with public networks."
    },
    "function_name": "documentdb_network_security_enabled_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.network.security.network_security_enabled"
  },
  {
    "id": "9.2",
    "title": "Ensure Data at Rest is Encrypted",
    "assessment": "Manual",
    "description": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
    "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster for which you want to enable encryption at rest. \u2022 Click on the cluster name to access its details page. 4. Enable Encryption at Rest \u2022 In the cluster details page, navigate to the Configuration or Encryption at Rest section. \u2022 Under Encryption at Rest, click on Modify. \u2022 In the Encryption at Rest dialog box, select the encryption option you prefer: o AWS managed key (default): Choose this option to use the default AWS managed key for encryption. o Customer-managed key (CMK): Choose this option if you want to use your own AWS Key Management Service (KMS) customer-managed key for encryption. Select the appropriate CMK from the dropdown menu.  \u2022 Click Apply Changes to enable encryption at rest for the Neptune cluster. 5. Verify Encryption Status \u2022 Wait a few minutes for the changes and configuration to take effect. \u2022 Refresh the cluster details page to see the updated encryption status. \u2022 Verify that encryption at rest is enabled for the Neptune cluster.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster for which you want to enable encryption at rest. \u2022 Click on the cluster name to access its details page. 4. Enable Encryption at Rest \u2022 In the cluster details page, navigate to the Configuration or Encryption at Rest section. \u2022 Under Encryption at Rest, click on Modify. \u2022 In the Encryption at Rest dialog box, select the encryption option you prefer: o AWS managed key (default): Choose this option to use the default AWS managed key for encryption. o Customer-managed key (CMK): Choose this option if you want to use your own AWS Key Management Service (KMS) customer-managed key for encryption. Select the appropriate CMK from the dropdown menu.  \u2022 Click Apply Changes to enable encryption at rest for the Neptune cluster. 5. Verify Encryption Status \u2022 Wait a few minutes for the changes and configuration to take effect. \u2022 Refresh the cluster details page to see the updated encryption status. \u2022 Verify that encryption at rest is enabled for the Neptune cluster.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
      "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext."
    },
    "function_name": "documentdb_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.encryption.at_rest.encryption_at_rest_enabled"
  },
  {
    "id": "9.3",
    "title": "Ensure Data in Transit is Encrypted",
    "assessment": "Manual",
    "description": "Enabling encryption in transit helps that the data is protected when it is moving from one location to another.",
    "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster for which you want to implement encryption in transit. \u2022 Click on the cluster name to access its details page. 4. Enable SSL/TLS Encryption \u2022 In the cluster details page, navigate to the Configuration or Encryption in Transit section. \u2022 Under Encryption in Transit, ensure that the Enable option is selected. \u2022 Optionally, you can also select the Enforce option to require SSL/TLS encryption for all client connections to the Neptune cluster. \u2022 Click Apply Changes to enable SSL/TLS encryption for the Neptune cluster. 5. Update Client Applications  \u2022 When connecting to the Neptune cluster, update your client applications to establish an SSL/TLS-encrypted connection. \u2022 Consult your client drivers or libraries documentation or configuration settings to enable SSL/TLS encryption. \u2022 Configure the necessary SSL/TLS settings, such as specifying the SSL/TLS certificate to use. 6. Verify Encryption in Transit \u2022 Test the connection to the Neptune cluster from your client application. \u2022 Ensure that the connection is established using SSL/TLS encryption. \u2022 Verify that all data transmitted between your client applications and the Neptune cluster is encrypted in transit.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enabling encryption in transit helps that the data is protected when it is moving from one location to another.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster for which you want to implement encryption in transit. \u2022 Click on the cluster name to access its details page. 4. Enable SSL/TLS Encryption \u2022 In the cluster details page, navigate to the Configuration or Encryption in Transit section. \u2022 Under Encryption in Transit, ensure that the Enable option is selected. \u2022 Optionally, you can also select the Enforce option to require SSL/TLS encryption for all client connections to the Neptune cluster. \u2022 Click Apply Changes to enable SSL/TLS encryption for the Neptune cluster. 5. Update Client Applications  \u2022 When connecting to the Neptune cluster, update your client applications to establish an SSL/TLS-encrypted connection. \u2022 Consult your client drivers or libraries documentation or configuration settings to enable SSL/TLS encryption. \u2022 Configure the necessary SSL/TLS settings, such as specifying the SSL/TLS certificate to use. 6. Verify Encryption in Transit \u2022 Test the connection to the Neptune cluster from your client application. \u2022 Ensure that the connection is established using SSL/TLS encryption. \u2022 Verify that all data transmitted between your client applications and the Neptune cluster is encrypted in transit.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
      "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext."
    },
    "function_name": "documentdb_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.encryption.in_transit.encryption_in_transit_enabled"
  },
  {
    "id": "9.4",
    "title": "Ensure Authentication and Access Control is Enabled",
    "assessment": "Manual",
    "description": "This helps ensure that there are specific IAM roles and policies that are given the necessary information within a Neptune DB cluster to operate as needed.",
    "rationale": "Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster on which you want to implement authentication and access control. \u2022 Click on the cluster name to access its details page. 4. Enable IAM Database Authentication \u2022 In the cluster details page, navigate to the Configuration or Database Authentication section. \u2022 Under Database Authentication, select the option to enable IAM database authentication. \u2022 Click Apply Changes to enable IAM database authentication for the Neptune cluster.   5. Configure IAM Roles and Policies \u2022 Open the AWS Identity and Access Management (IAM) console by navigating to IAM in the AWS Management Console. \u2022 Create IAM roles and policies that define the desired access control for your Neptune resources. \u2022 Assign the necessary permissions to the IAM roles to allow specific actions on the Neptune cluster, such as read, write, or manage operations. \u2022 Associate the IAM roles with the appropriate users, groups, or AWS services that need access to the Neptune cluster. 6. Test IAM Database Authentication \u2022 Update your client applications or tools to use IAM database authentication when connecting to the Neptune cluster. \u2022 Configure your applications to assume the necessary IAM roles before establishing a connection to Neptune. \u2022 Test the connection from your client application to the Neptune cluster to verify that IAM database authentication is working as expected. \u2022 Ensure that users or services are authenticated and authorized based on the IAM roles and policies defined. 7. Regularly Review and Update IAM Roles and Policies \u2022 Periodically review your IAM roles and policies to ensure they align with your security requirements and access control needs. \u2022 Make necessary updates to IAM roles and policies to adapt to changes in user access requirements or organizational security policies. \u2022 Follow the principle of least privilege and ensure that users or services have only the necessary permissions to perform their required actions on the Neptune cluster.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps ensure that there are specific IAM roles and policies that are given the necessary information within a Neptune DB cluster to operate as needed.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster on which you want to implement authentication and access control. \u2022 Click on the cluster name to access its details page. 4. Enable IAM Database Authentication \u2022 In the cluster details page, navigate to the Configuration or Database Authentication section. \u2022 Under Database Authentication, select the option to enable IAM database authentication. \u2022 Click Apply Changes to enable IAM database authentication for the Neptune cluster.   5. Configure IAM Roles and Policies \u2022 Open the AWS Identity and Access Management (IAM) console by navigating to IAM in the AWS Management Console. \u2022 Create IAM roles and policies that define the desired access control for your Neptune resources. \u2022 Assign the necessary permissions to the IAM roles to allow specific actions on the Neptune cluster, such as read, write, or manage operations. \u2022 Associate the IAM roles with the appropriate users, groups, or AWS services that need access to the Neptune cluster. 6. Test IAM Database Authentication \u2022 Update your client applications or tools to use IAM database authentication when connecting to the Neptune cluster. \u2022 Configure your applications to assume the necessary IAM roles before establishing a connection to Neptune. \u2022 Test the connection from your client application to the Neptune cluster to verify that IAM database authentication is working as expected. \u2022 Ensure that users or services are authenticated and authorized based on the IAM roles and policies defined. 7. Regularly Review and Update IAM Roles and Policies \u2022 Periodically review your IAM roles and policies to ensure they align with your security requirements and access control needs. \u2022 Make necessary updates to IAM roles and policies to adapt to changes in user access requirements or organizational security policies. \u2022 Follow the principle of least privilege and ensure that users or services have only the necessary permissions to perform their required actions on the Neptune cluster.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
      "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data."
    },
    "function_name": "documentdb_authentication_access_control_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.authentication.access_control.authentication_access_control_enabled"
  },
  {
    "id": "9.5",
    "title": "Ensure Audit Logging is Enabled",
    "assessment": "Manual",
    "description": "This control is important because it helps ensure activity within the cluster and identifies who has last modified the document and who has access to it, in case of breaches. It also ensures compliance with regulation requirements.",
    "rationale": "Impact: Reduces risks of any fraud since worker activity is being monitored and tracked.",
    "audit": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster on which you want to enable audit logging. Click on the cluster name to access its details page. 4. Enable Amazon CloudWatch Logs \u2022 In the cluster details page, navigate to the Monitoring or Logging section. \u2022 Under CloudWatch Logs, click Enable to enable logging for the Neptune cluster. \u2022 Select an existing CloudWatch Logs group or create a new one to store the logs. \u2022 Choose the appropriate retention period for the logs, considering your compliance and retention requirements. \u2022 Click Save or Apply Changes to enable CloudWatch Logs for the Neptune cluster. 5. Configure Log Levels (Optional) \u2022 In the cluster details page, navigate to the Configuration or Logging section.  \u2022 Under Logging, you may have the option to configure log levels for different components of Neptune, such as query logs or error logs. \u2022 Adjust the log levels according to your logging and troubleshooting needs. \u2022 Click Apply Changes to save the log level configuration. 6. Review and Analyze Logs \u2022 Access the Amazon CloudWatch console by navigating to CloudWatch in the AWS Management Console. \u2022 Go to the CloudWatch Logs section and locate the log group associated with your Neptune cluster. \u2022 Select the log group and review the logs generated by Neptune. \u2022 Analyze the logs for troubleshooting, performance monitoring, or auditing purposes. 7. Set Up Log Metric Filters and Alarms (Optional) \u2022 In the CloudWatch console, navigate to Metric Filters in the left-side menu. \u2022 Click on Create metric filter to set up filters to extract specific information from the Neptune logs. \u2022 Define the filter patterns to capture the desired log events and extract the required metrics. \u2022 Configure alarms based on the extracted metrics to trigger notifications or automated actions when specific conditions are met. 8. Regularly Monitor Logs \u2022 Continuously monitor the logs generated by Neptune using the CloudWatch Logs console or programmatically using the CloudWatch APIs. \u2022 Review the logs regularly to identify any abnormal or suspicious activities. \u2022 Set up appropriate notifications or alerts to proactively respond to critical log events.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Reduces risks of any fraud since worker activity is being monitored and tracked.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This control is important because it helps ensure activity within the cluster and identifies who has last modified the document and who has access to it, in case of breaches. It also ensures compliance with regulation requirements.",
      "audit_steps": "1. Sign into the AWS Management Console \u2022 Sign into the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster on which you want to enable audit logging. Click on the cluster name to access its details page. 4. Enable Amazon CloudWatch Logs \u2022 In the cluster details page, navigate to the Monitoring or Logging section. \u2022 Under CloudWatch Logs, click Enable to enable logging for the Neptune cluster. \u2022 Select an existing CloudWatch Logs group or create a new one to store the logs. \u2022 Choose the appropriate retention period for the logs, considering your compliance and retention requirements. \u2022 Click Save or Apply Changes to enable CloudWatch Logs for the Neptune cluster. 5. Configure Log Levels (Optional) \u2022 In the cluster details page, navigate to the Configuration or Logging section.  \u2022 Under Logging, you may have the option to configure log levels for different components of Neptune, such as query logs or error logs. \u2022 Adjust the log levels according to your logging and troubleshooting needs. \u2022 Click Apply Changes to save the log level configuration. 6. Review and Analyze Logs \u2022 Access the Amazon CloudWatch console by navigating to CloudWatch in the AWS Management Console. \u2022 Go to the CloudWatch Logs section and locate the log group associated with your Neptune cluster. \u2022 Select the log group and review the logs generated by Neptune. \u2022 Analyze the logs for troubleshooting, performance monitoring, or auditing purposes. 7. Set Up Log Metric Filters and Alarms (Optional) \u2022 In the CloudWatch console, navigate to Metric Filters in the left-side menu. \u2022 Click on Create metric filter to set up filters to extract specific information from the Neptune logs. \u2022 Define the filter patterns to capture the desired log events and extract the required metrics. \u2022 Configure alarms based on the extracted metrics to trigger notifications or automated actions when specific conditions are met. 8. Regularly Monitor Logs \u2022 Continuously monitor the logs generated by Neptune using the CloudWatch Logs console or programmatically using the CloudWatch APIs. \u2022 Review the logs regularly to identify any abnormal or suspicious activities. \u2022 Set up appropriate notifications or alerts to proactively respond to critical log events.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Reduces risks of any fraud since worker activity is being monitored and tracked.",
      "impact": "Reduces risks of any fraud since worker activity is being monitored and tracked."
    },
    "function_name": "documentdb_audit_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.logging.audit.audit_logging_enabled"
  },
  {
    "id": "9.6",
    "title": "Ensure Security Configurations are Reviewed Regularly",
    "assessment": "Manual",
    "description": "This helps by removing or updating any IAM roles, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
    "rationale": "Impact: By updating and revising the control within our Amazon Neptune cluster it would keep the system as secure as possible.",
    "audit": "1. Establish a Security Review Schedule \u2022 Determine a regular schedule for reviewing and updating the security configuration of your Amazon Neptune environment. \u2022 Consider factors such as the frequency of changes, compliance requirements, and industry best practices to determine the appropriate review interval. 2. Monitor AWS Security Bulletins \u2022 Stay informed about AWS security updates and announcements related to Amazon Neptune. \u2022 Regularly review AWS security bulletins and notifications to identify any security patches, updates, or new features relevant to your Neptune environment. \u2022 Take note of any security recommendations or best practices provided by AWS. 3. Review IAM Roles and Policies \u2022 Access the AWS Identity and Access Management (IAM) console by navigating to IAM in the AWS Management Console. \u2022 Review the IAM roles and policies associated with your Neptune resources. \u2022 Ensure that the assigned permissions align with the principle of least privilege and reflect the current access requirements. \u2022 Update the IAM roles and policies as needed to adapt to changes in user access or security requirements.  4. Review Security Groups and Network ACLs \u2022 Access the Amazon Neptune console by navigating to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. \u2022 In the Neptune console, navigate to the Connectivity & Security or Network & Security section. \u2022 Review the security groups and network ACLs associated with your Neptune clusters. \u2022 Ensure that the inbound and outbound rules are up to date and aligned with your security requirements. \u2022 Remove any unnecessary or outdated rules and add new rules if required. 5. Review Encryption Settings \u2022 Navigate to the Configuration section or relevant encryption settings in the Neptune console. \u2022 Review the encryption settings for both encryption at rest and encryption in transit. \u2022 Ensure that the appropriate encryption options and key management strategies are in place. \u2022 Consider rotating encryption keys periodically, following best practices and compliance requirements. 6. Review VPC Configuration \u2022 Access the Amazon VPC console by navigating to VPC in the AWS Management Console. \u2022 Review the VPC configuration associated with your Neptune clusters. \u2022 Ensure the subnets, routing tables, and VPC peering settings are configured correctly. \u2022 Verify that the network architecture provides your Neptune resources' desired isolation and connectivity. 7. Conduct Security Assessments \u2022 Periodically conduct security assessments and penetration testing on your Neptune environment. \u2022 Engage security experts or use appropriate security tools to identify vulnerabilities, weaknesses, or misconfigurations. \u2022 Analyze the assessment results and take necessary actions to remediate any security issues or risks. 8. Stay Up to Date with Best Practices  \u2022 Continuously educate yourself and your team on the latest security best practices for Amazon Neptune. \u2022 Stay informed about emerging security threats and vulnerabilities. -Regularly review AWS documentation, security blogs, and other relevant resources to enhance your understanding and implementation of security practices.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "By updating and revising the control within our Amazon Neptune cluster it would keep the system as secure as possible.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps by removing or updating any IAM roles, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
      "audit_steps": "1. Establish a Security Review Schedule \u2022 Determine a regular schedule for reviewing and updating the security configuration of your Amazon Neptune environment. \u2022 Consider factors such as the frequency of changes, compliance requirements, and industry best practices to determine the appropriate review interval. 2. Monitor AWS Security Bulletins \u2022 Stay informed about AWS security updates and announcements related to Amazon Neptune. \u2022 Regularly review AWS security bulletins and notifications to identify any security patches, updates, or new features relevant to your Neptune environment. \u2022 Take note of any security recommendations or best practices provided by AWS. 3. Review IAM Roles and Policies \u2022 Access the AWS Identity and Access Management (IAM) console by navigating to IAM in the AWS Management Console. \u2022 Review the IAM roles and policies associated with your Neptune resources. \u2022 Ensure that the assigned permissions align with the principle of least privilege and reflect the current access requirements. \u2022 Update the IAM roles and policies as needed to adapt to changes in user access or security requirements.  4. Review Security Groups and Network ACLs \u2022 Access the Amazon Neptune console by navigating to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. \u2022 In the Neptune console, navigate to the Connectivity & Security or Network & Security section. \u2022 Review the security groups and network ACLs associated with your Neptune clusters. \u2022 Ensure that the inbound and outbound rules are up to date and aligned with your security requirements. \u2022 Remove any unnecessary or outdated rules and add new rules if required. 5. Review Encryption Settings \u2022 Navigate to the Configuration section or relevant encryption settings in the Neptune console. \u2022 Review the encryption settings for both encryption at rest and encryption in transit. \u2022 Ensure that the appropriate encryption options and key management strategies are in place. \u2022 Consider rotating encryption keys periodically, following best practices and compliance requirements. 6. Review VPC Configuration \u2022 Access the Amazon VPC console by navigating to VPC in the AWS Management Console. \u2022 Review the VPC configuration associated with your Neptune clusters. \u2022 Ensure the subnets, routing tables, and VPC peering settings are configured correctly. \u2022 Verify that the network architecture provides your Neptune resources' desired isolation and connectivity. 7. Conduct Security Assessments \u2022 Periodically conduct security assessments and penetration testing on your Neptune environment. \u2022 Engage security experts or use appropriate security tools to identify vulnerabilities, weaknesses, or misconfigurations. \u2022 Analyze the assessment results and take necessary actions to remediate any security issues or risks. 8. Stay Up to Date with Best Practices  \u2022 Continuously educate yourself and your team on the latest security best practices for Amazon Neptune. \u2022 Stay informed about emerging security threats and vulnerabilities. -Regularly review AWS documentation, security blogs, and other relevant resources to enhance your understanding and implementation of security practices.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: By updating and revising the control within our Amazon Neptune cluster it would keep the system as secure as possible.",
      "impact": "By updating and revising the control within our Amazon Neptune cluster it would keep the system as secure as possible."
    },
    "function_name": "documentdb_security_configuration_review_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.security.configuration.security_configuration_reviewed"
  },
  {
    "id": "9.7",
    "title": "Ensure Monitoring and Alerting is Enabled",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster on which you want to implement monitoring and alerting. \u2022 Click on the cluster name to access its details page. 4. Set Up Amazon CloudWatch Metrics \u2022 In the cluster details page, navigate to the Monitoring or Metrics section. \u2022 Enable CloudWatch metrics for the Neptune cluster by clicking Enable or Configure. \u2022 Select the desired metrics to monitor, such as CPU utilization, storage usage, or network throughput. \u2022 Choose the appropriate granularity and sampling intervals for the metrics. \u2022 Click Save or Apply Changes to enable CloudWatch metrics for the Neptune cluster. 5. Configure CloudWatch Alarms \u2022 In the CloudWatch console, navigate to Alarms in the left-side menu. \u2022 Click Create alarm to configure alarms based on specific metric thresholds. \u2022 Select the desired metric to monitor and set the threshold values for triggering an alarm. \u2022 Define the actions to be taken when the alarm state changes, such as sending notifications or triggering automated actions.  \u2022 Configure the alarm settings, including alarm name, description, and notification recipients. \u2022 Click Create alarm to save the alarm configuration. 6. Set Up Amazon EventBridge Rules \u2022 In the Amazon EventBridge console, navigate to Rules in the left-side menu. \u2022 Click on Create rule to set up rules for specific events or log entries related to Neptune. \u2022 Define the event pattern or log filter to match the desired events. \u2022 Configure the target actions to be taken when the rule matches an event, such as sending notifications or invoking AWS Lambda functions. \u2022 Specify the rule settings, including rule name, description, and event source. \u2022 Click Create to save the rule configuration. 7. Review and Customize Metrics and Alarms \u2022 Periodically review the metrics and alarms configured for your Neptune cluster. \u2022 Adjust the metric thresholds and alarm settings based on your performance and alerting requirements. \u2022 Consider adding more metrics or alarms as needed to monitor additional aspects of your Neptune environment. 8. Regularly Monitor and Respond to Alerts \u2022 Continuously monitor the CloudWatch metrics and alarm states for your Neptune cluster. \u2022 Respond promptly to any alarms triggered by critical or abnormal conditions. \u2022 Investigate the root causes of the alerts and take appropriate actions to mitigate issues. 9. Utilize Additional Monitoring Tools \u2022 Explore and leverage additional monitoring and observability tools available in the AWS ecosystem, such as Amazon CloudWatch Logs Insights, AWS X-Ray, or third-party monitoring solutions. \u2022 Configure these tools to gather insights and detect any performance or security issues in your Neptune environment.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Sign in to the AWS Management Console \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. 2. Open the Amazon Neptune Console \u2022 Navigate to the service using the Find Services search bar or by directly accessing the console at https://console.aws.amazon.com/neptune/. 3. Select the Neptune Cluster \u2022 Choose the Amazon Neptune cluster on which you want to implement monitoring and alerting. \u2022 Click on the cluster name to access its details page. 4. Set Up Amazon CloudWatch Metrics \u2022 In the cluster details page, navigate to the Monitoring or Metrics section. \u2022 Enable CloudWatch metrics for the Neptune cluster by clicking Enable or Configure. \u2022 Select the desired metrics to monitor, such as CPU utilization, storage usage, or network throughput. \u2022 Choose the appropriate granularity and sampling intervals for the metrics. \u2022 Click Save or Apply Changes to enable CloudWatch metrics for the Neptune cluster. 5. Configure CloudWatch Alarms \u2022 In the CloudWatch console, navigate to Alarms in the left-side menu. \u2022 Click Create alarm to configure alarms based on specific metric thresholds. \u2022 Select the desired metric to monitor and set the threshold values for triggering an alarm. \u2022 Define the actions to be taken when the alarm state changes, such as sending notifications or triggering automated actions.  \u2022 Configure the alarm settings, including alarm name, description, and notification recipients. \u2022 Click Create alarm to save the alarm configuration. 6. Set Up Amazon EventBridge Rules \u2022 In the Amazon EventBridge console, navigate to Rules in the left-side menu. \u2022 Click on Create rule to set up rules for specific events or log entries related to Neptune. \u2022 Define the event pattern or log filter to match the desired events. \u2022 Configure the target actions to be taken when the rule matches an event, such as sending notifications or invoking AWS Lambda functions. \u2022 Specify the rule settings, including rule name, description, and event source. \u2022 Click Create to save the rule configuration. 7. Review and Customize Metrics and Alarms \u2022 Periodically review the metrics and alarms configured for your Neptune cluster. \u2022 Adjust the metric thresholds and alarm settings based on your performance and alerting requirements. \u2022 Consider adding more metrics or alarms as needed to monitor additional aspects of your Neptune environment. 8. Regularly Monitor and Respond to Alerts \u2022 Continuously monitor the CloudWatch metrics and alarm states for your Neptune cluster. \u2022 Respond promptly to any alarms triggered by critical or abnormal conditions. \u2022 Investigate the root causes of the alerts and take appropriate actions to mitigate issues. 9. Utilize Additional Monitoring Tools \u2022 Explore and leverage additional monitoring and observability tools available in the AWS ecosystem, such as Amazon CloudWatch Logs Insights, AWS X-Ray, or third-party monitoring solutions. \u2022 Configure these tools to gather insights and detect any performance or security issues in your Neptune environment.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "",
      "impact": ""
    },
    "function_name": "documentdb_monitoring_alerting_enabled_check",
    "coverage": 90,
    "rule_id": "aws.documentdb.monitoring.alerting.monitoring_alerting_enabled"
  },
  {
    "id": "10.1",
    "title": "Ensure Data Ingestion is Secure",
    "assessment": "Manual",
    "description": "",
    "rationale": "This helps ensure that the system is updated with any potential vulnerabilities that might pose a threat to the organization. Helps authenticate the sources that are coming to the database and ensures that only authorized users have the credential to access the data.",
    "audit": "1. Secure Data Sources Ensure that your data sources are protected with appropriate security measures. Implement secure network configurations, access controls, and authentication mechanisms for your data sources. Apply security patches and updates to your data source systems to prevent vulnerabilities. 2. Use HTTPS or AWS Direct Connect When ingesting data into Timestream, use secure communication protocols such as HTTPS. Encrypt data in transit to protect it from unauthorized interception. Consider using AWS Direct Connect for a dedicated private network connection to Timestream, ensuring data privacy. 3. Implement Client-Side Encryption Encrypt your data before sending it to Timestream using client-side encryption. Use industry-standard encryption algorithms and strong encryption keys to protect the confidentiality of your data. Store and manage the encryption keys securely using AWS Key Management Service (KMS). 4. Authenticate Data Sources Implement authentication mechanisms for your data sources to ensure only authorized sources can ingest data into Timestream. Use mechanisms such as API keys, access tokens, or client certificates to verify the authenticity of the data source. Consider integrating with AWS Identity and Access Management (IAM) for centralized authentication and access control. 5. Validate and Sanitize Data Implement data validation and sanitization mechanisms to prevent injection attacks or malformed data from being ingested into Timestream. Use input validation techniques and enforce data format requirements to ensure  the integrity of the ingested data. Implement data quality checks to identify and handle anomalies or outliers. 6. Monitor Data Ingestion Implement monitoring and logging for data ingestion processes. Regularly review logs and metrics related to data ingestion to detect anomalies or suspicious activities. Set up alarms and notifications for data ingestion failures or unexpected patterns. 7. Regularly Update Data Ingestion Components Keep your data ingestion components, such as APIs, scripts, or connectors, up to date with the latest security patches and updates. Follow safe coding practices and stay informed about security vulnerabilities and fixes specific to your data ingestion tools. 8. Implement Network Security Controls Use network security controls such as security groups, network ACLs, and VPC configurations to restrict access to your Timestream resources. Configure inbound and outbound traffic rules to allow only necessary network connections for data ingestion. Follow the principle of least privilege, granting access only to the required IPs or networks.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Secure Data Sources Ensure that your data sources are protected with appropriate security measures. Implement secure network configurations, access controls, and authentication mechanisms for your data sources. Apply security patches and updates to your data source systems to prevent vulnerabilities. 2. Use HTTPS or AWS Direct Connect When ingesting data into Timestream, use secure communication protocols such as HTTPS. Encrypt data in transit to protect it from unauthorized interception. Consider using AWS Direct Connect for a dedicated private network connection to Timestream, ensuring data privacy. 3. Implement Client-Side Encryption Encrypt your data before sending it to Timestream using client-side encryption. Use industry-standard encryption algorithms and strong encryption keys to protect the confidentiality of your data. Store and manage the encryption keys securely using AWS Key Management Service (KMS). 4. Authenticate Data Sources Implement authentication mechanisms for your data sources to ensure only authorized sources can ingest data into Timestream. Use mechanisms such as API keys, access tokens, or client certificates to verify the authenticity of the data source. Consider integrating with AWS Identity and Access Management (IAM) for centralized authentication and access control. 5. Validate and Sanitize Data Implement data validation and sanitization mechanisms to prevent injection attacks or malformed data from being ingested into Timestream. Use input validation techniques and enforce data format requirements to ensure  the integrity of the ingested data. Implement data quality checks to identify and handle anomalies or outliers. 6. Monitor Data Ingestion Implement monitoring and logging for data ingestion processes. Regularly review logs and metrics related to data ingestion to detect anomalies or suspicious activities. Set up alarms and notifications for data ingestion failures or unexpected patterns. 7. Regularly Update Data Ingestion Components Keep your data ingestion components, such as APIs, scripts, or connectors, up to date with the latest security patches and updates. Follow safe coding practices and stay informed about security vulnerabilities and fixes specific to your data ingestion tools. 8. Implement Network Security Controls Use network security controls such as security groups, network ACLs, and VPC configurations to restrict access to your Timestream resources. Configure inbound and outbound traffic rules to allow only necessary network connections for data ingestion. Follow the principle of least privilege, granting access only to the required IPs or networks.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps ensure that the system is updated with any potential vulnerabilities that might pose a threat to the organization. Helps authenticate the sources that are coming to the database and ensures that only authorized users have the credential to access the data.",
      "impact": ""
    },
    "function_name": "timestream_secure_data_ingestion_check",
    "coverage": 90,
    "rule_id": "aws.timestream.data.ingestion.secure_data_ingestion_enabled"
  },
  {
    "id": "10.2",
    "title": "Ensure Data at Rest is Encrypted",
    "assessment": "Manual",
    "description": "Enable encryption at rest for Amazon Timestream to protect your data while it is stored. Utilize AWS Key Management Service (KMS) to manage and control the encryption keys used for data encryption. Configure Timestream to encrypt your data using AWS- managed keys or customer-managed keys.",
    "rationale": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
    "audit": "1. Understand Encryption at Rest in Timestream Familiarize yourself with the concept of encryption at rest and its importance in securing your data in Timestream. Understand that encryption at rest ensures that your data remains protected even if the underlying storage media is compromised. 2. Create an AWS Key Management Service (KMS) Key Open the AWS Management Console and navigate to the AWS Key Management Service (KMS) service. Create a new KMS customer master key (CMK) or use an existing one to manage the encryption keys for Timestream. Follow the AWS documentation and best practices for creating and managing KMS keys. 3. Enable Encryption at Rest in Timestream Open the Amazon Timestream console. Select the Timestream database or table you want to enable encryption at rest. Click on the \"Encryption\" tab or section. Choose the option to enable encryption at rest. Select the KMS key that you created earlier to be used for encryption. 4. Verify Encryption at Rest Confirm that encryption at rest is enabled for the selected Timestream database or table. Review the encryption settings in the Timestream console to ensure the correct KMS key is associated. 5. Monitor and Audit Encryption at Rest Regularly monitor the encryption at rest status in the Timestream console. Leverage AWS CloudTrail and AWS CloudWatch to monitor and track encryption-related activities or events.  Set up appropriate alerts and notifications to detect any issues or unauthorized changes to the encryption settings. 6. Test Data Access and Decryption Access the Timestream data that is encrypted at rest. Verify that you can retrieve and decrypt the data using the appropriate access controls and KMS key permissions. Perform thorough testing to ensure data access and decryption functions as expected. 7. Review and Update Encryption Configuration Regularly review your encryption configuration and settings for Timestream. Ensure that the appropriate KMS key is still associated with the Timestream resources. Update the encryption settings if necessary, such as rotating encryption keys or modifying key policies.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enable encryption at rest for Amazon Timestream to protect your data while it is stored. Utilize AWS Key Management Service (KMS) to manage and control the encryption keys used for data encryption. Configure Timestream to encrypt your data using AWS- managed keys or customer-managed keys.",
      "audit_steps": "1. Understand Encryption at Rest in Timestream Familiarize yourself with the concept of encryption at rest and its importance in securing your data in Timestream. Understand that encryption at rest ensures that your data remains protected even if the underlying storage media is compromised. 2. Create an AWS Key Management Service (KMS) Key Open the AWS Management Console and navigate to the AWS Key Management Service (KMS) service. Create a new KMS customer master key (CMK) or use an existing one to manage the encryption keys for Timestream. Follow the AWS documentation and best practices for creating and managing KMS keys. 3. Enable Encryption at Rest in Timestream Open the Amazon Timestream console. Select the Timestream database or table you want to enable encryption at rest. Click on the \"Encryption\" tab or section. Choose the option to enable encryption at rest. Select the KMS key that you created earlier to be used for encryption. 4. Verify Encryption at Rest Confirm that encryption at rest is enabled for the selected Timestream database or table. Review the encryption settings in the Timestream console to ensure the correct KMS key is associated. 5. Monitor and Audit Encryption at Rest Regularly monitor the encryption at rest status in the Timestream console. Leverage AWS CloudTrail and AWS CloudWatch to monitor and track encryption-related activities or events.  Set up appropriate alerts and notifications to detect any issues or unauthorized changes to the encryption settings. 6. Test Data Access and Decryption Access the Timestream data that is encrypted at rest. Verify that you can retrieve and decrypt the data using the appropriate access controls and KMS key permissions. Perform thorough testing to ensure data access and decryption functions as expected. 7. Review and Update Encryption Configuration Regularly review your encryption configuration and settings for Timestream. Ensure that the appropriate KMS key is still associated with the Timestream resources. Update the encryption settings if necessary, such as rotating encryption keys or modifying key policies.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
      "impact": ""
    },
    "function_name": "timestream_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.timestream.encryption.at_rest.encryption_at_rest_enabled"
  },
  {
    "id": "10.3",
    "title": "Ensure Encryption in Transit is Configured",
    "assessment": "Manual",
    "description": "Configure your applications or tools to use secure communication protocols when interacting with Amazon Timestream. Utilize endpoints to establish private and secure connections to Timestream.",
    "rationale": "The database uses HTTPS/TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by HTTPS/TLS in order to configure it correctly. Also has an option for leverage, which creates a private connection between virtual private code (VPC) without interfering with public networks. Impact: If the client does not have the code configured correctly it would not be able to connect to the server.",
    "audit": "1. Understand Encryption in Transit in Timestream Familiarize yourself with the concept of encryption in transit and its importance in securing data communication. Understand that encryption in transit ensures that data transmitted between clients and Timestream remains confidential and protected from interception. 2. Use HTTPS for Communication Configure your client applications or tools to communicate with Amazon Timestream over HTTPS. Utilize the HTTPS protocol to establish secure encrypted connections between clients and the Timestream service. Ensure your client applications support the TLS (Transport Layer Security) protocol versions AWS recommends. 3. Leverage AWS PrivateLink (Optional) Consider using AWS PrivateLink to establish private and secure connections between your VPC and Timestream. Configure a VPC endpoint for Timestream to securely access the service without traversing the public internet. 4. Enable SSL/TLS Certificates Obtain and configure valid SSL/TLS certificates for your client applications or tools.  Install the SSL/TLS certificates on your client systems or load balancers. Use the configured certificates to establish secure connections with Timestream. 5. Verify Encryption in Transit Validate that your client applications or tools are using secure communication channels. Verify that HTTPS is being utilized for communication with Timestream. Confirm that SSL/TLS certificates are properly configured and used in communication. 6. Monitor Encryption in Transit Utilize Amazon CloudWatch to monitor the metrics and logs related to your Timestream resources. Set up appropriate alarms and notifications to alert you of any potential security incidents or anomalies in the encryption in transit process. Regularly review the CloudWatch logs and metrics to ensure the integrity and security of the data in transit. 7. Regularly Update Encryption Configuration Stay informed about the latest encryption standards, protocols, and best practices. Regularly review and update your encryption configurations and settings to align with industry standards and security recommendations. Apply any necessary updates or patches to client applications or tools to maintain strong encryption in transit.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the client does not have the code configured correctly it would not be able to connect to the server.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Configure your applications or tools to use secure communication protocols when interacting with Amazon Timestream. Utilize endpoints to establish private and secure connections to Timestream.",
      "audit_steps": "1. Understand Encryption in Transit in Timestream Familiarize yourself with the concept of encryption in transit and its importance in securing data communication. Understand that encryption in transit ensures that data transmitted between clients and Timestream remains confidential and protected from interception. 2. Use HTTPS for Communication Configure your client applications or tools to communicate with Amazon Timestream over HTTPS. Utilize the HTTPS protocol to establish secure encrypted connections between clients and the Timestream service. Ensure your client applications support the TLS (Transport Layer Security) protocol versions AWS recommends. 3. Leverage AWS PrivateLink (Optional) Consider using AWS PrivateLink to establish private and secure connections between your VPC and Timestream. Configure a VPC endpoint for Timestream to securely access the service without traversing the public internet. 4. Enable SSL/TLS Certificates Obtain and configure valid SSL/TLS certificates for your client applications or tools.  Install the SSL/TLS certificates on your client systems or load balancers. Use the configured certificates to establish secure connections with Timestream. 5. Verify Encryption in Transit Validate that your client applications or tools are using secure communication channels. Verify that HTTPS is being utilized for communication with Timestream. Confirm that SSL/TLS certificates are properly configured and used in communication. 6. Monitor Encryption in Transit Utilize Amazon CloudWatch to monitor the metrics and logs related to your Timestream resources. Set up appropriate alarms and notifications to alert you of any potential security incidents or anomalies in the encryption in transit process. Regularly review the CloudWatch logs and metrics to ensure the integrity and security of the data in transit. 7. Regularly Update Encryption Configuration Stay informed about the latest encryption standards, protocols, and best practices. Regularly review and update your encryption configurations and settings to align with industry standards and security recommendations. Apply any necessary updates or patches to client applications or tools to maintain strong encryption in transit.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "The database uses HTTPS/TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by HTTPS/TLS in order to configure it correctly. Also has an option for leverage, which creates a private connection between virtual private code (VPC) without interfering with public networks. Impact: If the client does not have the code configured correctly it would not be able to connect to the server.",
      "impact": "If the client does not have the code configured correctly it would not be able to connect to the server."
    },
    "function_name": "timestream_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.timestream.encryption.in_transit.encryption_in_transit_enabled"
  },
  {
    "id": "10.4",
    "title": "Ensure Access Control and Authentication is Enabled",
    "assessment": "Manual",
    "description": "Utilize AWS Identity and Access Management (IAM) to control access to your Amazon Timestream resources. Define IAM policies that grant or deny permissions for specific Timestream actions and resources.",
    "rationale": "Users should select whether they like to enable authentication. If they want to authenticate the user would be required to implement IAM roles would grant or deny permissions within that database. Users also have an option to enable multi-factor authentication, which adds an extra layer of security restricting access to unauthorized users. Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "audit": "1. Understand AWS Identity and Access Management (IAM) Familiarize yourself with IAM, the AWS service used to manage access to AWS resources. Understand IAM users, groups, roles, policies, and permissions, essential for access control in Timestream. 2. Create IAM Users, Groups, and Roles Access the AWS Management Console and navigate to the IAM service. Create IAM users, groups, and roles based on your organization's access control requirements for Timestream. Define appropriate permissions for these entities, limiting access to specific Timestream actions and resources. 3. Assign IAM Policies Create IAM policies that define the desired level of access to Timestream. Associate these policies with the respective IAM users, groups, and roles created earlier. Ensure that the policies provide the necessary permissions for users to interact with Timestream resources. 4. Use IAM Roles for External Applications If you have external applications or services accessing Timestream, create IAM roles specific to those applications.  Define the necessary permissions in the IAM roles and grant them to the respective applications or services. Configure the applications or services to assume these IAM roles when accessing Timestream. 5. Enable Multi-Factor Authentication (MFA) Enable MFA for IAM users who require access to Timestream. Configure MFA devices and enforce MFA usage for these users. MFA adds an extra layer of security by requiring an additional authentication factor during the login process. 6. Implement AWS Identity Federation (Optional) Consider implementing AWS Identity Federation if you need to grant access to Timestream to users from external identity providers. Configure the necessary trust relationships and establish a federation between the external identity provider and AWS. Ensure that the federated users have the appropriate IAM policies and permissions for Timestream. 7. Regularly Review and Update Access Controls Periodically review and update the IAM policies and permissions for Timestream. Remove unnecessary access permissions and ensure access controls align with your organization's security requirements. Monitor IAM activity logs and AWS CloudTrail to identify unauthorized access attempts or unusual activities.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Utilize AWS Identity and Access Management (IAM) to control access to your Amazon Timestream resources. Define IAM policies that grant or deny permissions for specific Timestream actions and resources.",
      "audit_steps": "1. Understand AWS Identity and Access Management (IAM) Familiarize yourself with IAM, the AWS service used to manage access to AWS resources. Understand IAM users, groups, roles, policies, and permissions, essential for access control in Timestream. 2. Create IAM Users, Groups, and Roles Access the AWS Management Console and navigate to the IAM service. Create IAM users, groups, and roles based on your organization's access control requirements for Timestream. Define appropriate permissions for these entities, limiting access to specific Timestream actions and resources. 3. Assign IAM Policies Create IAM policies that define the desired level of access to Timestream. Associate these policies with the respective IAM users, groups, and roles created earlier. Ensure that the policies provide the necessary permissions for users to interact with Timestream resources. 4. Use IAM Roles for External Applications If you have external applications or services accessing Timestream, create IAM roles specific to those applications.  Define the necessary permissions in the IAM roles and grant them to the respective applications or services. Configure the applications or services to assume these IAM roles when accessing Timestream. 5. Enable Multi-Factor Authentication (MFA) Enable MFA for IAM users who require access to Timestream. Configure MFA devices and enforce MFA usage for these users. MFA adds an extra layer of security by requiring an additional authentication factor during the login process. 6. Implement AWS Identity Federation (Optional) Consider implementing AWS Identity Federation if you need to grant access to Timestream to users from external identity providers. Configure the necessary trust relationships and establish a federation between the external identity provider and AWS. Ensure that the federated users have the appropriate IAM policies and permissions for Timestream. 7. Regularly Review and Update Access Controls Periodically review and update the IAM policies and permissions for Timestream. Remove unnecessary access permissions and ensure access controls align with your organization's security requirements. Monitor IAM activity logs and AWS CloudTrail to identify unauthorized access attempts or unusual activities.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Users should select whether they like to enable authentication. If they want to authenticate the user would be required to implement IAM roles would grant or deny permissions within that database. Users also have an option to enable multi-factor authentication, which adds an extra layer of security restricting access to unauthorized users. Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
      "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data."
    },
    "function_name": "timestream_access_control_authentication_check",
    "coverage": 90,
    "rule_id": "aws.timestream.authentication.access_control.access_control_authentication_enabled"
  },
  {
    "id": "10.5",
    "title": "Ensure Fine-Grained Access Control is Enabled",
    "assessment": "Manual",
    "description": "Leverage Timestream's fine-grained access control capabilities to control table or row level access. Define access policies that limit access to specific tables, columns, or rows based on user roles or conditions. Implement data filtering and row-level security to restrict access to sensitive information.",
    "rationale": "This helps by having specific permissions which can be denied due to multiple conditions of the database. This allows the user to control certain aspects of the database. Impact: This adds an extra layer for users to sign into with their credentials to the database.",
    "audit": "1. Understand Fine-Grained Access Control in Timestream Familiarize yourself with the concept of fine-grained access control and its benefits in Timestream. Understand that fine-grained access control allows you to control access to specific tables, columns, or rows within Timestream databases. 2. Define Timestream Database and Tables Create the necessary Timestream databases and tables that will be used for fine- grained access control. Design your database schema and define the tables, columns, and rows that need granular access control. 3. Create IAM Policies for Fine-Grained Access Access the AWS Management Console and navigate to the IAM service. Define IAM policies that grant or deny permissions for specific Timestream actions, databases, tables, columns, or rows. Leverage Timestream's fine-grained access control policy language to specify the conditions and restrictions for access. 4. Assign IAM Policies to IAM Users, Groups, or Roles Associate the IAM policies created earlier with the respective IAM users, groups, or roles. Assign the appropriate policies to grant access to specific Timestream databases, tables, columns, or rows. Follow the principle of least privilege and provide only the necessary permissions to users based on their requirements.  5. Test Fine-Grained Access Control Validate the fine-grained access control settings by attempting different actions on Timestream databases, tables, columns, or rows. Verify that the defined policies accurately restrict or allow access based on the specified conditions. Perform thorough testing to enforce the expected granularity and security level. 6. Regularly Review and Update Access Policies Periodically review the fine-grained access control policies to ensure they align with your organization's security requirements. Remove any unnecessary or outdated policies. Regularly monitor IAM activity logs and AWS CloudTrail to identify any unauthorized access attempts or unusual activities related to fine-grained access control.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "This adds an extra layer for users to sign into with their credentials to the database.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Leverage Timestream's fine-grained access control capabilities to control table or row level access. Define access policies that limit access to specific tables, columns, or rows based on user roles or conditions. Implement data filtering and row-level security to restrict access to sensitive information.",
      "audit_steps": "1. Understand Fine-Grained Access Control in Timestream Familiarize yourself with the concept of fine-grained access control and its benefits in Timestream. Understand that fine-grained access control allows you to control access to specific tables, columns, or rows within Timestream databases. 2. Define Timestream Database and Tables Create the necessary Timestream databases and tables that will be used for fine- grained access control. Design your database schema and define the tables, columns, and rows that need granular access control. 3. Create IAM Policies for Fine-Grained Access Access the AWS Management Console and navigate to the IAM service. Define IAM policies that grant or deny permissions for specific Timestream actions, databases, tables, columns, or rows. Leverage Timestream's fine-grained access control policy language to specify the conditions and restrictions for access. 4. Assign IAM Policies to IAM Users, Groups, or Roles Associate the IAM policies created earlier with the respective IAM users, groups, or roles. Assign the appropriate policies to grant access to specific Timestream databases, tables, columns, or rows. Follow the principle of least privilege and provide only the necessary permissions to users based on their requirements.  5. Test Fine-Grained Access Control Validate the fine-grained access control settings by attempting different actions on Timestream databases, tables, columns, or rows. Verify that the defined policies accurately restrict or allow access based on the specified conditions. Perform thorough testing to enforce the expected granularity and security level. 6. Regularly Review and Update Access Policies Periodically review the fine-grained access control policies to ensure they align with your organization's security requirements. Remove any unnecessary or outdated policies. Regularly monitor IAM activity logs and AWS CloudTrail to identify any unauthorized access attempts or unusual activities related to fine-grained access control.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps by having specific permissions which can be denied due to multiple conditions of the database. This allows the user to control certain aspects of the database. Impact: This adds an extra layer for users to sign into with their credentials to the database.",
      "impact": "This adds an extra layer for users to sign into with their credentials to the database."
    },
    "function_name": "timestream_fine_grained_access_control_check",
    "coverage": 90,
    "rule_id": "aws.timestream.access.control.fine_grained_access_control_enabled"
  },
  {
    "id": "10.6",
    "title": "Ensure Audit Logging is Enabled",
    "assessment": "Manual",
    "description": "Enable AWS CloudTrail to capture and log API calls and activities related to Amazon Timestream. Configure CloudTrail to store the logs in a secure location and regularly review the logs for any unauthorized or suspicious activities.",
    "rationale": "This captures and saves logs of activities that took place in the database. Impact: This reduces risks of any fraud since worker activity is being monitored and tracked.",
    "audit": "1. Understand Audit Logging in Timestream Familiarize yourself with audit logging and its importance in monitoring and tracking activities in Timestream. Understand that audit logs capture API calls and events related to Timestream actions and resources. 2. Enable AWS CloudTrail Access the AWS Management Console and navigate to the AWS CloudTrail service. Create a new CloudTrail trail or use an existing one to capture Timestream audit logs. Configure the trail to include Timestream as a data source for logging. 3. Configure CloudTrail Logging Options Specify the desired settings for the CloudTrail trail, such as the S3 bucket to store the audit logs and the log file encryption options. Enable logging of management and data events related to Timestream. Configure the trail to capture the necessary information for your audit and compliance requirements. 4. Set Up CloudTrail Notifications and Alerts Configure CloudTrail to send notifications or trigger actions based on specific events or conditions. Set up CloudWatch Alarms to monitor and receive notifications for critical Timestream audit events. Define the appropriate alert thresholds and actions to respond to specific events. 5. Access and Review Audit Logs Access the configured S3 bucket where the Timestream audit logs are stored. Retrieve and review the logs using AWS Management Console, AWS CLI, or any  preferred log analysis tools. Analyze the audit logs to track Timestream activities, detect anomalies, and investigate security incidents. 6. Retention and Compliance Considerations Determine the appropriate retention period for your Timestream audit logs based on compliance and regulatory requirements. Implement appropriate data lifecycle management policies for your audit logs stored in the S3 bucket. Ensure compliance with data protection and privacy regulations applicable to your organization. 7. Regularly Review and Monitor Audit Logs Establish a regular review process for your Timestream audit logs. Monitor the logs for unauthorized access attempts, unusual activities, or policy violations. Respond promptly to any identified security incidents or anomalies.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "This reduces risks of any fraud since worker activity is being monitored and tracked.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enable AWS CloudTrail to capture and log API calls and activities related to Amazon Timestream. Configure CloudTrail to store the logs in a secure location and regularly review the logs for any unauthorized or suspicious activities.",
      "audit_steps": "1. Understand Audit Logging in Timestream Familiarize yourself with audit logging and its importance in monitoring and tracking activities in Timestream. Understand that audit logs capture API calls and events related to Timestream actions and resources. 2. Enable AWS CloudTrail Access the AWS Management Console and navigate to the AWS CloudTrail service. Create a new CloudTrail trail or use an existing one to capture Timestream audit logs. Configure the trail to include Timestream as a data source for logging. 3. Configure CloudTrail Logging Options Specify the desired settings for the CloudTrail trail, such as the S3 bucket to store the audit logs and the log file encryption options. Enable logging of management and data events related to Timestream. Configure the trail to capture the necessary information for your audit and compliance requirements. 4. Set Up CloudTrail Notifications and Alerts Configure CloudTrail to send notifications or trigger actions based on specific events or conditions. Set up CloudWatch Alarms to monitor and receive notifications for critical Timestream audit events. Define the appropriate alert thresholds and actions to respond to specific events. 5. Access and Review Audit Logs Access the configured S3 bucket where the Timestream audit logs are stored. Retrieve and review the logs using AWS Management Console, AWS CLI, or any  preferred log analysis tools. Analyze the audit logs to track Timestream activities, detect anomalies, and investigate security incidents. 6. Retention and Compliance Considerations Determine the appropriate retention period for your Timestream audit logs based on compliance and regulatory requirements. Implement appropriate data lifecycle management policies for your audit logs stored in the S3 bucket. Ensure compliance with data protection and privacy regulations applicable to your organization. 7. Regularly Review and Monitor Audit Logs Establish a regular review process for your Timestream audit logs. Monitor the logs for unauthorized access attempts, unusual activities, or policy violations. Respond promptly to any identified security incidents or anomalies.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This captures and saves logs of activities that took place in the database. Impact: This reduces risks of any fraud since worker activity is being monitored and tracked.",
      "impact": "This reduces risks of any fraud since worker activity is being monitored and tracked."
    },
    "function_name": "timestream_audit_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.timestream.logging.audit.audit_logging_enabled"
  },
  {
    "id": "10.7",
    "title": "Ensure Regular Updates and Patches are Installed",
    "assessment": "Manual",
    "description": "Stay updated with the latest security patches and updates provided by AWS for Amazon Timestream. Follow AWS security best practices and recommendations to ensure your Timestream implementation remains secure.",
    "rationale": "Impact: This helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack.",
    "audit": "1. Stay Informed about Updates Stay updated with the latest announcements and releases related to Amazon Timestream. Subscribe to AWS notifications, blogs, and forums to learn about new features, enhancements, and security patches. 2. Review AWS Documentation Regularly review the official AWS documentation for Amazon Timestream. Pay attention to any updates or recommendations related to security, performance, and best practices. 3. Implement a Patch Management Process Establish a patch management process specific to Amazon Timestream within your organization. Define roles and responsibilities for managing patches, including testing and deployment procedures. 4. Test Patches in a Non-Production Environment Before deploying patches in production, create a non-production environment to test the patches. Set up a replica or a sandbox environment that resembles your production environment. Test the patches thoroughly to ensure they do not introduce compatibility issues or adverse effects. 5. Schedule Patching Maintenance Windows Identify suitable maintenance windows to apply patches to your Timestream resources. Consider the impact on system availability and plan the maintenance window  accordingly. Coordinate with relevant teams and stakeholders to ensure minimal disruption during the patching process. 6. Apply Patches Once you have successfully tested the patches in the non-production environment and scheduled a maintenance window. Apply the patches to your production Timestream resources. Follow the recommended patching procedures provided by AWS in the documentation. Ensure you follow any specific instructions or requirements for applying patches to Timestream. 7. Verify Patch Deployment After applying patches, monitor the Timestream resources to ensure they function as expected. Conduct thorough testing to validate that the patched resources operate correctly and have not introduced any issues. 8. Regularly Monitor for Updates Continuously monitor for new updates, patches, and security bulletins related to Amazon Timestream. Stay informed about any vulnerabilities or critical patches that require immediate attention. Adjust your patch management process and schedule to incorporate new updates and releases. 9. Automate Patch Management (Optional) Consider automating the patch management process using AWS tools or third- party solutions. Implement automation scripts or systems that handle patch deployments, testing, and monitoring.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "This helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Stay updated with the latest security patches and updates provided by AWS for Amazon Timestream. Follow AWS security best practices and recommendations to ensure your Timestream implementation remains secure.",
      "audit_steps": "1. Stay Informed about Updates Stay updated with the latest announcements and releases related to Amazon Timestream. Subscribe to AWS notifications, blogs, and forums to learn about new features, enhancements, and security patches. 2. Review AWS Documentation Regularly review the official AWS documentation for Amazon Timestream. Pay attention to any updates or recommendations related to security, performance, and best practices. 3. Implement a Patch Management Process Establish a patch management process specific to Amazon Timestream within your organization. Define roles and responsibilities for managing patches, including testing and deployment procedures. 4. Test Patches in a Non-Production Environment Before deploying patches in production, create a non-production environment to test the patches. Set up a replica or a sandbox environment that resembles your production environment. Test the patches thoroughly to ensure they do not introduce compatibility issues or adverse effects. 5. Schedule Patching Maintenance Windows Identify suitable maintenance windows to apply patches to your Timestream resources. Consider the impact on system availability and plan the maintenance window  accordingly. Coordinate with relevant teams and stakeholders to ensure minimal disruption during the patching process. 6. Apply Patches Once you have successfully tested the patches in the non-production environment and scheduled a maintenance window. Apply the patches to your production Timestream resources. Follow the recommended patching procedures provided by AWS in the documentation. Ensure you follow any specific instructions or requirements for applying patches to Timestream. 7. Verify Patch Deployment After applying patches, monitor the Timestream resources to ensure they function as expected. Conduct thorough testing to validate that the patched resources operate correctly and have not introduced any issues. 8. Regularly Monitor for Updates Continuously monitor for new updates, patches, and security bulletins related to Amazon Timestream. Stay informed about any vulnerabilities or critical patches that require immediate attention. Adjust your patch management process and schedule to incorporate new updates and releases. 9. Automate Patch Management (Optional) Consider automating the patch management process using AWS tools or third- party solutions. Implement automation scripts or systems that handle patch deployments, testing, and monitoring.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: This helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack.",
      "impact": "This helps the organization reduce their security risk by regularly updating and patching their database and database engine. Regularly updating and scanning for any weaknesses in the company can bring up possible vulnerabilities that could have led to potential cyber-attack."
    },
    "function_name": "timestream_regular_updates_patches_check",
    "coverage": 90,
    "rule_id": "aws.timestream.maintenance.updates.regular_updates_patches_enabled"
  },
  {
    "id": "10.8",
    "title": "Ensure Monitoring and Alerting is Enabled",
    "assessment": "Manual",
    "description": "Utilize Amazon CloudWatch to monitor key metrics, events, and logs related to Amazon Timestream. Set up appropriate alarms and notifications to detect security incidents or abnormal behavior proactively.",
    "rationale": "This helps the individual know what is being logged within the activity and determine what the next step should be if they spot any anomalies.",
    "audit": "1. Define Monitoring Objectives Determine the key metrics, events, and logs you want to monitor in Amazon Timestream. Identify the specific monitoring requirements based on your use case, workload, and business needs. 2. Choose Monitoring Tools Evaluate the available monitoring tools for Amazon Timestream, such as AWS CloudWatch, third-party monitoring solutions, or custom-built monitoring systems. Select the monitoring tool that best aligns with your monitoring objectives and requirements. 3. Configure CloudWatch Metrics Utilize Amazon CloudWatch to monitor key performance metrics of Timestream. Enable and configure CloudWatch metrics such as database CPU utilization, storage usage, query latency, and other relevant metrics. Set appropriate thresholds for these metrics to trigger alarms and notifications. 4. Create CloudWatch Alarms Set up CloudWatch alarms based on your defined thresholds and monitoring objectives. Define the conditions that trigger the alarms, such as CPU utilization exceeding a certain percentage or query latency exceeding a specific threshold. Configure the notification actions for the alarms, such as sending notifications via email, SMS, or triggering automated actions. 5. Enable Enhanced Monitoring (Optional) Consider enabling enhanced monitoring for Timestream, which provides more detailed performance metrics. Configure the enhanced monitoring settings to collect additional metrics that provide deeper insights into the health and performance of Timestream. 6. Configure Log Streams and Filters Enable Timestream's integration with AWS CloudWatch Logs.  Configure log streams and filters to capture and centralize Timestream logs into CloudWatch Logs. Define relevant log filters to extract and track specific log events for monitoring purposes. 7. Regularly Review and Analyze Monitoring Data Continuously review the monitoring data and metrics CloudWatch provides or your chosen monitoring tool. Analyze the data to identify performance bottlenecks, anomalies, or issues in your Timestream implementation. Take necessary actions based on the monitoring insights to optimize performance, improve resource utilization, or troubleshoot issues. 8. Periodically Review and Adjust Monitoring Configuration Regularly review your monitoring configuration to ensure it aligns with your evolving requirements and workload. Adjust your monitoring setup, such as adding or modifying metrics, updating alarm thresholds, or incorporating new log filters.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Utilize Amazon CloudWatch to monitor key metrics, events, and logs related to Amazon Timestream. Set up appropriate alarms and notifications to detect security incidents or abnormal behavior proactively.",
      "audit_steps": "1. Define Monitoring Objectives Determine the key metrics, events, and logs you want to monitor in Amazon Timestream. Identify the specific monitoring requirements based on your use case, workload, and business needs. 2. Choose Monitoring Tools Evaluate the available monitoring tools for Amazon Timestream, such as AWS CloudWatch, third-party monitoring solutions, or custom-built monitoring systems. Select the monitoring tool that best aligns with your monitoring objectives and requirements. 3. Configure CloudWatch Metrics Utilize Amazon CloudWatch to monitor key performance metrics of Timestream. Enable and configure CloudWatch metrics such as database CPU utilization, storage usage, query latency, and other relevant metrics. Set appropriate thresholds for these metrics to trigger alarms and notifications. 4. Create CloudWatch Alarms Set up CloudWatch alarms based on your defined thresholds and monitoring objectives. Define the conditions that trigger the alarms, such as CPU utilization exceeding a certain percentage or query latency exceeding a specific threshold. Configure the notification actions for the alarms, such as sending notifications via email, SMS, or triggering automated actions. 5. Enable Enhanced Monitoring (Optional) Consider enabling enhanced monitoring for Timestream, which provides more detailed performance metrics. Configure the enhanced monitoring settings to collect additional metrics that provide deeper insights into the health and performance of Timestream. 6. Configure Log Streams and Filters Enable Timestream's integration with AWS CloudWatch Logs.  Configure log streams and filters to capture and centralize Timestream logs into CloudWatch Logs. Define relevant log filters to extract and track specific log events for monitoring purposes. 7. Regularly Review and Analyze Monitoring Data Continuously review the monitoring data and metrics CloudWatch provides or your chosen monitoring tool. Analyze the data to identify performance bottlenecks, anomalies, or issues in your Timestream implementation. Take necessary actions based on the monitoring insights to optimize performance, improve resource utilization, or troubleshoot issues. 8. Periodically Review and Adjust Monitoring Configuration Regularly review your monitoring configuration to ensure it aligns with your evolving requirements and workload. Adjust your monitoring setup, such as adding or modifying metrics, updating alarm thresholds, or incorporating new log filters.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps the individual know what is being logged within the activity and determine what the next step should be if they spot any anomalies.",
      "impact": ""
    },
    "function_name": "timestream_monitoring_alerting_enabled_check",
    "coverage": 90,
    "rule_id": "aws.timestream.monitoring.alerting.monitoring_alerting_enabled"
  },
  {
    "id": "10.9",
    "title": "Ensure to Review and Update the Security Configuration",
    "assessment": "Manual",
    "description": "Conduct regular security reviews and assessments of your Amazon Timestream implementation. Evaluate access permissions, encryption settings, and security controls to ensure they align with your organization's security requirements.",
    "rationale": "By regularly reviewing security configuration it helps the businesses to detect any threat they might be hindering and address the threat in a timely manner. Impact: This helps by reviewing the database factors from database engine, review instance details, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
    "audit": "1. Understand Security Best Practices Familiarize yourself with the security best practices and recommendations provided by AWS for Timestream. Stay updated with the latest security guidelines and recommendations from AWS. 2. Review IAM Policies Regularly review the IAM policies associated with Timestream resources. Ensure that the assigned IAM policies provide the necessary permissions for users and roles while adhering to the principle of least privilege. 3. Audit User Access Periodically review the list of users and roles that have access to Timestream. Remove any unnecessary or unused accounts or permissions to minimize the attack surface. 4. Monitor Access Patterns Utilize AWS CloudTrail and Amazon CloudWatch logs to monitor access patterns and activities related to Timestream. Set up alerts and notifications to detect any suspicious or unauthorized access attempts. 5. Implement Security Controls Continuously assess and evaluate the security controls in place for Timestream.  Implement additional security measures, such as VPC peering, security groups, or network ACLs, to further secure access to Timestream resources. 6. Regularly Review Security Group Rules Regularly review the security group rules associated with Timestream instances. Remove any unnecessary open ports or protocols to minimize potential attack vectors. 7. Stay Informed about Security Updates Keep track of security updates, patches, and new features released by AWS for Timestream. Stay informed about any security vulnerabilities or fixes related to Timestream. 8. Conduct Security Assessments Perform periodic security assessments on your Timestream implementation, including vulnerability and penetration testing. Identify and remediate any security vulnerabilities or weaknesses discovered during the assessments. 9. Stay Compliant Regularly review and update your security configurations to meet compliance requirements and industry standards. Stay informed about any changes in compliance regulations that may impact your Timestream environment. 10. Educate and Train Provide regular security awareness training to users and administrators working with Timestream. Ensure that everyone involved understands their security responsibilities and follows security best practices.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "This helps by reviewing the database factors from database engine, review instance details, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Conduct regular security reviews and assessments of your Amazon Timestream implementation. Evaluate access permissions, encryption settings, and security controls to ensure they align with your organization's security requirements.",
      "audit_steps": "1. Understand Security Best Practices Familiarize yourself with the security best practices and recommendations provided by AWS for Timestream. Stay updated with the latest security guidelines and recommendations from AWS. 2. Review IAM Policies Regularly review the IAM policies associated with Timestream resources. Ensure that the assigned IAM policies provide the necessary permissions for users and roles while adhering to the principle of least privilege. 3. Audit User Access Periodically review the list of users and roles that have access to Timestream. Remove any unnecessary or unused accounts or permissions to minimize the attack surface. 4. Monitor Access Patterns Utilize AWS CloudTrail and Amazon CloudWatch logs to monitor access patterns and activities related to Timestream. Set up alerts and notifications to detect any suspicious or unauthorized access attempts. 5. Implement Security Controls Continuously assess and evaluate the security controls in place for Timestream.  Implement additional security measures, such as VPC peering, security groups, or network ACLs, to further secure access to Timestream resources. 6. Regularly Review Security Group Rules Regularly review the security group rules associated with Timestream instances. Remove any unnecessary open ports or protocols to minimize potential attack vectors. 7. Stay Informed about Security Updates Keep track of security updates, patches, and new features released by AWS for Timestream. Stay informed about any security vulnerabilities or fixes related to Timestream. 8. Conduct Security Assessments Perform periodic security assessments on your Timestream implementation, including vulnerability and penetration testing. Identify and remediate any security vulnerabilities or weaknesses discovered during the assessments. 9. Stay Compliant Regularly review and update your security configurations to meet compliance requirements and industry standards. Stay informed about any changes in compliance regulations that may impact your Timestream environment. 10. Educate and Train Provide regular security awareness training to users and administrators working with Timestream. Ensure that everyone involved understands their security responsibilities and follows security best practices.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "By regularly reviewing security configuration it helps the businesses to detect any threat they might be hindering and address the threat in a timely manner. Impact: This helps by reviewing the database factors from database engine, review instance details, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions.",
      "impact": "This helps by reviewing the database factors from database engine, review instance details, security networks, encryption settings, audit logging, and authentication. By updating or removing a few things from these lists it helps tighten security and ensures that the users do not have excessive permissions."
    },
    "function_name": "timestream_security_configuration_review_check",
    "coverage": 90,
    "rule_id": "aws.timestream.security.configuration.security_configuration_reviewed"
  },
  {
    "id": "11.1",
    "title": "Ensure to Implement Identity and Access Management (IAM)",
    "assessment": "Manual",
    "description": "This control is important because by having IAM roles implemented in the database it only allows certain people who are authenticated into the database to modify the database and would not give access to unauthorized personnel. This ensures that the data is being protected from any threat actor.",
    "rationale": "Impact: Only authorized personnel can access the database and configure the applications by using their IAM credentials. If the user credentials are compromised by an unauthorized user, it would limit them to access specific areas within the database due to the leverage IAM roles established.",
    "audit": "1. Understand IAM and QLDB Integration \u2022 Familiarize yourself with IAM and its role in controlling access to AWS services, including QLDB. \u2022 Understand how IAM policies define permissions and access control rules for QLDB resources. 2. Define IAM Users and Groups \u2022 Identify the users and groups that will need access to QLDB. \u2022 Create IAM user accounts for individuals who require direct access to QLDB. \u2022 Create IAM groups to organize users based on their roles or responsibilities logically. 3. Define IAM Policies \u2022 Determine the permissions and actions users and groups need to perform on QLDB resources. \u2022 Create custom IAM policies or leverage existing IAM-managed policies to define these permissions. \u2022 Consider the principle of least privilege and grant only the necessary permissions for each user or group.  4. Attach IAM Policies to Users and Groups \u2022 Associate the appropriate IAM policies with the IAM users and groups. \u2022 Ensure that each user or group has the necessary permissions to perform their tasks on QLDB. \u2022 Regularly review and update the assigned policies as access requirements evolve. 5. Leverage IAM Roles \u2022 Identify AWS services or applications that require access to QLDB. \u2022 Create IAM roles to provide temporary credentials and permissions for these services. \u2022 Define trust relationships and establish the necessary permissions in the IAM role policies. 6. Enable IAM Database Authentication \u2022 Configure IAM database authentication for QLDB to allow users to authenticate using their IAM credentials. \u2022 Enable the appropriate IAM authentication option in the QLDB configuration. \u2022 Configure your applications or clients to use IAM credentials when connecting to QLDB. 7. Test IAM Access \u2022 Use IAM user credentials to log in and test the access to QLDB. \u2022 Verify that users can perform their intended actions based on their assigned IAM policies. \u2022 Test IAM roles and authentication for applications or services requiring access to QLDB. 8. Monitor and Audit IAM Activity \u2022 Monitor IAM activity logs using AWS CloudTrail. \u2022 Set up appropriate CloudTrail trails to capture IAM-related events and API calls. \u2022 Regularly review IAM logs for any unauthorized access attempts or suspicious activities. 9. Regularly Review and Update IAM Configuration \u2022 Periodically review the IAM policies, users, groups, and roles associated with QLDB. \u2022 Ensure access is granted based on business requirements and follows the principle of least privilege.  \u2022 Remove or update IAM configurations when users or roles are no longer required.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Only authorized personnel can access the database and configure the applications by using their IAM credentials. If the user credentials are compromised by an unauthorized user, it would limit them to access specific areas within the database due to the leverage IAM roles established.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This control is important because by having IAM roles implemented in the database it only allows certain people who are authenticated into the database to modify the database and would not give access to unauthorized personnel. This ensures that the data is being protected from any threat actor.",
      "audit_steps": "1. Understand IAM and QLDB Integration \u2022 Familiarize yourself with IAM and its role in controlling access to AWS services, including QLDB. \u2022 Understand how IAM policies define permissions and access control rules for QLDB resources. 2. Define IAM Users and Groups \u2022 Identify the users and groups that will need access to QLDB. \u2022 Create IAM user accounts for individuals who require direct access to QLDB. \u2022 Create IAM groups to organize users based on their roles or responsibilities logically. 3. Define IAM Policies \u2022 Determine the permissions and actions users and groups need to perform on QLDB resources. \u2022 Create custom IAM policies or leverage existing IAM-managed policies to define these permissions. \u2022 Consider the principle of least privilege and grant only the necessary permissions for each user or group.  4. Attach IAM Policies to Users and Groups \u2022 Associate the appropriate IAM policies with the IAM users and groups. \u2022 Ensure that each user or group has the necessary permissions to perform their tasks on QLDB. \u2022 Regularly review and update the assigned policies as access requirements evolve. 5. Leverage IAM Roles \u2022 Identify AWS services or applications that require access to QLDB. \u2022 Create IAM roles to provide temporary credentials and permissions for these services. \u2022 Define trust relationships and establish the necessary permissions in the IAM role policies. 6. Enable IAM Database Authentication \u2022 Configure IAM database authentication for QLDB to allow users to authenticate using their IAM credentials. \u2022 Enable the appropriate IAM authentication option in the QLDB configuration. \u2022 Configure your applications or clients to use IAM credentials when connecting to QLDB. 7. Test IAM Access \u2022 Use IAM user credentials to log in and test the access to QLDB. \u2022 Verify that users can perform their intended actions based on their assigned IAM policies. \u2022 Test IAM roles and authentication for applications or services requiring access to QLDB. 8. Monitor and Audit IAM Activity \u2022 Monitor IAM activity logs using AWS CloudTrail. \u2022 Set up appropriate CloudTrail trails to capture IAM-related events and API calls. \u2022 Regularly review IAM logs for any unauthorized access attempts or suspicious activities. 9. Regularly Review and Update IAM Configuration \u2022 Periodically review the IAM policies, users, groups, and roles associated with QLDB. \u2022 Ensure access is granted based on business requirements and follows the principle of least privilege.  \u2022 Remove or update IAM configurations when users or roles are no longer required.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Only authorized personnel can access the database and configure the applications by using their IAM credentials. If the user credentials are compromised by an unauthorized user, it would limit them to access specific areas within the database due to the leverage IAM roles established.",
      "impact": "Only authorized personnel can access the database and configure the applications by using their IAM credentials. If the user credentials are compromised by an unauthorized user, it would limit them to access specific areas within the database due to the leverage IAM roles established."
    },
    "function_name": "memorydb_iam_implemented_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.iam.implementation.iam_implemented"
  },
  {
    "id": "11.2",
    "title": "Ensure Network Access is Secure",
    "assessment": "Manual",
    "description": "By applying certain network access such as Virtual Private Cloud (VPC) it protects the private network that has been established from any external networks from interfering. It allows internal networks to communicate with one another with the network that has been established. The Access Control List (ACLs) allows only specific individuals to access the resources. Also, by monitoring and logging the activity within the database it helps the individual know what is being logged within the activity and determine what next step they should take to address it.",
    "rationale": "Impact: Setting these certain rules in your network provides a strong security and prevents the organization suffering a ransomware attack.",
    "audit": "1. Deploy QLDB in a VPC \u2022 Create a Virtual Private Cloud (VPC) to isolate your QLDB resources. \u2022 Define the network CIDR blocks, subnets, and routing configurations for the VPC. \u2022 Ensure that the VPC is correctly configured with appropriate network access controls. 2. Configure Security Groups \u2022 Create security groups within your VPC to control inbound and outbound traffic to QLDB. \u2022 Determine the necessary protocols and ports for QLDB access. \u2022 Configure security group rules to allow access from trusted sources, such as specific IP ranges or other security groups within your VPC. 3. Set Up Network ACLs \u2022 Configure Network Access Control Lists (ACLs) within your VPC to provide an additional layer of network security. \u2022 Define inbound and outbound rules in the ACLs to allow or deny specific traffic based on IP addresses, ports, or protocols.  \u2022 Review and adjust the ACL rules to align with your organization's security policies and requirements. 4. Use VPC Endpoints or PrivateLink \u2022 Consider using VPC endpoints or AWS PrivateLink to securely access QLDB without traversing the public internet. \u2022 Set up a VPC endpoint for QLDB to allow private connectivity within your VPC. \u2022 Configure the routing and security group rules to enable traffic flow through the VPC endpoint or PrivateLink. 5. Secure External Connections \u2022 If external connections to QLDB are required, implement secure access methods such as Virtual Private Network (VPN) or AWS Direct Connect. \u2022 Configure VPN connections or Direct Connect links to establish encrypted and private connectivity between your on-premises network and the VPC hosting QLDB. \u2022 Apply appropriate security measures, such as strong authentication and encryption, to protect data transmitted over external connections. 6. Enable Logging and Monitoring \u2022 Enable logging for QLDB to capture important system events and database activity. \u2022 Utilize services like Amazon CloudWatch Logs to centralize and analyze QLDB logs. \u2022 Set up appropriate alarms and notifications to alert you of any suspicious network activity or potential security incidents. 7. Regularly Review and Update Network Security \u2022 Regularly review your VPC configurations, security groups, and network ACLs. \u2022 Stay informed about AWS security best practices and recommendations. \u2022 Update your network security measures as needed to address emerging threats or changes in your security requirements.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Setting these certain rules in your network provides a strong security and prevents the organization suffering a ransomware attack.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "By applying certain network access such as Virtual Private Cloud (VPC) it protects the private network that has been established from any external networks from interfering. It allows internal networks to communicate with one another with the network that has been established. The Access Control List (ACLs) allows only specific individuals to access the resources. Also, by monitoring and logging the activity within the database it helps the individual know what is being logged within the activity and determine what next step they should take to address it.",
      "audit_steps": "1. Deploy QLDB in a VPC \u2022 Create a Virtual Private Cloud (VPC) to isolate your QLDB resources. \u2022 Define the network CIDR blocks, subnets, and routing configurations for the VPC. \u2022 Ensure that the VPC is correctly configured with appropriate network access controls. 2. Configure Security Groups \u2022 Create security groups within your VPC to control inbound and outbound traffic to QLDB. \u2022 Determine the necessary protocols and ports for QLDB access. \u2022 Configure security group rules to allow access from trusted sources, such as specific IP ranges or other security groups within your VPC. 3. Set Up Network ACLs \u2022 Configure Network Access Control Lists (ACLs) within your VPC to provide an additional layer of network security. \u2022 Define inbound and outbound rules in the ACLs to allow or deny specific traffic based on IP addresses, ports, or protocols.  \u2022 Review and adjust the ACL rules to align with your organization's security policies and requirements. 4. Use VPC Endpoints or PrivateLink \u2022 Consider using VPC endpoints or AWS PrivateLink to securely access QLDB without traversing the public internet. \u2022 Set up a VPC endpoint for QLDB to allow private connectivity within your VPC. \u2022 Configure the routing and security group rules to enable traffic flow through the VPC endpoint or PrivateLink. 5. Secure External Connections \u2022 If external connections to QLDB are required, implement secure access methods such as Virtual Private Network (VPN) or AWS Direct Connect. \u2022 Configure VPN connections or Direct Connect links to establish encrypted and private connectivity between your on-premises network and the VPC hosting QLDB. \u2022 Apply appropriate security measures, such as strong authentication and encryption, to protect data transmitted over external connections. 6. Enable Logging and Monitoring \u2022 Enable logging for QLDB to capture important system events and database activity. \u2022 Utilize services like Amazon CloudWatch Logs to centralize and analyze QLDB logs. \u2022 Set up appropriate alarms and notifications to alert you of any suspicious network activity or potential security incidents. 7. Regularly Review and Update Network Security \u2022 Regularly review your VPC configurations, security groups, and network ACLs. \u2022 Stay informed about AWS security best practices and recommendations. \u2022 Update your network security measures as needed to address emerging threats or changes in your security requirements.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: Setting these certain rules in your network provides a strong security and prevents the organization suffering a ransomware attack.",
      "impact": "Setting these certain rules in your network provides a strong security and prevents the organization suffering a ransomware attack."
    },
    "function_name": "memorydb_secure_network_access_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.network.access.secure_network_access_enabled"
  },
  {
    "id": "11.3",
    "title": "Ensure Data at Rest is Encrypted",
    "assessment": "Manual",
    "description": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
    "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "audit": "1. Create an AWS Key Management Service (KMS) Key \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the AWS Key Management Service (KMS) console. \u2022 Create a new KMS key or select an existing one to encrypt your QLDB data at rest. \u2022 Configure the key policy to grant the appropriate IAM users or role permissions. 2. Enable Encryption for QLDB \u2022 Open the Amazon QLDB console. \u2022 Choose the QLDB ledger for which you want to enable encryption at rest. \u2022 Click on the Configuration tab. \u2022 Under the Encryption section. \u2022 Click on the Edit button or Modify option. \u2022 Enable encryption for the ledger. \u2022 Select the KMS key you created or chose in the first step for encrypting the QLDB data. \u2022 Save the changes to enable encryption at rest for the QLDB ledger. 3. Verify Encryption Status \u2022 Once the encryption at rest is enabled, the QLDB console will indicate the encryption status as Enabled for the selected ledger. \u2022 Ensure that the KMS key specified for encryption is the correct key you intended to use.  4. Testing and Verification \u2022 Perform read and write operations on your QLDB ledger to validate that the data is encrypted at rest. \u2022 Verify that you can access and query the encrypted data using appropriate authentication and authorization methods. 5. Key Management and Rotation \u2022 Follow AWS best practices for key management, including securely storing and managing the KMS key used for QLDB encryption. \u2022 Implement a key rotation policy, following AWS recommendations and compliance requirements if required. 6. Backup and Disaster Recovery \u2022 Ensure you have appropriate backup and disaster recovery mechanisms for your QLDB data. \u2022 Consider backing up the KMS key used for encryption to prevent data loss in case of a key compromise or accidental deletion.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This helps ensure that the data is kept secure and protected when at rest. The user must choose from two key options which then determine when the data is encrypted at rest.",
      "audit_steps": "1. Create an AWS Key Management Service (KMS) Key \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the AWS Key Management Service (KMS) console. \u2022 Create a new KMS key or select an existing one to encrypt your QLDB data at rest. \u2022 Configure the key policy to grant the appropriate IAM users or role permissions. 2. Enable Encryption for QLDB \u2022 Open the Amazon QLDB console. \u2022 Choose the QLDB ledger for which you want to enable encryption at rest. \u2022 Click on the Configuration tab. \u2022 Under the Encryption section. \u2022 Click on the Edit button or Modify option. \u2022 Enable encryption for the ledger. \u2022 Select the KMS key you created or chose in the first step for encrypting the QLDB data. \u2022 Save the changes to enable encryption at rest for the QLDB ledger. 3. Verify Encryption Status \u2022 Once the encryption at rest is enabled, the QLDB console will indicate the encryption status as Enabled for the selected ledger. \u2022 Ensure that the KMS key specified for encryption is the correct key you intended to use.  4. Testing and Verification \u2022 Perform read and write operations on your QLDB ledger to validate that the data is encrypted at rest. \u2022 Verify that you can access and query the encrypted data using appropriate authentication and authorization methods. 5. Key Management and Rotation \u2022 Follow AWS best practices for key management, including securely storing and managing the KMS key used for QLDB encryption. \u2022 Implement a key rotation policy, following AWS recommendations and compliance requirements if required. 6. Backup and Disaster Recovery \u2022 Ensure you have appropriate backup and disaster recovery mechanisms for your QLDB data. \u2022 Consider backing up the KMS key used for encryption to prevent data loss in case of a key compromise or accidental deletion.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext.",
      "impact": "If an unauthorized user steals the data, it would be unreadable for them because a key would be required to decrypt the message into plaintext."
    },
    "function_name": "memorydb_encryption_at_rest_enabled_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.encryption.at_rest.encryption_at_rest_enabled"
  },
  {
    "id": "11.4",
    "title": "Ensure Data in Transit is Encrypted",
    "assessment": "Manual",
    "description": "Use Transport Layer Security (TLS) to encrypt communications between clients and your QLDB instance. QLDB provides TLS support by default, allowing secure communication over the network. Configure your client applications to use TLS when connecting to QLDB.",
    "rationale": "Amazon Quantum Ledger Database (QLDB), uses TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by TLS in order to configure it correctly. Impact: If the user does not have the code configured correctly it would not be able to connect to the server.",
    "audit": "1. Understand TLS Encryption for QLDB \u2022 Learn about Transport Layer Security (TLS) and its role in securing data during transit. \u2022 Understand how TLS works to establish secure communication channels between clients and QLDB. 2. Configure Clients for TLS Encryption \u2022 Ensure that your client applications support TLS encryption for communication with QLDB. \u2022 Use the appropriate AWS SDK or QLDB driver that provides TLS encryption support. \u2022 Update your application code or configurations to enable TLS encryption. 3. Obtain the QLDB Endpoint \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the Amazon QLDB console. \u2022 Locate the QLDB ledger for which you want to enable encryption in transit. \u2022 Note down the QLDB endpoint for your ledger.  4. Establish TLS Connection \u2022 Use the QLDB endpoint obtained earlier to establish a TLS connection between your client application and QLDB. \u2022 Configure your client application to connect to QLDB using the secure HTTPS protocol. \u2022 Provide the necessary authentication credentials or tokens required to establish the connection. 5. Verify TLS Encryption \u2022 Once the TLS connection is established, verify that the connection is secured using TLS by checking for a valid TLS certificate. \u2022 Ensure that your client application can communicate securely with QLDB without any errors or warnings related to encryption. 6. Regularly Update Client Applications \u2022 Stay updated with the latest versions of the AWS SDKs or QLDB drivers used by your client applications. \u2022 Regularly update your client applications to leverage the latest TLS encryption features and security enhancements. 7. Monitor and Review TLS Connections \u2022 Utilize AWS CloudTrail and Amazon CloudWatch to monitor and log TLS-related events and errors. \u2022 Review the logs and alerts to identify potential security issues or anomalies related to TLS connections. 8. Secure Other Communication Channels \u2022 Ensure that other communication channels your client applications use, such as APIs or data transfers, also utilize TLS encryption. \u2022 Implement appropriate encryption and security measures to protect sensitive data during transit in all communication channels.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If the user does not have the code configured correctly it would not be able to connect to the server.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_cluster_storage_encrypted",
      "rds_instance_encryption_at_rest_check"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_cluster_storage_encrypted",
        "rds_instance_encryption_at_rest_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Use Transport Layer Security (TLS) to encrypt communications between clients and your QLDB instance. QLDB provides TLS support by default, allowing secure communication over the network. Configure your client applications to use TLS when connecting to QLDB.",
      "audit_steps": "1. Understand TLS Encryption for QLDB \u2022 Learn about Transport Layer Security (TLS) and its role in securing data during transit. \u2022 Understand how TLS works to establish secure communication channels between clients and QLDB. 2. Configure Clients for TLS Encryption \u2022 Ensure that your client applications support TLS encryption for communication with QLDB. \u2022 Use the appropriate AWS SDK or QLDB driver that provides TLS encryption support. \u2022 Update your application code or configurations to enable TLS encryption. 3. Obtain the QLDB Endpoint \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the Amazon QLDB console. \u2022 Locate the QLDB ledger for which you want to enable encryption in transit. \u2022 Note down the QLDB endpoint for your ledger.  4. Establish TLS Connection \u2022 Use the QLDB endpoint obtained earlier to establish a TLS connection between your client application and QLDB. \u2022 Configure your client application to connect to QLDB using the secure HTTPS protocol. \u2022 Provide the necessary authentication credentials or tokens required to establish the connection. 5. Verify TLS Encryption \u2022 Once the TLS connection is established, verify that the connection is secured using TLS by checking for a valid TLS certificate. \u2022 Ensure that your client application can communicate securely with QLDB without any errors or warnings related to encryption. 6. Regularly Update Client Applications \u2022 Stay updated with the latest versions of the AWS SDKs or QLDB drivers used by your client applications. \u2022 Regularly update your client applications to leverage the latest TLS encryption features and security enhancements. 7. Monitor and Review TLS Connections \u2022 Utilize AWS CloudTrail and Amazon CloudWatch to monitor and log TLS-related events and errors. \u2022 Review the logs and alerts to identify potential security issues or anomalies related to TLS connections. 8. Secure Other Communication Channels \u2022 Ensure that other communication channels your client applications use, such as APIs or data transfers, also utilize TLS encryption. \u2022 Implement appropriate encryption and security measures to protect sensitive data during transit in all communication channels.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Amazon Quantum Ledger Database (QLDB), uses TLS to encrypt data during transit. To secure your data in transit the individual should identify their client application and what is supported by TLS in order to configure it correctly. Impact: If the user does not have the code configured correctly it would not be able to connect to the server.",
      "impact": "If the user does not have the code configured correctly it would not be able to connect to the server."
    },
    "function_name": "memorydb_encryption_in_transit_enabled_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.encryption.in_transit.encryption_in_transit_enabled"
  },
  {
    "id": "11.5",
    "title": "Ensure to Implement Access Control and Authentication",
    "assessment": "Manual",
    "description": "Utilize QLDB's built-in authentication and access control mechanisms. Define IAM policies to control which users or roles can perform specific actions on QLDB resources. Leverage IAM roles for cross-service access, securely integrating QLDB with other AWS services.",
    "rationale": "Users should select whether they like to enable authentication. If they want to authenticate the user would be required to implement IAM roles would grant or deny permissions within that database. Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "audit": "1. Understand QLDB Authentication and Access Control \u2022 Familiarize yourself with the authentication and access control mechanisms provided by QLDB. \u2022 Understand the concepts of users, permissions, and roles in QLDB's access control model. 2. Configure IAM for QLDB \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the Amazon QLDB console. \u2022 Go to the Ledgers section. \u2022 Select the QLDB ledger for which you want to configure access control. \u2022 Under the Configuration tab. \u2022 Click on Edit or Modify to make changes. \u2022 Enable IAM-based authentication by selecting the appropriate option. \u2022 Define the IAM policies that grant or deny permissions for specific QLDB actions. \u2022 Configure fine-grained access control by associating IAM policies with IAM users or roles.  3. Create IAM Users or Roles \u2022 Identify the individuals or services that require access to QLDB. \u2022 Create IAM user accounts for individuals or IAM roles for services. \u2022 Assign appropriate IAM policies to these users or roles based on their required access levels. 4. Grant Required Permissions \u2022 Define IAM policies that grant the necessary permissions for QLDB operations. \u2022 Consider the principle of least privilege and only provide the minimum permissions required for each user or role. \u2022 Assign IAM policies to IAM users or roles to allow access to specific QLDB resources. 5. Test Access Control \u2022 Use IAM user credentials or IAM role credentials to test access to QLDB resources. \u2022 Verify that users or services can perform the expected actions based on their assigned IAM policies. \u2022 Test both read and write operations to ensure appropriate access permissions. 6. Monitor and Audit Access \u2022 Enable AWS CloudTrail for QLDB to capture and log all API calls and activities. \u2022 Use Amazon CloudWatch to monitor and analyze the logs for unauthorized access attempts or suspicious activities. \u2022 Implement additional logging and auditing mechanisms as per your organization's security requirements. 7. Regularly Review and Update Access Control \u2022 Conduct periodic reviews of IAM policies, users, and roles associated with QLDB. \u2022 Remove or update access for users or roles that no longer require QLDB access. \u2022 Stay updated with AWS security best practices and IAM and access control recommendations.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Utilize QLDB's built-in authentication and access control mechanisms. Define IAM policies to control which users or roles can perform specific actions on QLDB resources. Leverage IAM roles for cross-service access, securely integrating QLDB with other AWS services.",
      "audit_steps": "1. Understand QLDB Authentication and Access Control \u2022 Familiarize yourself with the authentication and access control mechanisms provided by QLDB. \u2022 Understand the concepts of users, permissions, and roles in QLDB's access control model. 2. Configure IAM for QLDB \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the Amazon QLDB console. \u2022 Go to the Ledgers section. \u2022 Select the QLDB ledger for which you want to configure access control. \u2022 Under the Configuration tab. \u2022 Click on Edit or Modify to make changes. \u2022 Enable IAM-based authentication by selecting the appropriate option. \u2022 Define the IAM policies that grant or deny permissions for specific QLDB actions. \u2022 Configure fine-grained access control by associating IAM policies with IAM users or roles.  3. Create IAM Users or Roles \u2022 Identify the individuals or services that require access to QLDB. \u2022 Create IAM user accounts for individuals or IAM roles for services. \u2022 Assign appropriate IAM policies to these users or roles based on their required access levels. 4. Grant Required Permissions \u2022 Define IAM policies that grant the necessary permissions for QLDB operations. \u2022 Consider the principle of least privilege and only provide the minimum permissions required for each user or role. \u2022 Assign IAM policies to IAM users or roles to allow access to specific QLDB resources. 5. Test Access Control \u2022 Use IAM user credentials or IAM role credentials to test access to QLDB resources. \u2022 Verify that users or services can perform the expected actions based on their assigned IAM policies. \u2022 Test both read and write operations to ensure appropriate access permissions. 6. Monitor and Audit Access \u2022 Enable AWS CloudTrail for QLDB to capture and log all API calls and activities. \u2022 Use Amazon CloudWatch to monitor and analyze the logs for unauthorized access attempts or suspicious activities. \u2022 Implement additional logging and auditing mechanisms as per your organization's security requirements. 7. Regularly Review and Update Access Control \u2022 Conduct periodic reviews of IAM policies, users, and roles associated with QLDB. \u2022 Remove or update access for users or roles that no longer require QLDB access. \u2022 Stay updated with AWS security best practices and IAM and access control recommendations.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Users should select whether they like to enable authentication. If they want to authenticate the user would be required to implement IAM roles would grant or deny permissions within that database. Impact: Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data.",
      "impact": "Allowing authentication verifies the identity of the person and who has appropriate access to a company\u2019s data."
    },
    "function_name": "memorydb_access_control_authentication_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.authentication.access_control.access_control_authentication_enabled"
  },
  {
    "id": "11.6",
    "title": "Ensure Monitoring and Logging is Enabled",
    "assessment": "Manual",
    "description": "Enable QLDB's built-in logging to capture important system events and database activity. Monitor the logs for any suspicious activities or errors. Leverage Amazon CloudWatch to collect and analyze logs, set up alarms, and receive notifications for potential security incidents.",
    "rationale": "This helps the individual know what is being logged within the activity and determine what next step they should take to address it.",
    "audit": "1. Enable AWS CloudTrail \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the AWS CloudTrail console. \u2022 Create a new trail or select an existing trail. \u2022 Configure the trail to capture QLDB API calls and relevant events. \u2022 Specify the Amazon S3 bucket where the CloudTrail logs will be stored. \u2022 Enable the trail to start capturing QLDB events. 2. Enable Amazon CloudWatch Logs \u2022 Open the Amazon CloudWatch console. \u2022 Create a new log group or select an existing log group. \u2022 Configure the log group to receive QLDB logs from CloudTrail. \u2022 Define the log retention period to retain the logs for the desired duration. \u2022 Enable CloudWatch Logs to start receiving and storing QLDB logs. 3. Configure Log Metric Filters \u2022 In the CloudWatch console, go to the log group that contains the QLDB logs. \u2022 Define log metric filters to extract specific information or patterns from the logs. \u2022 Create metric filters based on your monitoring and alerting requirements. \u2022 Specify the target metric and define the filter patterns to match the desired log events.   4. Create CloudWatch Dashboards and Alarms \u2022 Create CloudWatch dashboards to visualize and monitor important QLDB metrics. \u2022 Customize the dashboard widgets to display relevant log metrics, such as API calls or errors. \u2022 Set up CloudWatch alarms to trigger notifications or automated actions based on specific thresholds or conditions. \u2022 Configure alarm actions, such as sending email notifications or invoking AWS Lambda functions, to respond to critical events. 5. Enable EventBridge Integration (Optional) \u2022 Open the Amazon EventBridge console. \u2022 Create a new rule or select an existing rule. \u2022 Configure the rule to match specific QLDB events or patterns. \u2022 Define targets for the rule, such as invoking Lambda functions or sending notifications to other AWS services. 6. Monitor and Analyze Logs and Metrics \u2022 Regularly review the CloudWatch logs and metrics for QLDB. \u2022 Monitor key metrics and performance indicators to identify any issues or anomalies. \u2022 Use CloudWatch Logs Insights to query and analyze log data for troubleshooting. 7. Integrate with AWS Monitoring and Alerting Tools \u2022 Leverage other AWS monitoring and alerting services like AWS X-Ray or AWS ServiceLens to gain deeper insights into QLDB performance and behavior. \u2022 Configure additional alerts or notifications using AWS services like Amazon SNS or AWS Chatbot. 8. Regularly Review and Update Logging and Monitoring Configuration \u2022 Periodically review and update your CloudTrail, CloudWatch, and EventBridge configurations to align with changes in your monitoring requirements. \u2022 Stay informed about AWS security best practices and new features.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_enhanced_monitoring_enabled",
      "rds_cluster_integration_cloudwatch_logs"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_enhanced_monitoring_enabled",
        "rds_cluster_integration_cloudwatch_logs"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Enable QLDB's built-in logging to capture important system events and database activity. Monitor the logs for any suspicious activities or errors. Leverage Amazon CloudWatch to collect and analyze logs, set up alarms, and receive notifications for potential security incidents.",
      "audit_steps": "1. Enable AWS CloudTrail \u2022 Sign in to the AWS Management Console at https://console.aws.amazon.com/ with your AWS account credentials. \u2022 Open the AWS CloudTrail console. \u2022 Create a new trail or select an existing trail. \u2022 Configure the trail to capture QLDB API calls and relevant events. \u2022 Specify the Amazon S3 bucket where the CloudTrail logs will be stored. \u2022 Enable the trail to start capturing QLDB events. 2. Enable Amazon CloudWatch Logs \u2022 Open the Amazon CloudWatch console. \u2022 Create a new log group or select an existing log group. \u2022 Configure the log group to receive QLDB logs from CloudTrail. \u2022 Define the log retention period to retain the logs for the desired duration. \u2022 Enable CloudWatch Logs to start receiving and storing QLDB logs. 3. Configure Log Metric Filters \u2022 In the CloudWatch console, go to the log group that contains the QLDB logs. \u2022 Define log metric filters to extract specific information or patterns from the logs. \u2022 Create metric filters based on your monitoring and alerting requirements. \u2022 Specify the target metric and define the filter patterns to match the desired log events.   4. Create CloudWatch Dashboards and Alarms \u2022 Create CloudWatch dashboards to visualize and monitor important QLDB metrics. \u2022 Customize the dashboard widgets to display relevant log metrics, such as API calls or errors. \u2022 Set up CloudWatch alarms to trigger notifications or automated actions based on specific thresholds or conditions. \u2022 Configure alarm actions, such as sending email notifications or invoking AWS Lambda functions, to respond to critical events. 5. Enable EventBridge Integration (Optional) \u2022 Open the Amazon EventBridge console. \u2022 Create a new rule or select an existing rule. \u2022 Configure the rule to match specific QLDB events or patterns. \u2022 Define targets for the rule, such as invoking Lambda functions or sending notifications to other AWS services. 6. Monitor and Analyze Logs and Metrics \u2022 Regularly review the CloudWatch logs and metrics for QLDB. \u2022 Monitor key metrics and performance indicators to identify any issues or anomalies. \u2022 Use CloudWatch Logs Insights to query and analyze log data for troubleshooting. 7. Integrate with AWS Monitoring and Alerting Tools \u2022 Leverage other AWS monitoring and alerting services like AWS X-Ray or AWS ServiceLens to gain deeper insights into QLDB performance and behavior. \u2022 Configure additional alerts or notifications using AWS services like Amazon SNS or AWS Chatbot. 8. Regularly Review and Update Logging and Monitoring Configuration \u2022 Periodically review and update your CloudTrail, CloudWatch, and EventBridge configurations to align with changes in your monitoring requirements. \u2022 Stay informed about AWS security best practices and new features.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "This helps the individual know what is being logged within the activity and determine what next step they should take to address it.",
      "impact": ""
    },
    "function_name": "memorydb_monitoring_logging_enabled_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.monitoring.logging.monitoring_logging_enabled"
  },
  {
    "id": "11.7",
    "title": "Ensure to Enable Backup and Recovery",
    "assessment": "Manual",
    "description": "Having the data backed up ensures that all the crucial information is stored securely it defends against any human errors and system errors that resulted in data loss. An organization that has a disaster recovery plan is prepared for any disruption that would impact business operations.",
    "rationale": "Impact: If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, suffer data loss that cannot be restored, and loss of revenue.",
    "audit": "1. Understand QLDB Backup and Recovery Features \u2022 Familiarize yourself with the built-in backup and recovery capabilities provided by QLDB. \u2022 Understand the concepts of ledgers, revisions, and journal export for backup and restore operations. 2. Determine Backup and Recovery Requirements \u2022 Assess your organization's backup and recovery requirements for QLDB. \u2022 Define the recovery point objective (RPO) and recovery time objective (RTO) that align with your business needs. \u2022 Determine the desired backup frequency and retention period for your QLDB data. 3. Enable Automatic Backups \u2022 Open the Amazon QLDB console. \u2022 Select the QLDB ledger for which you want to enable automatic backups. \u2022 Click on the Configuration tab. \u2022 Under the Backup section, enable automatic backups. \u2022 Specify the desired backup retention period for the automatic backups.   4. Perform Manual Backups (Optional) \u2022 If you need additional backups or want to perform on-demand backups, initiate manual backups. \u2022 Open the Amazon QLDB console. \u2022 Select the QLDB ledger you want to back up. \u2022 Click on the Backups tab. \u2022 Choose the Create Backup option. \u2022 Provide a meaningful backup name and initiate the backup process. 5. Restore QLDB from Backups \u2022 Open the Amazon QLDB console. \u2022 Go to the Backups tab. \u2022 Select the backup from which you want to restore the QLDB ledger. \u2022 Click on the Restore option. \u2022 Specify the desired restoration name and initiate the restoration process. 6. Regularly Test Restore Process \u2022 Periodically test the restore process to ensure that backups are working correctly. \u2022 Select a backup and initiate the restoration to a separate QLDB ledger. \u2022 Verify that the restored ledger contains the expected data and is accessible. 7. Implement Data Archiving (Optional) \u2022 If you require long-term data retention or compliance with specific data retention policies, consider implementing data archiving strategies. \u2022 Leverage AWS services like Amazon S3 for long-term storage of QLDB journal exports or backups. 8. Disaster Recovery Planning \u2022 Develop a comprehensive disaster recovery plan for QLDB to mitigate the impact of catastrophic events. \u2022 Consider implementing cross-region replication or multi-region deployments to provide geographic redundancy. \u2022 Test the disaster recovery plan periodically to validate its effectiveness. 9. Monitor Backup and Recovery Operations \u2022 Regularly monitor backup and recovery operations using Amazon CloudWatch and AWS CloudTrail. \u2022 Set up appropriate alarms and notifications to ensure timely identification of any backup or recovery issues.",
    "remediation": "References: 1. https://aws.amazon.com/products/databases/",
    "profile_applicability": "\u2022  Level 1",
    "impact": "If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, suffer data loss that cannot be restored, and loss of revenue.",
    "references": "1. https://aws.amazon.com/products/databases/",
    "function_names": [
      "rds_instance_backup_enabled",
      "rds_cluster_protected_by_backup_plan"
    ],
    "implementation_guidance": {
      "boto3_client": "rds",
      "functions": [
        "rds_instance_backup_enabled",
        "rds_cluster_protected_by_backup_plan"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use rds boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Having the data backed up ensures that all the crucial information is stored securely it defends against any human errors and system errors that resulted in data loss. An organization that has a disaster recovery plan is prepared for any disruption that would impact business operations.",
      "audit_steps": "1. Understand QLDB Backup and Recovery Features \u2022 Familiarize yourself with the built-in backup and recovery capabilities provided by QLDB. \u2022 Understand the concepts of ledgers, revisions, and journal export for backup and restore operations. 2. Determine Backup and Recovery Requirements \u2022 Assess your organization's backup and recovery requirements for QLDB. \u2022 Define the recovery point objective (RPO) and recovery time objective (RTO) that align with your business needs. \u2022 Determine the desired backup frequency and retention period for your QLDB data. 3. Enable Automatic Backups \u2022 Open the Amazon QLDB console. \u2022 Select the QLDB ledger for which you want to enable automatic backups. \u2022 Click on the Configuration tab. \u2022 Under the Backup section, enable automatic backups. \u2022 Specify the desired backup retention period for the automatic backups.   4. Perform Manual Backups (Optional) \u2022 If you need additional backups or want to perform on-demand backups, initiate manual backups. \u2022 Open the Amazon QLDB console. \u2022 Select the QLDB ledger you want to back up. \u2022 Click on the Backups tab. \u2022 Choose the Create Backup option. \u2022 Provide a meaningful backup name and initiate the backup process. 5. Restore QLDB from Backups \u2022 Open the Amazon QLDB console. \u2022 Go to the Backups tab. \u2022 Select the backup from which you want to restore the QLDB ledger. \u2022 Click on the Restore option. \u2022 Specify the desired restoration name and initiate the restoration process. 6. Regularly Test Restore Process \u2022 Periodically test the restore process to ensure that backups are working correctly. \u2022 Select a backup and initiate the restoration to a separate QLDB ledger. \u2022 Verify that the restored ledger contains the expected data and is accessible. 7. Implement Data Archiving (Optional) \u2022 If you require long-term data retention or compliance with specific data retention policies, consider implementing data archiving strategies. \u2022 Leverage AWS services like Amazon S3 for long-term storage of QLDB journal exports or backups. 8. Disaster Recovery Planning \u2022 Develop a comprehensive disaster recovery plan for QLDB to mitigate the impact of catastrophic events. \u2022 Consider implementing cross-region replication or multi-region deployments to provide geographic redundancy. \u2022 Test the disaster recovery plan periodically to validate its effectiveness. 9. Monitor Backup and Recovery Operations \u2022 Regularly monitor backup and recovery operations using Amazon CloudWatch and AWS CloudTrail. \u2022 Set up appropriate alarms and notifications to ensure timely identification of any backup or recovery issues.",
      "remediation_steps": "References: 1. https://aws.amazon.com/products/databases/",
      "rationale": "Impact: If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, suffer data loss that cannot be restored, and loss of revenue.",
      "impact": "If a business does not have a backup and recovery plan it would have a negative impact on the business, which would result in less productivity, suffer data loss that cannot be restored, and loss of revenue."
    },
    "function_name": "memorydb_backup_recovery_enabled_check",
    "coverage": 90,
    "rule_id": "aws.memorydb.backup.recovery.backup_recovery_enabled"
  }
]