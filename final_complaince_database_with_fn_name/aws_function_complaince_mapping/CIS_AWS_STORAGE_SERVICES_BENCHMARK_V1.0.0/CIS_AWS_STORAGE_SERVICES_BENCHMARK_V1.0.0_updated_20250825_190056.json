[
  {
    "id": "1.1",
    "title": "AWS Storage Backups",
    "assessment": "Manual",
    "description": "AWS Storage Backups is a managed AWS Service that establishes high resiliency to your cloud resources. AWS Storage Backups are like making extra copies of your important stuff on Amazon's computers. It is an excellent strategy to ensure that the data and resources you use remain available in the event of unrecoverable damage or loss to your resources.",
    "rationale": "AWS Backups enable you to back up and restore all data lost during the attack,While AWS Storage Backups provide a level of security, there are numerous methods to fortify your backups, ensuring the protection of your data and services.",
    "audit": "",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "backup_plan_specification_check",
      "backup_resources_existence_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_plan_specification_check",
        "backup_resources_existence_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS Storage Backups is a managed AWS Service that establishes high resiliency to your cloud resources. AWS Storage Backups are like making extra copies of your important stuff on Amazon's computers. It is an excellent strategy to ensure that the data and resources you use remain available in the event of unrecoverable damage or loss to your resources.",
      "audit_steps": "",
      "remediation_steps": null,
      "rationale": "AWS Backups enable you to back up and restore all data lost during the attack,While AWS Storage Backups provide a level of security, there are numerous methods to fortify your backups, ensuring the protection of your data and services.",
      "impact": ""
    }
  },
  {
    "id": "1.2",
    "title": "Ensure securing AWS Backups",
    "assessment": "Manual",
    "description": "As an AWS administrator, it's important to know what you're responsible for. You're responsible for keeping things safe in the cloud, which means taking care of the resources and data on AWS. Here's what you need to secure, according to AWS documentation: 1. Responsible for alert communication with AWS. 2. Managing access credentials for AWS resources. 3. Configuring backup plans according to organization policies. 4. Ensuring backup recovery capability. 5. Including AWS Backups in the organization's disaster recovery procedures. 6. Ensuring user awareness and familiarity with AWS Backups platform usage",
    "rationale": "AWS will send periodic emails regarding the status of your backups and any service issues. The administrator must address any communicated issues from AWS, such as billing problems or backup inactivity, and take necessary steps to resolve them.",
    "audit": "CREATING AN AWS BACKUP: Creating an AWS Backup involves selecting the desired data, specifying backup frequency, and choosing storage options. Below we\u2019ll walk through how to create and configure an AWS Backup instance. 1. Sign into AWS Console: To sign into the AWS Console 'https://console.aws.amazon.com/billing/home#/', users navigate to the AWS Management Console website and enter their credentials, including their username and password. 2. Access the AWS Backup Service Dashboard in the AWS Management Console: AWS Management Console and type \"Backup\" or navigate through the services menu to find the \"Storage\" category, where AWS Backup is listed. 3. Create Backup Plan: Choose \"Create backup plan\" from the options provided. You can either create a custom plan tailored to your requirements or option for a per-defined template offered by AWS",
    "remediation": "References: 1. https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
    "function_names": [
      "backup_data_encryption_check",
      "backup_role_and_permissions_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_data_encryption_check",
        "backup_role_and_permissions_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "As an AWS administrator, it's important to know what you're responsible for. You're responsible for keeping things safe in the cloud, which means taking care of the resources and data on AWS. Here's what you need to secure, according to AWS documentation: 1. Responsible for alert communication with AWS. 2. Managing access credentials for AWS resources. 3. Configuring backup plans according to organization policies. 4. Ensuring backup recovery capability. 5. Including AWS Backups in the organization's disaster recovery procedures. 6. Ensuring user awareness and familiarity with AWS Backups platform usage",
      "audit_steps": "CREATING AN AWS BACKUP: Creating an AWS Backup involves selecting the desired data, specifying backup frequency, and choosing storage options. Below we\u2019ll walk through how to create and configure an AWS Backup instance. 1. Sign into AWS Console: To sign into the AWS Console 'https://console.aws.amazon.com/billing/home#/', users navigate to the AWS Management Console website and enter their credentials, including their username and password. 2. Access the AWS Backup Service Dashboard in the AWS Management Console: AWS Management Console and type \"Backup\" or navigate through the services menu to find the \"Storage\" category, where AWS Backup is listed. 3. Create Backup Plan: Choose \"Create backup plan\" from the options provided. You can either create a custom plan tailored to your requirements or option for a per-defined template offered by AWS",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html",
      "rationale": "AWS will send periodic emails regarding the status of your backups and any service issues. The administrator must address any communicated issues from AWS, such as billing problems or backup inactivity, and take necessary steps to resolve them.",
      "impact": ""
    }
  },
  {
    "id": "1.3",
    "title": "Ensure to create backup template and name",
    "assessment": "Manual",
    "description": "To create a backup plan, select a template and specify a name for the plan. Additionally, define backup rules according to your requirements and then click on create backup option.",
    "rationale": "",
    "audit": "Backup Resources: Once you've made your backup plan, it's time to put it into action and start backing up your stuff. Let's start by backing up an S3 storage bucket. To back up Elastic Beanstalk instance stored on AWS S3, we'll need to tag its Amazon Resource Name (ARN) with a backup plan. In S3, go to \"properties\" to attach the backup plan to the resource: 1. Copy the ARN from the console: From the AWS Management Console, copy the ARN (Amazon Resource Name) associated with the Elastic Beanstalk instance. This unique identifier will be used to tag the resource for backup. 2. Assign the resource: o After copying the ARN, return to the AWS Management Console and access Amazon Backup. o Choose the backup plan recently created, then proceed to assign the resource you wish to backup, such as the S3 bucket containing the Elastic Beanstalk resource. o Finally, navigate to \"Resource Assignments\" to complete the process. Choose \"Assign Resources\" and provide a name for the assignment. For now, maintain the role as Default. In subsequent sections, we'll explore implementing custom IAM roles and policies for your backup operations. Select the resource(s) that you want to backup. You have the option to backup all your resources, but we\u2019re just going to back up the specific Elastic Beanstalk resource for now. The resources are now being backed up according to the schedule established by your organization.",
    "remediation": "The AWS backup vault serves as the storage location for your backups. It's crucial to manage access to these backups to prevent unauthorized access and ensure data security. References: 1. https://docs.aws.amazon.com/aws-backup/latest/devguide/how-it-works.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/aws-backup/latest/devguide/how-it-works.html",
    "function_names": [
      "backup_plan_specification_check",
      "backup_rules_definition_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_plan_specification_check",
        "backup_rules_definition_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "To create a backup plan, select a template and specify a name for the plan. Additionally, define backup rules according to your requirements and then click on create backup option.",
      "audit_steps": "Backup Resources: Once you've made your backup plan, it's time to put it into action and start backing up your stuff. Let's start by backing up an S3 storage bucket. To back up Elastic Beanstalk instance stored on AWS S3, we'll need to tag its Amazon Resource Name (ARN) with a backup plan. In S3, go to \"properties\" to attach the backup plan to the resource: 1. Copy the ARN from the console: From the AWS Management Console, copy the ARN (Amazon Resource Name) associated with the Elastic Beanstalk instance. This unique identifier will be used to tag the resource for backup. 2. Assign the resource: o After copying the ARN, return to the AWS Management Console and access Amazon Backup. o Choose the backup plan recently created, then proceed to assign the resource you wish to backup, such as the S3 bucket containing the Elastic Beanstalk resource. o Finally, navigate to \"Resource Assignments\" to complete the process. Choose \"Assign Resources\" and provide a name for the assignment. For now, maintain the role as Default. In subsequent sections, we'll explore implementing custom IAM roles and policies for your backup operations. Select the resource(s) that you want to backup. You have the option to backup all your resources, but we\u2019re just going to back up the specific Elastic Beanstalk resource for now. The resources are now being backed up according to the schedule established by your organization.",
      "remediation_steps": "The AWS backup vault serves as the storage location for your backups. It's crucial to manage access to these backups to prevent unauthorized access and ensure data security. References: 1. https://docs.aws.amazon.com/aws-backup/latest/devguide/how-it-works.html",
      "rationale": "",
      "impact": ""
    }
  },
  {
    "id": "1.4",
    "title": "Ensure to create AWS IAM Policies",
    "assessment": "Manual",
    "description": "AWS IAM policies, specify the desired permissions for accessing AWS resources and define the conditions under which those permissions are granted. Configure the appropriate policies to keep your resources secure.",
    "rationale": "Managing AWS IAM policies is crucial to safeguard your backups from unauthorized access, ensuring that only approved users can manipulate or view sensitive data.",
    "audit": "To create a role for AWS Backup, follow these steps: 1. Navigate to the \"IAM Dashboard\" in the AWS Console. 2. Select \"Roles\" from the left-hand menu. 3. Click on the \"Create Role\" button. 4. Choose \"AWS Service\" as the trusted entity. 5. Select \"AWS Backup\" as the service that will use this role. 6. Choose a policy to apply to the role or create a custom policy. 7. Review the role details and provide a meaningful name for the role. 8. Click on \"Create Role\" to finalize the creation of the role for AWS Backup.",
    "remediation": "AWS IAM policies, restricting access to backup resources, and implementing additional security measures to prevent future incidents. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
    "function_names": [
      "backup_role_and_permissions_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_role_and_permissions_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS IAM policies, specify the desired permissions for accessing AWS resources and define the conditions under which those permissions are granted. Configure the appropriate policies to keep your resources secure.",
      "audit_steps": "To create a role for AWS Backup, follow these steps: 1. Navigate to the \"IAM Dashboard\" in the AWS Console. 2. Select \"Roles\" from the left-hand menu. 3. Click on the \"Create Role\" button. 4. Choose \"AWS Service\" as the trusted entity. 5. Select \"AWS Backup\" as the service that will use this role. 6. Choose a policy to apply to the role or create a custom policy. 7. Review the role details and provide a meaningful name for the role. 8. Click on \"Create Role\" to finalize the creation of the role for AWS Backup.",
      "remediation_steps": "AWS IAM policies, restricting access to backup resources, and implementing additional security measures to prevent future incidents. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html",
      "rationale": "Managing AWS IAM policies is crucial to safeguard your backups from unauthorized access, ensuring that only approved users can manipulate or view sensitive data.",
      "impact": ""
    }
  },
  {
    "id": "1.5",
    "title": "Ensure to create IAM roles for Backup",
    "assessment": "Manual",
    "description": "An AWS Identity and Access Management (IAM) role is similar to a user, in that it is an AWS identity with permissions policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.",
    "rationale": "While Service Linked Roles offer quick deployment, using default configurations isn't recommended for security best practices.",
    "audit": "To create a role for AWS Backup, follow these steps: 1. Navigate to the \"IAM Dashboard\" in the AWS Console. 2. Select \"Roles\" from the left-hand menu. 3. Click on the \"Create Role\" button. 4. Choose \"AWS Service\" as the trusted entity. 5. Select \"AWS Backup\" as the service that will use this role. 6. Choose a policy to apply to the role or create a custom policy. 7. Review the role details and provide a meaningful name for the role. 8. Click on \"Create Role\" to finalize the creation of the role for AWS Backup.",
    "remediation": "Assess your organization's needs to determine whether to utilize Service Linked Roles for AWS backups. Default Value: When using the AWS Backup console for the first time, you can choose to have AWS Backup create a default service role for you. This role has the permissions that AWS Backup needs to create and restore backups on your behalf. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked- roles.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked- roles.html",
    "function_names": [
      "backup_role_and_permissions_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_role_and_permissions_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "An AWS Identity and Access Management (IAM) role is similar to a user, in that it is an AWS identity with permissions policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.",
      "audit_steps": "To create a role for AWS Backup, follow these steps: 1. Navigate to the \"IAM Dashboard\" in the AWS Console. 2. Select \"Roles\" from the left-hand menu. 3. Click on the \"Create Role\" button. 4. Choose \"AWS Service\" as the trusted entity. 5. Select \"AWS Backup\" as the service that will use this role. 6. Choose a policy to apply to the role or create a custom policy. 7. Review the role details and provide a meaningful name for the role. 8. Click on \"Create Role\" to finalize the creation of the role for AWS Backup.",
      "remediation_steps": "Assess your organization's needs to determine whether to utilize Service Linked Roles for AWS backups. Default Value: When using the AWS Backup console for the first time, you can choose to have AWS Backup create a default service role for you. This role has the permissions that AWS Backup needs to create and restore backups on your behalf. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked- roles.html",
      "rationale": "While Service Linked Roles offer quick deployment, using default configurations isn't recommended for security best practices.",
      "impact": ""
    }
  },
  {
    "id": "1.6",
    "title": "Ensure AWS Backup with Service Linked Roles",
    "assessment": "Manual",
    "description": "AWS Service Linked Roles are IAM roles designed specifically for AWS Backup. These roles come with default configurations allowing access to all AWS resources by default.",
    "rationale": "While Service Linked Roles offer quick deployment, using default configurations isn't recommended for security best practices.",
    "audit": "Create service-linked role for AWS Backup: You don't need to create a service-linked role manually. AWS Backup automatically creates it when you list resources to back up, set up cross-account backup, or perform backups using the AWS Management Console, AWS CLI, or AWS API. If you delete this role, you can recreate it by following the same steps. AWS Backup will create it for you again when needed.",
    "remediation": "Assess your organization's needs to determine whether to utilize Service Linked Roles for AWS backups. References: 1. https://docs.aws.amazon.com/aws-backup/latest/devguide/using-service-linked- roles.html 2 Elastic Block Store (EBS) Amazon EBS is a block level file storage system that runs on EC2. EBS can be used as a hard drive that\u2019s mounted on an EC2 instance (virtual machine). EBS can store data as a standalone apart from EC2. This means that data can persist to the block storage service while an EC2 instance is offline. Out of the box EBS functions as an unformatted file system that needs to be configured and mounted on top of an EC2 instance. EBS can be used as both a storage and boot drive; for the purposes of this document, we will focus on EBS as a storage device. EBS comes with many different options to fit the specific needs of an application. EBS is most likely the right choice if you need quick access to read and write files to the cloud and you need these files to be stored long term. You can also rapidly unmount the file system from one EC2 instance and deploy it to another instance by using snapshots.",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/aws-backup/latest/devguide/using-service-linked- roles.html 2 Elastic Block Store (EBS) Amazon EBS is a block level file storage system that runs on EC2. EBS can be used as a hard drive that\u2019s mounted on an EC2 instance (virtual machine). EBS can store data as a standalone apart from EC2. This means that data can persist to the block storage service while an EC2 instance is offline. Out of the box EBS functions as an unformatted file system that needs to be configured and mounted on top of an EC2 instance. EBS can be used as both a storage and boot drive; for the purposes of this document, we will focus on EBS as a storage device. EBS comes with many different options to fit the specific needs of an application. EBS is most likely the right choice if you need quick access to read and write files to the cloud and you need these files to be stored long term. You can also rapidly unmount the file system from one EC2 instance and deploy it to another instance by using snapshots.",
    "function_names": [
      "backup_role_and_permissions_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_role_and_permissions_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS Service Linked Roles are IAM roles designed specifically for AWS Backup. These roles come with default configurations allowing access to all AWS resources by default.",
      "audit_steps": "Create service-linked role for AWS Backup: You don't need to create a service-linked role manually. AWS Backup automatically creates it when you list resources to back up, set up cross-account backup, or perform backups using the AWS Management Console, AWS CLI, or AWS API. If you delete this role, you can recreate it by following the same steps. AWS Backup will create it for you again when needed.",
      "remediation_steps": "Assess your organization's needs to determine whether to utilize Service Linked Roles for AWS backups. References: 1. https://docs.aws.amazon.com/aws-backup/latest/devguide/using-service-linked- roles.html 2 Elastic Block Store (EBS) Amazon EBS is a block level file storage system that runs on EC2. EBS can be used as a hard drive that\u2019s mounted on an EC2 instance (virtual machine). EBS can store data as a standalone apart from EC2. This means that data can persist to the block storage service while an EC2 instance is offline. Out of the box EBS functions as an unformatted file system that needs to be configured and mounted on top of an EC2 instance. EBS can be used as both a storage and boot drive; for the purposes of this document, we will focus on EBS as a storage device. EBS comes with many different options to fit the specific needs of an application. EBS is most likely the right choice if you need quick access to read and write files to the cloud and you need these files to be stored long term. You can also rapidly unmount the file system from one EC2 instance and deploy it to another instance by using snapshots.",
      "rationale": "While Service Linked Roles offer quick deployment, using default configurations isn't recommended for security best practices.",
      "impact": ""
    }
  },
  {
    "id": "2.1",
    "title": "Ensure creating EC2 instance with EBS",
    "assessment": "Manual",
    "description": "EBS are storage volumes that you attach to Amazon EC2 instances. After you attach a volume to an instance, you can use it in the same way you would use a local hard drive attached to a computer, for example to store files or to install applications.",
    "rationale": "",
    "audit": "Creating EC2 instance with Volume:- To create an EC2 instance with a volume in AWS, you can follow these general steps: 1. Initializing a Secure EC2 Instance: Navigate to the EC2 dashboard within your AWS console. Make sure you\u2019re in the region that\u2019s right for you. Select \u201cLaunch Instance\u201d. 2. Naming the EC2 instance: Name your EC2 instance according to the proper naming convention set by your organization. 3. Configure the operating system: You can choose any operating system according to your needs. In this tutorial, Ubuntu is the OS of choice. 4. Create a key pair Next, create a key pair. You will need this to login your EC2 instance. We\u2019re going to log in via SSH. Select \u201cCreate new key pair\u201d. Give your key a name, select RSA encryption, and select Open SSH. As you can see by the prompt, you will need to keep the private key that\u2019s generated secure on your local computer. This is how you will access your EC2 instance. Select \u201cCreate key pair\u201d your secret key will start downloading as a \u201c.pem\u201d file. Add Storage: 1. Click \"Add New Volume\" to add a new volume. 2. Specify the volume type (e.g., General Purpose SSD, Provisioned IOPS SSD, Magnetic). 3. Set the size of the volume in GB minimum of 8GB. 4. You can add multiple volumes if needed.",
    "remediation": "References: 1. https://aws.amazon.com/ebs/",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://aws.amazon.com/ebs/",
    "function_names": [
      "ec2_ebs_volume_creation_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_creation_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "EBS are storage volumes that you attach to Amazon EC2 instances. After you attach a volume to an instance, you can use it in the same way you would use a local hard drive attached to a computer, for example to store files or to install applications.",
      "audit_steps": "Creating EC2 instance with Volume:- To create an EC2 instance with a volume in AWS, you can follow these general steps: 1. Initializing a Secure EC2 Instance: Navigate to the EC2 dashboard within your AWS console. Make sure you\u2019re in the region that\u2019s right for you. Select \u201cLaunch Instance\u201d. 2. Naming the EC2 instance: Name your EC2 instance according to the proper naming convention set by your organization. 3. Configure the operating system: You can choose any operating system according to your needs. In this tutorial, Ubuntu is the OS of choice. 4. Create a key pair Next, create a key pair. You will need this to login your EC2 instance. We\u2019re going to log in via SSH. Select \u201cCreate new key pair\u201d. Give your key a name, select RSA encryption, and select Open SSH. As you can see by the prompt, you will need to keep the private key that\u2019s generated secure on your local computer. This is how you will access your EC2 instance. Select \u201cCreate key pair\u201d your secret key will start downloading as a \u201c.pem\u201d file. Add Storage: 1. Click \"Add New Volume\" to add a new volume. 2. Specify the volume type (e.g., General Purpose SSD, Provisioned IOPS SSD, Magnetic). 3. Set the size of the volume in GB minimum of 8GB. 4. You can add multiple volumes if needed.",
      "remediation_steps": "References: 1. https://aws.amazon.com/ebs/",
      "rationale": "",
      "impact": ""
    }
  },
  {
    "id": "2.2",
    "title": "Ensure configuring Security Groups",
    "assessment": "Manual",
    "description": "Security groups are your first line of defense for the EC2 instance. A security group is a firewall that controls inbound and outbound traffic.",
    "rationale": "Security groups play a critical role in maintaining the security of your AWS resources. It is advisable to restrict traffic to only what is necessary for accessing your instance, thereby minimizing potential security risks.",
    "audit": "Open traffic for SSH, HTTP, and HTTPS. Make sure to allow traffic from anywhere, unless you will be accessing the instance from a secure workstation or server with a static IP address.",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "ec2_securitygroup_rules_check",
      "ec2_default_security_group_no_inbound_rules"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check",
        "ec2_default_security_group_no_inbound_rules"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Security groups are your first line of defense for the EC2 instance. A security group is a firewall that controls inbound and outbound traffic.",
      "audit_steps": "Open traffic for SSH, HTTP, and HTTPS. Make sure to allow traffic from anywhere, unless you will be accessing the instance from a secure workstation or server with a static IP address.",
      "remediation_steps": null,
      "rationale": "Security groups play a critical role in maintaining the security of your AWS resources. It is advisable to restrict traffic to only what is necessary for accessing your instance, thereby minimizing potential security risks.",
      "impact": ""
    }
  },
  {
    "id": "2.3",
    "title": "Ensure the proper configuration of EBS storage",
    "assessment": "Manual",
    "description": "All computer instances need to have a device on which to store files. EBS is built on top of EC2 instances as a block storage device.",
    "rationale": "Remember that we are working with cloud computing. Rather than purchasing and manually installing disk drives on a server, AWS allows you to virtually add storage using Elastic Block Store (EBS). Impact: Failure to properly configure EBS storage can lead to data loss, performance issues, increased costs, security vulnerabilities, and operational downtime. Ensuring correct configuration is crucial to maintain data integrity, efficiency, cost-effectiveness, security, and reliability.",
    "audit": "",
    "remediation": "1. Open the Amazon EC2 Console : Navigate to the EC2 Dashboard in the AWS Management Console. 2. Select Volumes : Under the \"Elastic Block Store\" section, select \"Volumes\". 3. Create Volume : o Click on \"Create Volume\". o Choose the volume type (e.g., General Purpose SSD (gp2), Provisioned IOPS SSD (io1), etc.). o Specify the size and availability zone. o Optionally, configure additional settings such as IOPS, encryption, and tags. 4. Attach Volume to Instance : o Select the volume you created. o Click on \"Actions\" and choose \"Attach Volume\". o Select the instance to which you want to attach the volume and specify the device name. 5. Format and Mount the Volume (on the instance): o Connect to your instance using SSH. o List available disks using the command: lsblk. o Format the new volume (e.g., sudo mkfs -t ext4 /dev/xvdf for ext4 filesystem). o Create a mount point (e.g., sudo mkdir /mnt/data). o Mount the volume (e.g., sudo mount /dev/xvdf /mnt/data). 6. Configure Automatic Mounting (optional) : o Edit the /etc/fstab file to add an entry for the new volume to ensure it mounts automatically on reboot. o Example entry: /dev/xvdf /mnt/data ext4 defaults,nofail 0 2. By following these steps, you can effectively configure and manage EBS storage for your AWS instances.",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Failure to properly configure EBS storage can lead to data loss, performance issues, increased costs, security vulnerabilities, and operational downtime. Ensuring correct configuration is crucial to maintain data integrity, efficiency, cost-effectiveness, security, and reliability.",
    "function_names": [
      "ec2_ebs_volume_encryption_kms_key_check",
      "ec2_ebs_volume_protected_by_backup_plan"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_encryption_kms_key_check",
        "ec2_ebs_volume_protected_by_backup_plan"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "All computer instances need to have a device on which to store files. EBS is built on top of EC2 instances as a block storage device.",
      "audit_steps": "",
      "remediation_steps": "1. Open the Amazon EC2 Console : Navigate to the EC2 Dashboard in the AWS Management Console. 2. Select Volumes : Under the \"Elastic Block Store\" section, select \"Volumes\". 3. Create Volume : o Click on \"Create Volume\". o Choose the volume type (e.g., General Purpose SSD (gp2), Provisioned IOPS SSD (io1), etc.). o Specify the size and availability zone. o Optionally, configure additional settings such as IOPS, encryption, and tags. 4. Attach Volume to Instance : o Select the volume you created. o Click on \"Actions\" and choose \"Attach Volume\". o Select the instance to which you want to attach the volume and specify the device name. 5. Format and Mount the Volume (on the instance): o Connect to your instance using SSH. o List available disks using the command: lsblk. o Format the new volume (e.g., sudo mkfs -t ext4 /dev/xvdf for ext4 filesystem). o Create a mount point (e.g., sudo mkdir /mnt/data). o Mount the volume (e.g., sudo mount /dev/xvdf /mnt/data). 6. Configure Automatic Mounting (optional) : o Edit the /etc/fstab file to add an entry for the new volume to ensure it mounts automatically on reboot. o Example entry: /dev/xvdf /mnt/data ext4 defaults,nofail 0 2. By following these steps, you can effectively configure and manage EBS storage for your AWS instances.",
      "rationale": "Remember that we are working with cloud computing. Rather than purchasing and manually installing disk drives on a server, AWS allows you to virtually add storage using Elastic Block Store (EBS). Impact: Failure to properly configure EBS storage can lead to data loss, performance issues, increased costs, security vulnerabilities, and operational downtime. Ensuring correct configuration is crucial to maintain data integrity, efficiency, cost-effectiveness, security, and reliability.",
      "impact": "Failure to properly configure EBS storage can lead to data loss, performance issues, increased costs, security vulnerabilities, and operational downtime. Ensuring correct configuration is crucial to maintain data integrity, efficiency, cost-effectiveness, security, and reliability."
    }
  },
  {
    "id": "2.4",
    "title": "Ensure the creation of a new volume",
    "assessment": "Manual",
    "description": "Leave the root volume unchanged and create a new volume. To ensure the security of the instance and prevent data loss, select \"no\" under the \"delete on termination\" option and encrypt your volume using AWS KMS. A default key is available for encrypting the volume.",
    "rationale": "By leaving the root volume unchanged and creating a new volume, you separate critical data from the operating system. Selecting \"no\" for the \"delete on termination\" option ensures that data on the new volume is not automatically deleted when the instance is terminated, protecting against accidental data loss. Encrypting the volume using AWS KMS adds an additional layer of security, safeguarding the data against unauthorized access. The use of a default key for encryption simplifies the process while maintaining strong security measures. Impact: Not following these steps can lead to data loss, security risks, operational disruptions, and prolonged recovery times. Setting \"delete on termination\" to \"no\" prevents data deletion upon instance termination, while encrypting the volume with AWS KMS protects against unauthorized access. Storing critical data separately from the root volume ensures operational continuity and easier recovery.",
    "audit": "To audit this configuration in AWS, follow these steps: 1. Access the AWS Management Console : Log in to your AWS account and navigate to the AWS Management Console. 2. Review EBS Volumes : o Go to the EC2 Dashboard and select \"Volumes\" under the \"Elastic Block Store\" section. o Check the properties of each volume to ensure that the root volume is unchanged and new volumes are created as needed. 3. Check \"Delete on Termination\" Setting : o In the \"Volumes\" section, select each volume and click on the \"Actions\" button. o Select \"Modify Volume\" and ensure that \"Delete on Termination\" is set to \"no\" for the critical volumes. o Alternatively, go to the \"Instances\" section, select an instance, click on the \"Actions\" button, choose \"Instance Settings,\" and then \"Change Termination Protection.\" 4. Verify Encryption : o In the \"Volumes\" section, check the \"Encrypted\" column to confirm that the volumes are encrypted. o For detailed information, select a volume and view its details to ensure it is encrypted using AWS KMS. 5. Review IAM Policies : o Navigate to the IAM Dashboard and review the policies attached to users, groups, and roles to ensure they have appropriate permissions to create, modify, and encrypt EBS volumes. 6. Use AWS Config : o Enable AWS Config to continuously monitor and record AWS resource configurations. o Create AWS Config rules to check for compliance with best practices, such as ensuring volumes are encrypted and \"Delete on Termination\" is set to \"no.\" 7. Generate Reports : o Use AWS CloudTrail to review logs of API calls made to EBS volumes, ensuring compliance with the required configurations. o Generate compliance reports using AWS Config and AWS CloudTrail to provide evidence of adherence to best practices. By following these steps, you can effectively audit your EBS configurations to ensure data security, integrity, and operational reliability.",
    "remediation": "1. Volume Configurations : o After configuring your volume, ensure the settings meet your requirements. To secure your file system and prevent data loss, verify that the \"Delete on Termination\" option is set to \"no,\" the volume is encrypted, and the KMS key is correctly specified. For this EBS instance, we are using the default KMS key. 2. Availability Zone Consistency : o Ensure your EBS volume is in the same Availability Zone as your EC2 instance. An EBS volume can only be attached to an EC2 instance within the same Availability Zone. You can mount and unmount EBS volumes to any EC2 instance within the same zone as needed.",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not following these steps can lead to data loss, security risks, operational disruptions, and prolonged recovery times. Setting \"delete on termination\" to \"no\" prevents data deletion upon instance termination, while encrypting the volume with AWS KMS protects against unauthorized access. Storing critical data separately from the root volume ensures operational continuity and easier recovery.",
    "function_names": [
      "ec2_ebs_volume_encryption_kms_key_check",
      "ec2_ebs_snapshot_creation_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_encryption_kms_key_check",
        "ec2_ebs_snapshot_creation_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Leave the root volume unchanged and create a new volume. To ensure the security of the instance and prevent data loss, select \"no\" under the \"delete on termination\" option and encrypt your volume using AWS KMS. A default key is available for encrypting the volume.",
      "audit_steps": "To audit this configuration in AWS, follow these steps: 1. Access the AWS Management Console : Log in to your AWS account and navigate to the AWS Management Console. 2. Review EBS Volumes : o Go to the EC2 Dashboard and select \"Volumes\" under the \"Elastic Block Store\" section. o Check the properties of each volume to ensure that the root volume is unchanged and new volumes are created as needed. 3. Check \"Delete on Termination\" Setting : o In the \"Volumes\" section, select each volume and click on the \"Actions\" button. o Select \"Modify Volume\" and ensure that \"Delete on Termination\" is set to \"no\" for the critical volumes. o Alternatively, go to the \"Instances\" section, select an instance, click on the \"Actions\" button, choose \"Instance Settings,\" and then \"Change Termination Protection.\" 4. Verify Encryption : o In the \"Volumes\" section, check the \"Encrypted\" column to confirm that the volumes are encrypted. o For detailed information, select a volume and view its details to ensure it is encrypted using AWS KMS. 5. Review IAM Policies : o Navigate to the IAM Dashboard and review the policies attached to users, groups, and roles to ensure they have appropriate permissions to create, modify, and encrypt EBS volumes. 6. Use AWS Config : o Enable AWS Config to continuously monitor and record AWS resource configurations. o Create AWS Config rules to check for compliance with best practices, such as ensuring volumes are encrypted and \"Delete on Termination\" is set to \"no.\" 7. Generate Reports : o Use AWS CloudTrail to review logs of API calls made to EBS volumes, ensuring compliance with the required configurations. o Generate compliance reports using AWS Config and AWS CloudTrail to provide evidence of adherence to best practices. By following these steps, you can effectively audit your EBS configurations to ensure data security, integrity, and operational reliability.",
      "remediation_steps": "1. Volume Configurations : o After configuring your volume, ensure the settings meet your requirements. To secure your file system and prevent data loss, verify that the \"Delete on Termination\" option is set to \"no,\" the volume is encrypted, and the KMS key is correctly specified. For this EBS instance, we are using the default KMS key. 2. Availability Zone Consistency : o Ensure your EBS volume is in the same Availability Zone as your EC2 instance. An EBS volume can only be attached to an EC2 instance within the same Availability Zone. You can mount and unmount EBS volumes to any EC2 instance within the same zone as needed.",
      "rationale": "By leaving the root volume unchanged and creating a new volume, you separate critical data from the operating system. Selecting \"no\" for the \"delete on termination\" option ensures that data on the new volume is not automatically deleted when the instance is terminated, protecting against accidental data loss. Encrypting the volume using AWS KMS adds an additional layer of security, safeguarding the data against unauthorized access. The use of a default key for encryption simplifies the process while maintaining strong security measures. Impact: Not following these steps can lead to data loss, security risks, operational disruptions, and prolonged recovery times. Setting \"delete on termination\" to \"no\" prevents data deletion upon instance termination, while encrypting the volume with AWS KMS protects against unauthorized access. Storing critical data separately from the root volume ensures operational continuity and easier recovery.",
      "impact": "Not following these steps can lead to data loss, security risks, operational disruptions, and prolonged recovery times. Setting \"delete on termination\" to \"no\" prevents data deletion upon instance termination, while encrypting the volume with AWS KMS protects against unauthorized access. Storing critical data separately from the root volume ensures operational continuity and easier recovery."
    }
  },
  {
    "id": "2.5",
    "title": "Ensure creating snapshots of EBS volumes",
    "assessment": "Manual",
    "description": "A snapshot is a backup of your EBS volume that captures its state at a specific point in time, storing only the data changes since the last snapshot to optimize storage costs and speed. Snapshots are crucial for data recovery, creating new EBS volumes, and replicating data across AWS regions for disaster recovery and high availability. Restoring from a snapshot allows you to create a new EBS volume and attach it to an EC2 instance in the same availability zone, ensuring data integrity and accessibility.",
    "rationale": "The rationale behind using EBS snapshots is to ensure efficient and cost-effective data backup and recovery. By capturing only the data changes since the last snapshot, storage costs are minimized and the backup process is expedited. Snapshots are essential for maintaining data integrity, facilitating quick recovery, and enabling seamless data replication across regions, thereby enhancing disaster recovery capabilities and operational resilience. Impact: Not utilizing EBS snapshots can lead to significant risks and drawbacks. Without snapshots, data recovery becomes more complex and time-consuming, increasing the risk of prolonged downtime in the event of data loss or system failure. Additionally, the absence of incremental backups can lead to higher storage costs and inefficient use of resources. The lack of data replication across regions can severely compromise disaster recovery efforts, making it challenging to maintain high availability and operational continuity. Overall, failing to use snapshots undermines data integrity, security, and the ability to quickly restore critical information.",
    "audit": "To audit the use of EBS snapshots in AWS, follow these steps: 1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Review EBS Snapshots : o Go to the EC2 Dashboard and select \"Snapshots\" under the \"Elastic Block Store\" section. o Check the list of snapshots to ensure regular backups are being created for all critical volumes. 3. Verify Snapshot Policies : o Ensure that snapshot lifecycle policies are in place and configured correctly. o Go to the \"Lifecycle Manager\" under the EC2 Dashboard and review policies for automated snapshot creation and retention. 4. Check Snapshot Status and Details : o Review the status of each snapshot to ensure they are completed successfully. o Verify the details of snapshots, such as description, creation time, and the volume ID associated with each snapshot. 5. Inspect IAM Policies and Permissions : o Navigate to the IAM Dashboard and review the policies attached to users, groups, and roles. o Ensure that only authorized personnel have permissions to create, delete, and manage snapshots. 6. Use AWS Config Rules : o Enable AWS Config to continuously monitor and record AWS resource configurations. o Create AWS Config rules to check for compliance with best practices, such as ensuring snapshots are created regularly and are not older than a specific period. 7. Review AWS CloudTrail Logs : o Use AWS CloudTrail to review logs of API calls related to EBS snapshots. o Ensure that all snapshot activities are logged and can be traced back to authorized users and roles. 8. Generate Reports : o Utilize AWS Config and AWS CloudTrail to generate compliance and activity reports. o Review these reports to ensure adherence to snapshot policies and identify any anomalies or unauthorized activities. By following these steps, you can effectively audit the use of EBS snapshots to ensure data integrity, security, and compliance with best practices.",
    "remediation": "To create an EBS snapshot on AWS, follow these steps: 1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Navigate to the EC2 Dashboard : o In the AWS Management Console, select \"EC2\" from the services menu to open the EC2 Dashboard. 3. Select the Volume : o In the left-hand navigation pane, under \"Elastic Block Store,\" click on \"Volumes.\" o Find the volume you want to snapshot from the list and select it by clicking the checkbox next to it. 4. Create a Snapshot : o With the volume selected, click on the \"Actions\" button at the top of the page. o From the dropdown menu, select \"Create Snapshot.\" 5. Configure the Snapshot : o In the \"Create Snapshot\" dialog box, provide a description for the snapshot. This helps identify the snapshot later. o Review the volume ID to ensure it is the correct volume. 6. Initiate the Snapshot Creation : o Click the \"Create Snapshot\" button to start the snapshot creation process. 7. Monitor the Snapshot : o Navigate to the \"Snapshots\" section under \"Elastic Block Store\" in the left- hand navigation pane. o Find your snapshot in the list and monitor its status. The snapshot creation process might take some time, depending on the size of the volume and the amount of data. 8. Verify Completion : o Once the snapshot status changes to \"completed,\" it indicates that the snapshot has been successfully created and is available for use. By following these steps, you can create an EBS snapshot to ensure you have a backup of your volume at a specific point in time. References: 1. https://docs.aws.amazon.com/ebs/latest/userguide/ebs-creating-snapshot.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not utilizing EBS snapshots can lead to significant risks and drawbacks. Without snapshots, data recovery becomes more complex and time-consuming, increasing the risk of prolonged downtime in the event of data loss or system failure. Additionally, the absence of incremental backups can lead to higher storage costs and inefficient use of resources. The lack of data replication across regions can severely compromise disaster recovery efforts, making it challenging to maintain high availability and operational continuity. Overall, failing to use snapshots undermines data integrity, security, and the ability to quickly restore critical information.",
    "references": "1. https://docs.aws.amazon.com/ebs/latest/userguide/ebs-creating-snapshot.html",
    "function_names": [
      "ec2_ebs_snapshot_creation_check",
      "ec2_ebs_snapshot_account_block_public_access"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_snapshot_creation_check",
        "ec2_ebs_snapshot_account_block_public_access"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "A snapshot is a backup of your EBS volume that captures its state at a specific point in time, storing only the data changes since the last snapshot to optimize storage costs and speed. Snapshots are crucial for data recovery, creating new EBS volumes, and replicating data across AWS regions for disaster recovery and high availability. Restoring from a snapshot allows you to create a new EBS volume and attach it to an EC2 instance in the same availability zone, ensuring data integrity and accessibility.",
      "audit_steps": "To audit the use of EBS snapshots in AWS, follow these steps: 1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Review EBS Snapshots : o Go to the EC2 Dashboard and select \"Snapshots\" under the \"Elastic Block Store\" section. o Check the list of snapshots to ensure regular backups are being created for all critical volumes. 3. Verify Snapshot Policies : o Ensure that snapshot lifecycle policies are in place and configured correctly. o Go to the \"Lifecycle Manager\" under the EC2 Dashboard and review policies for automated snapshot creation and retention. 4. Check Snapshot Status and Details : o Review the status of each snapshot to ensure they are completed successfully. o Verify the details of snapshots, such as description, creation time, and the volume ID associated with each snapshot. 5. Inspect IAM Policies and Permissions : o Navigate to the IAM Dashboard and review the policies attached to users, groups, and roles. o Ensure that only authorized personnel have permissions to create, delete, and manage snapshots. 6. Use AWS Config Rules : o Enable AWS Config to continuously monitor and record AWS resource configurations. o Create AWS Config rules to check for compliance with best practices, such as ensuring snapshots are created regularly and are not older than a specific period. 7. Review AWS CloudTrail Logs : o Use AWS CloudTrail to review logs of API calls related to EBS snapshots. o Ensure that all snapshot activities are logged and can be traced back to authorized users and roles. 8. Generate Reports : o Utilize AWS Config and AWS CloudTrail to generate compliance and activity reports. o Review these reports to ensure adherence to snapshot policies and identify any anomalies or unauthorized activities. By following these steps, you can effectively audit the use of EBS snapshots to ensure data integrity, security, and compliance with best practices.",
      "remediation_steps": "To create an EBS snapshot on AWS, follow these steps: 1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Navigate to the EC2 Dashboard : o In the AWS Management Console, select \"EC2\" from the services menu to open the EC2 Dashboard. 3. Select the Volume : o In the left-hand navigation pane, under \"Elastic Block Store,\" click on \"Volumes.\" o Find the volume you want to snapshot from the list and select it by clicking the checkbox next to it. 4. Create a Snapshot : o With the volume selected, click on the \"Actions\" button at the top of the page. o From the dropdown menu, select \"Create Snapshot.\" 5. Configure the Snapshot : o In the \"Create Snapshot\" dialog box, provide a description for the snapshot. This helps identify the snapshot later. o Review the volume ID to ensure it is the correct volume. 6. Initiate the Snapshot Creation : o Click the \"Create Snapshot\" button to start the snapshot creation process. 7. Monitor the Snapshot : o Navigate to the \"Snapshots\" section under \"Elastic Block Store\" in the left- hand navigation pane. o Find your snapshot in the list and monitor its status. The snapshot creation process might take some time, depending on the size of the volume and the amount of data. 8. Verify Completion : o Once the snapshot status changes to \"completed,\" it indicates that the snapshot has been successfully created and is available for use. By following these steps, you can create an EBS snapshot to ensure you have a backup of your volume at a specific point in time. References: 1. https://docs.aws.amazon.com/ebs/latest/userguide/ebs-creating-snapshot.html",
      "rationale": "The rationale behind using EBS snapshots is to ensure efficient and cost-effective data backup and recovery. By capturing only the data changes since the last snapshot, storage costs are minimized and the backup process is expedited. Snapshots are essential for maintaining data integrity, facilitating quick recovery, and enabling seamless data replication across regions, thereby enhancing disaster recovery capabilities and operational resilience. Impact: Not utilizing EBS snapshots can lead to significant risks and drawbacks. Without snapshots, data recovery becomes more complex and time-consuming, increasing the risk of prolonged downtime in the event of data loss or system failure. Additionally, the absence of incremental backups can lead to higher storage costs and inefficient use of resources. The lack of data replication across regions can severely compromise disaster recovery efforts, making it challenging to maintain high availability and operational continuity. Overall, failing to use snapshots undermines data integrity, security, and the ability to quickly restore critical information.",
      "impact": "Not utilizing EBS snapshots can lead to significant risks and drawbacks. Without snapshots, data recovery becomes more complex and time-consuming, increasing the risk of prolonged downtime in the event of data loss or system failure. Additionally, the absence of incremental backups can lead to higher storage costs and inefficient use of resources. The lack of data replication across regions can severely compromise disaster recovery efforts, making it challenging to maintain high availability and operational continuity. Overall, failing to use snapshots undermines data integrity, security, and the ability to quickly restore critical information."
    }
  },
  {
    "id": "2.6",
    "title": "Ensure Proper IAM Configuration for EC2 Instances",
    "assessment": "Manual",
    "description": "IAM, or Identity and Access Management, is a vital security service used to control and manage access to AWS resources, ensuring only authorized users and services can interact with them. It allows you to create users and groups, set permissions, enforce multi-factor authentication, and implement least privilege principles to enhance security and compliance.",
    "rationale": "The rationale behind using IAM is to enhance security by controlling and managing access to AWS resources, ensuring that only authorized users and services can interact with them. This minimizes the risk of unauthorized access and potential security breaches, while also allowing for the implementation of best practices such as multi- factor authentication and least privilege principles, which further strengthen the security and compliance of your AWS environment. Impact: Not implementing IAM properly can lead to significant security vulnerabilities, including unauthorized access to AWS resources, data breaches, and potential loss of sensitive information. Without IAM, it is challenging to enforce access controls, monitor user activity, and implement security best practices such as multi-factor authentication and least privilege principles. This can result in increased risk of malicious attacks, operational disruptions, non-compliance with regulatory requirements, and substantial financial damage.",
    "audit": "1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Review IAM Users and Roles : o Go to the IAM Dashboard and select \"Users\" to review all user accounts. o Check each user for appropriate permissions, MFA enablement, and adherence to the principle of least privilege. o Similarly, review the \"Roles\" section to ensure roles have the correct permissions and are assigned appropriately. 3. Check IAM Policies : o In the IAM Dashboard, navigate to \"Policies\" and review both AWS managed and customer-managed policies. o Ensure that policies follow the principle of least privilege and do not grant excessive permissions. 4. Analyze IAM Groups : o Review the groups in the IAM Dashboard under \"Groups.\" o Ensure that users are grouped appropriately and that groups have suitable permissions. 5. Examine MFA Settings : o Verify that Multi-Factor Authentication (MFA) is enabled for all users with console access. o In the IAM Dashboard, click on \"Users\" and check the \"Security credentials\" tab for each user to confirm MFA setup. 6. Audit IAM Activity : o Use AWS CloudTrail to review logs of IAM activities, including user logins, policy changes, and other management activities. o Ensure that all IAM activities are logged and can be traced back to authorized users. 7. Implement AWS Config Rules : o Enable AWS Config to continuously monitor IAM configurations. o Create and apply AWS Config rules that check for compliance with best practices, such as ensuring all users have MFA enabled and policies are not overly permissive. 8. Generate IAM Reports : o Use AWS IAM Access Analyzer to identify permissions granted to resources that can be accessed from outside your AWS account. o Generate IAM credential reports from the IAM Dashboard to review the status of all IAM users, including when their passwords were last used and when their access keys were last rotated. 9. Conduct Regular Reviews : o Schedule regular audits to review IAM configurations and policies. o Periodically update and refine IAM policies and permissions to ensure ongoing compliance and security.",
    "remediation": "1. Restrict Overly Permissive Policies : o Identify and modify any IAM policies that are overly permissive. Update policies to grant the least privilege necessary for users to perform their tasks. o Use IAM policy simulator to test and validate the changes to ensure they do not disrupt operations. 2. Enable Multi-Factor Authentication (MFA) : o For all users with console access, enable MFA. This adds an additional layer of security. o Navigate to the IAM Dashboard, select \"Users,\" and enable MFA under the \"Security credentials\" tab for each user. 3. Rotate Access Keys : o Regularly rotate access keys for IAM users to reduce the risk of compromised credentials. o In the IAM Dashboard, select \"Users,\" go to the \"Security credentials\" tab, and create new access keys. Then, disable and delete old access keys after confirming the new keys are functioning correctly. 4. Remove Unnecessary Users and Roles : o Delete any IAM users or roles that are no longer needed to minimize potential security risks. o Review each user and role, and remove those that are inactive or no longer required. 5. Implement Role-Based Access Control (RBAC) : o Group users by their roles and assign permissions based on job functions. o Use IAM groups to manage permissions collectively rather than individually for each user. 6. Regularly Review and Update IAM Policies : o Set up a regular schedule to review and update IAM policies to ensure they remain aligned with security best practices and organizational changes. o Use AWS Config and AWS Config Rules to continuously monitor policy changes and ensure compliance. 7. Enable AWS CloudTrail and AWS Config : o Ensure that AWS CloudTrail is enabled to log all IAM activities. Configure it to capture and analyze logs for unauthorized access and policy changes. o Enable AWS Config to continuously monitor IAM resource configurations and compliance with best practices. 8. Conduct Security Awareness Training : o Provide regular security training for all users to educate them on best practices for using IAM and the importance of security measures like MFA and least privilege access. 9. Implement IAM Access Analyzer : o Use IAM Access Analyzer to identify and remediate permissions that allow external access to your resources. o Regularly review the findings and adjust permissions to ensure that only the necessary external access is granted. References: 1. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon- ec2.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not implementing IAM properly can lead to significant security vulnerabilities, including unauthorized access to AWS resources, data breaches, and potential loss of sensitive information. Without IAM, it is challenging to enforce access controls, monitor user activity, and implement security best practices such as multi-factor authentication and least privilege principles. This can result in increased risk of malicious attacks, operational disruptions, non-compliance with regulatory requirements, and substantial financial damage.",
    "references": "1. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon- ec2.html",
    "function_names": [
      "ec2_ebs_volume_protected_by_backup_plan",
      "ec2_ebs_snapshot_creation_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_protected_by_backup_plan",
        "ec2_ebs_snapshot_creation_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "IAM, or Identity and Access Management, is a vital security service used to control and manage access to AWS resources, ensuring only authorized users and services can interact with them. It allows you to create users and groups, set permissions, enforce multi-factor authentication, and implement least privilege principles to enhance security and compliance.",
      "audit_steps": "1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Review IAM Users and Roles : o Go to the IAM Dashboard and select \"Users\" to review all user accounts. o Check each user for appropriate permissions, MFA enablement, and adherence to the principle of least privilege. o Similarly, review the \"Roles\" section to ensure roles have the correct permissions and are assigned appropriately. 3. Check IAM Policies : o In the IAM Dashboard, navigate to \"Policies\" and review both AWS managed and customer-managed policies. o Ensure that policies follow the principle of least privilege and do not grant excessive permissions. 4. Analyze IAM Groups : o Review the groups in the IAM Dashboard under \"Groups.\" o Ensure that users are grouped appropriately and that groups have suitable permissions. 5. Examine MFA Settings : o Verify that Multi-Factor Authentication (MFA) is enabled for all users with console access. o In the IAM Dashboard, click on \"Users\" and check the \"Security credentials\" tab for each user to confirm MFA setup. 6. Audit IAM Activity : o Use AWS CloudTrail to review logs of IAM activities, including user logins, policy changes, and other management activities. o Ensure that all IAM activities are logged and can be traced back to authorized users. 7. Implement AWS Config Rules : o Enable AWS Config to continuously monitor IAM configurations. o Create and apply AWS Config rules that check for compliance with best practices, such as ensuring all users have MFA enabled and policies are not overly permissive. 8. Generate IAM Reports : o Use AWS IAM Access Analyzer to identify permissions granted to resources that can be accessed from outside your AWS account. o Generate IAM credential reports from the IAM Dashboard to review the status of all IAM users, including when their passwords were last used and when their access keys were last rotated. 9. Conduct Regular Reviews : o Schedule regular audits to review IAM configurations and policies. o Periodically update and refine IAM policies and permissions to ensure ongoing compliance and security.",
      "remediation_steps": "1. Restrict Overly Permissive Policies : o Identify and modify any IAM policies that are overly permissive. Update policies to grant the least privilege necessary for users to perform their tasks. o Use IAM policy simulator to test and validate the changes to ensure they do not disrupt operations. 2. Enable Multi-Factor Authentication (MFA) : o For all users with console access, enable MFA. This adds an additional layer of security. o Navigate to the IAM Dashboard, select \"Users,\" and enable MFA under the \"Security credentials\" tab for each user. 3. Rotate Access Keys : o Regularly rotate access keys for IAM users to reduce the risk of compromised credentials. o In the IAM Dashboard, select \"Users,\" go to the \"Security credentials\" tab, and create new access keys. Then, disable and delete old access keys after confirming the new keys are functioning correctly. 4. Remove Unnecessary Users and Roles : o Delete any IAM users or roles that are no longer needed to minimize potential security risks. o Review each user and role, and remove those that are inactive or no longer required. 5. Implement Role-Based Access Control (RBAC) : o Group users by their roles and assign permissions based on job functions. o Use IAM groups to manage permissions collectively rather than individually for each user. 6. Regularly Review and Update IAM Policies : o Set up a regular schedule to review and update IAM policies to ensure they remain aligned with security best practices and organizational changes. o Use AWS Config and AWS Config Rules to continuously monitor policy changes and ensure compliance. 7. Enable AWS CloudTrail and AWS Config : o Ensure that AWS CloudTrail is enabled to log all IAM activities. Configure it to capture and analyze logs for unauthorized access and policy changes. o Enable AWS Config to continuously monitor IAM resource configurations and compliance with best practices. 8. Conduct Security Awareness Training : o Provide regular security training for all users to educate them on best practices for using IAM and the importance of security measures like MFA and least privilege access. 9. Implement IAM Access Analyzer : o Use IAM Access Analyzer to identify and remediate permissions that allow external access to your resources. o Regularly review the findings and adjust permissions to ensure that only the necessary external access is granted. References: 1. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon- ec2.html",
      "rationale": "The rationale behind using IAM is to enhance security by controlling and managing access to AWS resources, ensuring that only authorized users and services can interact with them. This minimizes the risk of unauthorized access and potential security breaches, while also allowing for the implementation of best practices such as multi- factor authentication and least privilege principles, which further strengthen the security and compliance of your AWS environment. Impact: Not implementing IAM properly can lead to significant security vulnerabilities, including unauthorized access to AWS resources, data breaches, and potential loss of sensitive information. Without IAM, it is challenging to enforce access controls, monitor user activity, and implement security best practices such as multi-factor authentication and least privilege principles. This can result in increased risk of malicious attacks, operational disruptions, non-compliance with regulatory requirements, and substantial financial damage.",
      "impact": "Not implementing IAM properly can lead to significant security vulnerabilities, including unauthorized access to AWS resources, data breaches, and potential loss of sensitive information. Without IAM, it is challenging to enforce access controls, monitor user activity, and implement security best practices such as multi-factor authentication and least privilege principles. This can result in increased risk of malicious attacks, operational disruptions, non-compliance with regulatory requirements, and substantial financial damage."
    }
  },
  {
    "id": "2.7",
    "title": "Ensure creating IAM User",
    "assessment": "Manual",
    "description": "IAM users are individuals whose accounts have been created by the AWS administrator, providing them access to specific AWS resources. These users have undergone identity verification with your organization, ensuring that only authorized personnel can manage and interact with your AWS environment.",
    "rationale": "The purpose of creating IAM users and verifying their identities with your organization is to ensure that only authorized individuals have access to AWS resources, enhancing security and preventing unauthorized access. This practice helps maintain control over your AWS environment, ensuring that sensitive data and critical operations are managed by trusted and validated personnel. Impact: Not creating IAM users and verifying their identities can lead to unauthorized access to your AWS resources, increasing the risk of security breaches and data leaks. This lack of control can result in compromised sensitive data, unauthorized changes to critical systems, and overall reduced security posture, potentially causing significant operational and financial damage to your organization.",
    "audit": "1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Review IAM Users : o Go to the IAM Dashboard and select \"Users.\" o Check the list of IAM users to ensure that only authorized users are present. 3. Check User Details : o For each user, click on their name to view their details. o Verify the \"User ARN\" and ensure that the user was created by an authorized administrator. o Check the \"Security credentials\" tab to see if Multi-Factor Authentication (MFA) is enabled for added security. 4. Verify Identity Policies : o Review the policies attached to each user to ensure they are appropriate for the user's role. o Check that permissions follow the principle of least privilege, granting only the necessary access. 5. Monitor Login Activity : o Use AWS CloudTrail to review login activities for each IAM user. o Check for any unusual login patterns or unauthorized access attempts. 6. Use AWS IAM Access Analyzer : o Enable IAM Access Analyzer to identify any IAM resources shared outside your AWS account. o Review findings to ensure that only verified and authorized users have access to your resources. 7. Generate IAM Credential Reports : o In the IAM Dashboard, go to \"Credential reports\" and generate a report. o Review the report for details on all IAM users, including their access key age, password age, and MFA status. 8. Implement AWS Config Rules : o Enable AWS Config to continuously monitor IAM configurations. o Create and apply AWS Config rules to check for compliance with identity verification and user management best practices. 9. Review IAM Roles and Groups : o Ensure that roles and groups are properly configured and assigned only to authorized users. o Verify that roles have the correct trust relationships and that group memberships are appropriate for the user\u2019s responsibilities. 10. Schedule Regular Audits : o Set up regular intervals to audit IAM users and their access rights. o Keep records of audit findings and remediation actions to maintain a secure and compliant AWS environment.",
    "remediation": "1. Remove Unauthorized Users : o Go to the IAM Dashboard, select \"Users,\" and review the list of users. o Identify any unauthorized or unverified users and delete their accounts to prevent unauthorized access. 2. Enable Multi-Factor Authentication (MFA) : o For each IAM user, go to the \"Security credentials\" tab and enable MFA. o Ensure all users have MFA configured to enhance security and reduce the risk of unauthorized access. 3. Update User Policies : o Review the policies attached to each IAM user. o Modify policies to follow the principle of least privilege, ensuring users have only the permissions necessary for their role. o Remove any overly permissive policies that could lead to security risks. 4. Rotate Access Keys : o For IAM users with long-lived access keys, create new keys and update the applications or services using them. o Delete the old access keys to reduce the risk of compromised credentials. o Encourage regular rotation of access keys as a security best practice. 5. Review and Correct IAM Roles and Groups : o Ensure IAM roles are assigned only to authorized users and that trust relationships are properly configured. o Check group memberships and remove users who should not be part of specific groups. o Update role policies to adhere to the principle of least privilege. 6. Configure AWS IAM Access Analyzer : o Enable IAM Access Analyzer to continuously monitor and analyze access to your IAM resources. o Address any findings related to unauthorized or overly broad access permissions. 7. Implement and Enforce IAM Policies : o Create and enforce organizational IAM policies that require identity verification for all users. o Use AWS Organizations and Service Control Policies (SCPs) to enforce these policies across all accounts within your organization. 8. Enable AWS Config and Create Compliance Rules : o Enable AWS Config to monitor IAM configurations and compliance. o Create AWS Config rules to ensure all users have MFA enabled, policies adhere to least privilege, and access keys are rotated regularly. 9. Conduct Regular Training : o Provide regular security awareness training for all users to emphasize the importance of secure IAM practices. o Educate users on how to properly use IAM features and the significance of identity verification. 10. Schedule Regular Reviews and Audits : o Establish a schedule for regular audits of IAM configurations and access controls. o Document findings and remediation actions taken during each audit. o Continuously improve your IAM practices based on audit results and evolving security threats. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not creating IAM users and verifying their identities can lead to unauthorized access to your AWS resources, increasing the risk of security breaches and data leaks. This lack of control can result in compromised sensitive data, unauthorized changes to critical systems, and overall reduced security posture, potentially causing significant operational and financial damage to your organization.",
    "references": "1. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html",
    "function_names": [
      "ec2_ebs_volume_creation_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_creation_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "IAM users are individuals whose accounts have been created by the AWS administrator, providing them access to specific AWS resources. These users have undergone identity verification with your organization, ensuring that only authorized personnel can manage and interact with your AWS environment.",
      "audit_steps": "1. Access the AWS Management Console : o Log in to your AWS account and navigate to the AWS Management Console. 2. Review IAM Users : o Go to the IAM Dashboard and select \"Users.\" o Check the list of IAM users to ensure that only authorized users are present. 3. Check User Details : o For each user, click on their name to view their details. o Verify the \"User ARN\" and ensure that the user was created by an authorized administrator. o Check the \"Security credentials\" tab to see if Multi-Factor Authentication (MFA) is enabled for added security. 4. Verify Identity Policies : o Review the policies attached to each user to ensure they are appropriate for the user's role. o Check that permissions follow the principle of least privilege, granting only the necessary access. 5. Monitor Login Activity : o Use AWS CloudTrail to review login activities for each IAM user. o Check for any unusual login patterns or unauthorized access attempts. 6. Use AWS IAM Access Analyzer : o Enable IAM Access Analyzer to identify any IAM resources shared outside your AWS account. o Review findings to ensure that only verified and authorized users have access to your resources. 7. Generate IAM Credential Reports : o In the IAM Dashboard, go to \"Credential reports\" and generate a report. o Review the report for details on all IAM users, including their access key age, password age, and MFA status. 8. Implement AWS Config Rules : o Enable AWS Config to continuously monitor IAM configurations. o Create and apply AWS Config rules to check for compliance with identity verification and user management best practices. 9. Review IAM Roles and Groups : o Ensure that roles and groups are properly configured and assigned only to authorized users. o Verify that roles have the correct trust relationships and that group memberships are appropriate for the user\u2019s responsibilities. 10. Schedule Regular Audits : o Set up regular intervals to audit IAM users and their access rights. o Keep records of audit findings and remediation actions to maintain a secure and compliant AWS environment.",
      "remediation_steps": "1. Remove Unauthorized Users : o Go to the IAM Dashboard, select \"Users,\" and review the list of users. o Identify any unauthorized or unverified users and delete their accounts to prevent unauthorized access. 2. Enable Multi-Factor Authentication (MFA) : o For each IAM user, go to the \"Security credentials\" tab and enable MFA. o Ensure all users have MFA configured to enhance security and reduce the risk of unauthorized access. 3. Update User Policies : o Review the policies attached to each IAM user. o Modify policies to follow the principle of least privilege, ensuring users have only the permissions necessary for their role. o Remove any overly permissive policies that could lead to security risks. 4. Rotate Access Keys : o For IAM users with long-lived access keys, create new keys and update the applications or services using them. o Delete the old access keys to reduce the risk of compromised credentials. o Encourage regular rotation of access keys as a security best practice. 5. Review and Correct IAM Roles and Groups : o Ensure IAM roles are assigned only to authorized users and that trust relationships are properly configured. o Check group memberships and remove users who should not be part of specific groups. o Update role policies to adhere to the principle of least privilege. 6. Configure AWS IAM Access Analyzer : o Enable IAM Access Analyzer to continuously monitor and analyze access to your IAM resources. o Address any findings related to unauthorized or overly broad access permissions. 7. Implement and Enforce IAM Policies : o Create and enforce organizational IAM policies that require identity verification for all users. o Use AWS Organizations and Service Control Policies (SCPs) to enforce these policies across all accounts within your organization. 8. Enable AWS Config and Create Compliance Rules : o Enable AWS Config to monitor IAM configurations and compliance. o Create AWS Config rules to ensure all users have MFA enabled, policies adhere to least privilege, and access keys are rotated regularly. 9. Conduct Regular Training : o Provide regular security awareness training for all users to emphasize the importance of secure IAM practices. o Educate users on how to properly use IAM features and the significance of identity verification. 10. Schedule Regular Reviews and Audits : o Establish a schedule for regular audits of IAM configurations and access controls. o Document findings and remediation actions taken during each audit. o Continuously improve your IAM practices based on audit results and evolving security threats. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html",
      "rationale": "The purpose of creating IAM users and verifying their identities with your organization is to ensure that only authorized individuals have access to AWS resources, enhancing security and preventing unauthorized access. This practice helps maintain control over your AWS environment, ensuring that sensitive data and critical operations are managed by trusted and validated personnel. Impact: Not creating IAM users and verifying their identities can lead to unauthorized access to your AWS resources, increasing the risk of security breaches and data leaks. This lack of control can result in compromised sensitive data, unauthorized changes to critical systems, and overall reduced security posture, potentially causing significant operational and financial damage to your organization.",
      "impact": "Not creating IAM users and verifying their identities can lead to unauthorized access to your AWS resources, increasing the risk of security breaches and data leaks. This lack of control can result in compromised sensitive data, unauthorized changes to critical systems, and overall reduced security posture, potentially causing significant operational and financial damage to your organization."
    }
  },
  {
    "id": "2.8",
    "title": "Ensure the Creation of IAM Groups",
    "assessment": "Manual",
    "description": "IAM Groups are collections of users that share the same permissions for accessing AWS resources. For instance, you can create a group named \"Administrators,\" which includes users who require full access to your AWS environment. This simplifies permission management by assigning common access policies to all members of the group.",
    "rationale": "IAM groups in AWS simplify permission management by grouping users with similar access needs and applying common access policies, reducing administrative overhead and enhancing security through the principle of least privilege. This approach ensures consistency, scalability, and ease of auditing, strengthening the overall security posture of the AWS environment.",
    "audit": "1. Enable AWS CloudTrail : o Navigate to the AWS Management Console and open the CloudTrail service. o Create a new trail or ensure that an existing trail is configured to capture API activity in your AWS account. o Verify that CloudTrail is recording events related to IAM actions, including changes to IAM groups. 2. Review CloudTrail Logs : o Access the CloudTrail console and navigate to the Event History or Insights section. o Filter the logs to focus on IAM-related events, such as CreateGroup, AddUserToGroup, RemoveUserFromGroup, and PutGroupPolicy. o Analyze the logs to track changes made to IAM groups, including user additions/removals and modifications to group policies. 3. Utilize AWS Config : o Open the AWS Config console and ensure that AWS Config is enabled for your AWS account. o Set up AWS Config rules to monitor IAM configurations, including IAM groups. o Configure rules to check for compliance with security standards or organizational policies regarding IAM group settings and permissions. 4. Check IAM Console : o Access the IAM console in the AWS Management Console. o Navigate to the \"Groups\" section to view a list of IAM groups in your account. o Review the details of each group, including its members and attached policies, to ensure they align with your security requirements.",
    "remediation": "1. CloudTrail and AWS Config Configuration : o If CloudTrail or AWS Config is not enabled, configure them to capture and monitor IAM activities and configurations respectively. Enable logging and set up appropriate rules to track IAM group changes and ensure compliance. 2. Review CloudTrail Logs for Anomalies : o Regularly review CloudTrail logs to identify any unauthorized or unexpected changes to IAM groups. o Investigate any anomalies detected in the logs, such as unauthorized user additions or policy modifications, and take appropriate action to rectify them. 3. AWS Config Remediation Rules : o Define AWS Config rules to automatically detect non-compliant IAM group configurations. o Configure remediation actions within AWS Config to automatically revert any deviations from the desired IAM group settings back to the compliant state. 4. IAM Group Cleanup : o Periodically review IAM groups to ensure they are still necessary and relevant. o Remove any unused or obsolete IAM groups to reduce the attack surface and simplify permission management. 5. Permissions Review : o Regularly review the permissions assigned to IAM groups to ensure they follow the principle of least privilege. o Remove any excessive permissions or policies that are not required for the group's intended purpose. 6. Security Best Practices : o Implement security best practices for IAM, such as enforcing multi-factor authentication (MFA) for privileged IAM users and regularly rotating access keys. o Train IAM administrators and users on security best practices to prevent inadvertent misconfigurations and unauthorized access. 7. Documentation and Monitoring : o Document IAM group configurations, policies, and access controls to maintain an audit trail and facilitate future audits. o Set up monitoring alerts to notify administrators of any suspicious activities related to IAM groups. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_create.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_create.html",
    "function_names": [
      "ec2_ebs_volume_creation_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_creation_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "IAM Groups are collections of users that share the same permissions for accessing AWS resources. For instance, you can create a group named \"Administrators,\" which includes users who require full access to your AWS environment. This simplifies permission management by assigning common access policies to all members of the group.",
      "audit_steps": "1. Enable AWS CloudTrail : o Navigate to the AWS Management Console and open the CloudTrail service. o Create a new trail or ensure that an existing trail is configured to capture API activity in your AWS account. o Verify that CloudTrail is recording events related to IAM actions, including changes to IAM groups. 2. Review CloudTrail Logs : o Access the CloudTrail console and navigate to the Event History or Insights section. o Filter the logs to focus on IAM-related events, such as CreateGroup, AddUserToGroup, RemoveUserFromGroup, and PutGroupPolicy. o Analyze the logs to track changes made to IAM groups, including user additions/removals and modifications to group policies. 3. Utilize AWS Config : o Open the AWS Config console and ensure that AWS Config is enabled for your AWS account. o Set up AWS Config rules to monitor IAM configurations, including IAM groups. o Configure rules to check for compliance with security standards or organizational policies regarding IAM group settings and permissions. 4. Check IAM Console : o Access the IAM console in the AWS Management Console. o Navigate to the \"Groups\" section to view a list of IAM groups in your account. o Review the details of each group, including its members and attached policies, to ensure they align with your security requirements.",
      "remediation_steps": "1. CloudTrail and AWS Config Configuration : o If CloudTrail or AWS Config is not enabled, configure them to capture and monitor IAM activities and configurations respectively. Enable logging and set up appropriate rules to track IAM group changes and ensure compliance. 2. Review CloudTrail Logs for Anomalies : o Regularly review CloudTrail logs to identify any unauthorized or unexpected changes to IAM groups. o Investigate any anomalies detected in the logs, such as unauthorized user additions or policy modifications, and take appropriate action to rectify them. 3. AWS Config Remediation Rules : o Define AWS Config rules to automatically detect non-compliant IAM group configurations. o Configure remediation actions within AWS Config to automatically revert any deviations from the desired IAM group settings back to the compliant state. 4. IAM Group Cleanup : o Periodically review IAM groups to ensure they are still necessary and relevant. o Remove any unused or obsolete IAM groups to reduce the attack surface and simplify permission management. 5. Permissions Review : o Regularly review the permissions assigned to IAM groups to ensure they follow the principle of least privilege. o Remove any excessive permissions or policies that are not required for the group's intended purpose. 6. Security Best Practices : o Implement security best practices for IAM, such as enforcing multi-factor authentication (MFA) for privileged IAM users and regularly rotating access keys. o Train IAM administrators and users on security best practices to prevent inadvertent misconfigurations and unauthorized access. 7. Documentation and Monitoring : o Document IAM group configurations, policies, and access controls to maintain an audit trail and facilitate future audits. o Set up monitoring alerts to notify administrators of any suspicious activities related to IAM groups. References: 1. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups_create.html",
      "rationale": "IAM groups in AWS simplify permission management by grouping users with similar access needs and applying common access policies, reducing administrative overhead and enhancing security through the principle of least privilege. This approach ensures consistency, scalability, and ease of auditing, strengthening the overall security posture of the AWS environment.",
      "impact": ""
    }
  },
  {
    "id": "2.9",
    "title": "Ensure Granular Policy Creation",
    "assessment": "Manual",
    "description": "Granular policies are meticulously tailored to AWS resources, ensuring precision in access control measures.",
    "rationale": "Emphasizing granular policies in AWS ensures that access control measures are precisely aligned with the requirements of each resource, bolstering security and minimizing unauthorized access. By tailoring policies to specific resources, organizations can adhere more closely to the principle of least privilege, mitigating risks and maintaining compliance with regulatory standards.",
    "audit": "1. Review IAM Policies : o Access the IAM console in the AWS Management Console. o Navigate to the \"Policies\" section to view all IAM policies. o Examine each policy to ensure they are finely tuned and specific to the resources they are intended to control access to. 2. Utilize AWS Config : o Open the AWS Config console and ensure that AWS Config is enabled for your AWS account. o Set up Config rules to monitor IAM policies for granularity. o Configure rules to detect policies that are overly broad or provide unnecessary permissions. 3. CloudTrail Analysis : o Access the CloudTrail console and review the logs. o Look for API calls related to IAM policy modifications. o Analyze the logs to ensure that policy changes align with the principles of granular access control. 4. Manual Review : o Conduct manual reviews of IAM policies and their associated resources. o Verify that policies are scoped to specific resources and actions, rather than providing blanket permissions. 5. Automated Scanning : o Utilize third-party AWS security tools that offer automated scanning and analysis of IAM policies for granularity. o Configure these tools to regularly scan and identify any policies that may not adhere to granular access control principles. 6. Continuous Monitoring : o Implement continuous monitoring solutions to track changes to IAM policies in real-time. o Set up alerts to notify administrators of any policy modifications that may deviate from granular access control best practices.",
    "remediation": "1. Policy Refinement : o Review existing IAM policies to identify those that are overly broad or lack granularity. o Refine these policies to restrict permissions to only the resources and actions necessary for each user or group. 2. IAM Policy Simulator : o Utilize the IAM Policy Simulator in the AWS Management Console to test the effectiveness of policy changes. o Simulate various access scenarios to ensure that policies are granting the intended level of access without unintended consequences. 3. Access Reviews : o Conduct regular access reviews to ensure that IAM policies remain aligned with the principle of least privilege. o Identify and remove any unnecessary permissions or policies that grant excessive access to resources. 4. AWS Config Remediation : o Configure AWS Config rules to automatically remediate non-compliant IAM policies. o Set up remediation actions to adjust policies to adhere to granular access control principles automatically. 5. Employee Training : o Provide training and guidance to IAM administrators on best practices for crafting granular policies. o Ensure that administrators understand the importance of restricting permissions to only what is necessary for each user or group. 6. Monitoring and Alerting : o Implement continuous monitoring solutions to detect and alert on any deviations from granular access control policies. o Set up alerts to notify administrators of any unauthorized changes to IAM policies in real-time. 7. Documentation and Documentation : o Document changes made to IAM policies and keep records of policy adjustments. o Maintain up-to-date documentation on IAM policies and access controls for reference during audits and compliance assessments. References: 1. https://docs.aws.amazon.com/tag-editor/latest/userguide/tags-in-iam- policies.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/tag-editor/latest/userguide/tags-in-iam- policies.html",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Granular policies are meticulously tailored to AWS resources, ensuring precision in access control measures.",
      "audit_steps": "1. Review IAM Policies : o Access the IAM console in the AWS Management Console. o Navigate to the \"Policies\" section to view all IAM policies. o Examine each policy to ensure they are finely tuned and specific to the resources they are intended to control access to. 2. Utilize AWS Config : o Open the AWS Config console and ensure that AWS Config is enabled for your AWS account. o Set up Config rules to monitor IAM policies for granularity. o Configure rules to detect policies that are overly broad or provide unnecessary permissions. 3. CloudTrail Analysis : o Access the CloudTrail console and review the logs. o Look for API calls related to IAM policy modifications. o Analyze the logs to ensure that policy changes align with the principles of granular access control. 4. Manual Review : o Conduct manual reviews of IAM policies and their associated resources. o Verify that policies are scoped to specific resources and actions, rather than providing blanket permissions. 5. Automated Scanning : o Utilize third-party AWS security tools that offer automated scanning and analysis of IAM policies for granularity. o Configure these tools to regularly scan and identify any policies that may not adhere to granular access control principles. 6. Continuous Monitoring : o Implement continuous monitoring solutions to track changes to IAM policies in real-time. o Set up alerts to notify administrators of any policy modifications that may deviate from granular access control best practices.",
      "remediation_steps": "1. Policy Refinement : o Review existing IAM policies to identify those that are overly broad or lack granularity. o Refine these policies to restrict permissions to only the resources and actions necessary for each user or group. 2. IAM Policy Simulator : o Utilize the IAM Policy Simulator in the AWS Management Console to test the effectiveness of policy changes. o Simulate various access scenarios to ensure that policies are granting the intended level of access without unintended consequences. 3. Access Reviews : o Conduct regular access reviews to ensure that IAM policies remain aligned with the principle of least privilege. o Identify and remove any unnecessary permissions or policies that grant excessive access to resources. 4. AWS Config Remediation : o Configure AWS Config rules to automatically remediate non-compliant IAM policies. o Set up remediation actions to adjust policies to adhere to granular access control principles automatically. 5. Employee Training : o Provide training and guidance to IAM administrators on best practices for crafting granular policies. o Ensure that administrators understand the importance of restricting permissions to only what is necessary for each user or group. 6. Monitoring and Alerting : o Implement continuous monitoring solutions to detect and alert on any deviations from granular access control policies. o Set up alerts to notify administrators of any unauthorized changes to IAM policies in real-time. 7. Documentation and Documentation : o Document changes made to IAM policies and keep records of policy adjustments. o Maintain up-to-date documentation on IAM policies and access controls for reference during audits and compliance assessments. References: 1. https://docs.aws.amazon.com/tag-editor/latest/userguide/tags-in-iam- policies.html",
      "rationale": "Emphasizing granular policies in AWS ensures that access control measures are precisely aligned with the requirements of each resource, bolstering security and minimizing unauthorized access. By tailoring policies to specific resources, organizations can adhere more closely to the principle of least privilege, mitigating risks and maintaining compliance with regulatory standards.",
      "impact": ""
    }
  },
  {
    "id": "2.10",
    "title": "Ensure Resource Access via Tag-based Policies",
    "assessment": "Manual",
    "description": "For optimal granularity in EC2 access, configuring IAM policies via tags proves highly effective. This involves editing the JSON text editor to specify access permissions based on specific tags. In the provided example, I'm granting the \"developers\" group access exclusively to the newly created EC2 image, as illustrated in the attached screenshot depicting the policy creation process.",
    "rationale": "Implementing IAM policies based on tags in EC2 enables administrators to finely tailor access control, granting permissions dynamically according to resource attributes. This approach enhances security and scalability by aligning access rights with specific resource requirements while minimizing manual intervention.",
    "audit": "1. Review IAM Policies : o Access the IAM console in the AWS Management Console. o Navigate to the \"Policies\" section and review policies associated with EC2 resources. o Ensure that policies utilize condition keys related to EC2 tags for granting access. 2. IAM Policy Simulator : o Utilize the IAM Policy Simulator to simulate access scenarios based on EC2 tags. o Test various tag-based policy configurations to verify that access is granted or denied appropriately. 3. CloudTrail Analysis : o Access the CloudTrail console and review logs related to IAM policy changes. o Look for API calls related to modifications of policies using tag-based conditions. 4. AWS Config Rules : o Configure AWS Config rules to monitor IAM policies for tag-based conditions. o Set up rules to detect policies that do not include tag-based conditions or are overly permissive. 5. Manual Review : o Manually inspect IAM policies to ensure they include tag-based conditions where applicable. o Verify that policies accurately reflect the intended access control based on EC2 resource tags. 6. Automated Scanning : o Utilize third-party AWS security tools that offer automated scanning and analysis of IAM policies for tag-based conditions. o Configure these tools to regularly scan IAM policies and identify any deviations from best practices. 7. Continuous Monitoring : o Implement continuous monitoring solutions to track changes to IAM policies in real-time. o Set up alerts to notify administrators of any unauthorized modifications or policy changes that do not adhere to tag-based access control principles.",
    "remediation": "1. Policy Adjustment : o Review existing IAM policies associated with EC2 resources to ensure they include tag-based conditions where applicable. o Modify policies to incorporate tag-based conditions for granular access control, ensuring that access is granted or denied based on resource attributes. 2. IAM Policy Simulator Validation : o Utilize the IAM Policy Simulator to validate the effectiveness of policy adjustments. o Test various access scenarios to verify that policies accurately reflect the intended access control based on EC2 resource tags. 3. AWS Config Remediation : o Configure AWS Config rules to automatically remediate IAM policies that do not include tag-based conditions. o Set up remediation actions to adjust policies to adhere to tag-based access control principles automatically. 4. Employee Training : o Provide training to IAM administrators on best practices for crafting IAM policies based on tags. o Ensure that administrators understand the importance of utilizing tag- based conditions for granular access control in EC2. 5. Monitoring and Alerting : o Implement continuous monitoring solutions to detect and alert on any deviations from tag-based access control policies. o Set up alerts to notify administrators of any unauthorized modifications or policy changes that do not adhere to tag-based access control principles. 6. Documentation and Documentation : o Document changes made to IAM policies to include tag-based conditions. o Maintain up-to-date documentation on IAM policies and access controls for reference during audits and compliance assessments.",
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "For optimal granularity in EC2 access, configuring IAM policies via tags proves highly effective. This involves editing the JSON text editor to specify access permissions based on specific tags. In the provided example, I'm granting the \"developers\" group access exclusively to the newly created EC2 image, as illustrated in the attached screenshot depicting the policy creation process.",
      "audit_steps": "1. Review IAM Policies : o Access the IAM console in the AWS Management Console. o Navigate to the \"Policies\" section and review policies associated with EC2 resources. o Ensure that policies utilize condition keys related to EC2 tags for granting access. 2. IAM Policy Simulator : o Utilize the IAM Policy Simulator to simulate access scenarios based on EC2 tags. o Test various tag-based policy configurations to verify that access is granted or denied appropriately. 3. CloudTrail Analysis : o Access the CloudTrail console and review logs related to IAM policy changes. o Look for API calls related to modifications of policies using tag-based conditions. 4. AWS Config Rules : o Configure AWS Config rules to monitor IAM policies for tag-based conditions. o Set up rules to detect policies that do not include tag-based conditions or are overly permissive. 5. Manual Review : o Manually inspect IAM policies to ensure they include tag-based conditions where applicable. o Verify that policies accurately reflect the intended access control based on EC2 resource tags. 6. Automated Scanning : o Utilize third-party AWS security tools that offer automated scanning and analysis of IAM policies for tag-based conditions. o Configure these tools to regularly scan IAM policies and identify any deviations from best practices. 7. Continuous Monitoring : o Implement continuous monitoring solutions to track changes to IAM policies in real-time. o Set up alerts to notify administrators of any unauthorized modifications or policy changes that do not adhere to tag-based access control principles.",
      "remediation_steps": "1. Policy Adjustment : o Review existing IAM policies associated with EC2 resources to ensure they include tag-based conditions where applicable. o Modify policies to incorporate tag-based conditions for granular access control, ensuring that access is granted or denied based on resource attributes. 2. IAM Policy Simulator Validation : o Utilize the IAM Policy Simulator to validate the effectiveness of policy adjustments. o Test various access scenarios to verify that policies accurately reflect the intended access control based on EC2 resource tags. 3. AWS Config Remediation : o Configure AWS Config rules to automatically remediate IAM policies that do not include tag-based conditions. o Set up remediation actions to adjust policies to adhere to tag-based access control principles automatically. 4. Employee Training : o Provide training to IAM administrators on best practices for crafting IAM policies based on tags. o Ensure that administrators understand the importance of utilizing tag- based conditions for granular access control in EC2. 5. Monitoring and Alerting : o Implement continuous monitoring solutions to detect and alert on any deviations from tag-based access control policies. o Set up alerts to notify administrators of any unauthorized modifications or policy changes that do not adhere to tag-based access control principles. 6. Documentation and Documentation : o Document changes made to IAM policies to include tag-based conditions. o Maintain up-to-date documentation on IAM policies and access controls for reference during audits and compliance assessments.",
      "rationale": "Implementing IAM policies based on tags in EC2 enables administrators to finely tailor access control, granting permissions dynamically according to resource attributes. This approach enhances security and scalability by aligning access rights with specific resource requirements while minimizing manual intervention.",
      "impact": ""
    }
  },
  {
    "id": "2.11",
    "title": "Ensure Secure Password Policy Implementation",
    "assessment": "Manual",
    "description": "Password policies outline the appropriate parameters for password configuration within an organization.",
    "rationale": "Clear password policies provide essential guidelines for maintaining strong authentication practices, reducing the risk of unauthorized access and data breaches within an organization. By enforcing requirements for complex passwords and regular updates, these policies help bolster cybersecurity defenses and ensure compliance with industry standards and regulations.",
    "audit": "1. Review IAM Policies : o Access the IAM console in the AWS Management Console. o Navigate to the \"Password Policy\" section to review the current password policy settings. o Ensure that the password policy aligns with industry best practices and organizational security requirements, including parameters such as minimum length, complexity requirements, and password expiration. 2. AWS Config Rules : o Configure AWS Config rules to monitor IAM password policies. o Set up rules to check for compliance with password policy requirements, such as minimum length, complexity, and expiration settings. o Use AWS Config to continuously assess the configuration of IAM password policies and identify any non-compliant settings. 3. CloudTrail Analysis : o Access the CloudTrail console and review logs related to IAM password policy changes. o Look for API calls related to modifications of password policy settings. o Analyze the logs to ensure that password policy changes are authorized and adhere to organizational security standards. 4. Manual Review : o Manually inspect the IAM password policy settings to verify compliance with security requirements. o Check for parameters such as minimum password length, complexity requirements (e.g., uppercase, lowercase, special characters), and password expiration settings.",
    "remediation": "1. Revise and Update Password Policies : o Navigate to the IAM dashboard in the AWS Management Console. o Go to the \"Account settings\" section to review and adjust the password policy. o Strengthen the policy by setting requirements for password length, complexity (including uppercase, lowercase, numbers, and special characters), and rotation policies. 2. Enforce Password Changes : o If the audit reveals passwords that do not comply with the updated policy, require users to change their passwords immediately. o Implement mandatory password updates at regular intervals to ensure ongoing compliance with the policy. 3. Enable AWS Config for Continuous Compliance : o Use AWS Config to continuously monitor and record IAM password policies. o Set up AWS Config rules that automatically check compliance with your organization\u2019s password policy standards. 4. Utilize Multi-Factor Authentication (MFA) : o Enable MFA for an additional layer of security on all user accounts, especially for accounts with elevated permissions. o Regularly audit the use of MFA across your AWS environment to ensure it is enabled and functioning correctly. 5. Automate Alerts and Responses : o Set up real-time alerts for any non-compliant changes to password policies or unexpected password resets. o Automate responses where possible to enforce compliance immediately when a deviation from the password policy is detected.",
    "profile_applicability": "\u2022  Level 2",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Password policies outline the appropriate parameters for password configuration within an organization.",
      "audit_steps": "1. Review IAM Policies : o Access the IAM console in the AWS Management Console. o Navigate to the \"Password Policy\" section to review the current password policy settings. o Ensure that the password policy aligns with industry best practices and organizational security requirements, including parameters such as minimum length, complexity requirements, and password expiration. 2. AWS Config Rules : o Configure AWS Config rules to monitor IAM password policies. o Set up rules to check for compliance with password policy requirements, such as minimum length, complexity, and expiration settings. o Use AWS Config to continuously assess the configuration of IAM password policies and identify any non-compliant settings. 3. CloudTrail Analysis : o Access the CloudTrail console and review logs related to IAM password policy changes. o Look for API calls related to modifications of password policy settings. o Analyze the logs to ensure that password policy changes are authorized and adhere to organizational security standards. 4. Manual Review : o Manually inspect the IAM password policy settings to verify compliance with security requirements. o Check for parameters such as minimum password length, complexity requirements (e.g., uppercase, lowercase, special characters), and password expiration settings.",
      "remediation_steps": "1. Revise and Update Password Policies : o Navigate to the IAM dashboard in the AWS Management Console. o Go to the \"Account settings\" section to review and adjust the password policy. o Strengthen the policy by setting requirements for password length, complexity (including uppercase, lowercase, numbers, and special characters), and rotation policies. 2. Enforce Password Changes : o If the audit reveals passwords that do not comply with the updated policy, require users to change their passwords immediately. o Implement mandatory password updates at regular intervals to ensure ongoing compliance with the policy. 3. Enable AWS Config for Continuous Compliance : o Use AWS Config to continuously monitor and record IAM password policies. o Set up AWS Config rules that automatically check compliance with your organization\u2019s password policy standards. 4. Utilize Multi-Factor Authentication (MFA) : o Enable MFA for an additional layer of security on all user accounts, especially for accounts with elevated permissions. o Regularly audit the use of MFA across your AWS environment to ensure it is enabled and functioning correctly. 5. Automate Alerts and Responses : o Set up real-time alerts for any non-compliant changes to password policies or unexpected password resets. o Automate responses where possible to enforce compliance immediately when a deviation from the password policy is detected.",
      "rationale": "Clear password policies provide essential guidelines for maintaining strong authentication practices, reducing the risk of unauthorized access and data breaches within an organization. By enforcing requirements for complex passwords and regular updates, these policies help bolster cybersecurity defenses and ensure compliance with industry standards and regulations.",
      "impact": ""
    }
  },
  {
    "id": "2.12",
    "title": "Ensure Monitoring EC2 and EBS with CloudWatch",
    "assessment": "Manual",
    "description": "CloudWatch is an AWS monitoring service that allows you to keep an eye on your AWS resources. You can track metrics via log files or worldclass data visuals. AWS CloudWatch allows the administrator to keep an eye on his/her AWS resources. You can set up alarms, monitor activity, and analyze log data. CloudWatch is a must to keep your AWS EBS and EC2 resources secure.",
    "rationale": "Using CloudWatch to monitor EC2 instances and EBS volumes is essential for enhancing operational oversight and ensuring optimal performance within the AWS environment. This approach provides real-time insights into resource usage and system health, enabling proactive adjustments and timely responses to potential issues, thereby maintaining high availability and efficiency. Impact: Failing to monitor EC2 instances and EBS volumes with CloudWatch can lead to delayed detection of performance issues and resource bottlenecks, potentially causing system outages and degraded user experiences. Without this monitoring, organizations also miss opportunities for proactive optimizations, increasing the risk of unexpected downtime and higher operational costs.",
    "audit": "Creating an AWS CloudWatch Dashboard: 1. Navigate to the AWS CloudWatch Console - https://us-east- 2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#home. 2. Select the dashboard type that\u2019s right for you. Give the dashboard a name. Name the dashboard as something memorable. You can select which resources you want to monitor. Select \u201cEBS.\u201d 3. Create an alarm - Alarms are important to send you an alert as soon as something suspicious happens on your volume. You can create an alarm to alert you when a certain threshold of IOPS are reached. To create alarm, follow steps - o Go to \u201cAlarms\u201d on the left hand side of the CloudWatch dashboard. o Select \u201cCreate a new alarm\u201d. o Select \u201cEBS\u201d. o Select what you want to monitor. We\u2019re going to choose to monitor the write operations of an EBS volume. o Go back to the volume that was created in EC2 dashboard and copy the volume ID under the \u201cvolume ID\u201d field. o Configure the settings that you want to trigger an alarm. o Move onto the next step before continuing.",
    "remediation": "1. Enable CloudWatch Monitoring : o Access the AWS Management Console, navigate to the EC2 dashboard, and select the instances and EBS volumes. o Enable detailed monitoring on each EC2 instance and EBS volume to collect data at a higher granularity. 2. Configure CloudWatch Alarms : o In the CloudWatch console, set up alarms based on key performance metrics such as CPU utilization, disk read/write operations, and network traffic. o Configure these alarms to notify administrators via email or SMS when thresholds are breached, allowing for immediate action. 3. Establish Baselines : o Analyze historical performance data from CloudWatch to establish baseline performance metrics for each instance and volume. o Use these baselines to identify abnormal behavior or performance degradation over time. 4. Automate Responses : o Utilize AWS CloudWatch Events and AWS Lambda to automate responses to specific alarms, such as scaling operations or initiating recovery processes. o Ensure these automated scripts are tested and reflect the operational policies of your organization. References: 1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch _Embedded_Metric_Format_View.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Failing to monitor EC2 instances and EBS volumes with CloudWatch can lead to delayed detection of performance issues and resource bottlenecks, potentially causing system outages and degraded user experiences. Without this monitoring, organizations also miss opportunities for proactive optimizations, increasing the risk of unexpected downtime and higher operational costs.",
    "references": "1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch _Embedded_Metric_Format_View.html",
    "function_names": [
      "ec2_ebs_volume_creation_check",
      "ec2_ebs_volume_encryption_kms_key_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_creation_check",
        "ec2_ebs_volume_encryption_kms_key_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "CloudWatch is an AWS monitoring service that allows you to keep an eye on your AWS resources. You can track metrics via log files or worldclass data visuals. AWS CloudWatch allows the administrator to keep an eye on his/her AWS resources. You can set up alarms, monitor activity, and analyze log data. CloudWatch is a must to keep your AWS EBS and EC2 resources secure.",
      "audit_steps": "Creating an AWS CloudWatch Dashboard: 1. Navigate to the AWS CloudWatch Console - https://us-east- 2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#home. 2. Select the dashboard type that\u2019s right for you. Give the dashboard a name. Name the dashboard as something memorable. You can select which resources you want to monitor. Select \u201cEBS.\u201d 3. Create an alarm - Alarms are important to send you an alert as soon as something suspicious happens on your volume. You can create an alarm to alert you when a certain threshold of IOPS are reached. To create alarm, follow steps - o Go to \u201cAlarms\u201d on the left hand side of the CloudWatch dashboard. o Select \u201cCreate a new alarm\u201d. o Select \u201cEBS\u201d. o Select what you want to monitor. We\u2019re going to choose to monitor the write operations of an EBS volume. o Go back to the volume that was created in EC2 dashboard and copy the volume ID under the \u201cvolume ID\u201d field. o Configure the settings that you want to trigger an alarm. o Move onto the next step before continuing.",
      "remediation_steps": "1. Enable CloudWatch Monitoring : o Access the AWS Management Console, navigate to the EC2 dashboard, and select the instances and EBS volumes. o Enable detailed monitoring on each EC2 instance and EBS volume to collect data at a higher granularity. 2. Configure CloudWatch Alarms : o In the CloudWatch console, set up alarms based on key performance metrics such as CPU utilization, disk read/write operations, and network traffic. o Configure these alarms to notify administrators via email or SMS when thresholds are breached, allowing for immediate action. 3. Establish Baselines : o Analyze historical performance data from CloudWatch to establish baseline performance metrics for each instance and volume. o Use these baselines to identify abnormal behavior or performance degradation over time. 4. Automate Responses : o Utilize AWS CloudWatch Events and AWS Lambda to automate responses to specific alarms, such as scaling operations or initiating recovery processes. o Ensure these automated scripts are tested and reflect the operational policies of your organization. References: 1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch _Embedded_Metric_Format_View.html",
      "rationale": "Using CloudWatch to monitor EC2 instances and EBS volumes is essential for enhancing operational oversight and ensuring optimal performance within the AWS environment. This approach provides real-time insights into resource usage and system health, enabling proactive adjustments and timely responses to potential issues, thereby maintaining high availability and efficiency. Impact: Failing to monitor EC2 instances and EBS volumes with CloudWatch can lead to delayed detection of performance issues and resource bottlenecks, potentially causing system outages and degraded user experiences. Without this monitoring, organizations also miss opportunities for proactive optimizations, increasing the risk of unexpected downtime and higher operational costs.",
      "impact": "Failing to monitor EC2 instances and EBS volumes with CloudWatch can lead to delayed detection of performance issues and resource bottlenecks, potentially causing system outages and degraded user experiences. Without this monitoring, organizations also miss opportunities for proactive optimizations, increasing the risk of unexpected downtime and higher operational costs."
    }
  },
  {
    "id": "2.13",
    "title": "Ensure creating an SNS subscription",
    "assessment": "Manual",
    "description": "Create an SNS notification to send to the system administrator\u2019s email address.",
    "rationale": "",
    "audit": "Creating an SNS subscription: 1. Navigate to SNS service in the AWS console - https://us-east- 2.console.aws.amazon.com/sns/v3/home?region=us-east-2#/homepage (make sure you are in the correct region). 2. Navigate to \u201ctopics\u201d. 3. Create a new topic. 4. Select the ARN of the topic. 5. Select the \u201cEmail\u201d protocol if you wish to have the alarms delivered to your email. 6. Enter the correct email address of an administrator. 7. Select \u201cCreate Subscription\u201d. To attach the SNS notification service to the alarm - select the SNS subscription that you just created and create the alarm.",
    "remediation": "References: 1. https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html 3 Elastic File System (EFS) Amazon Elastic File System (EFS) automatically grows and shrinks as you add and remove files with no need for management or provisioning. AWS EFS is a serverless file storage service that allows users to easily configure filesystems. AWS takes care of provisioning, patching, and deploying the file system when you use EFS. This file system is built to scale without any user configuration changes. This means that the file system automatically gets bigger as the storage needs increase. The file system will also shrink with decreasing requirements, ensuring you only pay for what you need.",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html 3 Elastic File System (EFS) Amazon Elastic File System (EFS) automatically grows and shrinks as you add and remove files with no need for management or provisioning. AWS EFS is a serverless file storage service that allows users to easily configure filesystems. AWS takes care of provisioning, patching, and deploying the file system when you use EFS. This file system is built to scale without any user configuration changes. This means that the file system automatically gets bigger as the storage needs increase. The file system will also shrink with decreasing requirements, ensuring you only pay for what you need.",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Create an SNS notification to send to the system administrator\u2019s email address.",
      "audit_steps": "Creating an SNS subscription: 1. Navigate to SNS service in the AWS console - https://us-east- 2.console.aws.amazon.com/sns/v3/home?region=us-east-2#/homepage (make sure you are in the correct region). 2. Navigate to \u201ctopics\u201d. 3. Create a new topic. 4. Select the ARN of the topic. 5. Select the \u201cEmail\u201d protocol if you wish to have the alarms delivered to your email. 6. Enter the correct email address of an administrator. 7. Select \u201cCreate Subscription\u201d. To attach the SNS notification service to the alarm - select the SNS subscription that you just created and create the alarm.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html 3 Elastic File System (EFS) Amazon Elastic File System (EFS) automatically grows and shrinks as you add and remove files with no need for management or provisioning. AWS EFS is a serverless file storage service that allows users to easily configure filesystems. AWS takes care of provisioning, patching, and deploying the file system when you use EFS. This file system is built to scale without any user configuration changes. This means that the file system automatically gets bigger as the storage needs increase. The file system will also shrink with decreasing requirements, ensuring you only pay for what you need.",
      "rationale": "",
      "impact": ""
    }
  },
  {
    "id": "3.1",
    "title": "EFS",
    "assessment": "Manual",
    "description": "AWS EFS is a scalable and fully-managed storage service that enables you to quickly deploy file systems without the hassle of configuring, patching, or maintaining them.",
    "rationale": "Utilize AWS EFS to streamline your file system deployment, allowing the service to handle the heavy lifting for you. Impact: Not using AWS EFS for your file system deployment can lead to increased management overhead, as you'll need to manually configure, patch, and maintain the systems. This manual effort is time-consuming and complex, raising the potential for errors that could result in downtime and data loss. By not leveraging AWS EFS, you miss out on the streamlined, automated management and scalability that the service provides, potentially impacting your operational efficiency and reliability.",
    "audit": "To create an Amazon EFS (Elastic File System), you can follow these steps: 1. Sign in to the AWS Management Console and navigate to the Amazon EFS console - https://us-east-2.console.aws.amazon.com/efs?region=us-east-2#/get- started. 2. Click on the \"Create file system\" button. 3. Enter a name for your file system. 4. Choose a VPC for your file system 5. Then you have to go to File system settings to edit configurations , then you have to select Lifecycle management , performance settings and File system protection , and then click save changes.",
    "remediation": "To create an Amazon EFS (Elastic File System), follow these steps: 1. Open the Amazon EFS Console : Sign in to your AWS Management Console and navigate to the Amazon EFS service. 2. Create File System : Click on the \"Create file system\" button to start the creation process. 3. Configure File System : Select your desired VPC (Virtual Private Cloud) and availability zones for the file system. Optionally, you can configure settings like throughput mode and lifecycle management. 4. Configure Access Points : Set up access points if needed, to control access permissions and streamline access management. 5. Review and Create : Review your settings and click on the \"Create\" button to create the file system. 6. Mount the File System : Once created, use the provided mount targets and instructions to mount the file system to your EC2 instances or other resources. References: 1. https://us-east-2.console.aws.amazon.com/efs?region=us-east-2#/get-started",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not using AWS EFS for your file system deployment can lead to increased management overhead, as you'll need to manually configure, patch, and maintain the systems. This manual effort is time-consuming and complex, raising the potential for errors that could result in downtime and data loss. By not leveraging AWS EFS, you miss out on the streamlined, automated management and scalability that the service provides, potentially impacting your operational efficiency and reliability.",
    "references": "1. https://us-east-2.console.aws.amazon.com/efs?region=us-east-2#/get-started",
    "function_names": [
      "s3_bucket_unique_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS EFS is a scalable and fully-managed storage service that enables you to quickly deploy file systems without the hassle of configuring, patching, or maintaining them.",
      "audit_steps": "To create an Amazon EFS (Elastic File System), you can follow these steps: 1. Sign in to the AWS Management Console and navigate to the Amazon EFS console - https://us-east-2.console.aws.amazon.com/efs?region=us-east-2#/get- started. 2. Click on the \"Create file system\" button. 3. Enter a name for your file system. 4. Choose a VPC for your file system 5. Then you have to go to File system settings to edit configurations , then you have to select Lifecycle management , performance settings and File system protection , and then click save changes.",
      "remediation_steps": "To create an Amazon EFS (Elastic File System), follow these steps: 1. Open the Amazon EFS Console : Sign in to your AWS Management Console and navigate to the Amazon EFS service. 2. Create File System : Click on the \"Create file system\" button to start the creation process. 3. Configure File System : Select your desired VPC (Virtual Private Cloud) and availability zones for the file system. Optionally, you can configure settings like throughput mode and lifecycle management. 4. Configure Access Points : Set up access points if needed, to control access permissions and streamline access management. 5. Review and Create : Review your settings and click on the \"Create\" button to create the file system. 6. Mount the File System : Once created, use the provided mount targets and instructions to mount the file system to your EC2 instances or other resources. References: 1. https://us-east-2.console.aws.amazon.com/efs?region=us-east-2#/get-started",
      "rationale": "Utilize AWS EFS to streamline your file system deployment, allowing the service to handle the heavy lifting for you. Impact: Not using AWS EFS for your file system deployment can lead to increased management overhead, as you'll need to manually configure, patch, and maintain the systems. This manual effort is time-consuming and complex, raising the potential for errors that could result in downtime and data loss. By not leveraging AWS EFS, you miss out on the streamlined, automated management and scalability that the service provides, potentially impacting your operational efficiency and reliability.",
      "impact": "Not using AWS EFS for your file system deployment can lead to increased management overhead, as you'll need to manually configure, patch, and maintain the systems. This manual effort is time-consuming and complex, raising the potential for errors that could result in downtime and data loss. By not leveraging AWS EFS, you miss out on the streamlined, automated management and scalability that the service provides, potentially impacting your operational efficiency and reliability."
    }
  },
  {
    "id": "3.2",
    "title": "Ensure Implementation of EFS",
    "assessment": "Manual",
    "description": "AWS EFS is a fully managed storage service that enables rapid file system deployment without the need for configuration, patching, or maintenance.",
    "rationale": "The rationale behind using AWS EFS is to simplify and expedite the deployment of file systems, eliminating the need for manual configuration, patching, and maintenance. This allows you to focus on other critical aspects of your operations while benefiting from a reliable, scalable, and fully managed storage solution. Impact: Not using AWS EFS can lead to increased complexity and time-consuming manual management for configuration, patching, and maintenance. This raises the risk of human error, system downtime, and data loss, while also making it more challenging to scale your file systems efficiently.",
    "audit": "1. Navigate to console - https://us-east- 1.console.aws.amazon.com/efs/home?region=us-east-1#/get-started. 2. Select \u201cCreate File System\u201d. Give the file system a name and select the default VPC. Select \u201cCreate\u201d. 3. Encrypting data at rest - The EFS is encrypted automatically upon creation.. 4. Attach the EFS to an EC2 instance. 5. Navigate to file system details - Select the radio box next to the file system that was just created and select \u201cview details\u201d. 6. Creating an NFS directory on your EC2 instance - Launch your EC2 instance. Once connected, Type following command: \u201csudo mkdir efs\u201d to create a new efs directory. 7. Mounting an NFS directory on your EC2 instance - Navigate to find your EC2 DNS information Paste this command into the console after making the efs directory sudo mount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport mount-target-DNS:/   ~/efs-mount-point ``` NOTE: The encryption takes place as soon as you mount the directory. This encrypts the data in transit. 8. Terminating the EC2 instance - The EFS file system that was just mounted doesn\u2019t persist on reboot. You can consult the AWS documentation to see how you can write a script to automatically mount the file system upon every reboot.",
    "remediation": "To remediate the issues of manual file system management, follow these steps to create and use Amazon EFS: 1. Open the Amazon EFS Console : Sign in to the AWS Management Console and navigate to the Amazon EFS service. 2. Create a New File System : Click on \"Create file system\" to start the setup process. 3. Configure Settings : Select your desired VPC, availability zones, throughput mode, and any additional settings like lifecycle management. 4. Set Up Access Points : Configure access points to control permissions and simplify access management. 5. Review and Create : Verify your settings and click \"Create\" to finalize the file system setup. 6. Mount the File System : Use the provided mount targets and instructions to attach the file system to your EC2 instances or other resources. References: 1. https://aws.amazon.com/efs/",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not using AWS EFS can lead to increased complexity and time-consuming manual management for configuration, patching, and maintenance. This raises the risk of human error, system downtime, and data loss, while also making it more challenging to scale your file systems efficiently.",
    "references": "1. https://aws.amazon.com/efs/",
    "function_names": [
      "s3_bucket_default_encryption_check",
      "s3_data_transit_and_rest_protection_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_default_encryption_check",
        "s3_data_transit_and_rest_protection_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS EFS is a fully managed storage service that enables rapid file system deployment without the need for configuration, patching, or maintenance.",
      "audit_steps": "1. Navigate to console - https://us-east- 1.console.aws.amazon.com/efs/home?region=us-east-1#/get-started. 2. Select \u201cCreate File System\u201d. Give the file system a name and select the default VPC. Select \u201cCreate\u201d. 3. Encrypting data at rest - The EFS is encrypted automatically upon creation.. 4. Attach the EFS to an EC2 instance. 5. Navigate to file system details - Select the radio box next to the file system that was just created and select \u201cview details\u201d. 6. Creating an NFS directory on your EC2 instance - Launch your EC2 instance. Once connected, Type following command: \u201csudo mkdir efs\u201d to create a new efs directory. 7. Mounting an NFS directory on your EC2 instance - Navigate to find your EC2 DNS information Paste this command into the console after making the efs directory sudo mount -t nfs -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport mount-target-DNS:/   ~/efs-mount-point ``` NOTE: The encryption takes place as soon as you mount the directory. This encrypts the data in transit. 8. Terminating the EC2 instance - The EFS file system that was just mounted doesn\u2019t persist on reboot. You can consult the AWS documentation to see how you can write a script to automatically mount the file system upon every reboot.",
      "remediation_steps": "To remediate the issues of manual file system management, follow these steps to create and use Amazon EFS: 1. Open the Amazon EFS Console : Sign in to the AWS Management Console and navigate to the Amazon EFS service. 2. Create a New File System : Click on \"Create file system\" to start the setup process. 3. Configure Settings : Select your desired VPC, availability zones, throughput mode, and any additional settings like lifecycle management. 4. Set Up Access Points : Configure access points to control permissions and simplify access management. 5. Review and Create : Verify your settings and click \"Create\" to finalize the file system setup. 6. Mount the File System : Use the provided mount targets and instructions to attach the file system to your EC2 instances or other resources. References: 1. https://aws.amazon.com/efs/",
      "rationale": "The rationale behind using AWS EFS is to simplify and expedite the deployment of file systems, eliminating the need for manual configuration, patching, and maintenance. This allows you to focus on other critical aspects of your operations while benefiting from a reliable, scalable, and fully managed storage solution. Impact: Not using AWS EFS can lead to increased complexity and time-consuming manual management for configuration, patching, and maintenance. This raises the risk of human error, system downtime, and data loss, while also making it more challenging to scale your file systems efficiently.",
      "impact": "Not using AWS EFS can lead to increased complexity and time-consuming manual management for configuration, patching, and maintenance. This raises the risk of human error, system downtime, and data loss, while also making it more challenging to scale your file systems efficiently."
    }
  },
  {
    "id": "3.3",
    "title": "Ensure EFS and VPC Integration",
    "assessment": "Manual",
    "description": "You can use EFS as a network file system across availability zones on a virtual private cloud. This capability allows the organization to create a highly available file sharing solution. Leveraging AWS VPC and EC2 in tandem with AWS EFS makes for a highly available and scalable cloud file storage solution.",
    "rationale": "Redundancy and scalability are crucial for maintaining uninterrupted services. By integrating these AWS services, users can harness the full power of AWS, ensuring a resilient and scalable infrastructure. Impact: Not integrating AWS services for redundancy and scalability can lead to service disruptions and increased downtime. This approach also limits your ability to efficiently handle growing workloads, negatively impacting performance and user experience.",
    "audit": "Audit Procedures for AWS Redundancy and Scalability 1. Create Mount Targets in Each Availability Zone : Ensure EFS is attached in each availability zone by creating mount targets in each subnet. Although multiple subnets can exist per availability zone, verify that EFS is configured to work with one subnet per zone to maintain redundancy. 2. Monitor EFS with CloudWatch : Use AWS CloudWatch to automatically monitor your EFS service. Check that alarms are configured and logs and events are tracked effectively, providing real-time insights into the performance and health of your file systems.",
    "remediation": "Create an EC2 instance in each availability zone within your VPC. References: 1. https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works- conceptual",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not integrating AWS services for redundancy and scalability can lead to service disruptions and increased downtime. This approach also limits your ability to efficiently handle growing workloads, negatively impacting performance and user experience.",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works- conceptual",
    "function_names": [
      "s3_bucket_versioning_enabled_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_versioning_enabled_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "You can use EFS as a network file system across availability zones on a virtual private cloud. This capability allows the organization to create a highly available file sharing solution. Leveraging AWS VPC and EC2 in tandem with AWS EFS makes for a highly available and scalable cloud file storage solution.",
      "audit_steps": "Audit Procedures for AWS Redundancy and Scalability 1. Create Mount Targets in Each Availability Zone : Ensure EFS is attached in each availability zone by creating mount targets in each subnet. Although multiple subnets can exist per availability zone, verify that EFS is configured to work with one subnet per zone to maintain redundancy. 2. Monitor EFS with CloudWatch : Use AWS CloudWatch to automatically monitor your EFS service. Check that alarms are configured and logs and events are tracked effectively, providing real-time insights into the performance and health of your file systems.",
      "remediation_steps": "Create an EC2 instance in each availability zone within your VPC. References: 1. https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works- conceptual",
      "rationale": "Redundancy and scalability are crucial for maintaining uninterrupted services. By integrating these AWS services, users can harness the full power of AWS, ensuring a resilient and scalable infrastructure. Impact: Not integrating AWS services for redundancy and scalability can lead to service disruptions and increased downtime. This approach also limits your ability to efficiently handle growing workloads, negatively impacting performance and user experience.",
      "impact": "Not integrating AWS services for redundancy and scalability can lead to service disruptions and increased downtime. This approach also limits your ability to efficiently handle growing workloads, negatively impacting performance and user experience."
    }
  },
  {
    "id": "3.4",
    "title": "Ensure controlling Network access to EFS Services",
    "assessment": "Manual",
    "description": "It\u2019s important that you secure access to your resources on your AWS VPC network. There are several ways to ensure that you control what traffic is accessing your resources. Some of which include tightening down network layer security using a Security Group and a NACL within the VPC console. You can also tighten down Security Groups within your EC2 console and by using AWS IAM. Maintaining network security is a high priority to ensure that no unauthorized users can access the data stored on your EFS service.",
    "rationale": "Maintaining network security is a best practice essential for keeping your data safe and secure. Impact: Failing to maintain network security can lead to significant vulnerabilities, exposing your data to unauthorized access, breaches, and potential data loss. This can result in severe financial, operational, and reputational damage to your organization.",
    "audit": "",
    "remediation": "Implement network security access controls. References: 1. https://docs.aws.amazon.com/efs/latest/ug/NFS-access-control-efs.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Failing to maintain network security can lead to significant vulnerabilities, exposing your data to unauthorized access, breaches, and potential data loss. This can result in severe financial, operational, and reputational damage to your organization.",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/NFS-access-control-efs.html",
    "function_names": [
      "s3_account_public_access_block_check",
      "s3_bucket_public_acl_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_account_public_access_block_check",
        "s3_bucket_public_acl_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "It\u2019s important that you secure access to your resources on your AWS VPC network. There are several ways to ensure that you control what traffic is accessing your resources. Some of which include tightening down network layer security using a Security Group and a NACL within the VPC console. You can also tighten down Security Groups within your EC2 console and by using AWS IAM. Maintaining network security is a high priority to ensure that no unauthorized users can access the data stored on your EFS service.",
      "audit_steps": "",
      "remediation_steps": "Implement network security access controls. References: 1. https://docs.aws.amazon.com/efs/latest/ug/NFS-access-control-efs.html",
      "rationale": "Maintaining network security is a best practice essential for keeping your data safe and secure. Impact: Failing to maintain network security can lead to significant vulnerabilities, exposing your data to unauthorized access, breaches, and potential data loss. This can result in severe financial, operational, and reputational damage to your organization.",
      "impact": "Failing to maintain network security can lead to significant vulnerabilities, exposing your data to unauthorized access, breaches, and potential data loss. This can result in severe financial, operational, and reputational damage to your organization."
    }
  },
  {
    "id": "3.5",
    "title": "Ensure using Security Groups for VPC",
    "assessment": "Manual",
    "description": "A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.",
    "rationale": "",
    "audit": "1. Go to https://console.aws.amazon.com/vpc/ 2. Navigate to Security Groups and select on the VPC that houses your mount target. 3. Ensure that incoming traffic is restricted to SSH access on port 22 using TCP protocol and outbound traffic is accepting all traffic.",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "s3_bucket_audit_and_event_logging_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_audit_and_event_logging_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.",
      "audit_steps": "1. Go to https://console.aws.amazon.com/vpc/ 2. Navigate to Security Groups and select on the VPC that houses your mount target. 3. Ensure that incoming traffic is restricted to SSH access on port 22 using TCP protocol and outbound traffic is accepting all traffic.",
      "remediation_steps": null,
      "rationale": "",
      "impact": ""
    }
  },
  {
    "id": "3.6",
    "title": "Ensure Secure Ports",
    "assessment": "Manual",
    "description": "Securing network ports is essential for protecting AWS storage services like Amazon S3, EFS, and EBS. By configuring security groups and network access control lists (NACLs) to allow only necessary traffic, you minimize the risk of unauthorized access. Regular audits and monitoring of port usage ensure that only approved ports and protocols are operational, enhancing the overall security of your AWS storage environment.",
    "rationale": "By limiting traffic to only necessary and approved ports and protocols, you reduce the attack surface and enhance the overall security of your storage environment. Regular audits and monitoring further ensure that security measures remain effective and up-to- date, safeguarding your data from emerging threats. Impact: Not securing network ports in AWS storage services can lead to significant vulnerabilities, exposing your data to unauthorized access and potential breaches. This lack of control increases the risk of attacks, such as port scanning and exploitation of open ports, which can result in data loss, corruption, and theft. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation.",
    "audit": "1. Review Security Group Configurations : 1. Navigate to \"Security Groups\" under \"Network & Security\". 2. Verify that security groups are configured to allow only necessary inbound and outbound traffic. 3. Ensure rules are in place to restrict access to critical storage services, such as Amazon S3, EFS, and EBS. 2. Check Network Access Control Lists (NACLs) : o Steps : 1. Navigate to \"Network ACLs\" under \"Security\". 2. Ensure NACLs are configured to control traffic to and from subnets, allowing only necessary ports and protocols. 3. Verify that rules are implemented to deny unauthorized access. 3. Monitor VPC Flow Logs : o Steps : 1. Enable VPC Flow Logs for each VPC. 2. Regularly review flow logs to monitor traffic and identify any unauthorized access attempts or anomalies. 3. Investigate and remediate any unusual traffic patterns. 4. Inspect IAM Policies and Roles : o Steps : 1. Review IAM policies to ensure they enforce least privilege principles for access to storage services. 2. Verify that roles are appropriately assigned and used to control access to security groups and NACLs. 5. Enable and Review AWS CloudTrail Logs : o Steps : 1. Ensure CloudTrail is enabled in all regions. 2. Regularly review CloudTrail logs for any changes to security groups, NACLs, and IAM policies. 3. Set up alerts for critical security events related to port configurations. 6. Conduct Regular Penetration Testing : o Steps : 1. Conduct tests to identify vulnerabilities in port configurations. 2. Review findings and implement necessary security measures to address identified issues. 3. Ensure compliance with AWS penetration testing policies. 7. Verify Encryption in Transit : o Steps : 1. Ensure that data encryption is enabled for data in transit. 2. Verify that encryption keys are managed securely using AWS Key Management Service (KMS). 3. Check that all communication with storage services is encrypted. 8. Implement and Review Security Best Practices : o Steps : 1. Implement recommended best practices for securing network ports and storage services. 2. Regularly review and update security configurations to align with evolving best practices. 3. Conduct periodic training for staff on security best practices and AWS configurations.",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not securing network ports in AWS storage services can lead to significant vulnerabilities, exposing your data to unauthorized access and potential breaches. This lack of control increases the risk of attacks, such as port scanning and exploitation of open ports, which can result in data loss, corruption, and theft. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation.",
    "function_names": [
      "s3_bucket_backup_and_lifecycle_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_backup_and_lifecycle_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "MEDIUM",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Securing network ports is essential for protecting AWS storage services like Amazon S3, EFS, and EBS. By configuring security groups and network access control lists (NACLs) to allow only necessary traffic, you minimize the risk of unauthorized access. Regular audits and monitoring of port usage ensure that only approved ports and protocols are operational, enhancing the overall security of your AWS storage environment.",
      "audit_steps": "1. Review Security Group Configurations : 1. Navigate to \"Security Groups\" under \"Network & Security\". 2. Verify that security groups are configured to allow only necessary inbound and outbound traffic. 3. Ensure rules are in place to restrict access to critical storage services, such as Amazon S3, EFS, and EBS. 2. Check Network Access Control Lists (NACLs) : o Steps : 1. Navigate to \"Network ACLs\" under \"Security\". 2. Ensure NACLs are configured to control traffic to and from subnets, allowing only necessary ports and protocols. 3. Verify that rules are implemented to deny unauthorized access. 3. Monitor VPC Flow Logs : o Steps : 1. Enable VPC Flow Logs for each VPC. 2. Regularly review flow logs to monitor traffic and identify any unauthorized access attempts or anomalies. 3. Investigate and remediate any unusual traffic patterns. 4. Inspect IAM Policies and Roles : o Steps : 1. Review IAM policies to ensure they enforce least privilege principles for access to storage services. 2. Verify that roles are appropriately assigned and used to control access to security groups and NACLs. 5. Enable and Review AWS CloudTrail Logs : o Steps : 1. Ensure CloudTrail is enabled in all regions. 2. Regularly review CloudTrail logs for any changes to security groups, NACLs, and IAM policies. 3. Set up alerts for critical security events related to port configurations. 6. Conduct Regular Penetration Testing : o Steps : 1. Conduct tests to identify vulnerabilities in port configurations. 2. Review findings and implement necessary security measures to address identified issues. 3. Ensure compliance with AWS penetration testing policies. 7. Verify Encryption in Transit : o Steps : 1. Ensure that data encryption is enabled for data in transit. 2. Verify that encryption keys are managed securely using AWS Key Management Service (KMS). 3. Check that all communication with storage services is encrypted. 8. Implement and Review Security Best Practices : o Steps : 1. Implement recommended best practices for securing network ports and storage services. 2. Regularly review and update security configurations to align with evolving best practices. 3. Conduct periodic training for staff on security best practices and AWS configurations.",
      "remediation_steps": null,
      "rationale": "By limiting traffic to only necessary and approved ports and protocols, you reduce the attack surface and enhance the overall security of your storage environment. Regular audits and monitoring further ensure that security measures remain effective and up-to- date, safeguarding your data from emerging threats. Impact: Not securing network ports in AWS storage services can lead to significant vulnerabilities, exposing your data to unauthorized access and potential breaches. This lack of control increases the risk of attacks, such as port scanning and exploitation of open ports, which can result in data loss, corruption, and theft. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation.",
      "impact": "Not securing network ports in AWS storage services can lead to significant vulnerabilities, exposing your data to unauthorized access and potential breaches. This lack of control increases the risk of attacks, such as port scanning and exploitation of open ports, which can result in data loss, corruption, and theft. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation."
    }
  },
  {
    "id": "3.7",
    "title": "Ensure File-Level Access Control with Mount Targets",
    "assessment": "Manual",
    "description": "Mount targets act as gateways, enabling resources to be accessed across different availability zones within a VPC. When you create an EFS file system, mount targets are automatically provisioned in each availability zone associated with the VPC. This ensures high availability and redundancy, allowing seamless and efficient access to the EFS file system from any availability zone.",
    "rationale": "Using mount targets ensures seamless access to the EFS file system across different availability zones within a VPC. This automatic provisioning of mount targets in each availability zone provides high availability and redundancy, essential for maintaining uninterrupted data access. It simplifies configuration and enhances the resilience and scalability of the file system architecture. Impact: Not using mount targets can lead to inefficient and unreliable access to the EFS file system across availability zones. This lack of automatic provisioning reduces high availability and redundancy, increasing the risk of service interruptions and data access issues. Consequently, your infrastructure may suffer from decreased performance, higher latency, and potential data loss or downtime.",
    "audit": "",
    "remediation": "Control access by modifying mount targets in each availability zone. References: 1. https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not using mount targets can lead to inefficient and unreliable access to the EFS file system across availability zones. This lack of automatic provisioning reduces high availability and redundancy, increasing the risk of service interruptions and data access issues. Consequently, your infrastructure may suffer from decreased performance, higher latency, and potential data loss or downtime.",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html",
    "function_names": [
      "s3_bucket_mfa_delete_enabled_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_mfa_delete_enabled_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Mount targets act as gateways, enabling resources to be accessed across different availability zones within a VPC. When you create an EFS file system, mount targets are automatically provisioned in each availability zone associated with the VPC. This ensures high availability and redundancy, allowing seamless and efficient access to the EFS file system from any availability zone.",
      "audit_steps": "",
      "remediation_steps": "Control access by modifying mount targets in each availability zone. References: 1. https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html",
      "rationale": "Using mount targets ensures seamless access to the EFS file system across different availability zones within a VPC. This automatic provisioning of mount targets in each availability zone provides high availability and redundancy, essential for maintaining uninterrupted data access. It simplifies configuration and enhances the resilience and scalability of the file system architecture. Impact: Not using mount targets can lead to inefficient and unreliable access to the EFS file system across availability zones. This lack of automatic provisioning reduces high availability and redundancy, increasing the risk of service interruptions and data access issues. Consequently, your infrastructure may suffer from decreased performance, higher latency, and potential data loss or downtime.",
      "impact": "Not using mount targets can lead to inefficient and unreliable access to the EFS file system across availability zones. This lack of automatic provisioning reduces high availability and redundancy, increasing the risk of service interruptions and data access issues. Consequently, your infrastructure may suffer from decreased performance, higher latency, and potential data loss or downtime."
    }
  },
  {
    "id": "3.8",
    "title": "Ensure managing mount target security groups",
    "assessment": "Manual",
    "description": "Managing security groups for mount targets is essential for controlling access to your Amazon EFS file systems. By configuring these security groups, you ensure that only authorized network traffic can access your file systems, enhancing security. Regular reviews and updates of security group rules maintain strict access control, protecting your data from unauthorized access and potential breaches.",
    "rationale": "The rationale for managing security groups for mount targets is to ensure robust access control and security for your Amazon EFS file systems. By configuring these security groups, you restrict access to only authorized network traffic, thereby minimizing the risk of unauthorized access and potential data breaches. Regularly reviewing and updating these rules helps maintain strong security measures and compliance with organizational policies and industry standards. Impact: Not managing security groups for mount targets can lead to significant vulnerabilities, exposing your Amazon EFS file systems to unauthorized access and potential breaches. This lack of control increases the risk of malicious attacks, data theft, and data corruption. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation.",
    "audit": "1. Navigate to EFS. 2. Select file systems. 3. Click the radio box and select \u201cview details\u201d. 4. Select the \u201cmanage\u201d button. 5. Select \u201cNetworking\u201d tab. 6. This will bring up a screen for each of your mount points. 7. To edit Security Groups, select \u201cManage\u201d.From here, you can edit security groups for each mount point. This gives you control of how traffic can flow between each subnet.",
    "remediation": "References: 1. https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not managing security groups for mount targets can lead to significant vulnerabilities, exposing your Amazon EFS file systems to unauthorized access and potential breaches. This lack of control increases the risk of malicious attacks, data theft, and data corruption. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation.",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html",
    "function_names": [
      "s3_bucket_object_lock_enabled_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_object_lock_enabled_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Managing security groups for mount targets is essential for controlling access to your Amazon EFS file systems. By configuring these security groups, you ensure that only authorized network traffic can access your file systems, enhancing security. Regular reviews and updates of security group rules maintain strict access control, protecting your data from unauthorized access and potential breaches.",
      "audit_steps": "1. Navigate to EFS. 2. Select file systems. 3. Click the radio box and select \u201cview details\u201d. 4. Select the \u201cmanage\u201d button. 5. Select \u201cNetworking\u201d tab. 6. This will bring up a screen for each of your mount points. 7. To edit Security Groups, select \u201cManage\u201d.From here, you can edit security groups for each mount point. This gives you control of how traffic can flow between each subnet.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/efs/latest/ug/accessing-fs.html",
      "rationale": "The rationale for managing security groups for mount targets is to ensure robust access control and security for your Amazon EFS file systems. By configuring these security groups, you restrict access to only authorized network traffic, thereby minimizing the risk of unauthorized access and potential data breaches. Regularly reviewing and updating these rules helps maintain strong security measures and compliance with organizational policies and industry standards. Impact: Not managing security groups for mount targets can lead to significant vulnerabilities, exposing your Amazon EFS file systems to unauthorized access and potential breaches. This lack of control increases the risk of malicious attacks, data theft, and data corruption. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation.",
      "impact": "Not managing security groups for mount targets can lead to significant vulnerabilities, exposing your Amazon EFS file systems to unauthorized access and potential breaches. This lack of control increases the risk of malicious attacks, data theft, and data corruption. Consequently, your organization may face severe financial losses, operational disruptions, and damage to its reputation."
    }
  },
  {
    "id": "3.9",
    "title": "Ensure using VPC endpoints - EFS",
    "assessment": "Manual",
    "description": "With AWS PrivateLink, VPC Endpoints allow services to communicate within AWS using private IP addresses within approved CIDR ranges. This communication can be achieved without the need for a VPN, ensuring secure and efficient data transfer.",
    "rationale": "The rationale behind using AWS PrivateLink with VPC Endpoints is to enable secure and efficient communication between services within AWS. By using private IP addresses within approved CIDR ranges, it eliminates the need for a VPN, reducing complexity and potential points of failure. This approach enhances security, reduces latency, and ensures data remains within the AWS network, aligning with best practices for secure and reliable cloud architecture. Impact: Not using AWS PrivateLink with VPC Endpoints can lead to several issues, including increased security risks and potential data exposure since services would need to communicate over the public internet or through more complex VPN setups. This can result in higher latency, reduced performance, and greater vulnerability to attacks. Additionally, managing VPN connections adds complexity and potential points of failure, compromising the overall efficiency and reliability of your network architecture.",
    "audit": "Creating a FIPS compliant interface endpoint for EFS: 1. Navigate to VPC Console: https://console.aws.amazon.com/vpc/. 2. Select \u201cEndpoints\u201d on the sidebar. 3. Select \u201cCreate endpoint. 4. Name the endpoint. 5. Copy and paste this services into the services bar: com.amazonaws.region.elasticfilesystem-fips \u2013 replace \u201cregion: with us-east-1 or whatever region you\u2019re using. 6. Select your VPC. 7. For subnets, select the availability zone and then select private subnet. 8. Select the Security Group for the VPC endpoint. 9. For policy: select \u201cfull access\u201d. 10. Create a tag for future reference / granular IAM permissions. 11. Create endpoint.",
    "remediation": "Use VPC Endpoints in tandem with AWS Private Link to secure your EFS connections. References: 1. https://docs.aws.amazon.com/efs/latest/ug/efs-vpc- endpoints.html#:~:text=To%20establish%20a%20private%20connection,private %20network%20(VPN)%20connection.",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not using AWS PrivateLink with VPC Endpoints can lead to several issues, including increased security risks and potential data exposure since services would need to communicate over the public internet or through more complex VPN setups. This can result in higher latency, reduced performance, and greater vulnerability to attacks. Additionally, managing VPN connections adds complexity and potential points of failure, compromising the overall efficiency and reliability of your network architecture.",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/efs-vpc- endpoints.html#:~:text=To%20establish%20a%20private%20connection,private %20network%20(VPN)%20connection.",
    "function_names": [
      "s3_bucket_policy_privilege_and_public_access_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_policy_privilege_and_public_access_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "With AWS PrivateLink, VPC Endpoints allow services to communicate within AWS using private IP addresses within approved CIDR ranges. This communication can be achieved without the need for a VPN, ensuring secure and efficient data transfer.",
      "audit_steps": "Creating a FIPS compliant interface endpoint for EFS: 1. Navigate to VPC Console: https://console.aws.amazon.com/vpc/. 2. Select \u201cEndpoints\u201d on the sidebar. 3. Select \u201cCreate endpoint. 4. Name the endpoint. 5. Copy and paste this services into the services bar: com.amazonaws.region.elasticfilesystem-fips \u2013 replace \u201cregion: with us-east-1 or whatever region you\u2019re using. 6. Select your VPC. 7. For subnets, select the availability zone and then select private subnet. 8. Select the Security Group for the VPC endpoint. 9. For policy: select \u201cfull access\u201d. 10. Create a tag for future reference / granular IAM permissions. 11. Create endpoint.",
      "remediation_steps": "Use VPC Endpoints in tandem with AWS Private Link to secure your EFS connections. References: 1. https://docs.aws.amazon.com/efs/latest/ug/efs-vpc- endpoints.html#:~:text=To%20establish%20a%20private%20connection,private %20network%20(VPN)%20connection.",
      "rationale": "The rationale behind using AWS PrivateLink with VPC Endpoints is to enable secure and efficient communication between services within AWS. By using private IP addresses within approved CIDR ranges, it eliminates the need for a VPN, reducing complexity and potential points of failure. This approach enhances security, reduces latency, and ensures data remains within the AWS network, aligning with best practices for secure and reliable cloud architecture. Impact: Not using AWS PrivateLink with VPC Endpoints can lead to several issues, including increased security risks and potential data exposure since services would need to communicate over the public internet or through more complex VPN setups. This can result in higher latency, reduced performance, and greater vulnerability to attacks. Additionally, managing VPN connections adds complexity and potential points of failure, compromising the overall efficiency and reliability of your network architecture.",
      "impact": "Not using AWS PrivateLink with VPC Endpoints can lead to several issues, including increased security risks and potential data exposure since services would need to communicate over the public internet or through more complex VPN setups. This can result in higher latency, reduced performance, and greater vulnerability to attacks. Additionally, managing VPN connections adds complexity and potential points of failure, compromising the overall efficiency and reliability of your network architecture."
    }
  },
  {
    "id": "3.10",
    "title": "Ensure managing AWS EFS access points",
    "assessment": "Manual",
    "description": "EFS access points serve as gateways to your EFS file system, allowing applications to interact with the file system across various resources. Proper configuration of these access points within your applications is crucial to ensure seamless and secure access. By configuring EFS access points, you can control and manage which users have access to specific resources in your EFS environment, enhancing security and operational efficiency.",
    "rationale": "The rationale behind properly configuring EFS access points is to ensure secure and efficient interaction between your applications and the EFS file system. By setting up these access points correctly, you can control and manage user permissions, ensuring that only authorized users can access specific resources. This not only enhances the security of your data but also improves operational efficiency by preventing unauthorized access and potential data breaches.",
    "audit": "1. Creating an EFS access point: You can create an EFS access point through the amazon CLI, AWS console, and with the EFS API. An EFS can only have up to 1,000 access points. 2. Mounting an EFS access point: Consult the section where we mounted an EFS file system on an EC2 instance. While inside the resource you want to configure an access point for, type in this command: mount -t efs -o tls,iam,accesspoint=fsap-abcdef0123456789a fs- abc0123def456789a: /localmountpoint  3. Enforcing a User Identity with an EFS access point: You can enforce user identity to ensure that users and groups with proper permissions are able to access the EFS file system. In order to do this, you must specify the user and group ID you wish to have ownership of the files. When enforcement is enabled, that file that is was created by the user will automatically show ownership to belong to the user. When enforcement is enabled, the access point considers the User ID, group ID, and secondary group ID. It ignored the NFS client\u2019s ID. Note: enforcing the user ID is subject to the \u201cClientRootAccess\u201d IAM permission. If either the User ID or Group ID = 0, then you must explicitly allow \u201cClientRootAccess\u201d permission.  4. Enforcing a root directory with an access point: If you wish to override the root directory of the EFS, you can make the root directory that of the access point. To enforce the root directory with an access point, you must specify three things upon provisioning the EFS mount point: o Owner UID o Group GID o Permissions To access an EFS from an access point, a root directory must be created and enforced. Reminder: You must specify permissions for the access point root directory. If these permissions are not defined, a root directory will not be created on the mount point, and you will not be able to access EFS from an access point. 5. Security Model for access point root directories: When a root directory override is in effect, the EFS behaves like a Linux server with a no_subtree_check option enabled.",
    "remediation": "Implement AWS EFS access points References: 1. https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html",
    "function_names": [
      "s3_bucket_secure_transport_and_network_security_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_secure_transport_and_network_security_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "CRITICAL",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "EFS access points serve as gateways to your EFS file system, allowing applications to interact with the file system across various resources. Proper configuration of these access points within your applications is crucial to ensure seamless and secure access. By configuring EFS access points, you can control and manage which users have access to specific resources in your EFS environment, enhancing security and operational efficiency.",
      "audit_steps": "1. Creating an EFS access point: You can create an EFS access point through the amazon CLI, AWS console, and with the EFS API. An EFS can only have up to 1,000 access points. 2. Mounting an EFS access point: Consult the section where we mounted an EFS file system on an EC2 instance. While inside the resource you want to configure an access point for, type in this command: mount -t efs -o tls,iam,accesspoint=fsap-abcdef0123456789a fs- abc0123def456789a: /localmountpoint  3. Enforcing a User Identity with an EFS access point: You can enforce user identity to ensure that users and groups with proper permissions are able to access the EFS file system. In order to do this, you must specify the user and group ID you wish to have ownership of the files. When enforcement is enabled, that file that is was created by the user will automatically show ownership to belong to the user. When enforcement is enabled, the access point considers the User ID, group ID, and secondary group ID. It ignored the NFS client\u2019s ID. Note: enforcing the user ID is subject to the \u201cClientRootAccess\u201d IAM permission. If either the User ID or Group ID = 0, then you must explicitly allow \u201cClientRootAccess\u201d permission.  4. Enforcing a root directory with an access point: If you wish to override the root directory of the EFS, you can make the root directory that of the access point. To enforce the root directory with an access point, you must specify three things upon provisioning the EFS mount point: o Owner UID o Group GID o Permissions To access an EFS from an access point, a root directory must be created and enforced. Reminder: You must specify permissions for the access point root directory. If these permissions are not defined, a root directory will not be created on the mount point, and you will not be able to access EFS from an access point. 5. Security Model for access point root directories: When a root directory override is in effect, the EFS behaves like a Linux server with a no_subtree_check option enabled.",
      "remediation_steps": "Implement AWS EFS access points References: 1. https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html",
      "rationale": "The rationale behind properly configuring EFS access points is to ensure secure and efficient interaction between your applications and the EFS file system. By setting up these access points correctly, you can control and manage user permissions, ensuring that only authorized users can access specific resources. This not only enhances the security of your data but also improves operational efficiency by preventing unauthorized access and potential data breaches.",
      "impact": ""
    }
  },
  {
    "id": "3.11",
    "title": "Ensure accessing Points and IAM Policies",
    "assessment": "Manual",
    "description": "You can use IAM policies to control access to your EFS access points. To achieve this, utilize the elasticfilesystem:AccessPointArn IAM condition key. The AccessPointArn represents the Amazon Resource Name (ARN) of the access point that the file system is mounted with.",
    "rationale": "The rationale for using IAM policies with the elasticfilesystem:AccessPointArn condition key is to ensure precise and secure access control to EFS access points. By specifying the access point's ARN, you can restrict interactions to authorized users and resources only, thereby enhancing data security and preventing unauthorized access. This approach maintains the integrity and confidentiality of your data within the AWS environment.",
    "audit": "Below is a same IAM policy copied from the AWS documentation: { \"Version\": \"2012-10-17\", \"Id\": \"MyFileSystemPolicy\", \"Statement\": [ { \"Sid\": \"App1Access\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::111122223333:role/app1\" }, \"Action\": [ \"elasticfilesystem:ClientMount\", \"elasticfilesystem:ClientWrite\" ], \"Condition\": { \"StringEquals\": { \"elasticfilesystem:AccessPointArn\":\"arn:aws:elasticfilesystem:us-east- 1:222233334444:access-point/fsap-01234567\" } } }, { \"Sid\": \"App2Access\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::111122223333:role/app2\" }, \"Action\": [ \"elasticfilesystem:ClientMount\", \"elasticfilesystem:ClientWrite\" ], \"Condition\": { \"StringEquals\": { \"elasticfilesystem:AccessPointArn\":\"arn:aws:elasticfilesystem:us-east 1:222233334444:access-point/fsap-89abcdef\" } } } ] }",
    "remediation": "References: 1. https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "You can use IAM policies to control access to your EFS access points. To achieve this, utilize the elasticfilesystem:AccessPointArn IAM condition key. The AccessPointArn represents the Amazon Resource Name (ARN) of the access point that the file system is mounted with.",
      "audit_steps": "Below is a same IAM policy copied from the AWS documentation: { \"Version\": \"2012-10-17\", \"Id\": \"MyFileSystemPolicy\", \"Statement\": [ { \"Sid\": \"App1Access\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::111122223333:role/app1\" }, \"Action\": [ \"elasticfilesystem:ClientMount\", \"elasticfilesystem:ClientWrite\" ], \"Condition\": { \"StringEquals\": { \"elasticfilesystem:AccessPointArn\":\"arn:aws:elasticfilesystem:us-east- 1:222233334444:access-point/fsap-01234567\" } } }, { \"Sid\": \"App2Access\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::111122223333:role/app2\" }, \"Action\": [ \"elasticfilesystem:ClientMount\", \"elasticfilesystem:ClientWrite\" ], \"Condition\": { \"StringEquals\": { \"elasticfilesystem:AccessPointArn\":\"arn:aws:elasticfilesystem:us-east 1:222233334444:access-point/fsap-89abcdef\" } } } ] }",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html",
      "rationale": "The rationale for using IAM policies with the elasticfilesystem:AccessPointArn condition key is to ensure precise and secure access control to EFS access points. By specifying the access point's ARN, you can restrict interactions to authorized users and resources only, thereby enhancing data security and preventing unauthorized access. This approach maintains the integrity and confidentiality of your data within the AWS environment.",
      "impact": ""
    }
  },
  {
    "id": "3.12",
    "title": "Ensure configuring IAM for AWS Elastic Disaster Recovery",
    "assessment": "Manual",
    "description": "Before installing the AWS Elastic Disaster Recovery client, you need to configure AWS IAM permissions and users for both the AWS Replication and AWS Failback Client.",
    "rationale": "Configuring AWS IAM permissions and users before installing the AWS Elastic Disaster Recovery client ensures that the AWS Replication and AWS Failback Client have the necessary access rights. This setup is essential for maintaining security and preventing unauthorized access. Proper IAM configuration guarantees the smooth operation of disaster recovery processes, safeguarding your data and ensuring system reliability.",
    "audit": "To create DRS Agent User, follow following steps: 1. Navigate to the AWS IAM Console - https://us-east- 1.console.aws.amazon.com/iam/home?region=us-east-1#/home. 2. Create new user. This user will only be able to access the Elastic disaster recovery agent installation resource. Accordingly, name the user \u201cDSRuser\u201d. 3. Allow Programmatic access: This allows the user to access resources programmatically with a secure key rather than having to enter a password. 4. elect \u201cattach policies directly\u201d and search for \u201cAWSElasticDisasterRecoveryAgentInstallationPolicy\u201d. 5. Create user. To create Failback Agent User, Follow the steps above with these two modifications: 1. Name the user \u201cFailbackAgentuser\u201d. 2. Apply the \u201cAWSElasticDisasterRecoveryFailbackInstallationPolicy\u201d.",
    "remediation": "Configure IAM Credentials for AWS Elastic Disaster Recovery. References: 1. https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Before installing the AWS Elastic Disaster Recovery client, you need to configure AWS IAM permissions and users for both the AWS Replication and AWS Failback Client.",
      "audit_steps": "To create DRS Agent User, follow following steps: 1. Navigate to the AWS IAM Console - https://us-east- 1.console.aws.amazon.com/iam/home?region=us-east-1#/home. 2. Create new user. This user will only be able to access the Elastic disaster recovery agent installation resource. Accordingly, name the user \u201cDSRuser\u201d. 3. Allow Programmatic access: This allows the user to access resources programmatically with a secure key rather than having to enter a password. 4. elect \u201cattach policies directly\u201d and search for \u201cAWSElasticDisasterRecoveryAgentInstallationPolicy\u201d. 5. Create user. To create Failback Agent User, Follow the steps above with these two modifications: 1. Name the user \u201cFailbackAgentuser\u201d. 2. Apply the \u201cAWSElasticDisasterRecoveryFailbackInstallationPolicy\u201d.",
      "remediation_steps": "Configure IAM Credentials for AWS Elastic Disaster Recovery. References: 1. https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home",
      "rationale": "Configuring AWS IAM permissions and users before installing the AWS Elastic Disaster Recovery client ensures that the AWS Replication and AWS Failback Client have the necessary access rights. This setup is essential for maintaining security and preventing unauthorized access. Proper IAM configuration guarantees the smooth operation of disaster recovery processes, safeguarding your data and ensuring system reliability.",
      "impact": ""
    }
  },
  {
    "id": "4.1",
    "title": "FSX (AWS Elastic File Cache)",
    "assessment": "Manual",
    "description": "Amazon File Cache is a fully managed, high speed cache on AWS that is used to process file data, regardless of where the data is stored. AWS File Cache is a serverless service on AWS that spares the administrators from the burden of managing file servers and storage volumes, updating hardware, configuring software, running out of capacity, or tuning performance. AWS Elastic cache is capable of handling hundreds of GB/s of throughput and up to millions of operations per second. AWS FSx is an excellent service for cost optimization and high scalability. Amazon File Cache automatically loads data into the cache when it\u2019s accessed for the first time and automatically releases data when it\u2019s not used.",
    "rationale": "Amazon File Cache is used as a temporary, high performance storage location for data that\u2019s stored in on-premises file systems, AWS file systems, and Amazon S3 buckets. This service is used for data processing and is best suited for applications that need high data processing speeds. This is not a long term storage option.",
    "audit": "",
    "remediation": "You can link your cache to S3 data repositories or to any file system that supports the NFSv3 protocol. The NFS data repository can either be on premises or in the cloud and you can link a maximum of eight repositories. All the linked repositories must be using the same file system; either S3 or NFS. When linked to a data repository, Amazon File Cache transparently presents S3 or NFS objects as files and directories. Amazon File Cache is compatible to be used interchangeably with Amazon Elastic Compute Service, Amazon Elastic Container Service, and Amazon Elastic Kubernetes Service. References: 1. https://aws.amazon.com/fsx/",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://aws.amazon.com/fsx/",
    "function_names": [
      "ec2_ebs_volume_creation_check",
      "ec2_ebs_volume_encryption_kms_key_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_creation_check",
        "ec2_ebs_volume_encryption_kms_key_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon File Cache is a fully managed, high speed cache on AWS that is used to process file data, regardless of where the data is stored. AWS File Cache is a serverless service on AWS that spares the administrators from the burden of managing file servers and storage volumes, updating hardware, configuring software, running out of capacity, or tuning performance. AWS Elastic cache is capable of handling hundreds of GB/s of throughput and up to millions of operations per second. AWS FSx is an excellent service for cost optimization and high scalability. Amazon File Cache automatically loads data into the cache when it\u2019s accessed for the first time and automatically releases data when it\u2019s not used.",
      "audit_steps": "",
      "remediation_steps": "You can link your cache to S3 data repositories or to any file system that supports the NFSv3 protocol. The NFS data repository can either be on premises or in the cloud and you can link a maximum of eight repositories. All the linked repositories must be using the same file system; either S3 or NFS. When linked to a data repository, Amazon File Cache transparently presents S3 or NFS objects as files and directories. Amazon File Cache is compatible to be used interchangeably with Amazon Elastic Compute Service, Amazon Elastic Container Service, and Amazon Elastic Kubernetes Service. References: 1. https://aws.amazon.com/fsx/",
      "rationale": "Amazon File Cache is used as a temporary, high performance storage location for data that\u2019s stored in on-premises file systems, AWS file systems, and Amazon S3 buckets. This service is used for data processing and is best suited for applications that need high data processing speeds. This is not a long term storage option.",
      "impact": ""
    }
  },
  {
    "id": "4.2",
    "title": "Amazon Elastic File Cache",
    "assessment": "Manual",
    "description": "Amazon File Cache is available in the following AWS Regions: 1. US East (N. Virginia) 2. US East (Ohio) 3. US West (Oregon) 4. Canada (Central) 5. Europe (Frankfurt) 6. Europe (Ireland) 7. Europe (London) 8. Europe (Stockholm) 9. Asia Pacific (Hong Kong) 10. Asia Pacific (Mumbai) 11. Asia Pacific (Seoul) 12. Asia Pacific (Tokyo) 13. Asia Pacific (Singapore) 14. Asia Pacific (Sydney) Amazon Elastic File Cache Compatibility: In order to use AWS FSx, you must ensure that the operating system you\u2019re using on the compute instance is compatible with AWS FSx. Below are the compatible operating systems: 1. Amazon Linux 2 and Amazon Linux 2. Red Hat Enterprise Linux (RHEL) 3. CentOS 4. Rocky Linux 5. Ubuntu. The Lustre client must be installed on these systems in order for the FSx service to work.",
    "rationale": "The rationale behind creating Amazon Elastic File Cache is to enhance the performance and scalability of cloud-based applications by providing a high-speed, scalable file caching solution. This service reduces latency and improves access times for frequently accessed data, thereby optimizing application performance and user experience. Additionally, it helps manage and reduce storage costs by efficiently utilizing cached data, ensuring that resources are used effectively while maintaining high performance standards. Impact: Not implementing Amazon Elastic File Cache can lead to increased latency and slower access times for frequently accessed data, resulting in suboptimal performance for cloud-based applications. This can negatively affect user experience and productivity. Additionally, without an efficient caching solution, there may be higher storage costs due to inefficient use of resources, and the system may struggle to handle high demand, leading to potential performance bottlenecks and scalability issues.",
    "audit": "Creating Amazon Elastic File Cache: Before you can start using Amazon Elastic File Cache, you must set up an Amazon Elastic Compute Instance and an S3 bucket. We\u2019re going to create a new EC2 instance and S3 bucket for the sake of this tutorial. Creating an EC2 instance for FSx: Make sure that whatever AMI you select is compatible with Lustre 2.12 client. - Navigate to the Amazon EC2 console. - Select \u201cLaunch Instance\u201d. - Give your server a name. - Select \u201cUbuntu\u201d or an operating system that\u2019s compatible with FSx. - Select default VPC and security group. - Select or create private SSH keys. - Leave the rest of the settings default. - Create Instance.",
    "remediation": "References: 1. https://aws.amazon.com/fsx/",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Not implementing Amazon Elastic File Cache can lead to increased latency and slower access times for frequently accessed data, resulting in suboptimal performance for cloud-based applications. This can negatively affect user experience and productivity. Additionally, without an efficient caching solution, there may be higher storage costs due to inefficient use of resources, and the system may struggle to handle high demand, leading to potential performance bottlenecks and scalability issues.",
    "references": "1. https://aws.amazon.com/fsx/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon File Cache is available in the following AWS Regions: 1. US East (N. Virginia) 2. US East (Ohio) 3. US West (Oregon) 4. Canada (Central) 5. Europe (Frankfurt) 6. Europe (Ireland) 7. Europe (London) 8. Europe (Stockholm) 9. Asia Pacific (Hong Kong) 10. Asia Pacific (Mumbai) 11. Asia Pacific (Seoul) 12. Asia Pacific (Tokyo) 13. Asia Pacific (Singapore) 14. Asia Pacific (Sydney) Amazon Elastic File Cache Compatibility: In order to use AWS FSx, you must ensure that the operating system you\u2019re using on the compute instance is compatible with AWS FSx. Below are the compatible operating systems: 1. Amazon Linux 2 and Amazon Linux 2. Red Hat Enterprise Linux (RHEL) 3. CentOS 4. Rocky Linux 5. Ubuntu. The Lustre client must be installed on these systems in order for the FSx service to work.",
      "audit_steps": "Creating Amazon Elastic File Cache: Before you can start using Amazon Elastic File Cache, you must set up an Amazon Elastic Compute Instance and an S3 bucket. We\u2019re going to create a new EC2 instance and S3 bucket for the sake of this tutorial. Creating an EC2 instance for FSx: Make sure that whatever AMI you select is compatible with Lustre 2.12 client. - Navigate to the Amazon EC2 console. - Select \u201cLaunch Instance\u201d. - Give your server a name. - Select \u201cUbuntu\u201d or an operating system that\u2019s compatible with FSx. - Select default VPC and security group. - Select or create private SSH keys. - Leave the rest of the settings default. - Create Instance.",
      "remediation_steps": "References: 1. https://aws.amazon.com/fsx/",
      "rationale": "The rationale behind creating Amazon Elastic File Cache is to enhance the performance and scalability of cloud-based applications by providing a high-speed, scalable file caching solution. This service reduces latency and improves access times for frequently accessed data, thereby optimizing application performance and user experience. Additionally, it helps manage and reduce storage costs by efficiently utilizing cached data, ensuring that resources are used effectively while maintaining high performance standards. Impact: Not implementing Amazon Elastic File Cache can lead to increased latency and slower access times for frequently accessed data, resulting in suboptimal performance for cloud-based applications. This can negatively affect user experience and productivity. Additionally, without an efficient caching solution, there may be higher storage costs due to inefficient use of resources, and the system may struggle to handle high demand, leading to potential performance bottlenecks and scalability issues.",
      "impact": "Not implementing Amazon Elastic File Cache can lead to increased latency and slower access times for frequently accessed data, resulting in suboptimal performance for cloud-based applications. This can negatively affect user experience and productivity. Additionally, without an efficient caching solution, there may be higher storage costs due to inefficient use of resources, and the system may struggle to handle high demand, leading to potential performance bottlenecks and scalability issues."
    }
  },
  {
    "id": "4.3",
    "title": "Ensure the creation of an FSX Bucket",
    "assessment": "Manual",
    "description": "An S3 bucket will store the data that Amazon Elastic File Cache accesses",
    "rationale": "Storing data in S3 ensures scalability, durability, and cost-efficiency, while Amazon Elastic File Cache enhances access speed by caching frequently accessed data. This combination leverages the strengths of both services, providing a seamless and efficient data storage and retrieval solution.",
    "audit": "1. Navigate to the Amazon S3 bucket console. https://s3.console.aws.amazon.com/s3/. 2. Select \u201cCreate Bucket\u201d. 3. Give your bucket a name and select the region. Note: your bucket must be a unique name that\u2019s not used anywhere else on AWS. 4. Block public access: This is an internal service that will not be accessed outside of our internal AWS network. Keep the \u201cblock public access\u201d setting checked. 5. Enable bucket versioning. 6. Leave the rest of the settings as default. 7. Select \u201ccreate bucket.\u201d 8. Create a path in your bucket, give it a name and leave the encryption as default for now.",
    "remediation": "References: 1. https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data- repo.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data- repo.html",
    "function_names": [
      "s3_bucket_unique_check",
      "s3_bucket_default_encryption_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check",
        "s3_bucket_default_encryption_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "An S3 bucket will store the data that Amazon Elastic File Cache accesses",
      "audit_steps": "1. Navigate to the Amazon S3 bucket console. https://s3.console.aws.amazon.com/s3/. 2. Select \u201cCreate Bucket\u201d. 3. Give your bucket a name and select the region. Note: your bucket must be a unique name that\u2019s not used anywhere else on AWS. 4. Block public access: This is an internal service that will not be accessed outside of our internal AWS network. Keep the \u201cblock public access\u201d setting checked. 5. Enable bucket versioning. 6. Leave the rest of the settings as default. 7. Select \u201ccreate bucket.\u201d 8. Create a path in your bucket, give it a name and leave the encryption as default for now.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data- repo.html",
      "rationale": "Storing data in S3 ensures scalability, durability, and cost-efficiency, while Amazon Elastic File Cache enhances access speed by caching frequently accessed data. This combination leverages the strengths of both services, providing a seamless and efficient data storage and retrieval solution.",
      "impact": ""
    }
  },
  {
    "id": "4.4",
    "title": "Ensure the creation of Elastic File Cache",
    "assessment": "Manual",
    "description": "With the prerequisites completed, we can now proceed to create our Elastic File Cache.",
    "rationale": "By implementing an Elastic File Cache, frequently accessed data is stored closer to the application, reducing latency and speeding up access times. This approach optimizes resource utilization, improves user experience, and ensures that the system can handle high-demand workloads effectively.",
    "audit": "1. Navigate to the AWS Elastic File Cache console: https://console.aws.amazon.com/fsx/. 2. Click the hamburger menu on the left side of the screen and select \u201ccaches\u201d. 3. Select \u201cCreate Cache\u201d 4. Give your Cache a name. Choose a name that you will remember. 5. Select the amount of storage capacity you need for your cache. We\u2019ll select 1.2 TiB for this tutorial. You can select storage capacity in increments of 1.2 TiB. 6. Select the amount of throughput capacity. The amount of Throughput capacity is calculated by multiplying the cache storage capacity by the throughput tier. For example, for a 1.2 TiB cache, it's 1200 MB/s; for a 9.6 TiB cache, it's 9600 MB/s. Throughput capacity is the sustained speed at which the file server that hosts your cache can serve data. 7. In the Network & Security section, provide networking and security group information: o For Virtual Private Cloud (VPC) choose the correct amazon VPC that you want to associate with your cache. We\u2019re going to use the default VPC. o For VPC Security Groups, the ID for the default security group for your VPC should already be added. o For Subnet, you can choose any of the available subnets. 8. In the Encryption section, choose the Default aws/fsx KMS encryption keys to protect your data by encrypting your data at-rest. 9. You have the option to create tags; this is an optional step. 10. Select \u201cnext\u201d. 11. In the Data repository associations (DRAs) section, there are no DRAs linking your cache to S3 or NFS repositories. We need to link the cache that we\u2019re creating to the Amazon S3 bucket that we created earlier. o For Data repository type, choose S3 o For Data repository path, type the path of the S3 bucket that you want to associate with this cache. For example: s3://{example-bucket}/{example-prefix} ```       - To access this URL, go back to the S3 bucket that was just created and navigate to the directory of the folder that you created. Select \u201ccopy AWS URI\u201d. - For cache path, enter the name of a high-level directory such as /ns1 or subdirectory such as ns1/subdir within Amazon File Cache to associate with the S3 data repository. The first forward slash in the path is required. 12. Select \u201cnext\u201d this will take you to the summary page. 13. Choose\u201d Create Cache.\u201d You will see your cache in the FSx dashboard.",
    "remediation": "References: 1. https://aws.amazon.com/filecache/",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://aws.amazon.com/filecache/",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "With the prerequisites completed, we can now proceed to create our Elastic File Cache.",
      "audit_steps": "1. Navigate to the AWS Elastic File Cache console: https://console.aws.amazon.com/fsx/. 2. Click the hamburger menu on the left side of the screen and select \u201ccaches\u201d. 3. Select \u201cCreate Cache\u201d 4. Give your Cache a name. Choose a name that you will remember. 5. Select the amount of storage capacity you need for your cache. We\u2019ll select 1.2 TiB for this tutorial. You can select storage capacity in increments of 1.2 TiB. 6. Select the amount of throughput capacity. The amount of Throughput capacity is calculated by multiplying the cache storage capacity by the throughput tier. For example, for a 1.2 TiB cache, it's 1200 MB/s; for a 9.6 TiB cache, it's 9600 MB/s. Throughput capacity is the sustained speed at which the file server that hosts your cache can serve data. 7. In the Network & Security section, provide networking and security group information: o For Virtual Private Cloud (VPC) choose the correct amazon VPC that you want to associate with your cache. We\u2019re going to use the default VPC. o For VPC Security Groups, the ID for the default security group for your VPC should already be added. o For Subnet, you can choose any of the available subnets. 8. In the Encryption section, choose the Default aws/fsx KMS encryption keys to protect your data by encrypting your data at-rest. 9. You have the option to create tags; this is an optional step. 10. Select \u201cnext\u201d. 11. In the Data repository associations (DRAs) section, there are no DRAs linking your cache to S3 or NFS repositories. We need to link the cache that we\u2019re creating to the Amazon S3 bucket that we created earlier. o For Data repository type, choose S3 o For Data repository path, type the path of the S3 bucket that you want to associate with this cache. For example: s3://{example-bucket}/{example-prefix} ```       - To access this URL, go back to the S3 bucket that was just created and navigate to the directory of the folder that you created. Select \u201ccopy AWS URI\u201d. - For cache path, enter the name of a high-level directory such as /ns1 or subdirectory such as ns1/subdir within Amazon File Cache to associate with the S3 data repository. The first forward slash in the path is required. 12. Select \u201cnext\u201d this will take you to the summary page. 13. Choose\u201d Create Cache.\u201d You will see your cache in the FSx dashboard.",
      "remediation_steps": "References: 1. https://aws.amazon.com/filecache/",
      "rationale": "By implementing an Elastic File Cache, frequently accessed data is stored closer to the application, reducing latency and speeding up access times. This approach optimizes resource utilization, improves user experience, and ensures that the system can handle high-demand workloads effectively.",
      "impact": ""
    }
  },
  {
    "id": "4.5",
    "title": "Ensure installation and configuration of Lustre Client",
    "assessment": "Manual",
    "description": "To utilize the newly created File Cache, you must install the Lustre Client on your EC2 instance.",
    "rationale": "The Lustre Client facilitates efficient communication between the EC2 instance and the File Cache, ensuring high-performance data access and improved overall system efficiency. This setup is crucial for optimizing data processing and leveraging the benefits of the File Cache.",
    "audit": "Follow along to install the Lustre Client on Ubuntu 22.04: 1. Launch your EC2 instance. Navigate to the folder of your secure key and ssh into the instance using this command: - ssh -i \"{KEY.pem}\" ubuntu@{your ec2 instance} - When prompted to log in with the SSH key, enter in \u201cyes\u201d - You should now be connected to your EC2 instance. 2. Run the following command to download and install the public Lustre key: wget -O - https://fsx-lustre-client-repo-public-keys.s3.amazonaws.com/fsx- ubuntu-public-key.asc | gpg --dearmor | sudo tee /usr/share/keyrings/fsx- ubuntu-public-key.gpg >/dev/null  3. Add the AWS Lustre package repository to your local package manager using the following command: sudo bash -c 'echo \"deb [signed-by=/usr/share/keyrings/fsx-ubuntu-public- key.gpg] https://fsx-lustre-client-repo.s3.amazonaws.com/ubuntu jammy main\" > /etc/apt/sources.list.d/fsxlustreclientrepo.list && apt-get update'  4. Determine which kernel is currently running on your client instance and update as needed. The AWS Lustre client on Ubuntu 22.02 requires kernel 5.15.0.1020- aws or later for both x86 based EC2 instances and Arm-based EC2 instanced powered by AWS Graviton processors: a. Run the following command to find out which kernel your machine is running: uname -r o If your kernel is not up to date, run the following command: This will install the kernel update, Lustre client update, as well as reboot your system. sudo apt install -y linux-aws lustre-client-modules-aws && sudo reboot - If your kernel is up to date and you just want to install the latest Lustre version, run this command: sudo apt install -y lustre-client-modules-$(uname -r)",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "To utilize the newly created File Cache, you must install the Lustre Client on your EC2 instance.",
      "audit_steps": "Follow along to install the Lustre Client on Ubuntu 22.04: 1. Launch your EC2 instance. Navigate to the folder of your secure key and ssh into the instance using this command: - ssh -i \"{KEY.pem}\" ubuntu@{your ec2 instance} - When prompted to log in with the SSH key, enter in \u201cyes\u201d - You should now be connected to your EC2 instance. 2. Run the following command to download and install the public Lustre key: wget -O - https://fsx-lustre-client-repo-public-keys.s3.amazonaws.com/fsx- ubuntu-public-key.asc | gpg --dearmor | sudo tee /usr/share/keyrings/fsx- ubuntu-public-key.gpg >/dev/null  3. Add the AWS Lustre package repository to your local package manager using the following command: sudo bash -c 'echo \"deb [signed-by=/usr/share/keyrings/fsx-ubuntu-public- key.gpg] https://fsx-lustre-client-repo.s3.amazonaws.com/ubuntu jammy main\" > /etc/apt/sources.list.d/fsxlustreclientrepo.list && apt-get update'  4. Determine which kernel is currently running on your client instance and update as needed. The AWS Lustre client on Ubuntu 22.02 requires kernel 5.15.0.1020- aws or later for both x86 based EC2 instances and Arm-based EC2 instanced powered by AWS Graviton processors: a. Run the following command to find out which kernel your machine is running: uname -r o If your kernel is not up to date, run the following command: This will install the kernel update, Lustre client update, as well as reboot your system. sudo apt install -y linux-aws lustre-client-modules-aws && sudo reboot - If your kernel is up to date and you just want to install the latest Lustre version, run this command: sudo apt install -y lustre-client-modules-$(uname -r)",
      "remediation_steps": null,
      "rationale": "The Lustre Client facilitates efficient communication between the EC2 instance and the File Cache, ensuring high-performance data access and improved overall system efficiency. This setup is crucial for optimizing data processing and leveraging the benefits of the File Cache.",
      "impact": ""
    }
  },
  {
    "id": "4.6",
    "title": "Ensure EC2 Kernel compatibility with Lustre",
    "assessment": "Manual",
    "description": "The latest kernel included with the Ubuntu Amazon EC2 AMI is not compatible with the Lustre service, which is crucial for mounting the cache on your EC2 instance. To downgrade your kernel, specific prerequisites must be met if you are using the default Ubuntu machine image as of November 8, 2023.",
    "rationale": "The latest kernel version is not supported by Lustre, and meeting the prerequisites for downgrading will allow you to leverage Lustre's high-performance file system capabilities effectively. This ensures optimal data access and processing efficiency on your EC2 instance.",
    "audit": "Follow the steps to downgrade your kernel: 1. List all of the available Lustre packages by typing in this command: sudo apt- cache search lustre-client-modules. This will show a list of supported modules with corresponding kernel versions in ascending order from top to bottom. The most recent version in this case is \u201clustre-client-modules-5.15.0-1049-aws. Save this information for the next commands. 2. Install the most recent linux image that supports the Lustre client with this command: sudo apt-get install -y linux-image-5.15.0-1049-aws sudo sed -i 's/GRUB_DEFAULT=.\\+/GRUB\\_DEFAULT=\"Advanced options for Ubuntu>Ubuntu, with Linux 5.15.0-1049-aws\"/' /etc/default/grub  3. Reboot your system by typing \u201csudo reboot\u201d. 4. Install the correct Lustre module: . sudo apt-get install -y lustre-client-modules-$(uname -r)",
    "remediation": "References: 1. https://docs.aws.amazon.com/fsx/latest/LustreGuide/install-lustre-client.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/fsx/latest/LustreGuide/install-lustre-client.html",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "The latest kernel included with the Ubuntu Amazon EC2 AMI is not compatible with the Lustre service, which is crucial for mounting the cache on your EC2 instance. To downgrade your kernel, specific prerequisites must be met if you are using the default Ubuntu machine image as of November 8, 2023.",
      "audit_steps": "Follow the steps to downgrade your kernel: 1. List all of the available Lustre packages by typing in this command: sudo apt- cache search lustre-client-modules. This will show a list of supported modules with corresponding kernel versions in ascending order from top to bottom. The most recent version in this case is \u201clustre-client-modules-5.15.0-1049-aws. Save this information for the next commands. 2. Install the most recent linux image that supports the Lustre client with this command: sudo apt-get install -y linux-image-5.15.0-1049-aws sudo sed -i 's/GRUB_DEFAULT=.\\+/GRUB\\_DEFAULT=\"Advanced options for Ubuntu>Ubuntu, with Linux 5.15.0-1049-aws\"/' /etc/default/grub  3. Reboot your system by typing \u201csudo reboot\u201d. 4. Install the correct Lustre module: . sudo apt-get install -y lustre-client-modules-$(uname -r)",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/fsx/latest/LustreGuide/install-lustre-client.html",
      "rationale": "The latest kernel version is not supported by Lustre, and meeting the prerequisites for downgrading will allow you to leverage Lustre's high-performance file system capabilities effectively. This ensures optimal data access and processing efficiency on your EC2 instance.",
      "impact": ""
    }
  },
  {
    "id": "4.7",
    "title": "Ensure mounting FSx cache",
    "assessment": "Manual",
    "description": "Mounting the FSx cache is a crucial step to optimize data retrieval and system performance. This process involves connecting the FSx file system to your compute instances, allowing them to access cached data efficiently. Properly mounting the FSx cache ensures low-latency access to frequently used data, enhances overall application performance, and leverages the full capabilities of the AWS FSx service. This setup is essential for achieving high performance and efficient data processing in your AWS environment.",
    "rationale": "By connecting the FSx file system to your compute instances, you enable low-latency access to frequently used data, significantly improving application performance. This setup leverages the full capabilities of the AWS FSx service, ensuring efficient data processing and resource utilization in your AWS environment. Properly mounting the FSx cache is essential for achieving high performance and operational efficiency.",
    "audit": "To mount your cache, follow the next steps: 1. Make a directory for the mount point with the following command: sudo mkdir -p /mnt  2. Mount the Amazon file cache to the directory that you just created. Use the following command and replace these names: o Replace cache_dns_name with the actual file cache\u2019s Domain Name System (DNS) name o Replace mountname with the cache\u2019s mount name, which you can get by running the describe-file-caches AWS CLI command or DescribeFileCaches API operation sudo mount -t lustre -o relatime,flock cache_dns_name@tcp:/mountname /mnt Note: Make sure your EC2 instance is in the same VPC as your cache. If done correctly, the path of your folder will show up in the /mnt folder. You can also use the df command to see the DNS and mount point is attached to your file system:",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Mounting the FSx cache is a crucial step to optimize data retrieval and system performance. This process involves connecting the FSx file system to your compute instances, allowing them to access cached data efficiently. Properly mounting the FSx cache ensures low-latency access to frequently used data, enhances overall application performance, and leverages the full capabilities of the AWS FSx service. This setup is essential for achieving high performance and efficient data processing in your AWS environment.",
      "audit_steps": "To mount your cache, follow the next steps: 1. Make a directory for the mount point with the following command: sudo mkdir -p /mnt  2. Mount the Amazon file cache to the directory that you just created. Use the following command and replace these names: o Replace cache_dns_name with the actual file cache\u2019s Domain Name System (DNS) name o Replace mountname with the cache\u2019s mount name, which you can get by running the describe-file-caches AWS CLI command or DescribeFileCaches API operation sudo mount -t lustre -o relatime,flock cache_dns_name@tcp:/mountname /mnt Note: Make sure your EC2 instance is in the same VPC as your cache. If done correctly, the path of your folder will show up in the /mnt folder. You can also use the df command to see the DNS and mount point is attached to your file system:",
      "remediation_steps": null,
      "rationale": "By connecting the FSx file system to your compute instances, you enable low-latency access to frequently used data, significantly improving application performance. This setup leverages the full capabilities of the AWS FSx service, ensuring efficient data processing and resource utilization in your AWS environment. Properly mounting the FSx cache is essential for achieving high performance and operational efficiency.",
      "impact": ""
    }
  },
  {
    "id": "4.8",
    "title": "Ensure exporting cache to S3",
    "assessment": "Manual",
    "description": "The S3 bucket we created earlier will store the files generated at this mount point.",
    "rationale": "The rationale behind using the S3 bucket to store files generated at the mount point is to ensure scalable, durable, and cost-effective storage for your data. By exporting files to S3, you benefit from its high availability and robust data management features, which enhances data security and accessibility. This approach also optimizes storage resource utilization and simplifies data backup and retrieval processes.",
    "audit": "We can export the files that were created to the S3 bucket using the following steps: 1. Create a file on the FSx mount point: 2. Run the command: sudo touch efx.txt  3. Now run the command: sudo lsm hsm_archive efx.txt  4. Now check your S3 bucket that was created earlier.",
    "remediation": "References: 1. https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups- exporting.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups- exporting.html",
    "function_names": [
      "s3_bucket_unique_check",
      "s3_bucket_default_encryption_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check",
        "s3_bucket_default_encryption_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "The S3 bucket we created earlier will store the files generated at this mount point.",
      "audit_steps": "We can export the files that were created to the S3 bucket using the following steps: 1. Create a file on the FSx mount point: 2. Run the command: sudo touch efx.txt  3. Now run the command: sudo lsm hsm_archive efx.txt  4. Now check your S3 bucket that was created earlier.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/backups- exporting.html",
      "rationale": "The rationale behind using the S3 bucket to store files generated at the mount point is to ensure scalable, durable, and cost-effective storage for your data. By exporting files to S3, you benefit from its high availability and robust data management features, which enhances data security and accessibility. This approach also optimizes storage resource utilization and simplifies data backup and retrieval processes.",
      "impact": ""
    }
  },
  {
    "id": "4.9",
    "title": "Ensure cleaning up FSx Resources",
    "assessment": "Manual",
    "description": "Cleaning up FSx resources involves removing unused or unnecessary FSx file systems and associated components to optimize costs and maintain a secure cloud environment. This includes deleting redundant file systems, snapshots, and mount targets, while ensuring all data is backed up or migrated. Regular cleanup prevents resource sprawl, reduces expenses, and maintains the overall health and performance of your AWS infrastructure.",
    "rationale": "The rationale for cleaning up FSx resources is to optimize costs and ensure a secure and efficient cloud environment. By removing unused or unnecessary file systems, snapshots, and mount targets, you prevent resource sprawl and reduce unnecessary expenses. Regular cleanup also helps maintain the overall health and performance of your AWS infrastructure, ensuring it remains organized and secure.",
    "audit": "To clean the FSx resources - 1. Terminate the EC2 instance. 2. Delete Fsx cache - On the actions drop down, select delete cache. 3. Verify that you want to delete the service. 4. Select Delete. It will take some time to delete the cache. 5. Delete the S3 Bucket Before you can delete the bucket you must first empty the bucket. Check the radio box and select Empty Select the bucket that you want to delete and select Delete n the S3 console.",
    "remediation": "References: 1. https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/getting-started-step3.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/getting-started-step3.html",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Cleaning up FSx resources involves removing unused or unnecessary FSx file systems and associated components to optimize costs and maintain a secure cloud environment. This includes deleting redundant file systems, snapshots, and mount targets, while ensuring all data is backed up or migrated. Regular cleanup prevents resource sprawl, reduces expenses, and maintains the overall health and performance of your AWS infrastructure.",
      "audit_steps": "To clean the FSx resources - 1. Terminate the EC2 instance. 2. Delete Fsx cache - On the actions drop down, select delete cache. 3. Verify that you want to delete the service. 4. Select Delete. It will take some time to delete the cache. 5. Delete the S3 Bucket Before you can delete the bucket you must first empty the bucket. Check the radio box and select Empty Select the bucket that you want to delete and select Delete n the S3 console.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/fsx/latest/ONTAPGuide/getting-started-step3.html",
      "rationale": "The rationale for cleaning up FSx resources is to optimize costs and ensure a secure and efficient cloud environment. By removing unused or unnecessary file systems, snapshots, and mount targets, you prevent resource sprawl and reduce unnecessary expenses. Regular cleanup also helps maintain the overall health and performance of your AWS infrastructure, ensuring it remains organized and secure.",
      "impact": ""
    }
  },
  {
    "id": "5.1",
    "title": "Amazon Simple Storage Service",
    "assessment": "Manual",
    "description": "Amazon Simple Storage Service (Amazon S3) is an object storage service that provides industry-leading scalability, data availability, security, and performance. It allows customers of all sizes and industries to store and protect any amount of data for virtually any use case, including data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and intuitive management features, you can optimize costs, organize data, and configure precise access controls to meet your specific business, organizational, and compliance requirements.",
    "rationale": "By utilizing S3, businesses of all sizes can efficiently store and protect large amounts of data, ensuring it is accessible when needed. The service's cost-effective storage classes and user-friendly management features help optimize costs and streamline data organization. Additionally, S3's fine-tuned access controls allow organizations to meet specific business, organizational, and compliance requirements, enhancing overall data management and security.",
    "audit": "How Amazon S3 works: 1. To store your data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object has a key (or key name), which is the unique identifier for the object within the bucket. 2. S3 provides features that you can configure to support your specific use case. For example, you can use S3 Versioning to keep multiple versions of an object in the same bucket, which allows you to restore objects that are accidentally deleted or overwritten. Buckets and the objects in them are private and can be accessed only if you explicitly grant access permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies, access control lists (ACLs), and S3 Access Points to manage access.",
    "remediation": "References: 1. https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
    "function_names": [
      "s3_bucket_unique_check",
      "s3_bucket_default_encryption_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check",
        "s3_bucket_default_encryption_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon Simple Storage Service (Amazon S3) is an object storage service that provides industry-leading scalability, data availability, security, and performance. It allows customers of all sizes and industries to store and protect any amount of data for virtually any use case, including data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and intuitive management features, you can optimize costs, organize data, and configure precise access controls to meet your specific business, organizational, and compliance requirements.",
      "audit_steps": "How Amazon S3 works: 1. To store your data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object has a key (or key name), which is the unique identifier for the object within the bucket. 2. S3 provides features that you can configure to support your specific use case. For example, you can use S3 Versioning to keep multiple versions of an object in the same bucket, which allows you to restore objects that are accidentally deleted or overwritten. Buckets and the objects in them are private and can be accessed only if you explicitly grant access permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies, access control lists (ACLs), and S3 Access Points to manage access.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
      "rationale": "By utilizing S3, businesses of all sizes can efficiently store and protect large amounts of data, ensuring it is accessible when needed. The service's cost-effective storage classes and user-friendly management features help optimize costs and streamline data organization. Additionally, S3's fine-tuned access controls allow organizations to meet specific business, organizational, and compliance requirements, enhancing overall data management and security.",
      "impact": ""
    }
  },
  {
    "id": "5.2",
    "title": "Ensure direct data addition to S3",
    "assessment": "Manual",
    "description": "Your bucket name must be unique and not already in use on AWS. Click on your bucket name, and in the right corner, you will find an option to upload data directly to your S3 bucket. You can choose the file option to upload individual files, images, or even entire folders.",
    "rationale": "Accessing the upload option within your bucket simplifies the process of adding data, making it easy to manage and organize your files. This streamlined approach allows for efficient data storage, retrieval, and management within the AWS S3 environment, enhancing overall operational efficiency.",
    "audit": "Access Point in S3 Bucket: Access points are named network endpoints that are attached to buckets which simplify managing data access at scale in S3. To see if any of the access points attached to this bucket grant public or cross-account access, go to IAM Access Analyzer for S3. 1. Enter a name for the access point. The name must be unique within the AWS account and Region. 2. Choose the VPC (Virtual Private Cloud) and subnet where you want the access point to be accessible. This determines the network traffic routing for the access point. 3. Optionally, you can configure additional settings such as permissions, bucket policy, and endpoint policy for the access point. 4. Review the settings, and click on \"Create access point\" to create the access point",
    "remediation": "References: 1. https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data-upload- files.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data-upload- files.html",
    "function_names": [
      "s3_bucket_unique_check",
      "s3_bucket_default_encryption_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check",
        "s3_bucket_default_encryption_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Your bucket name must be unique and not already in use on AWS. Click on your bucket name, and in the right corner, you will find an option to upload data directly to your S3 bucket. You can choose the file option to upload individual files, images, or even entire folders.",
      "audit_steps": "Access Point in S3 Bucket: Access points are named network endpoints that are attached to buckets which simplify managing data access at scale in S3. To see if any of the access points attached to this bucket grant public or cross-account access, go to IAM Access Analyzer for S3. 1. Enter a name for the access point. The name must be unique within the AWS account and Region. 2. Choose the VPC (Virtual Private Cloud) and subnet where you want the access point to be accessible. This determines the network traffic routing for the access point. 3. Optionally, you can configure additional settings such as permissions, bucket policy, and endpoint policy for the access point. 4. Review the settings, and click on \"Create access point\" to create the access point",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data-upload- files.html",
      "rationale": "Accessing the upload option within your bucket simplifies the process of adding data, making it easy to manage and organize your files. This streamlined approach allows for efficient data storage, retrieval, and management within the AWS S3 environment, enhancing overall operational efficiency.",
      "impact": ""
    }
  },
  {
    "id": "5.3",
    "title": "Ensure Storage Classes are Configured",
    "assessment": "Manual",
    "description": "Amazon S3 offers various storage classes to optimize cost and performance based on data access patterns and retention needs. Standard Storage is for frequently accessed data, while Standard-IA and One Zone-IA are for infrequent access, with the latter offering cost savings by storing in a single Availability Zone. Intelligent-Tiering automatically moves data between access tiers based on usage, and Glacier and Glacier Deep Archive provide low-cost options for long-term archival storage with varying retrieval times. Each class balances availability, durability, performance, and cost, enabling a tailored storage strategy to meet specific requirements.",
    "rationale": "This approach ensures frequently accessed data is readily available, while infrequently accessed data is stored cost-effectively, balancing availability, durability, and cost.",
    "audit": "",
    "remediation": "References: 1. https://aws.amazon.com/s3/storage-classes/",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://aws.amazon.com/s3/storage-classes/",
    "function_names": [
      "s3_bucket_unique_check",
      "s3_bucket_default_encryption_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check",
        "s3_bucket_default_encryption_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Amazon S3 offers various storage classes to optimize cost and performance based on data access patterns and retention needs. Standard Storage is for frequently accessed data, while Standard-IA and One Zone-IA are for infrequent access, with the latter offering cost savings by storing in a single Availability Zone. Intelligent-Tiering automatically moves data between access tiers based on usage, and Glacier and Glacier Deep Archive provide low-cost options for long-term archival storage with varying retrieval times. Each class balances availability, durability, performance, and cost, enabling a tailored storage strategy to meet specific requirements.",
      "audit_steps": "",
      "remediation_steps": "References: 1. https://aws.amazon.com/s3/storage-classes/",
      "rationale": "This approach ensures frequently accessed data is readily available, while infrequently accessed data is stored cost-effectively, balancing availability, durability, and cost.",
      "impact": ""
    }
  },
  {
    "id": "6.1",
    "title": "Ensure Elastic Disaster Recovery is Configured",
    "assessment": "Manual",
    "description": "AWS Elastic Disaster Recovery is a service that enables you to create and maintain backups of your workloads on AWS, particularly your servers. This service is crucial for ensuring high resilience for your AWS workloads. It operates by establishing and maintaining backups in selected AWS regions, guaranteeing that your data is safe, durable, and highly available in the event of issues in the primary availability zone or region where your AWS server is located.",
    "rationale": "AWS Elastic Disaster Recovery is crucial for establishing high resiliency in the cloud, synonymous with effective disaster recovery. High resiliency measures your organization's ability to respond to and recover from disasters impacting IT infrastructure. Achieving high resiliency minimizes downtime and long-term costs associated with outages, while low resiliency can result in prolonged downtime, potential data loss, and even permanent infrastructure damage.",
    "audit": "1. Review Disaster Recovery Plans : o Log in to the AWS Management Console. o Navigate to the AWS Elastic Disaster Recovery service. o Locate and open the disaster recovery plans. o Verify that the plans are current and comprehensive, covering all critical workloads. o Ensure that the plans specify clear recovery time objectives (RTO) and recovery point objectives (RPO). 2. Check Backup Configurations : o In the AWS Elastic Disaster Recovery dashboard, review the list of protected servers and workloads. o Confirm that backups are enabled for all critical servers and workloads. o Verify the backup schedule and frequency to ensure they meet organizational requirements. o Check that backups are being stored in the correct AWS regions as specified in the disaster recovery plan. 3. Test Recovery Procedures : o Identify a non-production environment to conduct recovery drills. o Initiate a simulated disaster scenario to test the recovery procedures. o Execute the recovery process for each critical workload. o Measure and document the time taken to recover each workload. o Compare the measured recovery times against the RTO and RPO. o Identify and document any issues or delays encountered during the recovery process. 4. Monitor Backup Integrity : o Open the AWS CloudWatch console. o Set up CloudWatch Alarms to monitor the status of backups. o Configure alerts for any failed or incomplete backups. o Regularly review the CloudWatch logs to verify that backups are successfully completed and stored. 5. Evaluate Backup Storage and Security : o Access the AWS S3 or Glacier console, depending on where backups are stored. o Verify that all backup data is encrypted in transit and at rest. o Check the storage settings to confirm that data is being stored in secure, durable storage solutions. o Review the access control policies to ensure that only authorized personnel have access to backup data. 6. Ensure Compliance with Policies and Regulations : o Review organizational and regulatory compliance requirements relevant to disaster recovery. o Ensure that the disaster recovery practices and configurations comply with these requirements. o Document the compliance efforts, including any specific steps taken to meet industry standards and regulations. o Prepare reports or evidence of compliance for any upcoming audits or assessments.",
    "remediation": "1. Update Disaster Recovery Plans : o Action : Log in to the AWS Management Console. o Procedure : \u25aa Navigate to the AWS Elastic Disaster Recovery service. \u25aa Locate and review the current disaster recovery plans. \u25aa Update the plans to ensure they are comprehensive and cover all critical workloads. \u25aa Ensure that the plans specify clear recovery time objectives (RTO) and recovery point objectives (RPO). \u25aa Save and document the updated plans. 2. Correct Backup Configurations : o Action : Verify and adjust backup settings. o Procedure : \u25aa In the AWS Elastic Disaster Recovery dashboard, review the list of protected servers and workloads. \u25aa Enable backups for any critical servers and workloads that are not currently being backed up. \u25aa Adjust the backup schedule and frequency to meet organizational requirements. \u25aa Ensure backups are stored in the correct AWS regions as specified in the disaster recovery plan. 3. Conduct Recovery Procedure Drills : o Action : Test and refine recovery procedures. o Procedure : \u25aa Identify a non-production environment to conduct recovery drills. \u25aa Simulate a disaster scenario to test the recovery procedures. \u25aa Execute the recovery process for each critical workload. \u25aa Measure and document the time taken to recover each workload. \u25aa Compare the measured recovery times against the RTO and RPO. \u25aa Identify and address any issues or delays encountered during the recovery process. \u25aa Update the recovery procedures based on the findings from the drill. 4. Ensure Backup Integrity : o Action : Monitor and verify the integrity of backups. o Procedure : \u25aa Open the AWS CloudWatch console. \u25aa Set up CloudWatch Alarms to monitor the status of backups. \u25aa Configure alerts for any failed or incomplete backups. \u25aa Regularly review CloudWatch logs to verify that backups are successfully completed and stored. \u25aa Resolve any issues identified in the logs, such as incomplete or failed backups. 5. Enhance Backup Storage and Security : o Action : Improve the storage and security of backup data. o Procedure : \u25aa Access the AWS S3 or Glacier console, depending on where backups are stored. \u25aa Ensure all backup data is encrypted in transit and at rest. \u25aa Adjust storage settings to confirm that data is being stored in secure, durable storage solutions. \u25aa Review and update access control policies to ensure only authorized personnel can access backup data. \u25aa Implement any additional security measures necessary to protect the backup data. 6. Ensure Compliance with Policies and Regulations : o Action : Align disaster recovery practices with compliance requirements. o Procedure : \u25aa Review organizational and regulatory compliance requirements relevant to disaster recovery. \u25aa Adjust disaster recovery practices and configurations to ensure compliance with these requirements. \u25aa Document the compliance efforts, including specific steps taken to meet industry standards and regulations. \u25aa Prepare and maintain reports or evidence of compliance for any upcoming audits or assessments.",
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "backup_plan_specification_check",
      "backup_resources_existence_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_plan_specification_check",
        "backup_resources_existence_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "AWS Elastic Disaster Recovery is a service that enables you to create and maintain backups of your workloads on AWS, particularly your servers. This service is crucial for ensuring high resilience for your AWS workloads. It operates by establishing and maintaining backups in selected AWS regions, guaranteeing that your data is safe, durable, and highly available in the event of issues in the primary availability zone or region where your AWS server is located.",
      "audit_steps": "1. Review Disaster Recovery Plans : o Log in to the AWS Management Console. o Navigate to the AWS Elastic Disaster Recovery service. o Locate and open the disaster recovery plans. o Verify that the plans are current and comprehensive, covering all critical workloads. o Ensure that the plans specify clear recovery time objectives (RTO) and recovery point objectives (RPO). 2. Check Backup Configurations : o In the AWS Elastic Disaster Recovery dashboard, review the list of protected servers and workloads. o Confirm that backups are enabled for all critical servers and workloads. o Verify the backup schedule and frequency to ensure they meet organizational requirements. o Check that backups are being stored in the correct AWS regions as specified in the disaster recovery plan. 3. Test Recovery Procedures : o Identify a non-production environment to conduct recovery drills. o Initiate a simulated disaster scenario to test the recovery procedures. o Execute the recovery process for each critical workload. o Measure and document the time taken to recover each workload. o Compare the measured recovery times against the RTO and RPO. o Identify and document any issues or delays encountered during the recovery process. 4. Monitor Backup Integrity : o Open the AWS CloudWatch console. o Set up CloudWatch Alarms to monitor the status of backups. o Configure alerts for any failed or incomplete backups. o Regularly review the CloudWatch logs to verify that backups are successfully completed and stored. 5. Evaluate Backup Storage and Security : o Access the AWS S3 or Glacier console, depending on where backups are stored. o Verify that all backup data is encrypted in transit and at rest. o Check the storage settings to confirm that data is being stored in secure, durable storage solutions. o Review the access control policies to ensure that only authorized personnel have access to backup data. 6. Ensure Compliance with Policies and Regulations : o Review organizational and regulatory compliance requirements relevant to disaster recovery. o Ensure that the disaster recovery practices and configurations comply with these requirements. o Document the compliance efforts, including any specific steps taken to meet industry standards and regulations. o Prepare reports or evidence of compliance for any upcoming audits or assessments.",
      "remediation_steps": "1. Update Disaster Recovery Plans : o Action : Log in to the AWS Management Console. o Procedure : \u25aa Navigate to the AWS Elastic Disaster Recovery service. \u25aa Locate and review the current disaster recovery plans. \u25aa Update the plans to ensure they are comprehensive and cover all critical workloads. \u25aa Ensure that the plans specify clear recovery time objectives (RTO) and recovery point objectives (RPO). \u25aa Save and document the updated plans. 2. Correct Backup Configurations : o Action : Verify and adjust backup settings. o Procedure : \u25aa In the AWS Elastic Disaster Recovery dashboard, review the list of protected servers and workloads. \u25aa Enable backups for any critical servers and workloads that are not currently being backed up. \u25aa Adjust the backup schedule and frequency to meet organizational requirements. \u25aa Ensure backups are stored in the correct AWS regions as specified in the disaster recovery plan. 3. Conduct Recovery Procedure Drills : o Action : Test and refine recovery procedures. o Procedure : \u25aa Identify a non-production environment to conduct recovery drills. \u25aa Simulate a disaster scenario to test the recovery procedures. \u25aa Execute the recovery process for each critical workload. \u25aa Measure and document the time taken to recover each workload. \u25aa Compare the measured recovery times against the RTO and RPO. \u25aa Identify and address any issues or delays encountered during the recovery process. \u25aa Update the recovery procedures based on the findings from the drill. 4. Ensure Backup Integrity : o Action : Monitor and verify the integrity of backups. o Procedure : \u25aa Open the AWS CloudWatch console. \u25aa Set up CloudWatch Alarms to monitor the status of backups. \u25aa Configure alerts for any failed or incomplete backups. \u25aa Regularly review CloudWatch logs to verify that backups are successfully completed and stored. \u25aa Resolve any issues identified in the logs, such as incomplete or failed backups. 5. Enhance Backup Storage and Security : o Action : Improve the storage and security of backup data. o Procedure : \u25aa Access the AWS S3 or Glacier console, depending on where backups are stored. \u25aa Ensure all backup data is encrypted in transit and at rest. \u25aa Adjust storage settings to confirm that data is being stored in secure, durable storage solutions. \u25aa Review and update access control policies to ensure only authorized personnel can access backup data. \u25aa Implement any additional security measures necessary to protect the backup data. 6. Ensure Compliance with Policies and Regulations : o Action : Align disaster recovery practices with compliance requirements. o Procedure : \u25aa Review organizational and regulatory compliance requirements relevant to disaster recovery. \u25aa Adjust disaster recovery practices and configurations to ensure compliance with these requirements. \u25aa Document the compliance efforts, including specific steps taken to meet industry standards and regulations. \u25aa Prepare and maintain reports or evidence of compliance for any upcoming audits or assessments.",
      "rationale": "AWS Elastic Disaster Recovery is crucial for establishing high resiliency in the cloud, synonymous with effective disaster recovery. High resiliency measures your organization's ability to respond to and recover from disasters impacting IT infrastructure. Achieving high resiliency minimizes downtime and long-term costs associated with outages, while low resiliency can result in prolonged downtime, potential data loss, and even permanent infrastructure damage.",
      "impact": ""
    }
  },
  {
    "id": "6.2",
    "title": "Ensure AWS Disaster Recovery Configuration",
    "assessment": "Manual",
    "description": "It\u2019s important to understand how the network on EDR works. This isn\u2019t a simple service to configure, but it works with multiple work loads over the network. You can connect your on-premises or third-party cloud service to AWS EDR over the network. Below are the descriptions of the AWS network architecture: 1. Your local network inside the data center or cloud a.Connect an AWS Replication Agent to each of your resources. 2. AWS Cloud Architecture a. Choose the AWS Region that you want to house your disaster recovery instances. b.Create AWS API Endpoints for EC2, Disaster Recovery, and S3. c.Upon creation of Disaster Recovery endpoints, two subnets will be created in your VPC i.Staging Area Subnets: Replication servers with EBS volumes attached to each disk on the replication servers. ii.Recovery Subnets: Recovery EC2 instances attached to EBS volumes/ d.Connect local network over TCP port 443 to EDR and S3 e.Connect local replication agent to AWS replication servers over TCP port 1500 f.Connectivity out of staging area: Connect staging area on AWS to EDR over TCP port 443 g.Allow connection to S3 over TCP 443 h.Allow connectivity to EC2 over TCP 443 to connect to API Endpoint",
    "rationale": "",
    "audit": "",
    "remediation": "References: 1. https://aws.amazon.com/disaster-recovery/",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://aws.amazon.com/disaster-recovery/",
    "function_names": [
      "s3_bucket_unique_check",
      "s3_bucket_default_encryption_check"
    ],
    "implementation_guidance": {
      "boto3_client": "s3",
      "functions": [
        "s3_bucket_unique_check",
        "s3_bucket_default_encryption_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use s3 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "It\u2019s important to understand how the network on EDR works. This isn\u2019t a simple service to configure, but it works with multiple work loads over the network. You can connect your on-premises or third-party cloud service to AWS EDR over the network. Below are the descriptions of the AWS network architecture: 1. Your local network inside the data center or cloud a.Connect an AWS Replication Agent to each of your resources. 2. AWS Cloud Architecture a. Choose the AWS Region that you want to house your disaster recovery instances. b.Create AWS API Endpoints for EC2, Disaster Recovery, and S3. c.Upon creation of Disaster Recovery endpoints, two subnets will be created in your VPC i.Staging Area Subnets: Replication servers with EBS volumes attached to each disk on the replication servers. ii.Recovery Subnets: Recovery EC2 instances attached to EBS volumes/ d.Connect local network over TCP port 443 to EDR and S3 e.Connect local replication agent to AWS replication servers over TCP port 1500 f.Connectivity out of staging area: Connect staging area on AWS to EDR over TCP port 443 g.Allow connection to S3 over TCP 443 h.Allow connectivity to EC2 over TCP 443 to connect to API Endpoint",
      "audit_steps": "",
      "remediation_steps": "References: 1. https://aws.amazon.com/disaster-recovery/",
      "rationale": "",
      "impact": ""
    }
  },
  {
    "id": "6.3",
    "title": "Ensure functionality of Endpoint Detection and Response (EDR)",
    "assessment": "Manual",
    "description": "Establish and maintain an effective Endpoint Detection and Response (EDR) system to proactively monitor, detect, and respond to security threats on endpoints such as computers, mobile devices, and servers. This involves deploying EDR software that continuously collects data from endpoints, analyzes this data for signs of malicious activity, and provides real-time alerts and detailed incident reports. Regularly test and update the EDR system to ensure it can accurately identify and mitigate advanced threats, including zero-day exploits and sophisticated malware, ensuring comprehensive protection and swift response to potential security incidents.",
    "rationale": "Ensuring the functionality of Endpoint Detection and Response (EDR) systems is essential for early detection and swift response to security threats on endpoints. These systems continuously monitor and analyze endpoint data, providing real-time alerts and detailed incident reports to identify and mitigate potential threats. Regular testing and updates of the EDR system ensure it remains effective against advanced threats, maintaining comprehensive protection for the organization's assets.",
    "audit": "1. Preparing the Environment for EDR - Before getting started with EDR, you must prepare the environment that you want to back up. 2. Preparing the Source Server - Allow direct access to Elastic Disaster Recovery and Amazon S3 AWS service API endpoints through HTTPS protocol (TCP port 443). Direct outbound TCP port 1500 from the source server to the staging area subnet, which contains the replication servers. 3. Preparing the Staging Area Subnet - Allow Direct access to EDR, S3, and EC2 through HTTPS protocol (TCP port 443) Direct inbound TCP port 1500 for replication traffic 4. Accessing the AWS Elastic Disaster Recovery Console - o Search for \u201cAWS Elastic Disaster Recovery\u201d in the AWS Console. o Select \u201cElastic Disaster Recovery\u201d 5. Configuring the Replication Settings Template - Select Configure and Initializein in the AWS Elastic Disaster Recovery screen. You will be navigated to setup your replication settings template. This will create a staging area in a subnet of your choice and a replication server instance types. The default replication server instance type will be a t3 micro EC2 instance. This is good for normal workloads with small I/O operations. 6. Next, configure EBS encryption and volume types. This will depend on your workload requirements. 7. To encrypt EBS volumes, leave the setting as \u201cdefault.\u201d If you wish to make a custom encryption setting, you will need to create an AWS KMS key. 8. Configure the security group to your specific needs. Remember what ports need to be opened on inbound / outbound traffic that was specified in previous steps: You can choose how you want your data routed and if you want to throttle network traffic to reserve bandwidth. To keep your data as secure as possible, it\u2019s recommended to get set up with a VPN or AWS direct connect, so your backups are not traveling over the public internet. Point in time policy defines the snapshot retention time. Because Elastic Disaster Recovery service uses incremental backups, it\u2019s not necessary to keep old copies of backups. Now, you\u2019re ready to launch this template.",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Establish and maintain an effective Endpoint Detection and Response (EDR) system to proactively monitor, detect, and respond to security threats on endpoints such as computers, mobile devices, and servers. This involves deploying EDR software that continuously collects data from endpoints, analyzes this data for signs of malicious activity, and provides real-time alerts and detailed incident reports. Regularly test and update the EDR system to ensure it can accurately identify and mitigate advanced threats, including zero-day exploits and sophisticated malware, ensuring comprehensive protection and swift response to potential security incidents.",
      "audit_steps": "1. Preparing the Environment for EDR - Before getting started with EDR, you must prepare the environment that you want to back up. 2. Preparing the Source Server - Allow direct access to Elastic Disaster Recovery and Amazon S3 AWS service API endpoints through HTTPS protocol (TCP port 443). Direct outbound TCP port 1500 from the source server to the staging area subnet, which contains the replication servers. 3. Preparing the Staging Area Subnet - Allow Direct access to EDR, S3, and EC2 through HTTPS protocol (TCP port 443) Direct inbound TCP port 1500 for replication traffic 4. Accessing the AWS Elastic Disaster Recovery Console - o Search for \u201cAWS Elastic Disaster Recovery\u201d in the AWS Console. o Select \u201cElastic Disaster Recovery\u201d 5. Configuring the Replication Settings Template - Select Configure and Initializein in the AWS Elastic Disaster Recovery screen. You will be navigated to setup your replication settings template. This will create a staging area in a subnet of your choice and a replication server instance types. The default replication server instance type will be a t3 micro EC2 instance. This is good for normal workloads with small I/O operations. 6. Next, configure EBS encryption and volume types. This will depend on your workload requirements. 7. To encrypt EBS volumes, leave the setting as \u201cdefault.\u201d If you wish to make a custom encryption setting, you will need to create an AWS KMS key. 8. Configure the security group to your specific needs. Remember what ports need to be opened on inbound / outbound traffic that was specified in previous steps: You can choose how you want your data routed and if you want to throttle network traffic to reserve bandwidth. To keep your data as secure as possible, it\u2019s recommended to get set up with a VPN or AWS direct connect, so your backups are not traveling over the public internet. Point in time policy defines the snapshot retention time. Because Elastic Disaster Recovery service uses incremental backups, it\u2019s not necessary to keep old copies of backups. Now, you\u2019re ready to launch this template.",
      "remediation_steps": null,
      "rationale": "Ensuring the functionality of Endpoint Detection and Response (EDR) systems is essential for early detection and swift response to security threats on endpoints. These systems continuously monitor and analyze endpoint data, providing real-time alerts and detailed incident reports to identify and mitigate potential threats. Regular testing and updates of the EDR system ensure it remains effective against advanced threats, maintaining comprehensive protection for the organization's assets.",
      "impact": ""
    }
  },
  {
    "id": "6.4",
    "title": "Ensure configuration of replication settings",
    "assessment": "Manual",
    "description": "Set up and maintain the replication settings to ensure accurate and efficient data duplication across systems. Proper configuration includes specifying source and target locations, defining replication schedules, and setting bandwidth limits to optimize performance. Regularly review and update these settings to accommodate changes in data volume and network conditions, ensuring data integrity and availability during replication processes.",
    "rationale": "Proper configuration of replication settings is essential to ensure data consistency and availability across systems. Accurate replication schedules and bandwidth management optimize performance and prevent network congestion. Regular reviews and updates of these settings help adapt to changes in data volume and network conditions, maintaining efficient and reliable data replication processes.",
    "audit": "1. Select \u201cConfigure and Initialize\u201d in in the AWS Elastic Disaster Recovery screen. You will be navigated to setup your replication settings template. This will create a staging area in a subnet of your choice and a replication server instance types. The default replication server instance type will be a t3 micro EC2 instance. This is good for normal workloads with small I/O operations. 2. Next, configure EBS encryption and volume types. This will depend on your workload requirements. To encrypt EBS volumes, leave the setting as \u201cdefault.\u201d If you wish to make a custom encryption setting, you will need to create an AWS KMS key. 3. Configure the security group to your specific needs. Remember what ports need to be opened on inbound / outbound traffic that was specified in previous steps: o Configure Additional Replication settings. o You can choose how you want your data routed and if you want to throttle network traffic to reserve bandwidth. To keep your data as secure as possible, it\u2019s recommended to get set up with a VPN or AWS direct connect, so your backups are not traveling over the public internet. o Point in time policy defines the snapshot retention time. Because Elastic Disaster Recovery service uses incremental backups, it\u2019s not necessary to keep old copies of backups. Now, you\u2019re ready to launch this template.",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "ec2_ebs_volume_creation_check",
      "ec2_ebs_volume_encryption_kms_key_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_ebs_volume_creation_check",
        "ec2_ebs_volume_encryption_kms_key_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Set up and maintain the replication settings to ensure accurate and efficient data duplication across systems. Proper configuration includes specifying source and target locations, defining replication schedules, and setting bandwidth limits to optimize performance. Regularly review and update these settings to accommodate changes in data volume and network conditions, ensuring data integrity and availability during replication processes.",
      "audit_steps": "1. Select \u201cConfigure and Initialize\u201d in in the AWS Elastic Disaster Recovery screen. You will be navigated to setup your replication settings template. This will create a staging area in a subnet of your choice and a replication server instance types. The default replication server instance type will be a t3 micro EC2 instance. This is good for normal workloads with small I/O operations. 2. Next, configure EBS encryption and volume types. This will depend on your workload requirements. To encrypt EBS volumes, leave the setting as \u201cdefault.\u201d If you wish to make a custom encryption setting, you will need to create an AWS KMS key. 3. Configure the security group to your specific needs. Remember what ports need to be opened on inbound / outbound traffic that was specified in previous steps: o Configure Additional Replication settings. o You can choose how you want your data routed and if you want to throttle network traffic to reserve bandwidth. To keep your data as secure as possible, it\u2019s recommended to get set up with a VPN or AWS direct connect, so your backups are not traveling over the public internet. o Point in time policy defines the snapshot retention time. Because Elastic Disaster Recovery service uses incremental backups, it\u2019s not necessary to keep old copies of backups. Now, you\u2019re ready to launch this template.",
      "remediation_steps": null,
      "rationale": "Proper configuration of replication settings is essential to ensure data consistency and availability across systems. Accurate replication schedules and bandwidth management optimize performance and prevent network congestion. Regular reviews and updates of these settings help adapt to changes in data volume and network conditions, maintaining efficient and reliable data replication processes.",
      "impact": ""
    }
  },
  {
    "id": "6.5",
    "title": "Ensure proper IAM configuration for AWS Elastic Disaster Recovery",
    "assessment": "Manual",
    "description": "Set up and maintain Identity and Access Management (IAM) roles and policies specifically for AWS Elastic Disaster Recovery. This includes defining least-privilege access for users and services, creating roles for automated processes, and enforcing multi-factor authentication (MFA) for added security. Regularly review and update IAM policies to adapt to changes in the organization and to maintain compliance with security best practices, ensuring that only authorized personnel and services can access and manage disaster recovery resources.",
    "rationale": "Proper IAM configuration for AWS Elastic Disaster Recovery ensures that only authorized users and services have access to critical recovery functions, reducing the risk of unauthorized access and potential security breaches. Implementing least- privilege access and MFA enhances security by limiting permissions and adding an extra layer of authentication. Regular reviews and updates of IAM policies help maintain security compliance and adapt to organizational changes, ensuring continuous protection of disaster recovery resources.",
    "audit": "To create DRS Agent User, follow following steps: 1. Navigate to the AWS IAM Console - https://us-east- 1.console.aws.amazon.com/iam/home?region=us-east-1#/home. 2. Create new user. This user will only be able to access the Elastic disaster recovery agent installation resource. Accordingly, name the user \u201cDSRuser\u201d. 3. Allow Programmatic access: This allows the user to access resources programmatically with a secure key rather than having to enter a password. 4. elect \u201cattach policies directly\u201d and search for \u201cAWSElasticDisasterRecoveryAgentInstallationPolicy\u201d. 5. Create user. To create Failback Agent User, Follow the steps above with these two modifications: 1. Name the user \u201cFailbackAgentuser\u201d. 2. Apply the \u201cAWSElasticDisasterRecoveryFailbackInstallationPolicy\u201d.",
    "remediation": "Default Value: Configure IAM Credentials for AWS Elastic Disaster Recovery. References: 1. https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Set up and maintain Identity and Access Management (IAM) roles and policies specifically for AWS Elastic Disaster Recovery. This includes defining least-privilege access for users and services, creating roles for automated processes, and enforcing multi-factor authentication (MFA) for added security. Regularly review and update IAM policies to adapt to changes in the organization and to maintain compliance with security best practices, ensuring that only authorized personnel and services can access and manage disaster recovery resources.",
      "audit_steps": "To create DRS Agent User, follow following steps: 1. Navigate to the AWS IAM Console - https://us-east- 1.console.aws.amazon.com/iam/home?region=us-east-1#/home. 2. Create new user. This user will only be able to access the Elastic disaster recovery agent installation resource. Accordingly, name the user \u201cDSRuser\u201d. 3. Allow Programmatic access: This allows the user to access resources programmatically with a secure key rather than having to enter a password. 4. elect \u201cattach policies directly\u201d and search for \u201cAWSElasticDisasterRecoveryAgentInstallationPolicy\u201d. 5. Create user. To create Failback Agent User, Follow the steps above with these two modifications: 1. Name the user \u201cFailbackAgentuser\u201d. 2. Apply the \u201cAWSElasticDisasterRecoveryFailbackInstallationPolicy\u201d.",
      "remediation_steps": "Default Value: Configure IAM Credentials for AWS Elastic Disaster Recovery. References: 1. https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home",
      "rationale": "Proper IAM configuration for AWS Elastic Disaster Recovery ensures that only authorized users and services have access to critical recovery functions, reducing the risk of unauthorized access and potential security breaches. Implementing least- privilege access and MFA enhances security by limiting permissions and adding an extra layer of authentication. Regular reviews and updates of IAM policies help maintain security compliance and adapt to organizational changes, ensuring continuous protection of disaster recovery resources.",
      "impact": ""
    }
  },
  {
    "id": "6.6",
    "title": "Ensure installation of the AWS Replication Agent",
    "assessment": "Manual",
    "description": "Set up and verify the installation of the AWS Replication Agent on all relevant systems to facilitate efficient and reliable data replication. This process includes downloading the agent, configuring it according to best practices, and ensuring it is correctly integrated with your AWS environment. Regularly check the agent\u2019s performance and update it as needed to maintain optimal functionality and data integrity during replication processes.",
    "rationale": "Installing the AWS Replication Agent is crucial for enabling efficient and reliable data replication, ensuring that critical data is accurately duplicated across systems. Proper configuration and integration with your AWS environment optimize the agent\u2019s performance, enhancing data availability and disaster recovery capabilities. Regular checks and updates of the replication agent help maintain its effectiveness, ensuring data integrity and minimizing the risk of replication failures.",
    "audit": "1. On the source servers page, from Actions, choose add servers to obtain the agent installer link. 2. On your source server (in our case, the EC2 instance that was already created) download the appropriate agent installer for your operating system. o For Linux instance on US-East-1. Substitute your region in the {Region} brackets of this command: wget -O ./aws-replication-installer-init https://aws-elastic-disaster- recovery-us-east-1.s3.us-east-1.amazonaws.com/latest/linux/aws-replication- installer-init  3. Run following command: chmod +x aws-replication-installer-init; sudo ./aws-replication-installer- init  4. Type in your region. Region is case sensitive: if you\u2019re in us-east-1, make sure you type \u201cus-east-1\u201d. 5. If you\u2019re using SSH, you will be prompted with your activation ID and secret activation key. Make sure you have those accessible for the IAM user you\u2019re using. You can generate a new key from the IAM dashboard if you forgot to save your key. 6. Select \u201cEnter\u201d to replicate all servers. 7. All servers should replicate. 8. Make sure your OS is up to date. If you run into an error replicating your devices, view the documentation on troubleshooting the AWS replication installation here: https://docs.aws.amazon.com/mgn/latest/ug/installation-requirements.html. 9. If install runs successfully, the source server will appear in your Elastic Disaster Recovery Console dashboard on the \u201csource servers\u201d page. This will signify the beginning of the replication process.",
    "remediation": "References: 1. https://docs.aws.amazon.com/mgn/latest/ug/agent-installation.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/mgn/latest/ug/agent-installation.html",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Set up and verify the installation of the AWS Replication Agent on all relevant systems to facilitate efficient and reliable data replication. This process includes downloading the agent, configuring it according to best practices, and ensuring it is correctly integrated with your AWS environment. Regularly check the agent\u2019s performance and update it as needed to maintain optimal functionality and data integrity during replication processes.",
      "audit_steps": "1. On the source servers page, from Actions, choose add servers to obtain the agent installer link. 2. On your source server (in our case, the EC2 instance that was already created) download the appropriate agent installer for your operating system. o For Linux instance on US-East-1. Substitute your region in the {Region} brackets of this command: wget -O ./aws-replication-installer-init https://aws-elastic-disaster- recovery-us-east-1.s3.us-east-1.amazonaws.com/latest/linux/aws-replication- installer-init  3. Run following command: chmod +x aws-replication-installer-init; sudo ./aws-replication-installer- init  4. Type in your region. Region is case sensitive: if you\u2019re in us-east-1, make sure you type \u201cus-east-1\u201d. 5. If you\u2019re using SSH, you will be prompted with your activation ID and secret activation key. Make sure you have those accessible for the IAM user you\u2019re using. You can generate a new key from the IAM dashboard if you forgot to save your key. 6. Select \u201cEnter\u201d to replicate all servers. 7. All servers should replicate. 8. Make sure your OS is up to date. If you run into an error replicating your devices, view the documentation on troubleshooting the AWS replication installation here: https://docs.aws.amazon.com/mgn/latest/ug/installation-requirements.html. 9. If install runs successfully, the source server will appear in your Elastic Disaster Recovery Console dashboard on the \u201csource servers\u201d page. This will signify the beginning of the replication process.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/mgn/latest/ug/agent-installation.html",
      "rationale": "Installing the AWS Replication Agent is crucial for enabling efficient and reliable data replication, ensuring that critical data is accurately duplicated across systems. Proper configuration and integration with your AWS environment optimize the agent\u2019s performance, enhancing data availability and disaster recovery capabilities. Regular checks and updates of the replication agent help maintain its effectiveness, ensuring data integrity and minimizing the risk of replication failures.",
      "impact": ""
    }
  },
  {
    "id": "6.7",
    "title": "Ensure proper configuration of the Launch Settings",
    "assessment": "Manual",
    "description": "Set up and verify the launch settings to ensure systems and applications start correctly and securely. This includes defining startup parameters, specifying required resources, and configuring security settings to prevent unauthorized changes. Regularly review and update these settings to align with best practices and organizational requirements, ensuring optimal performance and security at launch.",
    "rationale": "Proper configuration of launch settings is crucial for ensuring that systems and applications start securely and perform optimally. Defining startup parameters and resource requirements prevents potential issues and enhances efficiency. Regular reviews and updates to these settings help maintain alignment with best practices and evolving organizational needs, thereby strengthening security and operational reliability from the moment of launch.",
    "audit": "The settings can be changed after instances have been launched, but a new instance must be launched for new launch settings to take effect. 1. Select launch settings on the source server page 2. Configure launch settings o On the launch settings page, next to general launch, select \u201cedit\u201d 3. Configure EC2 launch template o Enable auto assign public IP and change the instance type to a t2.medium 4. Set version to default in the console 5. Set the default version that was just created to default version 6. Return to the dashboard and confirm your configurations are correct.",
    "remediation": null,
    "profile_applicability": "\u2022  Level 2",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Set up and verify the launch settings to ensure systems and applications start correctly and securely. This includes defining startup parameters, specifying required resources, and configuring security settings to prevent unauthorized changes. Regularly review and update these settings to align with best practices and organizational requirements, ensuring optimal performance and security at launch.",
      "audit_steps": "The settings can be changed after instances have been launched, but a new instance must be launched for new launch settings to take effect. 1. Select launch settings on the source server page 2. Configure launch settings o On the launch settings page, next to general launch, select \u201cedit\u201d 3. Configure EC2 launch template o Enable auto assign public IP and change the instance type to a t2.medium 4. Set version to default in the console 5. Set the default version that was just created to default version 6. Return to the dashboard and confirm your configurations are correct.",
      "remediation_steps": null,
      "rationale": "Proper configuration of launch settings is crucial for ensuring that systems and applications start securely and perform optimally. Defining startup parameters and resource requirements prevents potential issues and enhances efficiency. Regular reviews and updates to these settings help maintain alignment with best practices and evolving organizational needs, thereby strengthening security and operational reliability from the moment of launch.",
      "impact": ""
    }
  },
  {
    "id": "6.8",
    "title": "Ensure execution of a recovery drill",
    "assessment": "Manual",
    "description": "To ensure your organization is prepared for a disaster, it's crucial to verify that your disaster recovery services function as expected. Your IT team should conduct regular recovery drills on your AWS Elastic Recovery Instance to confirm everything operates smoothly and according to plan.",
    "rationale": "Regular recovery drills are essential to verify the functionality of your disaster recovery services and ensure your organization is well-prepared for any disruptions. By conducting these drills on your AWS Elastic Recovery Instance, you can identify and address potential issues before they impact operations. This proactive approach enhances the reliability and effectiveness of your disaster recovery plan, providing confidence that your systems can recover swiftly and efficiently in the event of a disaster.",
    "audit": "Steps to perform a recovery drill: 1. Navigate to source servers tab in AWS Elastic Disaster Recovery Dashboard. 2. Make sure that all servers you launch show as \u201cReady\u201d under \u201cstatus,\u201d report as \u201chealthy\u201d in the data replication status column, and that pending actions show as \u201cinitiate drill\u201d. 3. Select \u201cinitiate drill\u201d under the orange dropdown menu. Make sure that you don\u2019t initiate a real recovery job. 4. Choose a recovery point. Normally, it makes sense to choose the most recent recovery point, but you can also choose a recovery point from earlier. 5. Select the orange \u201cinitiate drill\u201d to initiate the recovery drill. 6. To complete the recovery drill, clean up your resources by deleting the recovery instance by selecting actions and \u201cterminate recovery instances\u201d.",
    "remediation": "References: 1. https://docs.aws.amazon.com/drs/latest/userguide/failback-preparing.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/drs/latest/userguide/failback-preparing.html",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "To ensure your organization is prepared for a disaster, it's crucial to verify that your disaster recovery services function as expected. Your IT team should conduct regular recovery drills on your AWS Elastic Recovery Instance to confirm everything operates smoothly and according to plan.",
      "audit_steps": "Steps to perform a recovery drill: 1. Navigate to source servers tab in AWS Elastic Disaster Recovery Dashboard. 2. Make sure that all servers you launch show as \u201cReady\u201d under \u201cstatus,\u201d report as \u201chealthy\u201d in the data replication status column, and that pending actions show as \u201cinitiate drill\u201d. 3. Select \u201cinitiate drill\u201d under the orange dropdown menu. Make sure that you don\u2019t initiate a real recovery job. 4. Choose a recovery point. Normally, it makes sense to choose the most recent recovery point, but you can also choose a recovery point from earlier. 5. Select the orange \u201cinitiate drill\u201d to initiate the recovery drill. 6. To complete the recovery drill, clean up your resources by deleting the recovery instance by selecting actions and \u201cterminate recovery instances\u201d.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/drs/latest/userguide/failback-preparing.html",
      "rationale": "Regular recovery drills are essential to verify the functionality of your disaster recovery services and ensure your organization is well-prepared for any disruptions. By conducting these drills on your AWS Elastic Recovery Instance, you can identify and address potential issues before they impact operations. This proactive approach enhances the reliability and effectiveness of your disaster recovery plan, providing confidence that your systems can recover swiftly and efficiently in the event of a disaster.",
      "impact": ""
    }
  },
  {
    "id": "6.9",
    "title": "Ensure Continuous Disaster Recovery Operations",
    "assessment": "Manual",
    "description": "Maintain ongoing disaster recovery operations to ensure that systems and data can be swiftly restored in the event of a disruption. This involves regularly updating and testing recovery plans, monitoring replication processes, and verifying the integrity and accessibility of backups. Continuously evaluate and improve disaster recovery strategies to adapt to evolving threats and organizational changes, ensuring resilience and minimal downtime during incidents.",
    "rationale": "Maintaining continuous disaster recovery operations is essential for ensuring that systems and data can be quickly and effectively restored following a disruption. Regular updates and tests of recovery plans, along with constant monitoring of replication processes, help verify the integrity and availability of backups. This proactive approach allows organizations to adapt to evolving threats and changes, ensuring resilience and minimizing downtime during incidents, which ultimately protects business continuity and reduces potential losses.",
    "audit": "1. Review Disaster Recovery Plan: o Verify that a comprehensive disaster recovery (DR) plan exists and is regularly updated. o Ensure the DR plan includes detailed procedures for data backup, system recovery, and failover processes. o Check for documentation of roles and responsibilities during a disaster event. 2. Check Backup and Replication Settings: o Confirm that AWS Backup is configured correctly for all critical systems and data. o Review the settings for Amazon RDS, EBS snapshots, S3 versioning, and other AWS services to ensure backups are automated and scheduled appropriately. o Ensure that replication settings are configured to replicate data across multiple AWS regions for added redundancy. 3. Test Recovery Procedures: o Verify that regular recovery drills are conducted to test the DR plan\u2019s effectiveness. o Check the logs and reports from these drills to ensure that any issues identified are addressed promptly. o Ensure that the most recent recovery drill results are documented and reviewed by relevant stakeholders. 4. Monitor and Log Review: o Ensure CloudWatch logs and alarms are set up to monitor backup and replication processes. o Review CloudTrail logs to verify that DR-related actions are being logged and monitored. o Check for alerts and notifications related to backup failures, replication issues, or any anomalies in the DR processes. 5. Evaluate Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO): o Verify that the DR plan specifies RTO and RPO for all critical systems and data. o Ensure that actual recovery times and points from recent drills meet or exceed the defined objectives. 6. Review Access Controls: o Check IAM policies to ensure that only authorized personnel have access to manage and initiate disaster recovery operations. o Verify that multi-factor authentication (MFA) is enabled for accounts with access to DR resources. 7. Assess Security and Compliance: o Ensure that data encryption is enabled for all backups and replicated data. o Verify compliance with industry standards and regulations (e.g., GDPR, HIPAA) concerning data protection and disaster recovery. 8. Continuous Improvement: o Review post-mortem reports from actual incidents and recovery drills to identify areas for improvement. o Ensure that feedback loops are in place for continuous enhancement of the DR plan and procedures. o Confirm that lessons learned from incidents and drills are incorporated into the DR plan. 9. Regular Updates and Communication: o Ensure the DR plan is reviewed and updated at least annually or whenever significant changes occur in the IT environment. o Verify that all relevant personnel are trained on the DR procedures and aware of their roles. o Check that regular communication channels are established for DR updates and training sessions.",
    "remediation": "References: 1. https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads- on-aws/disaster-recovery-options-in-the-cloud.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads- on-aws/disaster-recovery-options-in-the-cloud.html",
    "function_names": [
      "backup_plan_specification_check",
      "backup_resources_existence_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_plan_specification_check",
        "backup_resources_existence_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Maintain ongoing disaster recovery operations to ensure that systems and data can be swiftly restored in the event of a disruption. This involves regularly updating and testing recovery plans, monitoring replication processes, and verifying the integrity and accessibility of backups. Continuously evaluate and improve disaster recovery strategies to adapt to evolving threats and organizational changes, ensuring resilience and minimal downtime during incidents.",
      "audit_steps": "1. Review Disaster Recovery Plan: o Verify that a comprehensive disaster recovery (DR) plan exists and is regularly updated. o Ensure the DR plan includes detailed procedures for data backup, system recovery, and failover processes. o Check for documentation of roles and responsibilities during a disaster event. 2. Check Backup and Replication Settings: o Confirm that AWS Backup is configured correctly for all critical systems and data. o Review the settings for Amazon RDS, EBS snapshots, S3 versioning, and other AWS services to ensure backups are automated and scheduled appropriately. o Ensure that replication settings are configured to replicate data across multiple AWS regions for added redundancy. 3. Test Recovery Procedures: o Verify that regular recovery drills are conducted to test the DR plan\u2019s effectiveness. o Check the logs and reports from these drills to ensure that any issues identified are addressed promptly. o Ensure that the most recent recovery drill results are documented and reviewed by relevant stakeholders. 4. Monitor and Log Review: o Ensure CloudWatch logs and alarms are set up to monitor backup and replication processes. o Review CloudTrail logs to verify that DR-related actions are being logged and monitored. o Check for alerts and notifications related to backup failures, replication issues, or any anomalies in the DR processes. 5. Evaluate Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO): o Verify that the DR plan specifies RTO and RPO for all critical systems and data. o Ensure that actual recovery times and points from recent drills meet or exceed the defined objectives. 6. Review Access Controls: o Check IAM policies to ensure that only authorized personnel have access to manage and initiate disaster recovery operations. o Verify that multi-factor authentication (MFA) is enabled for accounts with access to DR resources. 7. Assess Security and Compliance: o Ensure that data encryption is enabled for all backups and replicated data. o Verify compliance with industry standards and regulations (e.g., GDPR, HIPAA) concerning data protection and disaster recovery. 8. Continuous Improvement: o Review post-mortem reports from actual incidents and recovery drills to identify areas for improvement. o Ensure that feedback loops are in place for continuous enhancement of the DR plan and procedures. o Confirm that lessons learned from incidents and drills are incorporated into the DR plan. 9. Regular Updates and Communication: o Ensure the DR plan is reviewed and updated at least annually or whenever significant changes occur in the IT environment. o Verify that all relevant personnel are trained on the DR procedures and aware of their roles. o Check that regular communication channels are established for DR updates and training sessions.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads- on-aws/disaster-recovery-options-in-the-cloud.html",
      "rationale": "Maintaining continuous disaster recovery operations is essential for ensuring that systems and data can be quickly and effectively restored following a disruption. Regular updates and tests of recovery plans, along with constant monitoring of replication processes, help verify the integrity and availability of backups. This proactive approach allows organizations to adapt to evolving threats and changes, ensuring resilience and minimizing downtime during incidents, which ultimately protects business continuity and reduces potential losses.",
      "impact": ""
    }
  },
  {
    "id": "6.10",
    "title": "Ensure execution of a Disaster Recovery Failover",
    "assessment": "Manual",
    "description": "Execute a comprehensive disaster recovery failover to transition operations from the primary system to a backup system during disruptions. This process includes ensuring all critical data and applications are accurately replicated to the backup site for seamless operational continuity. Regularly test and document the failover process to identify and resolve any issues, maintaining readiness to minimize downtime and data loss during real disasters.",
    "rationale": "Executing a comprehensive disaster recovery failover is essential to ensure operational continuity during disruptions. Accurate replication of critical data and applications to the backup site guarantees that business operations can continue seamlessly. Regular testing and documentation of the failover process help identify and resolve potential issues, maintaining a state of readiness and minimizing downtime and data loss in actual disaster scenarios.",
    "audit": "Follow the steps where we learned how to conduct a recovery drill with the below modifications: 1. Choose the server that you want to recover and failover. On the initiate recovery job menu, choose \u201cinitiate recovery.\u201d 2. Choose a point in time to recover from backup. 3. Choose initiate recovery to create a recovery job. Note: You can use the job details to monitor the progress and status of the recovery job. After the recovery job has completed, the last recovery result of your source server will report \u201csuccessful.\u201d The EC2 instance ID of the launched recovery instance will also be listed in the source server overview. You can test if the recovery instance is functioning by testing the EC2 instance that is in the source server overview.",
    "remediation": "Default Value: Implement a disaster recovery failover. References: 1. https://docs.aws.amazon.com/drs/latest/userguide/failback-preparing- failover.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/drs/latest/userguide/failback-preparing- failover.html",
    "function_names": [
      "backup_plan_specification_check",
      "backup_resources_existence_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_plan_specification_check",
        "backup_resources_existence_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Execute a comprehensive disaster recovery failover to transition operations from the primary system to a backup system during disruptions. This process includes ensuring all critical data and applications are accurately replicated to the backup site for seamless operational continuity. Regularly test and document the failover process to identify and resolve any issues, maintaining readiness to minimize downtime and data loss during real disasters.",
      "audit_steps": "Follow the steps where we learned how to conduct a recovery drill with the below modifications: 1. Choose the server that you want to recover and failover. On the initiate recovery job menu, choose \u201cinitiate recovery.\u201d 2. Choose a point in time to recover from backup. 3. Choose initiate recovery to create a recovery job. Note: You can use the job details to monitor the progress and status of the recovery job. After the recovery job has completed, the last recovery result of your source server will report \u201csuccessful.\u201d The EC2 instance ID of the launched recovery instance will also be listed in the source server overview. You can test if the recovery instance is functioning by testing the EC2 instance that is in the source server overview.",
      "remediation_steps": "Default Value: Implement a disaster recovery failover. References: 1. https://docs.aws.amazon.com/drs/latest/userguide/failback-preparing- failover.html",
      "rationale": "Executing a comprehensive disaster recovery failover is essential to ensure operational continuity during disruptions. Accurate replication of critical data and applications to the backup site guarantees that business operations can continue seamlessly. Regular testing and documentation of the failover process help identify and resolve potential issues, maintaining a state of readiness and minimizing downtime and data loss in actual disaster scenarios.",
      "impact": ""
    }
  },
  {
    "id": "6.11",
    "title": "Ensure execution of a failback",
    "assessment": "Manual",
    "description": "This method involves transitioning operations back from the backup or recovery system to the primary system after the resolution of a disruption or disaster. You can execute a failback either to the original server, ensuring continuity and restoring the previous state, or to a new server, which might be necessary if the original server is compromised or no longer functional. The failback process ensures that all updated data and configurations are transferred back, maintaining the integrity and functionality of the primary system.",
    "rationale": "A failback is crucial for restoring normal operations after a disaster recovery scenario. Transitioning operations back to the primary system ensures continuity and leverages the original environment's configurations and settings. This process can be directed either to the original server, maintaining the existing infrastructure, or to a new server if the original is compromised. Ensuring all data and configurations are accurately transferred back preserves system integrity and functionality, reducing downtime and allowing the organization to resume normal operations efficiently. Impact: Failback Prerequisites: \u2022 The volumes on the server you are failing back to are the same size or larger than the recovery instance if failing back to a new server. \u2022 The failback client has the proper permissions to access both Elastic Disaster Recovery and S3 services on TCP port 1500 inbound and TCP port 443 outbound to communicate with the failback client. \u2022 A public IP is added to the recovery instance.",
    "audit": "Performing the failback: 1. Download the failback client ISO 2. Attach the ISO to your original server and boot up the server. o The failback client will prompt for the IAM access key and secret key generated when making the user with the permission to access the failback. It will also ask for the region of the recovery instance. Remember: regions are case sensitive. If you\u2019re in US east 1, type \u201cus-east-1.\u201d o If you are failing back to the original server, the failback client will automatically detect the recovery instance and map the data volumes for replication. o If you are failing back to a new server, you may need to manually specify from a list of available recovery instances and map the data volumes. o The failback client will verify that the chosen recovery instance has connectivity to the Elastic Disaster Recovery service. o The replication software will be downloaded to the failback client and then configured. Connectivity will be made between the failback client and the replication agent on the recovery instance to begin data replication. 3. Return to the elastic disaster recovery console and recovery instances to see the current state of replication. Failing back to the original server will show \u201crescan\u201d in the console, while failing back o a new instance will perform an \u201cinitial sync.\u201d 4. After the data replication is completed, you will be able to perform the failback. o Check the state of the recovery instance to ensure that it\u2019s ready to complete a failback. o Select your recovery instance, then choose failback for the chosen recovery instance(s). o Choose failback again the complete a failback for the chosen recovery instance(s). During the failback process, the failback client will prepare your source server for normal operation. After it has completed successfully, the failback client will return \u201cfailback completed successfully\u201d in the console. 5. Reboot the server and return to normal operations. 6. Clean up failback job; terminate recovery job by following the steps outlined above when we ran a drill.",
    "remediation": "References: 1. https://docs.aws.amazon.com/drs/latest/userguide/failback-performing-main.html",
    "profile_applicability": "\u2022  Level 2",
    "impact": "Failback Prerequisites: \u2022 The volumes on the server you are failing back to are the same size or larger than the recovery instance if failing back to a new server. \u2022 The failback client has the proper permissions to access both Elastic Disaster Recovery and S3 services on TCP port 1500 inbound and TCP port 443 outbound to communicate with the failback client. \u2022 A public IP is added to the recovery instance.",
    "references": "1. https://docs.aws.amazon.com/drs/latest/userguide/failback-performing-main.html",
    "function_names": [
      "backup_plan_specification_check",
      "backup_resources_existence_check"
    ],
    "implementation_guidance": {
      "boto3_client": "backup",
      "functions": [
        "backup_plan_specification_check",
        "backup_resources_existence_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use backup boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "This method involves transitioning operations back from the backup or recovery system to the primary system after the resolution of a disruption or disaster. You can execute a failback either to the original server, ensuring continuity and restoring the previous state, or to a new server, which might be necessary if the original server is compromised or no longer functional. The failback process ensures that all updated data and configurations are transferred back, maintaining the integrity and functionality of the primary system.",
      "audit_steps": "Performing the failback: 1. Download the failback client ISO 2. Attach the ISO to your original server and boot up the server. o The failback client will prompt for the IAM access key and secret key generated when making the user with the permission to access the failback. It will also ask for the region of the recovery instance. Remember: regions are case sensitive. If you\u2019re in US east 1, type \u201cus-east-1.\u201d o If you are failing back to the original server, the failback client will automatically detect the recovery instance and map the data volumes for replication. o If you are failing back to a new server, you may need to manually specify from a list of available recovery instances and map the data volumes. o The failback client will verify that the chosen recovery instance has connectivity to the Elastic Disaster Recovery service. o The replication software will be downloaded to the failback client and then configured. Connectivity will be made between the failback client and the replication agent on the recovery instance to begin data replication. 3. Return to the elastic disaster recovery console and recovery instances to see the current state of replication. Failing back to the original server will show \u201crescan\u201d in the console, while failing back o a new instance will perform an \u201cinitial sync.\u201d 4. After the data replication is completed, you will be able to perform the failback. o Check the state of the recovery instance to ensure that it\u2019s ready to complete a failback. o Select your recovery instance, then choose failback for the chosen recovery instance(s). o Choose failback again the complete a failback for the chosen recovery instance(s). During the failback process, the failback client will prepare your source server for normal operation. After it has completed successfully, the failback client will return \u201cfailback completed successfully\u201d in the console. 5. Reboot the server and return to normal operations. 6. Clean up failback job; terminate recovery job by following the steps outlined above when we ran a drill.",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/drs/latest/userguide/failback-performing-main.html",
      "rationale": "A failback is crucial for restoring normal operations after a disaster recovery scenario. Transitioning operations back to the primary system ensures continuity and leverages the original environment's configurations and settings. This process can be directed either to the original server, maintaining the existing infrastructure, or to a new server if the original is compromised. Ensuring all data and configurations are accurately transferred back preserves system integrity and functionality, reducing downtime and allowing the organization to resume normal operations efficiently. Impact: Failback Prerequisites: \u2022 The volumes on the server you are failing back to are the same size or larger than the recovery instance if failing back to a new server. \u2022 The failback client has the proper permissions to access both Elastic Disaster Recovery and S3 services on TCP port 1500 inbound and TCP port 443 outbound to communicate with the failback client. \u2022 A public IP is added to the recovery instance.",
      "impact": "Failback Prerequisites: \u2022 The volumes on the server you are failing back to are the same size or larger than the recovery instance if failing back to a new server. \u2022 The failback client has the proper permissions to access both Elastic Disaster Recovery and S3 services on TCP port 1500 inbound and TCP port 443 outbound to communicate with the failback client. \u2022 A public IP is added to the recovery instance."
    }
  },
  {
    "id": "6.12",
    "title": "Ensure CloudWatch Metrics for AWS EDR",
    "assessment": "Manual",
    "description": "Set up and monitor AWS CloudWatch metrics for Endpoint Detection and Response (EDR) to track and analyze the performance and security of your AWS environment. This involves configuring CloudWatch to collect detailed logs and metrics on EDR activities, such as threat detections, response actions, and system health. Regularly review these metrics to identify trends, anomalies, and potential security issues, enabling proactive management and timely responses to ensure the effectiveness of your EDR solution.",
    "rationale": "Implementing AWS CloudWatch metrics for Endpoint Detection and Response (EDR) is essential for maintaining a secure and efficient AWS environment. By collecting detailed logs and metrics on EDR activities, you gain valuable insights into the performance and health of your security measures. Regular review of these metrics allows for the early detection of trends, anomalies, and potential security threats, enabling proactive management and swift responses to maintain the integrity and effectiveness of your EDR solution. This continuous monitoring ensures that your security posture remains robust and adaptive to evolving threats.",
    "audit": "1. Sign in to the AWS Management Console: o Open the AWS Management Console and sign in with your credentials. 2. Navigate to CloudWatch: o In the AWS Management Console, navigate to the CloudWatch service. 3. Create a CloudWatch Log Group: o Select Logs from the navigation pane. o Click on Create log group . o Enter a name for the log group and click Create . 4. Configure AWS EDR to Send Logs to CloudWatch: o Go to the AWS EDR (Elastic Disaster Recovery) console. o In the AWS EDR console, configure your settings to send logs and metrics to the CloudWatch log group you created. 5. Set Up CloudWatch Alarms: o In the CloudWatch console, select Alarms from the navigation pane. o Click on Create Alarm . o Select the metric you want to monitor from the list of AWS EDR metrics. o Configure the conditions for the alarm (e.g., threshold, period, etc.). o Set the actions to take when the alarm state is triggered (e.g., send a notification). o Review and create the alarm. 6. Create CloudWatch Dashboards: o In the CloudWatch console, select Dashboards from the navigation pane. o Click on Create dashboard . o Enter a name for your dashboard and click Create . o Add widgets to the dashboard by selecting the relevant AWS EDR metrics. o Customize the widgets to display the data in a meaningful way (e.g., graphs, numbers). 7. Enable CloudWatch Logs Insights: o In the CloudWatch console, select Logs Insights from the navigation pane. o Choose the log group you created for AWS EDR. o Use CloudWatch Logs Insights queries to analyze the log data and extract meaningful insights. 8. Set Up CloudWatch Events: o In the CloudWatch console, select Events from the navigation pane. o Click on Create rule . o Define the event source and the specific events you want to capture (e.g., changes in EDR status). o Set the target for the event (e.g., send a notification, invoke a Lambda function). o Configure the rule and click Create rule .",
    "remediation": "References: 1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsClou dWatch.html",
    "profile_applicability": "\u2022  Level 2",
    "references": "1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsClou dWatch.html",
    "function_names": [
      "ec2_securitygroup_rules_check"
    ],
    "implementation_guidance": {
      "boto3_client": "ec2",
      "functions": [
        "ec2_securitygroup_rules_check"
      ],
      "automation_level": "FULLY_AUTOMATED",
      "priority": "HIGH",
      "implementation_notes": "Use ec2 boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "Set up and monitor AWS CloudWatch metrics for Endpoint Detection and Response (EDR) to track and analyze the performance and security of your AWS environment. This involves configuring CloudWatch to collect detailed logs and metrics on EDR activities, such as threat detections, response actions, and system health. Regularly review these metrics to identify trends, anomalies, and potential security issues, enabling proactive management and timely responses to ensure the effectiveness of your EDR solution.",
      "audit_steps": "1. Sign in to the AWS Management Console: o Open the AWS Management Console and sign in with your credentials. 2. Navigate to CloudWatch: o In the AWS Management Console, navigate to the CloudWatch service. 3. Create a CloudWatch Log Group: o Select Logs from the navigation pane. o Click on Create log group . o Enter a name for the log group and click Create . 4. Configure AWS EDR to Send Logs to CloudWatch: o Go to the AWS EDR (Elastic Disaster Recovery) console. o In the AWS EDR console, configure your settings to send logs and metrics to the CloudWatch log group you created. 5. Set Up CloudWatch Alarms: o In the CloudWatch console, select Alarms from the navigation pane. o Click on Create Alarm . o Select the metric you want to monitor from the list of AWS EDR metrics. o Configure the conditions for the alarm (e.g., threshold, period, etc.). o Set the actions to take when the alarm state is triggered (e.g., send a notification). o Review and create the alarm. 6. Create CloudWatch Dashboards: o In the CloudWatch console, select Dashboards from the navigation pane. o Click on Create dashboard . o Enter a name for your dashboard and click Create . o Add widgets to the dashboard by selecting the relevant AWS EDR metrics. o Customize the widgets to display the data in a meaningful way (e.g., graphs, numbers). 7. Enable CloudWatch Logs Insights: o In the CloudWatch console, select Logs Insights from the navigation pane. o Choose the log group you created for AWS EDR. o Use CloudWatch Logs Insights queries to analyze the log data and extract meaningful insights. 8. Set Up CloudWatch Events: o In the CloudWatch console, select Events from the navigation pane. o Click on Create rule . o Define the event source and the specific events you want to capture (e.g., changes in EDR status). o Set the target for the event (e.g., send a notification, invoke a Lambda function). o Configure the rule and click Create rule .",
      "remediation_steps": "References: 1. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsClou dWatch.html",
      "rationale": "Implementing AWS CloudWatch metrics for Endpoint Detection and Response (EDR) is essential for maintaining a secure and efficient AWS environment. By collecting detailed logs and metrics on EDR activities, you gain valuable insights into the performance and health of your security measures. Regular review of these metrics allows for the early detection of trends, anomalies, and potential security threats, enabling proactive management and swift responses to maintain the integrity and effectiveness of your EDR solution. This continuous monitoring ensures that your security posture remains robust and adaptive to evolving threats.",
      "impact": ""
    }
  },
  {
    "id": "6.13",
    "title": "Ensure working of EDR",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "1. Preparing the Environment for EDR - Before getting started with EDR, you must prepare the environment that you want to back up. 2. Preparing the Source Server - Allow direct access to Elastic Disaster Recovery and Amazon S3 AWS service API endpoints through HTTPS protocol (TCP port 443). Direct outbound TCP port 1500 from the source server to the staging area subnet, which contains the replication servers. 3. Preparing the Staging Area Subnet - Allow Direct access to EDR, S3, and EC2 through HTTPS protocol (TCP port 443) Direct inbound TCP port 1500 for replication traffic 4. Accessing the AWS Elastic Disaster Recovery Console - o Search for \u201cAWS Elastic Disaster Recovery\u201d in the AWS Console. o Select \u201cElastic Disaster Recovery\u201d 5. Configuring the Replication Settings Template - Select Configure and Initializein in the AWS Elastic Disaster Recovery screen. You will be navigated to setup your replication settings template. This will create a staging area in a subnet of your choice and a replication server instance types. The default replication server instance type will be a t3 micro EC2 instance. This is good for normal workloads with small I/O operations. 6. Next, configure EBS encryption and volume types. This will depend on your workload requirements. 7. To encrypt EBS volumes, leave the setting as \u201cdefault.\u201d If you wish to make a custom encryption setting, you will need to create an AWS KMS key. 8. Configure the security group to your specific needs. Remember what ports need to be opened on inbound / outbound traffic that was specified in previous steps: You can choose how you want your data routed and if you want to throttle network traffic to reserve bandwidth. To keep your data as secure as possible, it\u2019s recommended to get set up with a VPN or AWS direct connect, so your backups are not traveling over the public internet. Point in time policy defines the snapshot retention time. Because Elastic Disaster Recovery service uses incremental backups, it\u2019s not necessary to keep old copies of backups. Now, you\u2019re ready to launch this template.",
    "remediation": "Appendix: Summary Table CIS Benchmark Recommendation Set Correctly Yes No 1 Introduction 1.1 AWS Storage Backups (Manual) \uf06f \uf06f 1.2 Ensure securing AWS Backups (Manual) \uf06f \uf06f 1.3 Ensure to create backup template and name (Manual) \uf06f \uf06f 1.4 Ensure to create AWS IAM Policies (Manual) \uf06f \uf06f 1.5 Ensure to create IAM roles for Backup (Manual) \uf06f \uf06f 1.6 Ensure AWS Backup with Service Linked Roles (Manual) \uf06f \uf06f 2 Elastic Block Store (EBS) 2.1 Ensure creating EC2 instance with EBS (Manual) \uf06f \uf06f 2.2 Ensure configuring Security Groups (Manual) \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage (Manual) \uf06f \uf06f 2.4 Ensure the creation of a new volume (Manual) \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes (Manual) \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances (Manual) \uf06f \uf06f 2.7 Ensure creating IAM User (Manual) \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups (Manual) \uf06f \uf06f 2.9 Ensure Granular Policy Creation (Manual) \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies (Manual) \uf06f \uf06f CIS Benchmark Recommendation Set Correctly Yes No 2.11 Ensure Secure Password Policy Implementation (Manual) \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch (Manual) \uf06f \uf06f 2.13 Ensure creating an SNS subscription (Manual) \uf06f \uf06f 3 Elastic File System (EFS) 3.1 EFS (Manual) \uf06f \uf06f 3.2 Ensure Implementation of EFS (Manual) \uf06f \uf06f 3.3 Ensure EFS and VPC Integration (Manual) \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services (Manual) \uf06f \uf06f 3.5 Ensure using Security Groups for VPC (Manual) \uf06f \uf06f 3.6 Ensure Secure Ports (Manual) \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets (Manual) \uf06f \uf06f 3.8 Ensure managing mount target security groups (Manual) \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS (Manual) \uf06f \uf06f 3.10 Ensure managing AWS EFS access points (Manual) \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies (Manual) \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery (Manual) \uf06f \uf06f 4 FSx 4.1 FSX (AWS Elastic File Cache) (Manual) \uf06f \uf06f 4.2 Amazon Elastic File Cache (Manual) \uf06f \uf06f CIS Benchmark Recommendation Set Correctly Yes No 4.3 Ensure the creation of an FSX Bucket (Manual) \uf06f \uf06f 4.4 Ensure the creation of Elastic File Cache (Manual) \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client (Manual) \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre (Manual) \uf06f \uf06f 4.7 Ensure mounting FSx cache (Manual) \uf06f \uf06f 4.8 Ensure exporting cache to S3 (Manual) \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources (Manual) \uf06f \uf06f 5 Simple Storage Service (S3) 5.1 Amazon Simple Storage Service (Manual) \uf06f \uf06f 5.2 Ensure direct data addition to S3 (Manual) \uf06f \uf06f 5.3 Ensure Storage Classes are Configured (Manual) \uf06f \uf06f 6 Elastic Disaster Recovery (EDR) 6.1 Ensure Elastic Disaster Recovery is Configured (Manual) \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration (Manual) \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) (Manual) \uf06f \uf06f 6.4 Ensure configuration of replication settings (Manual) \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery (Manual) \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent (Manual) \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings (Manual) \uf06f \uf06f CIS Benchmark Recommendation Set Correctly Yes No 6.8 Ensure execution of a recovery drill (Manual) \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations (Manual) \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover (Manual) \uf06f \uf06f 6.11 Ensure execution of a failback (Manual) \uf06f \uf06f 6.12 Ensure CloudWatch Metrics for AWS EDR (Manual) \uf06f \uf06f 6.13 Ensure working of EDR (Manual) \uf06f \uf06f   Appendix: CIS Controls v7 IG 1 Mapped Recommendations Recommendation Set Correctly Yes No 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v7 IG 2 Mapped Recommendations Recommendation Set Correctly Yes No 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups \uf06f \uf06f 2.9 Ensure Granular Policy Creation \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f Recommendation Set Correctly Yes No 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.7 Ensure mounting FSx cache \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v7 IG 3 Mapped Recommendations Recommendation Set Correctly Yes No 2.1 Ensure creating EC2 instance with EBS \uf06f \uf06f 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups \uf06f \uf06f 2.9 Ensure Granular Policy Creation \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f Recommendation Set Correctly Yes No 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.7 Ensure mounting FSx cache \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f   Appendix: CIS Controls v7 Unmapped Recommendations Recommendation Set Correctly Yes No 1.1 AWS Storage Backups \uf06f \uf06f 1.2 Ensure securing AWS Backups \uf06f \uf06f 1.3 Ensure to create backup template and name \uf06f \uf06f 1.4 Ensure to create AWS IAM Policies \uf06f \uf06f 1.5 Ensure to create IAM roles for Backup \uf06f \uf06f 1.6 Ensure AWS Backup with Service Linked Roles \uf06f \uf06f 2.13 Ensure creating an SNS subscription \uf06f \uf06f 4.4 Ensure the creation of Elastic File Cache \uf06f \uf06f 4.8 Ensure exporting cache to S3 \uf06f \uf06f 6.12 Ensure CloudWatch Metrics for AWS EDR \uf06f \uf06f 6.13 Ensure working of EDR \uf06f \uf06f    Appendix: CIS Controls v8 IG 1 Mapped Recommendations Recommendation Set Correctly Yes No 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f Recommendation Set Correctly Yes No 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v8 IG 2 Mapped Recommendations Recommendation Set Correctly Yes No 2.1 Ensure creating EC2 instance with EBS \uf06f \uf06f 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f Recommendation Set Correctly Yes No 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v8 IG 3 Mapped Recommendations Recommendation Set Correctly Yes No 2.1 Ensure creating EC2 instance with EBS \uf06f \uf06f 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups \uf06f \uf06f 2.9 Ensure Granular Policy Creation \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f Recommendation Set Correctly Yes No 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f   Appendix: CIS Controls v8 Unmapped Recommendations Recommendation Set Correctly Yes No 1.1 AWS Storage Backups \uf06f \uf06f 1.2 Ensure securing AWS Backups \uf06f \uf06f 1.3 Ensure to create backup template and name \uf06f \uf06f 1.4 Ensure to create AWS IAM Policies \uf06f \uf06f 1.5 Ensure to create IAM roles for Backup \uf06f \uf06f 1.6 Ensure AWS Backup with Service Linked Roles \uf06f \uf06f 2.13 Ensure creating an SNS subscription \uf06f \uf06f 4.4 Ensure the creation of Elastic File Cache \uf06f \uf06f 4.7 Ensure mounting FSx cache \uf06f \uf06f 4.8 Ensure exporting cache to S3 \uf06f \uf06f 6.12 Ensure CloudWatch Metrics for AWS EDR \uf06f \uf06f 6.13 Ensure working of EDR \uf06f \uf06f   Appendix: Change History Date Version Changes for this version 07/03/2024 V1.0.0 Document Created",
    "profile_applicability": "\u2022  Level 2",
    "function_names": [],
    "implementation_guidance": {
      "boto3_client": "unknown",
      "functions": [],
      "automation_level": "MANUAL",
      "priority": "LOW",
      "implementation_notes": "Use unknown boto3 client to implement these functions"
    },
    "manual_requirements": {
      "requires_manual_intervention": true,
      "description": "",
      "audit_steps": "1. Preparing the Environment for EDR - Before getting started with EDR, you must prepare the environment that you want to back up. 2. Preparing the Source Server - Allow direct access to Elastic Disaster Recovery and Amazon S3 AWS service API endpoints through HTTPS protocol (TCP port 443). Direct outbound TCP port 1500 from the source server to the staging area subnet, which contains the replication servers. 3. Preparing the Staging Area Subnet - Allow Direct access to EDR, S3, and EC2 through HTTPS protocol (TCP port 443) Direct inbound TCP port 1500 for replication traffic 4. Accessing the AWS Elastic Disaster Recovery Console - o Search for \u201cAWS Elastic Disaster Recovery\u201d in the AWS Console. o Select \u201cElastic Disaster Recovery\u201d 5. Configuring the Replication Settings Template - Select Configure and Initializein in the AWS Elastic Disaster Recovery screen. You will be navigated to setup your replication settings template. This will create a staging area in a subnet of your choice and a replication server instance types. The default replication server instance type will be a t3 micro EC2 instance. This is good for normal workloads with small I/O operations. 6. Next, configure EBS encryption and volume types. This will depend on your workload requirements. 7. To encrypt EBS volumes, leave the setting as \u201cdefault.\u201d If you wish to make a custom encryption setting, you will need to create an AWS KMS key. 8. Configure the security group to your specific needs. Remember what ports need to be opened on inbound / outbound traffic that was specified in previous steps: You can choose how you want your data routed and if you want to throttle network traffic to reserve bandwidth. To keep your data as secure as possible, it\u2019s recommended to get set up with a VPN or AWS direct connect, so your backups are not traveling over the public internet. Point in time policy defines the snapshot retention time. Because Elastic Disaster Recovery service uses incremental backups, it\u2019s not necessary to keep old copies of backups. Now, you\u2019re ready to launch this template.",
      "remediation_steps": "Appendix: Summary Table CIS Benchmark Recommendation Set Correctly Yes No 1 Introduction 1.1 AWS Storage Backups (Manual) \uf06f \uf06f 1.2 Ensure securing AWS Backups (Manual) \uf06f \uf06f 1.3 Ensure to create backup template and name (Manual) \uf06f \uf06f 1.4 Ensure to create AWS IAM Policies (Manual) \uf06f \uf06f 1.5 Ensure to create IAM roles for Backup (Manual) \uf06f \uf06f 1.6 Ensure AWS Backup with Service Linked Roles (Manual) \uf06f \uf06f 2 Elastic Block Store (EBS) 2.1 Ensure creating EC2 instance with EBS (Manual) \uf06f \uf06f 2.2 Ensure configuring Security Groups (Manual) \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage (Manual) \uf06f \uf06f 2.4 Ensure the creation of a new volume (Manual) \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes (Manual) \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances (Manual) \uf06f \uf06f 2.7 Ensure creating IAM User (Manual) \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups (Manual) \uf06f \uf06f 2.9 Ensure Granular Policy Creation (Manual) \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies (Manual) \uf06f \uf06f CIS Benchmark Recommendation Set Correctly Yes No 2.11 Ensure Secure Password Policy Implementation (Manual) \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch (Manual) \uf06f \uf06f 2.13 Ensure creating an SNS subscription (Manual) \uf06f \uf06f 3 Elastic File System (EFS) 3.1 EFS (Manual) \uf06f \uf06f 3.2 Ensure Implementation of EFS (Manual) \uf06f \uf06f 3.3 Ensure EFS and VPC Integration (Manual) \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services (Manual) \uf06f \uf06f 3.5 Ensure using Security Groups for VPC (Manual) \uf06f \uf06f 3.6 Ensure Secure Ports (Manual) \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets (Manual) \uf06f \uf06f 3.8 Ensure managing mount target security groups (Manual) \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS (Manual) \uf06f \uf06f 3.10 Ensure managing AWS EFS access points (Manual) \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies (Manual) \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery (Manual) \uf06f \uf06f 4 FSx 4.1 FSX (AWS Elastic File Cache) (Manual) \uf06f \uf06f 4.2 Amazon Elastic File Cache (Manual) \uf06f \uf06f CIS Benchmark Recommendation Set Correctly Yes No 4.3 Ensure the creation of an FSX Bucket (Manual) \uf06f \uf06f 4.4 Ensure the creation of Elastic File Cache (Manual) \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client (Manual) \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre (Manual) \uf06f \uf06f 4.7 Ensure mounting FSx cache (Manual) \uf06f \uf06f 4.8 Ensure exporting cache to S3 (Manual) \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources (Manual) \uf06f \uf06f 5 Simple Storage Service (S3) 5.1 Amazon Simple Storage Service (Manual) \uf06f \uf06f 5.2 Ensure direct data addition to S3 (Manual) \uf06f \uf06f 5.3 Ensure Storage Classes are Configured (Manual) \uf06f \uf06f 6 Elastic Disaster Recovery (EDR) 6.1 Ensure Elastic Disaster Recovery is Configured (Manual) \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration (Manual) \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) (Manual) \uf06f \uf06f 6.4 Ensure configuration of replication settings (Manual) \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery (Manual) \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent (Manual) \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings (Manual) \uf06f \uf06f CIS Benchmark Recommendation Set Correctly Yes No 6.8 Ensure execution of a recovery drill (Manual) \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations (Manual) \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover (Manual) \uf06f \uf06f 6.11 Ensure execution of a failback (Manual) \uf06f \uf06f 6.12 Ensure CloudWatch Metrics for AWS EDR (Manual) \uf06f \uf06f 6.13 Ensure working of EDR (Manual) \uf06f \uf06f   Appendix: CIS Controls v7 IG 1 Mapped Recommendations Recommendation Set Correctly Yes No 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v7 IG 2 Mapped Recommendations Recommendation Set Correctly Yes No 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups \uf06f \uf06f 2.9 Ensure Granular Policy Creation \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f Recommendation Set Correctly Yes No 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.7 Ensure mounting FSx cache \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v7 IG 3 Mapped Recommendations Recommendation Set Correctly Yes No 2.1 Ensure creating EC2 instance with EBS \uf06f \uf06f 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups \uf06f \uf06f 2.9 Ensure Granular Policy Creation \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f Recommendation Set Correctly Yes No 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.7 Ensure mounting FSx cache \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f   Appendix: CIS Controls v7 Unmapped Recommendations Recommendation Set Correctly Yes No 1.1 AWS Storage Backups \uf06f \uf06f 1.2 Ensure securing AWS Backups \uf06f \uf06f 1.3 Ensure to create backup template and name \uf06f \uf06f 1.4 Ensure to create AWS IAM Policies \uf06f \uf06f 1.5 Ensure to create IAM roles for Backup \uf06f \uf06f 1.6 Ensure AWS Backup with Service Linked Roles \uf06f \uf06f 2.13 Ensure creating an SNS subscription \uf06f \uf06f 4.4 Ensure the creation of Elastic File Cache \uf06f \uf06f 4.8 Ensure exporting cache to S3 \uf06f \uf06f 6.12 Ensure CloudWatch Metrics for AWS EDR \uf06f \uf06f 6.13 Ensure working of EDR \uf06f \uf06f    Appendix: CIS Controls v8 IG 1 Mapped Recommendations Recommendation Set Correctly Yes No 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f Recommendation Set Correctly Yes No 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v8 IG 2 Mapped Recommendations Recommendation Set Correctly Yes No 2.1 Ensure creating EC2 instance with EBS \uf06f \uf06f 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f Recommendation Set Correctly Yes No 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f    Appendix: CIS Controls v8 IG 3 Mapped Recommendations Recommendation Set Correctly Yes No 2.1 Ensure creating EC2 instance with EBS \uf06f \uf06f 2.2 Ensure configuring Security Groups \uf06f \uf06f 2.3 Ensure the proper configuration of EBS storage \uf06f \uf06f 2.4 Ensure the creation of a new volume \uf06f \uf06f 2.5 Ensure creating snapshots of EBS volumes \uf06f \uf06f 2.6 Ensure Proper IAM Configuration for EC2 Instances \uf06f \uf06f 2.7 Ensure creating IAM User \uf06f \uf06f 2.8 Ensure the Creation of IAM Groups \uf06f \uf06f 2.9 Ensure Granular Policy Creation \uf06f \uf06f 2.10 Ensure Resource Access via Tag-based Policies \uf06f \uf06f 2.11 Ensure Secure Password Policy Implementation \uf06f \uf06f 2.12 Ensure Monitoring EC2 and EBS with CloudWatch \uf06f \uf06f 3.1 EFS \uf06f \uf06f 3.2 Ensure Implementation of EFS \uf06f \uf06f 3.3 Ensure EFS and VPC Integration \uf06f \uf06f 3.4 Ensure controlling Network access to EFS Services \uf06f \uf06f 3.5 Ensure using Security Groups for VPC \uf06f \uf06f 3.6 Ensure Secure Ports \uf06f \uf06f 3.7 Ensure File-Level Access Control with Mount Targets \uf06f \uf06f 3.8 Ensure managing mount target security groups \uf06f \uf06f 3.9 Ensure using VPC endpoints - EFS \uf06f \uf06f 3.10 Ensure managing AWS EFS access points \uf06f \uf06f 3.11 Ensure accessing Points and IAM Policies \uf06f \uf06f 3.12 Ensure configuring IAM for AWS Elastic Disaster Recovery \uf06f \uf06f 4.1 FSX (AWS Elastic File Cache) \uf06f \uf06f 4.2 Amazon Elastic File Cache \uf06f \uf06f Recommendation Set Correctly Yes No 4.3 Ensure the creation of an FSX Bucket \uf06f \uf06f 4.5 Ensure installation and configuration of Lustre Client \uf06f \uf06f 4.6 Ensure EC2 Kernel compatibility with Lustre \uf06f \uf06f 4.9 Ensure cleaning up FSx Resources \uf06f \uf06f 5.1 Amazon Simple Storage Service \uf06f \uf06f 5.2 Ensure direct data addition to S3 \uf06f \uf06f 5.3 Ensure Storage Classes are Configured \uf06f \uf06f 6.1 Ensure Elastic Disaster Recovery is Configured \uf06f \uf06f 6.2 Ensure AWS Disaster Recovery Configuration \uf06f \uf06f 6.3 Ensure functionality of Endpoint Detection and Response (EDR) \uf06f \uf06f 6.4 Ensure configuration of replication settings \uf06f \uf06f 6.5 Ensure proper IAM configuration for AWS Elastic Disaster Recovery \uf06f \uf06f 6.6 Ensure installation of the AWS Replication Agent \uf06f \uf06f 6.7 Ensure proper configuration of the Launch Settings \uf06f \uf06f 6.8 Ensure execution of a recovery drill \uf06f \uf06f 6.9 Ensure Continuous Disaster Recovery Operations \uf06f \uf06f 6.10 Ensure execution of a Disaster Recovery Failover \uf06f \uf06f 6.11 Ensure execution of a failback \uf06f \uf06f   Appendix: CIS Controls v8 Unmapped Recommendations Recommendation Set Correctly Yes No 1.1 AWS Storage Backups \uf06f \uf06f 1.2 Ensure securing AWS Backups \uf06f \uf06f 1.3 Ensure to create backup template and name \uf06f \uf06f 1.4 Ensure to create AWS IAM Policies \uf06f \uf06f 1.5 Ensure to create IAM roles for Backup \uf06f \uf06f 1.6 Ensure AWS Backup with Service Linked Roles \uf06f \uf06f 2.13 Ensure creating an SNS subscription \uf06f \uf06f 4.4 Ensure the creation of Elastic File Cache \uf06f \uf06f 4.7 Ensure mounting FSx cache \uf06f \uf06f 4.8 Ensure exporting cache to S3 \uf06f \uf06f 6.12 Ensure CloudWatch Metrics for AWS EDR \uf06f \uf06f 6.13 Ensure working of EDR \uf06f \uf06f   Appendix: Change History Date Version Changes for this version 07/03/2024 V1.0.0 Document Created",
      "rationale": "",
      "impact": ""
    }
  }
]