[
  {
    "id": "2.1.1",
    "title": "Enable audit Logs",
    "assessment": "Manual",
    "description": "With Azure Kubernetes Service (AKS), the control plane components such as the kube- apiserver and kube-controller-manager are provided as a managed service. You create and manage the nodes that run the kubelet and container runtime, and deploy your applications through the managed Kubernetes API server. To help troubleshoot your application and services, you may need to view the logs generated by these control plane components. To help collect and review data from multiple sources, Azure Monitor logs provides a query language and analytics engine that provides insights to your environment. A workspace is used to collate and analyze the data, and can integrate with other Azure services such as Application Insights and Security Center.",
    "rationale": "Exporting logs and metrics to a dedicated, persistent datastore ensures availability of audit data following a cluster security event, and provides a central location for analysis of log and metric data collated from multiple sources. Impact: What is collected from Kubernetes clusters Container insights includes a predefined set of metrics and inventory items collected that are written as log data in your Log Analytics workspace. All metrics listed below are collected by default every one minute. Node metrics collected The following list is the 24 metrics per node that are collected: cpuUsageNanoCores cpuCapacityNanoCores cpuAllocatableNanoCores memoryRssBytes memoryWorkingSetBytes memoryCapacityBytes memoryAllocatableBytes restartTimeEpoch used (disk) free (disk) used_percent (disk) io_time (diskio) writes (diskio) reads (diskio) write_bytes (diskio) write_time (diskio) iops_in_progress (diskio) read_bytes (diskio) read_time (diskio) err_in (net) err_out (net) bytes_recv (net) bytes_sent (net) Kubelet_docker_operations (kubelet) Container metrics The following list is the eight metrics per container collected: cpuUsageNanoCores cpuRequestNanoCores cpuLimitNanoCores memoryRssBytes memoryWorkingSetBytes memoryRequestBytes memoryLimitBytes restartTimeEpoch Cluster inventory The following list is the cluster inventory data collected by default: KubePodInventory – 1 per minute per container KubeNodeInventory – 1 per node per minute KubeServices – 1 per service per minute ContainerInventory – 1 per container per minute",
    "audit": "",
    "remediation": "Azure audit logs are enabled and managed in the Azure portal. To enable log collection for the Kubernetes master components in your AKS cluster, open the Azure portal in a web browser and complete the following steps: 1. Select the resource group for your AKS cluster, such as myResourceGroup. Don't select the resource group that contains your individual AKS cluster resources, such as MC_myResourceGroup_myAKSCluster_eastus. 2. On the left-hand side, choose Diagnostic settings. 3. Select your AKS cluster, such as myAKSCluster, then choose to Add diagnostic setting. 4. Enter a name, such as myAKSClusterLogs, then select the option to Send to Log Analytics. 5. Select an existing workspace or create a new one. If you create a workspace, provide a workspace name, a resource group, and a location. 6. In the list of available logs, select the logs you wish to enable. For this example, enable the kube-audit and kube-audit-admin logs. Common logs include the kube-apiserver, kube-controller-manager, and kube-scheduler. You can return and change the collected logs once Log Analytics workspaces are enabled. 7. When ready, select Save to enable collection of the selected logs. Default Value: By default, cluster control plane logs aren't sent to be Logged. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ 2. https://docs.microsoft.com/en-us/azure/aks/view-master-logs 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- logging-threat-detection#lt-4-enable-logging-for-azure-resources",
    "profile_applicability": "•  Level 1",
    "impact": "What is collected from Kubernetes clusters Container insights includes a predefined set of metrics and inventory items collected that are written as log data in your Log Analytics workspace. All metrics listed below are collected by default every one minute. Node metrics collected The following list is the 24 metrics per node that are collected: cpuUsageNanoCores cpuCapacityNanoCores cpuAllocatableNanoCores memoryRssBytes memoryWorkingSetBytes memoryCapacityBytes memoryAllocatableBytes restartTimeEpoch used (disk) free (disk) used_percent (disk) io_time (diskio) writes (diskio) reads (diskio) write_bytes (diskio) write_time (diskio) iops_in_progress (diskio) read_bytes (diskio) read_time (diskio) err_in (net) err_out (net) bytes_recv (net) bytes_sent (net) Kubelet_docker_operations (kubelet) Container metrics The following list is the eight metrics per container collected: cpuUsageNanoCores cpuRequestNanoCores cpuLimitNanoCores memoryRssBytes memoryWorkingSetBytes memoryRequestBytes memoryLimitBytes restartTimeEpoch Cluster inventory The following list is the cluster inventory data collected by default: KubePodInventory – 1 per minute per container KubeNodeInventory – 1 per node per minute KubeServices – 1 per service per minute ContainerInventory – 1 per container per minute",
    "references": "1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ 2. https://docs.microsoft.com/en-us/azure/aks/view-master-logs 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- logging-threat-detection#lt-4-enable-logging-for-azure-resources",
    "function_names": [
      "aks_control_plane_audit_logs_enabled",
      "aks_control_plane_logging_enabled"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "All existing functions related to AKS control plane logging have been mapped and consolidated into one function, `aks_control_plane_logging_enabled`, which encompasses all logging requirements. This ensures compliance with the requirement to enable audit logs while adhering to naming standards and minimizing function count.",
    "last_mapped": "2025-08-18T01:05:33.829714",
    "consolidation_info": [
      {
        "old_functions": [
          "aks_control_plane_logging_to_monitor_enabled",
          "aks_control_plane_logs_workspace_integrated",
          "aks_control_plane_logs_retention_configured",
          "aks_control_plane_logs_all_categories_enabled",
          "aks_control_plane_logs_immutable_storage_enabled",
          "aks_control_plane_logs_encryption_enabled",
          "aks_control_plane_logs_centralized_workspace_configured"
        ],
        "new_consolidated_function": "aks_control_plane_logging_enabled",
        "rationale": "Consolidation improves efficiency by reducing duplication of logging-related checks into a single function that covers all aspects of logging for the control plane."
      }
    ]
  },
  {
    "id": "3.1.1",
    "title": "Ensure that the kubeconfig file permissions are set to 644 or more restrictive",
    "assessment": "Automated",
    "description": "If kubelet is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644 or more restrictive.",
    "rationale": "The kubelet kubeconfig file controls various parameters of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kubelet with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None.",
    "audit": "Method 1 SSH to the worker nodes To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate kubeconfig file: ps -ef | grep kubelet The output of the above command should return something similar to --kubeconfig /var/lib/kubelet/kubeconfig which is the location of the kubeconfig file. Run this command to obtain the kubeconfig file permissions: stat -c %a /var/lib/kubelet/kubeconfig The output of the above command gives you the kubeconfig file's permissions. Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file permissions on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the permission level of the file: ls -l /host/var/lib/kubelet/kubeconfig Verify that if a file is specified and it exists, the permissions are 644 or more restrictive.",
    "remediation": "Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 <kubeconfig file> Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "kubernetes_kubeconfig_file_permissions_restrictive",
      "kubernetes_kubeconfig_file_permissions_644_or_stricter"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The existing functions 'kubernetes_kubeconfig_file_permissions_restrictive' and 'kubernetes_kubeconfig_file_permissions_644_or_stricter' were identified as relevant to the compliance requirement. The other functions related to kubeconfig file permissions were consolidated into 'kubernetes_kubeconfig_file_permissions_restrictive' to minimize function count while ensuring all aspects of the compliance requirement are covered.",
    "last_mapped": "2025-08-18T01:05:33.829784",
    "consolidation_info": [
      {
        "old_functions": [
          "kubernetes_kubeconfig_file_permissions_secure",
          "kubernetes_kubeconfig_file_permissions_compliant",
          "kubernetes_kubeconfig_file_permissions_protected"
        ],
        "new_consolidated_function": "kubernetes_kubeconfig_file_permissions_restrictive",
        "rationale": "Consolidation improves efficiency by reducing duplication of functions that check similar requirements regarding kubeconfig file permissions."
      }
    ]
  },
  {
    "id": "3.1.2",
    "title": "Ensure that the kubelet kubeconfig file ownership is set to root:root",
    "assessment": "Automated",
    "description": "If kubelet is running, ensure that the file ownership of its kubeconfig file is set to root:root.",
    "rationale": "The kubeconfig file for kubelet controls various parameters for the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None",
    "audit": "Method 1 SSH to the worker nodes To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate kubeconfig file: ps -ef | grep kubelet The output of the above command should return something similar to --kubeconfig /var/lib/kubelet/kubeconfig which is the location of the kubeconfig file. Run this command to obtain the kubeconfig file ownership: stat -c %U:%G /var/lib/kubelet/kubeconfig The output of the above command gives you the kubeconfig file's ownership. Verify that the ownership is set to root:root. Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file ownership on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the ownership of the file: ls -l /host/var/lib/kubelet/kubeconfig The output of the above command gives you the kubeconfig file's ownership. Verify that the ownership is set to root:root.",
    "remediation": "Run the below command (based on the file location on your system) on each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "kubernetes_kubelet_kubeconfig_file_ownership_root"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The existing functions were assessed for coverage of the compliance requirement. The function 'kubernetes_kubelet_kubeconfig_file_ownership_root' was selected as the best representative for the compliance check, and all similar functions were consolidated into this one to minimize redundancy. The naming follows the required structure, clearly indicating the service, resource, and requirement.",
    "last_mapped": "2025-08-18T01:05:33.829810",
    "consolidation_info": [
      {
        "old_functions": [
          "kubernetes_kubelet_kubeconfig_file_ownership_root",
          "kubernetes_kubeconfig_file_ownership_root",
          "kubelet_kubeconfig_file_ownership_root_root",
          "kubelet_kubeconfig_root_ownership",
          "kubernetes_kubelet_config_file_ownership_root"
        ],
        "new_consolidated_function": "kubernetes_kubelet_kubeconfig_file_ownership_root",
        "rationale": "Consolidation reduces duplication and ensures a single source of truth for checking kubelet kubeconfig file ownership."
      }
    ]
  },
  {
    "id": "3.1.3",
    "title": "Ensure that the azure.json file has permissions set to 644 or more restrictive",
    "assessment": "Automated",
    "description": "The azure.json file in an Azure Kubernetes Service (AKS) cluster is a configuration file used by the Kubernetes cloud provider integration for Azure. This file contains essential details that allow the Kubernetes cluster to interact with Azure resources effectively. It's part of the Azure Cloud Provider configuration, enabling Kubernetes components to communicate with Azure services for features like load balancers, storage, and networking. Ensure the file has permissions of 644 or more restrictive.",
    "rationale": "The azure.json file in AKS structure typically includes: • Tenant ID: The Azure Tenant ID where the AKS cluster resides. • Subscription ID: The Azure Subscription ID used for billing and resource management. • AAD Client ID: The Azure Active Directory (AAD) application client ID used by the Kubernetes cloud provider to interact with Azure resources. • AAD Client Secret: The secret for the AAD application. • Resource Group: The name of the resource group where the AKS cluster resources are located. • Location: The Azure region where the AKS cluster is deployed. • VM Type: Specifies the type of VMs used by the cluster (e.g., standard VMs or Virtual Machine Scale Sets). • Subnet Name, Security Group Name, Vnet Name, and Vnet Resource Group: Networking details for the cluster. • Route Table Name: The name of the route table for the cluster. • Storage Account Type: The default type of storage account to use for Kubernetes persistent volumes. Impact: None.",
    "audit": "Method 1 First, SSH to the relevant worker node: To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/azure.json which is the location of the Kubelet config file. Run the following command: stat -c %a /etc/kubernetes/azure.json The output of the above command is the Kubelet config file's permissions. Verify that the permissions are 644 or more restrictive. Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file permissions on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the permission level of the file: ls -l /host/etc/kubernetes/azure.json Verify that if a file is specified and it exists, the permissions are 644 or more restrictive.",
    "remediation": "Run the following command (using the config file location identified in the Audit step) chmod 644 /etc/kubernetes/azure.json Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "aks_config_file_permissions_644_or_stricter"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The existing function 'aks_config_file_permissions_644_or_stricter' was identified as the best match for the compliance requirement. Other similar functions were consolidated into this one to minimize redundancy and maintain clarity in compliance checks.",
    "last_mapped": "2025-08-18T01:05:33.829825",
    "consolidation_info": [
      {
        "old_functions": [
          "aks_config_file_permissions_restrictive",
          "aks_config_file_permissions_secure",
          "aks_config_file_permissions_compliant",
          "aks_config_file_permissions_protected"
        ],
        "new_consolidated_function": "aks_config_file_permissions_644_or_stricter",
        "rationale": "Consolidation improves efficiency by reducing duplication of similar functions that check for file permissions, ensuring a single source of truth for compliance checks."
      }
    ]
  },
  {
    "id": "3.1.4",
    "title": "Ensure that the azure.json file ownership is set to root:root",
    "assessment": "Automated",
    "description": "The azure.json file in an Azure Kubernetes Service (AKS) cluster is a configuration file used by the Kubernetes cloud provider integration for Azure. This file contains essential details that allow the Kubernetes cluster to interact with Azure resources effectively. It's part of the Azure Cloud Provider configuration, enabling Kubernetes components to communicate with Azure services for features like load balancers, storage, and networking. Ensure that the file is owned by root:root.",
    "rationale": "The azure.json file in AKS structure typically includes: • Tenant ID: The Azure Tenant ID where the AKS cluster resides. • Subscription ID: The Azure Subscription ID used for billing and resource management. • AAD Client ID: The Azure Active Directory (AAD) application client ID used by the Kubernetes cloud provider to interact with Azure resources. • AAD Client Secret: The secret for the AAD application. • Resource Group: The name of the resource group where the AKS cluster resources are located. • Location: The Azure region where the AKS cluster is deployed. • VM Type: Specifies the type of VMs used by the cluster (e.g., standard VMs or Virtual Machine Scale Sets). • Subnet Name, Security Group Name, Vnet Name, and Vnet Resource Group: Networking details for the cluster. • Route Table Name: The name of the route table for the cluster. • Storage Account Type: The default type of storage account to use for Kubernetes persistent volumes. Impact: None",
    "audit": "Method 1 First, SSH to the relevant worker node: To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/azure.json which is the location of the Kubelet config file. Run the following command: stat -c %U:%G /etc/kubernetes/azure.json The output of the above command is the Kubelet config file's ownership. Verify that the ownership is set to root:root Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file ownership on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the ownership of the file: ls -l /etc/kubernetes/azure.json The output of the above command gives you the azure.json file's ownership. Verify that the ownership is set to root:root.",
    "remediation": "Run the following command (using the config file location identified in the Audit step) chown root:root /etc/kubernetes/azure.json Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "aks_azure_json_file_owner_root"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The existing function 'aks_azure_json_file_owner_root' effectively covers the compliance requirement of ensuring the azure.json file ownership is set to root:root. All similar functions have been consolidated into this single function, which adheres to the naming standards and provides complete coverage for the compliance item.",
    "last_mapped": "2025-08-18T01:05:33.829842",
    "consolidation_info": [
      {
        "old_functions": [
          "aks_config_file_root_ownership",
          "aks_azure_json_file_secure_ownership",
          "aks_cloud_provider_config_root_owned",
          "aks_config_file_owner_root",
          "aks_azure_json_root_ownership",
          "aks_cloud_config_file_secure_owner",
          "aks_config_file_root_user_owned"
        ],
        "new_consolidated_function": "aks_azure_json_file_owner_root",
        "rationale": "Consolidation reduces duplication and maintains clarity by focusing on the specific requirement of ensuring the azure.json file is owned by root:root."
      }
    ]
  },
  {
    "id": "3.2.1",
    "title": "Ensure that the --anonymous-auth argument is set to false",
    "assessment": "Automated",
    "description": "Disable anonymous requests to the Kubelet server.",
    "rationale": "When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: sudo more /etc/kubernetes/kubelet/kubelet-config.json Verify that the \"authentication\": { \"anonymous\": { \"enabled\": false } argument is set to false. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication... \"anonymous\":{\"enabled\":false} by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to false \"anonymous\": \"enabled\": false Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --anonymous-auth=false Remediation Method 3: If using the api configz endpoint consider searching for the status of \"authentication.*anonymous\":{\"enabled\":false}\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- governance-strategy#gs-6-define-identity-and-privileged-access-strategy",
    "profile_applicability": "•  Level 1",
    "impact": "Anonymous requests will be rejected.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- governance-strategy#gs-6-define-identity-and-privileged-access-strategy",
    "function_names": [
      "kubernetes_kubelet_anonymous_auth_disabled"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The existing functions related to anonymous authentication for the Kubelet server were consolidated into one function, 'kubernetes_kubelet_anonymous_auth_disabled', which effectively covers the compliance requirement to ensure that anonymous requests are disabled. This consolidation reduces redundancy and adheres to the naming standards.",
    "last_mapped": "2025-08-18T01:05:33.829857",
    "consolidation_info": [
      {
        "old_functions": [
          "kubernetes_kubelet_anonymous_auth_disabled",
          "kubernetes_kubelet_anonymous_auth_set_false",
          "kubernetes_kubelet_anonymous_auth_denied",
          "kubernetes_kubelet_auth_anonymous_disabled"
        ],
        "new_consolidated_function": "kubernetes_kubelet_anonymous_auth_disabled",
        "rationale": "Consolidation improves efficiency by reducing duplication of similar functions that check the same requirement."
      }
    ]
  },
  {
    "id": "3.2.2",
    "title": "Ensure that the --authorization-mode argument is not set to AlwaysAllow",
    "assessment": "Automated",
    "description": "Do not allow all requests. Enable explicit authorization.",
    "rationale": "Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for \"authentication\": \"webhook\": \"enabled\" set to true. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: sudo more /etc/kubernetes/kubelet/kubelet-config.json Verify that the \"authentication\": {\"webhook\": { \"enabled\": is set to true. If the \"authentication\": {\"mode\": { argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by -- config, and that file sets \"authentication\": {\"mode\": { to something other than AlwaysAllow. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication... \"webhook\":{\"enabled\":true} by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to false \"authentication\"... \"webhook\":{\"enabled\":true Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --authorization-mode=Webhook Remediation Method 3: If using the api configz endpoint consider searching for the status of \"authentication.*webhook\":{\"enabled\":true\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- governance-strategy#gs-6-define-identity-and-privileged-access-strategy",
    "profile_applicability": "•  Level 1",
    "impact": "Unauthorized requests will be denied.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- governance-strategy#gs-6-define-identity-and-privileged-access-strategy",
    "function_names": [
      "eks_cluster_authorization_mode_restricted"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "All existing functions related to the authorization mode have been consolidated into one function, `eks_cluster_authorization_mode_restricted`, which effectively covers the compliance requirement of ensuring that the authorization mode is not set to AlwaysAllow. This consolidation reduces redundancy and maintains clarity in function naming.",
    "last_mapped": "2025-08-18T01:05:33.829872",
    "consolidation_info": [
      {
        "old_functions": [
          "eks_cluster_authorization_mode_not_always_allow",
          "eks_cluster_explicit_authorization_required",
          "eks_cluster_authorization_restricted",
          "eks_cluster_no_always_allow_auth",
          "eks_cluster_auth_mode_restricted"
        ],
        "new_consolidated_function": "eks_cluster_authorization_mode_restricted",
        "rationale": "Consolidation improves efficiency by reducing duplication of similar functions that check for the same requirement regarding authorization mode."
      }
    ]
  },
  {
    "id": "3.2.3",
    "title": "Ensure that the --client-ca-file argument is set as appropriate",
    "assessment": "Automated",
    "description": "Enable Kubelet authentication using certificates.",
    "rationale": "The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for \"x509\": {\"clientCAFile:\" set to the location of the client certificate authority file. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: sudo more /etc/kubernetes/kubelet/kubelet-config.json Verify that the \"x509\": {\"clientCAFile:\" argument exists and is set to the location of the client certificate authority file. If the \"x509\": {\"clientCAFile:\" argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets \"authentication\": { \"x509\": {\"clientCAFile:\" to the location of the client certificate authority file. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication.. x509\":(\"clientCAFile\":\"/etc/kubernetes/pki/ca.crt by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to false \"authentication\": { \"x509\": {\"clientCAFile:\" to the location of the client CA file. Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --client-ca-file=<path/to/client-ca-file> Remediation Method 3: If using the api configz endpoint consider searching for the status of \"authentication.*x509\":(\"clientCAFile\":\"/etc/kubernetes/pki/ca.crt\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/ 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-4-encrypt-sensitive-information-in-transit",
    "profile_applicability": "•  Level 1",
    "impact": "You require TLS to be configured on apiserver as well as kubelets.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/ 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-4-encrypt-sensitive-information-in-transit",
    "function_names": [
      "kubernetes_kubelet_client_ca_file_set",
      "kubernetes_kubelet_certificate_authentication_enabled",
      "kubernetes_kubelet_client_ca_file_configured",
      "kubernetes_kubelet_client_ca_file_appropriate"
    ],
    "mapped_coverage": "partial",
    "mapping_notes": "Mapped existing functions that cover the compliance requirement. Consolidated similar functions related to certificate authentication to reduce duplication. A new function is needed to ensure complete coverage of the compliance item regarding the client-ca-file argument.",
    "last_mapped": "2025-08-18T01:05:33.829890",
    "consolidation_info": [
      {
        "old_functions": [
          "kubernetes_kubelet_certificate_authentication_enabled",
          "kubernetes_kubelet_authentication_certificates_valid"
        ],
        "new_consolidated_function": "kubernetes_kubelet_certificate_authentication_enabled",
        "rationale": "Consolidation improves efficiency by combining similar checks related to certificate authentication into one function."
      }
    ]
  },
  {
    "id": "3.2.4",
    "title": "Ensure that the --read-only-port is secured",
    "assessment": "Automated",
    "description": "Disable the read-only port.",
    "rationale": "The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.",
    "audit": "If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to 0. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: cat /etc/kubernetes/kubelet/kubelet-config.json Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.",
    "remediation": "If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to false readOnlyPort to 0 If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --read-only-port=0 For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 1",
    "impact": "Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "mongodb_instance_read_only_port_disabled"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The compliance requirement to ensure that the read-only port is secured is fully covered by the consolidated function 'mongodb_instance_read_only_port_disabled'. This function effectively communicates the desired state of the read-only port being disabled, aligning with the compliance requirement.",
    "last_mapped": "2025-08-18T01:05:33.829916",
    "consolidation_info": [
      {
        "old_functions": [
          "mongodb_instance_read_only_port_disabled",
          "mongodb_instance_read_only_port_secured",
          "mongodb_instance_read_only_port_restricted",
          "mongodb_instance_read_only_port_unexposed",
          "mongodb_instance_read_only_port_inaccessible"
        ],
        "new_consolidated_function": "mongodb_instance_read_only_port_disabled",
        "rationale": "Consolidating all similar functions into one function named 'mongodb_instance_read_only_port_disabled' improves efficiency and reduces duplication, as they all check for the same compliance requirement regarding the read-only port."
      }
    ]
  },
  {
    "id": "3.2.5",
    "title": "Ensure that the --streaming-connection-idle-timeout argument is not set to 0",
    "assessment": "Automated",
    "description": "Do not disable timeouts on streaming connections.",
    "rationale": "Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.",
    "audit": "Audit Method 1: First, SSH to the relevant node: Run the following command on each node to find the running kubelet process: ps -ef | grep kubelet If the command line for the process includes the argument streaming-connection- idle-timeout verify that it is not set to 0. If the streaming-connection-idle-timeout argument is not present in the output of the above command, refer instead to the config argument that specifies the location of the Kubelet config file e.g. --config /etc/kubernetes/kubelet/kubelet- config.json. Open the Kubelet config file: cat /etc/kubernetes/kubelet/kubelet-config.json Verify that the streamingConnectionIdleTimeout argument is not set to 0. Audit Method 2: If using the api configz endpoint consider searching for the status of \"streamingConnectionIdleTimeout\":\"4h0m0s\" by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to a non-zero value in the format of #h#m#s \"streamingConnectionIdleTimeout\": \"4h0m0s\" You should ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not specify a --streaming-connection-idle-timeout argument because it would override the Kubelet config file. Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --streaming-connection-idle-timeout=4h0m0s Remediation Method 3: If using the api configz endpoint consider searching for the status of \"streamingConnectionIdleTimeout\": by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 1",
    "impact": "Long-lived connections could be interrupted.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "kubernetes_api_server_streaming_connection_idle_timeout_non_zero",
      "kubernetes_api_server_streaming_connection_idle_timeout_not_disabled",
      "kubernetes_api_server_streaming_idle_timeout_non_zero"
    ],
    "mapped_coverage": "complete",
    "mapping_notes": "The existing functions were assessed for coverage of the compliance requirement. The functions related to streaming connection idle timeout were consolidated into one function, ensuring that all aspects of the requirement are covered while adhering to naming standards. The consolidated function clearly indicates that the idle timeout must not be set to zero, aligning with the compliance requirement.",
    "last_mapped": "2025-08-18T01:05:33.829931",
    "consolidation_info": [
      {
        "old_functions": [
          "kubernetes_api_server_streaming_connection_timeout_configured",
          "kubernetes_api_server_connection_idle_timeout_enabled",
          "kubernetes_api_server_streaming_timeout_not_zero"
        ],
        "new_consolidated_function": "kubernetes_api_server_streaming_connection_idle_timeout_non_zero",
        "rationale": "Consolidation improves efficiency by reducing duplication of similar checks related to streaming connection idle timeout."
      }
    ]
  },
  {
    "id": "3.2.6",
    "title": "Ensure that the --make-iptables-util-chains argument is set to true",
    "assessment": "Automated",
    "description": "Allow Kubelet to manage iptables.",
    "rationale": "Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for makeIPTablesUtilChains set to true. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: cat /etc/kubernetes/kubelet/kubelet-config.json Verify that if the makeIPTablesUtilChains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication... \"makeIPTablesUtilChains\":true by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to false \"makeIPTablesUtilChains\": true Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --make-iptables-util-chains:true Remediation Method 3: If using the api configz endpoint consider searching for the status of \"makeIPTablesUtilChains\": true by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the Azure AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic",
    "profile_applicability": "•  Level 1",
    "impact": "Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic",
    "function_names": [
      "kubernetes_kubelet_iptables_util_chains_enabled",
      "kubernetes_kubelet_iptables_util_chains_configured",
      "kubernetes_kubelet_iptables_util_chains_set_true"
    ]
  },
  {
    "id": "3.2.7",
    "title": "Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture",
    "assessment": "Automated",
    "description": "Security relevant information should be captured. The --eventRecordQPS flag on the Kubelet can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",
    "rationale": "It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.",
    "audit": "Audit Method 1: First, SSH to each node. Run the following command on each node to find the Kubelet process: ps -ef | grep kubelet In the output of the above command review the value set for the --eventRecordQPS argument and determine whether this has been set to an appropriate level for the cluster. The value of 0 can be used to ensure that all events are captured. If the --eventRecordQPS argument does not exist, check that there is a Kubelet config file specified by --config and review the value in this location. The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: cat /etc/kubernetes/kubelet/kubelet-config.json If there is an entry for eventRecordQPS check that it is set to 0 or an appropriate level for the cluster. Audit Method 2: If using the api configz endpoint consider searching for the status of eventRecordQPS by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to 5 or a value greater or equal to 0 \"eventRecordQPS\": 5 Check that /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not define an executable argument for eventRecordQPS because this would override your Kubelet config. Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --eventRecordQPS=5 Remediation Method 3: If using the api configz endpoint consider searching for the status of \"eventRecordQPS\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the AKS documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- logging-threat-detection",
    "profile_applicability": "•  Level 2",
    "impact": "Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go 3. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- logging-threat-detection",
    "function_names": [
      "kubernetes_kubelet_event_record_qps_disabled",
      "kubernetes_kubelet_event_record_qps_within_safe_limit",
      "kubernetes_kubelet_event_capture_rate_configured",
      "kubernetes_kubelet_event_record_qps_no_dos_risk",
      "kubernetes_kubelet_event_logging_rate_optimized"
    ]
  },
  {
    "id": "3.2.8",
    "title": "Ensure that the --rotate-certificates argument is not set to false",
    "assessment": "Automated",
    "description": "Enable kubelet client certificate rotation.",
    "rationale": "The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA (Confidentiality, Integrity, and Availability) security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to implement rotation yourself. Note: This feature also requires the RotateKubeletClientCertificate feature gate to be enabled. Impact: None",
    "audit": "Audit Method 1: SSH to each node and run the following command to find the Kubelet process: ps -ef | grep kubelet If the output of the command above includes the --RotateCertificate executable argument, verify that it is set to true. If the output of the command above does not include the --RotateCertificate executable argument then check the Kubelet config file. The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: cat /etc/kubernetes/kubelet/kubelet-config.json Verify that the RotateCertificate argument is not present, or is set to true.",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to true \"RotateCertificate\":true Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the -- RotateCertificate executable argument to false because this would override the Kubelet config file. Remediation Method 2: If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --RotateCertificate=true Default Value: See the AKS documentation for the default value. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ 5. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 6. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-4-encrypt-sensitive-information-in-transit",
    "profile_applicability": "•  Level 2",
    "impact": "None",
    "references": "1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ 5. https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 6. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-4-encrypt-sensitive-information-in-transit",
    "function_names": [
      "kubernetes_kubelet_certificate_rotation_enabled",
      "kubernetes_kubelet_rotate_certificates_not_disabled",
      "kubernetes_kubelet_client_cert_rotation_required",
      "kubernetes_kubelet_certificate_rotation_active",
      "kubernetes_kubelet_rotate_certificates_set_true"
    ]
  },
  {
    "id": "3.2.9",
    "title": "Ensure that the RotateKubeletServerCertificate argument is set to true",
    "assessment": "Automated",
    "description": "Enable kubelet server certificate rotation.",
    "rationale": "RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for RotateKubeletServerCertificate is set to true. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file. Open the Kubelet config file: cat /etc/kubernetes/kubelet/kubelet-config.json Verify that RotateKubeletServerCertificate argument exists and is set to true. Audit Method 2: If using the api configz endpoint consider searching for the status of \"RotateKubeletServerCertificate\":true by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to true \"RotateKubeletServerCertificate\":true Remediation Method 2: If using a Kubelet config file, edit the file to set RotateKubeletServerCertificate to true. If using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string. --rotate-kubelet-server-certificate=true Remediation Method 3: If using the api configz endpoint consider searching for the status of \"RotateKubeletServerCertificate\": by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all three remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the AKS documentation for the default value. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",
    "function_names": [
      "compute_kubelet_certificate_rotation_enabled",
      "compute_kubelet_server_certificate_rotation_enabled",
      "kubernetes_kubelet_certificate_rotation_enabled",
      "kubernetes_kubelet_server_certificate_rotation_enabled",
      "kubelet_server_certificate_rotation_enabled"
    ]
  },
  {
    "id": "4.1.1",
    "title": "Ensure that the cluster-admin role is only used where required",
    "assessment": "Automated",
    "description": "The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.",
    "rationale": "Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",
    "audit": "Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.",
    "remediation": "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",
    "references": "1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "function_names": [
      "kubernetes_role_no_cluster_admin_unnecessary",
      "kubernetes_role_cluster_admin_restricted",
      "kubernetes_role_cluster_admin_minimal_usage",
      "kubernetes_role_cluster_admin_least_privilege",
      "kubernetes_role_cluster_admin_limited_scope"
    ]
  },
  {
    "id": "4.1.2",
    "title": "Minimize access to secrets",
    "assessment": "Automated",
    "description": "The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",
    "rationale": "Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation",
    "audit": "Review the users who have get, list or watch access to secrets objects in the Kubernetes API.",
    "remediation": "Where possible, remove get, list and watch access to secret objects in the cluster. Default Value: By default, the following list of principals have get privileges on secret objects CLUSTERROLEBINDING                                    SUBJECT TYPE            SA-NAMESPACE cluster-admin                                         system:masters Group system:controller:clusterrole-aggregation-controller  clusterrole- aggregation-controller  ServiceAccount  kube-system system:controller:expand-controller                   expand-controller ServiceAccount  kube-system system:controller:generic-garbage-collector           generic-garbage- collector           ServiceAccount  kube-system system:controller:namespace-controller                namespace-controller ServiceAccount  kube-system system:controller:persistent-volume-binder            persistent-volume- binder            ServiceAccount  kube-system system:kube-controller-manager                        system:kube-controller- manager      User References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-7-eliminate-unintended-credential-exposure",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken not to remove access to secrets to system components which require this for their operation",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-7-eliminate-unintended-credential-exposure",
    "function_names": [
      "kubernetes_secret_access_restricted",
      "kubernetes_secret_minimal_access",
      "kubernetes_secret_no_public_access",
      "kubernetes_secret_no_anonymous_access",
      "kubernetes_secret_no_wildcard_access",
      "kubernetes_secret_no_default_access",
      "kubernetes_secret_no_broad_access",
      "kubernetes_secret_no_unrestricted_access",
      "kubernetes_secret_no_excessive_access",
      "kubernetes_secret_no_unauthorized_access"
    ]
  },
  {
    "id": "4.1.3",
    "title": "Minimize wildcard use in Roles and ClusterRoles",
    "assessment": "Automated",
    "description": "Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard \"*\" which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.",
    "rationale": "The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.",
    "audit": "Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml",
    "remediation": "Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions. References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "profile_applicability": "•  Level 1",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "function_names": [
      "kubernetes_role_wildcard_restricted",
      "kubernetes_clusterrole_wildcard_restricted",
      "kubernetes_role_no_wildcard_resources",
      "kubernetes_clusterrole_no_wildcard_resources",
      "kubernetes_role_no_wildcard_verbs",
      "kubernetes_clusterrole_no_wildcard_verbs",
      "kubernetes_role_minimal_permissions",
      "kubernetes_clusterrole_minimal_permissions",
      "kubernetes_role_no_wildcard_api_groups",
      "kubernetes_clusterrole_no_wildcard_api_groups"
    ]
  },
  {
    "id": "4.1.4",
    "title": "Minimize access to create pods",
    "assessment": "Automated",
    "description": "The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.",
    "rationale": "The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",
    "audit": "Review the users who have create access to pod objects in the Kubernetes API.",
    "remediation": "Where possible, remove create access to pod objects in the cluster. Default Value: By default, the following list of principals have create privileges on pod objects CLUSTERROLEBINDING                                    SUBJECT TYPE            SA-NAMESPACE cluster-admin                                         system:masters Group system:controller:clusterrole-aggregation-controller  clusterrole- aggregation-controller  ServiceAccount  kube-system system:controller:daemon-set-controller               daemon-set-controller ServiceAccount  kube-system system:controller:job-controller                      job-controller ServiceAccount  kube-system system:controller:persistent-volume-binder            persistent-volume- binder            ServiceAccount  kube-system system:controller:replicaset-controller               replicaset-controller ServiceAccount  kube-system system:controller:replication-controller              replication-controller ServiceAccount  kube-system system:controller:statefulset-controller              statefulset-controller ServiceAccount  kube-system References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken not to remove access to pods to system components which require this for their operation",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "function_names": [
      "kubernetes_namespace_pod_creation_restricted",
      "kubernetes_role_pod_creation_minimized",
      "kubernetes_cluster_pod_creation_limited",
      "kubernetes_service_account_pod_creation_restricted",
      "kubernetes_rbac_pod_creation_minimized",
      "kubernetes_policy_pod_creation_limited",
      "kubernetes_user_pod_creation_restricted",
      "kubernetes_group_pod_creation_minimized"
    ]
  },
  {
    "id": "4.1.5",
    "title": "Ensure that default service accounts are not actively used",
    "assessment": "Automated",
    "description": "The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.",
    "rationale": "Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.",
    "audit": "For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",
    "remediation": "Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Automatic remediation for the default account: kubectl patch serviceaccount default -p $'automountServiceAccountToken: false' Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-2-manage-application-identities-securely-and- automatically",
    "profile_applicability": "•  Level 1",
    "impact": "All workloads which require access to the Kubernetes API will require an explicit service account to be created.",
    "references": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-2-manage-application-identities-securely-and- automatically",
    "function_names": [
      "iam_service_account_default_not_used",
      "iam_service_account_no_default_usage",
      "iam_service_account_default_inactive",
      "compute_service_account_default_disabled",
      "compute_service_account_no_default_usage",
      "compute_service_account_default_not_used"
    ]
  },
  {
    "id": "4.1.6",
    "title": "Ensure that Service Account Tokens are only mounted where necessary",
    "assessment": "Automated",
    "description": "Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server",
    "rationale": "Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",
    "audit": "Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. Set SERVICE_ACCOUNT and POD variables to appropriate values automountServiceAccountToken: false",
    "remediation": "Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-2-manage-application-identities-securely-and- automatically",
    "profile_applicability": "•  Level 1",
    "impact": "Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",
    "references": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-2-manage-application-identities-securely-and- automatically",
    "function_names": [
      "kubernetes_pod_service_account_token_unmounted",
      "kubernetes_pod_service_account_token_restricted",
      "kubernetes_pod_service_account_token_minimal",
      "kubernetes_pod_service_account_token_disabled",
      "kubernetes_pod_service_account_token_required_only"
    ]
  },
  {
    "id": "4.2.1",
    "title": "Minimize the admission of privileged containers",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the securityContext.privileged flag set to true.",
    "rationale": "Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of privileged containers. Since manually searching through each pod's configuration might be tedious, especially in environments with many pods, you can use a more automated approach with grep or other command-line tools. Here's an example of how you might approach this with a combination of kubectl, grep, and shell scripting for a more automated solution: kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[].securityContext.privileged == true) | .metadata.name' OR kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[].securityContext.privileged == true) | select(.metadata.namespace != \"kube-system\" and .metadata.namespace != \"gatekeeper-system\" and .metadata.namespace != \"azure-arc\" and .metadata.namespace != \"azure-extensions-usage-system\") | \"\\(.metadata.name) \\(.metadata.namespace)\"' When creating a Pod Security Policy, [\"kube-system\", \"gatekeeper-system\", \"azure-arc\", \"azure-extensions-usage-system\"] namespaces are excluded by default. This command uses jq, a command-line JSON processor, to parse the JSON output from kubectl get pods and filter out pods where any container has the securityContext.privileged flag set to true. Please note that you might need to adjust the command depending on your specific requirements and the structure of your pod specifications.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. To enable PSA for a namespace in your cluster, set the pod- security.kubernetes.io/enforce label with the policy value you want to enforce. kubectl label --overwrite ns NAMESPACE pod- security.kubernetes.io/enforce=restricted The above command enforces the restricted policy for the NAMESPACE namespace. You can also enable Pod Security Admission for all your namespaces. For example: kubectl label --overwrite ns --all pod- security.kubernetes.io/warn=baseline Pod Security Policies and Assignments can be found by searching for Policies in the Azure Portal. A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://learn.microsoft.com/en-us/azure/aks/use-azure-policy 3. https://kubernetes.io/docs/concepts/security/pod-security-admission/",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",
    "references": "1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://learn.microsoft.com/en-us/azure/aks/use-azure-policy 3. https://kubernetes.io/docs/concepts/security/pod-security-admission/",
    "function_names": [
      "compute_container_privileged_disabled",
      "compute_container_privileged_escalation_disabled",
      "compute_container_security_context_restricted",
      "compute_container_privileged_flag_false",
      "compute_container_privileged_mode_disabled"
    ]
  },
  {
    "id": "4.2.2",
    "title": "Minimize the admission of containers wishing to share the host process ID namespace",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the hostPID flag set to true.",
    "rationale": "A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostPID containers Search for the hostPID Flag: In the YAML output, look for the hostPID setting under the spec section to check if it is set to true. kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostPID == true) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostPID == true) | select(.metadata.namespace != \"kube- system\" and .metadata.namespace != \"gatekeeper-system\" and .metadata.namespace != \"azure-arc\" and .metadata.namespace != \"azure- extensions-usage-system\") | \"\\(.metadata.name) \\(.metadata.namespace)\"' When creating a Pod Security Policy, [\"kube-system\", \"gatekeeper-system\", \"azure-arc\", \"azure-extensions-usage-system\"] namespaces are excluded by default. This command retrieves all pods across all namespaces in JSON format, then uses jq to filter out those with the hostPID flag set to true, and finally formats the output to show the namespace and name of each matching pod.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostPID containers. Pod Security Policies and Assignments can be found by searching for Policies in the Azure Portal. A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://kubernetes.io/docs/concepts/security/pod-security-admission/",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://kubernetes.io/docs/concepts/security/pod-security-admission/",
    "function_names": [
      "compute_container_host_pid_disabled",
      "compute_container_host_pid_restricted",
      "compute_container_host_pid_not_shared",
      "compute_container_host_pid_isolated",
      "compute_container_host_pid_protected"
    ]
  },
  {
    "id": "4.2.3",
    "title": "Minimize the admission of containers wishing to share the host IPC namespace",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the hostIPC flag set to true.",
    "rationale": "A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostIPC containers Search for the hostIPC Flag: In the YAML output, look for the hostIPC setting under the spec section to check if it is set to true. kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostIPC == true) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostIPC == true) | select(.metadata.namespace != \"kube- system\" and .metadata.namespace != \"gatekeeper-system\" and .metadata.namespace != \"azure-arc\" and .metadata.namespace != \"azure- extensions-usage-system\") | \"\\(.metadata.name) \\(.metadata.namespace)\"' When creating a Pod Security Policy, [\"kube-system\", \"gatekeeper-system\", \"azure-arc\", \"azure-extensions-usage-system\"] namespaces are excluded by default. This command retrieves all pods across all namespaces in JSON format, then uses jq to filter out those with the hostIPC flag set to true, and finally formats the output to show the namespace and name of each matching pod.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Pod Security Policies and Assignments can be found by searching for Policies in the Azure Portal. A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 3. https://learn.microsoft.com/en-us/azure/aks/use-psa",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 3. https://learn.microsoft.com/en-us/azure/aks/use-psa",
    "function_names": [
      "compute_container_host_ipc_disabled",
      "compute_container_host_ipc_not_shared",
      "compute_container_host_ipc_restricted",
      "compute_container_host_ipc_denied",
      "compute_container_host_ipc_protected"
    ]
  },
  {
    "id": "4.2.4",
    "title": "Minimize the admission of containers wishing to share the host network namespace",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the hostNetwork flag set to true.",
    "rationale": "A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namespaces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostNetwork containers Given that manually checking each pod can be time-consuming, especially in large environments, you can use a more automated approach to filter out pods where hostNetwork is set to true. Here’s a command using kubectl and jq: kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostNetwork == true) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostNetwork == true) | select(.metadata.namespace != \"kube-system\" and .metadata.namespace != \"gatekeeper-system\" and .metadata.namespace != \"azure-arc\" and .metadata.namespace != \"azure- extensions-usage-system\") | \"\\(.metadata.name) \\(.metadata.namespace)\"' When creating a Pod Security Policy, [\"kube-system\", \"gatekeeper-system\", \"azure-arc\", \"azure-extensions-usage-system\"] namespaces are excluded by default. This command retrieves all pods across all namespaces in JSON format, then uses jq to filter out those with the hostNetwork flag set to true, and finally formats the output to show the namespace and name of each matching pod.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Pod Security Policies and Assignments can be found by searching for Policies in the Azure Portal. A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://learn.microsoft.com/en-us/azure/aks/use-azure-policy 3. https://learn.microsoft.com/en-us/azure/aks/use-psa",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://learn.microsoft.com/en-us/azure/aks/use-azure-policy 3. https://learn.microsoft.com/en-us/azure/aks/use-psa",
    "function_names": [
      "compute_container_host_network_disabled",
      "compute_container_host_network_restricted",
      "compute_container_host_network_prohibited",
      "compute_container_host_network_denied",
      "compute_container_host_network_unshared"
    ]
  },
  {
    "id": "4.2.5",
    "title": "Minimize the admission of containers with allowPrivilegeEscalation",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.",
    "rationale": "A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation. This command gets all pods across all namespaces, outputs their details in JSON format, and uses jq to parse and filter the output for containers with allowPrivilegeEscalation set to true. kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(any(.spec.containers[]; .securityContext.allowPrivilegeEscalation == true)) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(any(.spec.containers[]; .securityContext.allowPrivilegeEscalation == true)) | select(.metadata.namespace != \"kube-system\" and .metadata.namespace != \"gatekeeper-system\" and .metadata.namespace != \"azure-arc\" and .metadata.namespace != \"azure-extensions-usage-system\") | \"\\(.metadata.name) \\(.metadata.namespace)\"' When creating a Pod Security Policy, [\"kube-system\", \"gatekeeper-system\", \"azure-arc\", \"azure-extensions-usage-system\"] namespaces are excluded by default. This command uses jq, a command-line JSON processor, to parse the JSON output from kubectl get pods and filter out pods where any container has the securityContext.privileged flag set to true. Please note that you might need to adjust the command depending on your specific requirements and the structure of your pod specifications.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with .spec.allowPrivilegeEscalation set to true. Pod Security Policies and Assignments can be found by searching for Policies in the Azure Portal. A detailed step-by-step guide can be found here: https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://learn.microsoft.com/en-us/azure/aks/use-azure-policy 3. https://learn.microsoft.com/en-us/azure/aks/use-psa",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for- kubernetes 2. https://learn.microsoft.com/en-us/azure/aks/use-azure-policy 3. https://learn.microsoft.com/en-us/azure/aks/use-psa",
    "function_names": [
      "compute_container_privilege_escalation_disabled",
      "compute_container_allow_privilege_escalation_false",
      "compute_container_privilege_escalation_restricted",
      "compute_container_privilege_escalation_minimized",
      "compute_container_privilege_escalation_denied"
    ]
  },
  {
    "id": "4.4.1",
    "title": "Ensure latest CNI version is used",
    "assessment": "Manual",
    "description": "There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.",
    "rationale": "Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None.",
    "audit": "Ensure CNI plugin supports network policies. Set Environment Variables to run export RESOURCE_GROUP=Resource Group Name export CLUSTER_NAME=Cluster Name Azure command to check for CNI plugin: az aks show --resource-group ${RESOURCE_GROUP} --name ${CLUSTER_NAME} --query \"networkProfile\"",
    "remediation": "As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Cilium or Calico. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic 3. https://learn.microsoft.com/en-us/azure/aks/use-network-policies Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic 3. https://learn.microsoft.com/en-us/azure/aks/use-network-policies Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",
    "function_names": [
      "kubernetes_cni_latest_version_used",
      "kubernetes_network_policy_supported",
      "kubernetes_cni_network_policy_enabled",
      "kubernetes_cni_plugin_up_to_date",
      "kubernetes_cni_plugin_supports_network_policies"
    ]
  },
  {
    "id": "4.4.2",
    "title": "Ensure that all Namespaces have Network Policies defined",
    "assessment": "Automated",
    "description": "Use network policies to isolate traffic in your cluster network.",
    "rationale": "Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\" Impact: Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\"",
    "audit": "Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces Ensure that each namespace defined in the cluster has at least one Network Policy.",
    "remediation": "Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic",
    "profile_applicability": "•  Level 2",
    "impact": "Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\"",
    "references": "1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/ 4. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic",
    "function_names": [
      "kubernetes_namespace_network_policy_defined",
      "kubernetes_namespace_network_policy_required",
      "kubernetes_namespace_traffic_isolation_enabled",
      "kubernetes_network_policy_namespace_coverage",
      "kubernetes_namespace_network_restrictions_enforced"
    ]
  },
  {
    "id": "4.5.1",
    "title": "Prefer using secrets as files over secrets as environment variables",
    "assessment": "Automated",
    "description": "Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.",
    "rationale": "It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",
    "audit": "Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A",
    "remediation": "If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-7-eliminate-unintended-credential-exposure Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",
    "profile_applicability": "•  Level 2",
    "impact": "Application code which expects to read secrets in the form of environment variables would need modification",
    "references": "1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-7-eliminate-unintended-credential-exposure Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",
    "function_names": [
      "kubernetes_secret_files_preferred",
      "kubernetes_secret_environment_variables_avoided",
      "kubernetes_secret_volume_mount_enabled",
      "kubernetes_secret_environment_variables_minimized",
      "kubernetes_secret_files_over_environment_variables"
    ]
  },
  {
    "id": "4.5.2",
    "title": "Consider external secret storage",
    "assessment": "Manual",
    "description": "Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.",
    "rationale": "Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",
    "audit": "Review your secrets management implementation.",
    "remediation": "Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured. References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-7-eliminate-unintended-credential-exposure",
    "profile_applicability": "•  Level 2",
    "impact": "None",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-7-eliminate-unintended-credential-exposure",
    "function_names": [
      "secrets_manager_external_storage_used",
      "secrets_manager_authentication_required",
      "secrets_manager_audit_logging_enabled",
      "secrets_manager_encryption_enabled",
      "secrets_manager_rotation_enabled",
      "secrets_manager_compliance_standards_met"
    ]
  },
  {
    "id": "4.6.1",
    "title": "Create administrative boundaries between resources using namespaces",
    "assessment": "Manual",
    "description": "Use namespaces to isolate your Kubernetes objects.",
    "rationale": "Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in an Azure AKS cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",
    "audit": "Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.",
    "remediation": "Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: When you create an AKS cluster, the following namespaces are available: NAMESPACES Namespace Description default Where pods and deployments are created by default when none is provided. In smaller environments, you can deploy applications directly into the default namespace without creating additional logical separations. When you interact with the Kubernetes API, such as with kubectl get pods, the default namespace is used when none is specified. kube-system Where core resources exist, such as network features like DNS and proxy, or the Kubernetes dashboard. You typically don't deploy your own applications into this namespace. kube- public Typically not used, but can be used for resources to be visible across the whole cluster, and can be viewed by any user. References: 1. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- governance-strategy#gs-1-define-asset-management-and-data-protection- strategy 4. https://docs.microsoft.com/en-us/azure/aks/concepts-clusters- workloads#:~:text=Kubernetes%20resources%2C%20such%20as%20pods,or% 20manage%20access%20to%20resources.&text=When%20you%20interact%20 with%20the,used%20when%20none%20is%20specified.",
    "profile_applicability": "•  Level 1",
    "impact": "You need to switch between namespaces for administration.",
    "references": "1. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- governance-strategy#gs-1-define-asset-management-and-data-protection- strategy 4. https://docs.microsoft.com/en-us/azure/aks/concepts-clusters- workloads#:~:text=Kubernetes%20resources%2C%20such%20as%20pods,or% 20manage%20access%20to%20resources.&text=When%20you%20interact%20 with%20the,used%20when%20none%20is%20specified.",
    "function_names": [
      "kubernetes_namespace_isolation_enabled",
      "kubernetes_namespace_admin_boundaries_enabled",
      "kubernetes_namespace_resource_separation_enabled",
      "kubernetes_namespace_security_boundaries_enabled",
      "kubernetes_namespace_isolation_for_resources_enabled"
    ]
  },
  {
    "id": "4.6.2",
    "title": "Apply Security Context to Your Pods and Containers",
    "assessment": "Manual",
    "description": "Apply Security Context to Your Pods and Containers",
    "rationale": "A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",
    "audit": "Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.",
    "remediation": "As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we recommend implementing a more restrictive policy such as this: apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' spec: privileged: false # Required to prevent escalations to root. allowPrivilegeEscalation: false # This is redundant with non-root + disallow privilege escalation, # but we can provide it for defense in depth. requiredDropCapabilities: - ALL # Allow core volume types. volumes: - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' # Assume that persistentVolumes set up by the cluster admin are safe to use. - 'persistentVolumeClaim' hostNetwork: false hostIPC: false hostPID: false runAsUser: # Require the container to run without root privileges. rule: 'MustRunAsNonRoot' seLinux: # This policy assumes the nodes are using AppArmor rather than SELinux. rule: 'RunAsAny' supplementalGroups: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 readOnlyRootFilesystem: false This policy prevents pods from running as privileged or escalating privileges. It also restricts the types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar, approach is to start with policy that locks everything down and incrementally add exceptions for applications that need looser restrictions such as logging agents which need the ability to mount a host path. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 2",
    "impact": "If you incorrectly apply security contexts, you may have trouble running the pods.",
    "references": "1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "kubernetes_pod_security_context_configured",
      "kubernetes_container_security_context_configured",
      "kubernetes_pod_read_only_root_filesystem_enabled",
      "kubernetes_container_read_only_root_filesystem_enabled",
      "kubernetes_pod_run_as_non_root_enabled",
      "kubernetes_container_run_as_non_root_enabled",
      "kubernetes_pod_capabilities_dropped",
      "kubernetes_container_capabilities_dropped",
      "kubernetes_pod_privilege_escalation_disabled",
      "kubernetes_container_privilege_escalation_disabled"
    ]
  },
  {
    "id": "4.6.3",
    "title": "The default namespace should not be used",
    "assessment": "Automated",
    "description": "Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.",
    "rationale": "Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None",
    "audit": "Run this command to list objects in default namespace kubectl get all -n default The only entries there should be system managed resources such as the kubernetes service",
    "remediation": "Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "profile_applicability": "•  Level 2",
    "impact": "None",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-3-establish-secure-configurations-for- compute-resources",
    "function_names": [
      "kubernetes_namespace_default_not_used",
      "kubernetes_namespace_default_avoided",
      "kubernetes_namespace_non_default_required",
      "kubernetes_namespace_default_prohibited",
      "kubernetes_namespace_custom_required"
    ]
  },
  {
    "id": "5.1.1",
    "title": "Ensure Image Vulnerability Scanning using Microsoft Defender for Cloud (MDC) image scanning or a third party provider",
    "assessment": "Automated",
    "description": "Scan images being deployed to Azure (AKS) for vulnerabilities. Vulnerability scanning for images stored in Microsoft Defender for Cloud (MDC). This capability is powered by Microsoft Defender for Endpoint's MDVM, a leading provider of information security. When you push an image to Container Registry, MDC automatically scans it, then checks for known vulnerabilities in packages or dependencies defined in the file. When the scan completes (after about 10 minutes), MDC provides details and a security classification for each vulnerability detected, along with guidance on how to remediate issues and protect vulnerable attack surfaces.",
    "rationale": "Vulnerabilities in software packages can be exploited by hackers or malicious users to obtain unauthorized access to local cloud resources. Impact: When using an MDC, you might occasionally encounter problems. For example, you might not be able to pull a container image because of an issue with Docker in your local environment. Or, a network issue might prevent you from connecting to the registry.",
    "audit": "Check MDC for Container Registries: This command shows whether container registries is enabled, which includes the image scanning feature. az security pricing show --name ContainerRegistry or az security pricing list --query \"[ type=='Microsoft.ContainerRegistry/registries'].{Name:name, Status:properties.status}\" -o table",
    "remediation": "Enable MDC for Container Registries: If you find that container registries is not enabled and you wish to enable it, you can do so using the following command: az security pricing create --name ContainerRegistry --tier Standard or az resource update --ids /subscriptions/{subscription- id}/resourceGroups/{resource-group- name}/providers/Microsoft.ContainerRegistry/registries/{registry-name} --set properties.enabled=true replacing subscription-id, resource-group-name and registry-name with the appropriate values. Please note, enabling MDC for container registries incurs additional costs, so be sure to review the pricing details on the official Azure documentation before enabling it. Default Value: Images are not scanned by Default. References: 1. https://docs.microsoft.com/en-us/azure/security-center/defender-for-container- registries-usage 2. https://docs.microsoft.com/en-us/azure/container-registry/container-registry- check-health 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-6-perform-software-vulnerability- assessments",
    "profile_applicability": "•  Level 1",
    "impact": "When using an MDC, you might occasionally encounter problems. For example, you might not be able to pull a container image because of an issue with Docker in your local environment. Or, a network issue might prevent you from connecting to the registry.",
    "references": "1. https://docs.microsoft.com/en-us/azure/security-center/defender-for-container- registries-usage 2. https://docs.microsoft.com/en-us/azure/container-registry/container-registry- check-health 3. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- posture-vulnerability-management#pv-6-perform-software-vulnerability- assessments",
    "function_names": [
      "container_registry_image_vulnerability_scanning_enabled",
      "container_registry_image_scanning_defender_enabled",
      "container_registry_image_scanning_third_party_enabled",
      "container_registry_image_scan_automated",
      "container_registry_image_scan_remediation_guidance_provided",
      "container_registry_image_scan_security_classification_provided",
      "container_registry_image_scan_defender_endpoint_integrated",
      "container_registry_image_scan_vulnerability_details_provided"
    ]
  },
  {
    "id": "5.1.2",
    "title": "Minimize user access to Azure Container Registry (ACR)",
    "assessment": "Manual",
    "description": "Restrict user access to Azure Container Registry (ACR), limiting interaction with build images to only authorized personnel and service accounts.",
    "rationale": "Weak access control to Azure Container Registry (ACR) may allow malicious users to replace built images with vulnerable containers. Impact: Care should be taken not to remove access to Azure ACR for accounts that require this for their operation.",
    "audit": "",
    "remediation": "Azure Container Registry If you use Azure Container Registry (ACR) as your container image store, you need to grant permissions to the service principal for your AKS cluster to read and pull images. Currently, the recommended configuration is to use the az aks create or az aks update command to integrate with a registry and assign the appropriate role for the service principal. For detailed steps, see Authenticate with Azure Container Registry from Azure Kubernetes Service. To avoid needing an Owner or Azure account administrator role, you can configure a service principal manually or use an existing service principal to authenticate ACR from AKS. For more information, see ACR authentication with service principals or Authenticate from Kubernetes with a pull secret. References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken not to remove access to Azure ACR for accounts that require this for their operation.",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "function_names": [
      "acr_registry_access_restricted",
      "acr_registry_user_access_minimized",
      "acr_registry_authorized_accounts_only",
      "acr_registry_build_images_restricted",
      "acr_registry_service_accounts_only"
    ]
  },
  {
    "id": "5.1.3",
    "title": "Minimize cluster access to read-only for Azure Container Registry (ACR)",
    "assessment": "Manual",
    "description": "Configure the Cluster Service Account with Storage Object Viewer Role to only allow read-only access to Azure Container Registry (ACR)",
    "rationale": "The Cluster Service Account does not require administrative access to Azure ACR, only requiring pull access to containers to deploy onto Azure AKS. Restricting permissions follows the principles of least privilege and prevents credentials from being abused beyond the required role. Impact: A separate dedicated service account may be required for use by build servers and other robot users pushing or managing container images.",
    "audit": "",
    "remediation": "References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-2-protect-sensitive-data",
    "profile_applicability": "•  Level 1",
    "impact": "A separate dedicated service account may be required for use by build servers and other robot users pushing or managing container images.",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-2-protect-sensitive-data",
    "function_names": [
      "container_registry_cluster_read_only_access",
      "container_registry_cluster_storage_object_viewer_role",
      "container_registry_cluster_minimal_access",
      "container_registry_cluster_no_write_access",
      "container_registry_cluster_service_account_read_only"
    ]
  },
  {
    "id": "5.1.4",
    "title": "Minimize Container Registries to only those approved",
    "assessment": "Manual",
    "description": "Use approved container registries.",
    "rationale": "Allowing unrestricted access to external container registries provides the opportunity for malicious or unapproved containers to be deployed into the cluster. Allowlisting only approved container registries reduces this risk. Impact: All container images to be deployed to the cluster must be hosted within an approved container image registry.",
    "audit": "",
    "remediation": "If you are using Azure Container Registry you have this option: https://docs.microsoft.com/en-us/azure/container-registry/container-registry-firewall- access-rules For other non-AKS repos using admission controllers or Azure Policy will also work. Limiting or locking down egress traffic is also recommended: https://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-asset- management#am-6-use-only-approved-applications-in-compute-resources 2. https://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic 3. https://docs.microsoft.com/en-us/azure/container-registry/container-registry- firewall-access-rules",
    "profile_applicability": "•  Level 2",
    "impact": "All container images to be deployed to the cluster must be hosted within an approved container image registry.",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-asset- management#am-6-use-only-approved-applications-in-compute-resources 2. https://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic 3. https://docs.microsoft.com/en-us/azure/container-registry/container-registry- firewall-access-rules",
    "function_names": [
      "compute_container_registry_approved_only",
      "compute_registry_approved_list",
      "container_registry_approved_sources",
      "compute_registry_approved_only",
      "container_registry_approved_whitelist"
    ]
  },
  {
    "id": "5.2.1",
    "title": "Prefer using dedicated AKS Service Accounts",
    "assessment": "Manual",
    "description": "Kubernetes workloads should not use cluster node service accounts to authenticate to Azure AKS APIs. Each Kubernetes workload that needs to authenticate to other Azure Web Services using IAM should be provisioned with a dedicated Service account.",
    "rationale": "Manual approaches for authenticating Kubernetes workloads running on Azure AKS against Azure APIs are: storing service account keys as a Kubernetes secret (which introduces manual key rotation and potential for key compromise); or use of the underlying nodes' IAM Service account, which violates the principle of least privilege on a multi-tenanted node, when one pod needs to have access to a service, but every other pod on the node that uses the Service account does not.",
    "audit": "For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults.",
    "remediation": "Azure Active Directory integration The security of AKS clusters can be enhanced with the integration of Azure Active Directory (AD). Built on decades of enterprise identity management, Azure AD is a multi-tenant, cloud-based directory, and identity management service that combines core directory services, application access management, and identity protection. With Azure AD, you can integrate on-premises identities into AKS clusters to provide a single source for account management and security. Azure Active Directory integration with AKS clusters With Azure AD-integrated AKS clusters, you can grant users or groups access to Kubernetes resources within a namespace or across the cluster. To obtain a kubectl configuration context, a user can run the az aks get-credentials command. When a user then interacts with the AKS cluster with kubectl, they're prompted to sign in with their Azure AD credentials. This approach provides a single source for user account management and password credentials. The user can only access the resources as defined by the cluster administrator. Azure AD authentication is provided to AKS clusters with OpenID Connect. OpenID Connect is an identity layer built on top of the OAuth 2.0 protocol. For more information on OpenID Connect, see the Open ID connect documentation. From inside of the Kubernetes cluster, Webhook Token Authentication is used to verify authentication tokens. Webhook token authentication is configured and managed as part of the AKS cluster. References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-2-manage-application-identities-securely-and- automatically",
    "profile_applicability": "•  Level 1",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- identity-management#im-2-manage-application-identities-securely-and- automatically",
    "function_names": [
      "aks_workload_dedicated_service_account",
      "aks_workload_no_node_service_account",
      "aks_service_account_workload_isolation",
      "aks_workload_iam_dedicated_credentials",
      "aks_workload_service_account_unique_per_pod"
    ]
  },
  {
    "id": "5.3.1",
    "title": "Ensure Kubernetes Secrets are encrypted",
    "assessment": "Manual",
    "description": "Encryption at Rest is a common security requirement. In Azure, organizations can encrypt data at rest without the risk or cost of a custom key management solution. Organizations have the option of letting Azure completely manage Encryption at Rest. Additionally, organizations have various options to closely manage encryption or encryption keys.",
    "rationale": "",
    "audit": "",
    "remediation": "References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-5-encrypt-sensitive-data-at-rest",
    "profile_applicability": "•  Level 1",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-5-encrypt-sensitive-data-at-rest",
    "function_names": [
      "kubernetes_secret_encryption_enabled",
      "kubernetes_secret_encryption_azure_managed",
      "kubernetes_secret_encryption_custom_key_managed",
      "kubernetes_secret_encryption_at_rest_enabled",
      "kubernetes_secret_encryption_key_rotation_enabled"
    ]
  },
  {
    "id": "5.4.1",
    "title": "Restrict Access to the Control Plane Endpoint",
    "assessment": "Automated",
    "description": "Enable Endpoint Private Access to restrict access to the cluster's control plane to only an allowlist of authorized IPs.",
    "rationale": "Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your cluster's control plane. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster's control plane from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network. Restricting access to an authorized network can provide additional security benefits for your container cluster, including: • Better protection from outsider attacks: Authorized networks provide an additional layer of security by limiting external access to a specific set of addresses you designate, such as those that originate from your premises. This helps protect access to your cluster in the case of a vulnerability in the cluster's authentication or authorization mechanism. • Better protection from insider attacks: Authorized networks help protect your cluster from accidental leaks of master certificates from your company's premises. Leaked certificates used from outside Azure virtual machines and outside the authorized IP ranges (for example, from addresses outside your company) are still denied access. Impact: When implementing Endpoint Private Access, be careful to ensure all desired networks are on the allowlist (whitelist) to prevent inadvertently blocking external access to your cluster's control plane. Limitations IP authorized ranges can't be applied to the private api server endpoint, they only apply to the public API server Availability Zones are currently supported for certain regions. Azure Private Link service limitations apply to private clusters. No support for Azure DevOps Microsoft-hosted Agents with private clusters. Consider to use Self- hosted Agents. For customers that need to enable Azure Container Registry to work with private AKS, the Container Registry virtual network must be peered with the agent cluster virtual network.",
    "audit": "Check for the following to be 'enabled: true' export CLUSTER_NAME=<your cluster name> export RESOURCE_GROUP=<your resource group name> az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"apiServerAccessProfile.enablePublicFqdn\" This command queries for the enablePublicFqdn property within the apiServerAccessProfile of your AKS cluster. The output will be true if endpointPublicAccess is enabled, allowing access to the AKS cluster API server from the internet. If it's false, endpointPublicAccess is disabled, meaning the API server is not accessible over the internet, which is a common configuration for private clusters. az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"apiServerAccessProfile.enablePrivateCluster\" This command queries the enablePrivateCluster property within the apiServerAccessProfile of your AKS cluster. If the output is true, it indicates that endpointPrivateAccess is enabled, and the AKS cluster API server is configured to be accessible only via a private endpoint. If the output is false, the cluster is not configured for private access only, and the API server might be accessible over the internet depending on other settings. Check for the following is not null: az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"apiServerAccessProfile.authorizedIpRanges\" This command queries for the authorizedIpRanges property within the apiServerAccessProfile of your AKS cluster. The output will list the IP ranges that are authorized to access the AKS cluster's API server over the internet. If the list is empty, it means there are no restrictions, and any IP can access the AKS cluster's API server, assuming other network and security configurations allow it.",
    "remediation": "By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API server stays within your VPC. You can also limit the IP addresses that can access your API server from the internet, or completely disable internet access to the API server. With this in mind, you can update your cluster accordingly using the AKS CLI to ensure that Private Endpoint Access is enabled. If you choose to also enable Public Endpoint Access then you should also configure a list of allowable CIDR blocks, resulting in restricted access from the internet. If you specify no CIDR blocks, then the public API server endpoint is able to receive and process requests from all IP addresses by defaulting to ['0.0.0.0/0']. For example, the following command would enable private access to the Kubernetes API as well as limited public access over the internet from a single IP address (noting the /32 CIDR suffix): Default Value: By default, Endpoint Private Access is disabled. References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic",
    "profile_applicability": "•  Level 1",
    "impact": "When implementing Endpoint Private Access, be careful to ensure all desired networks are on the allowlist (whitelist) to prevent inadvertently blocking external access to your cluster's control plane. Limitations IP authorized ranges can't be applied to the private api server endpoint, they only apply to the public API server Availability Zones are currently supported for certain regions. Azure Private Link service limitations apply to private clusters. No support for Azure DevOps Microsoft-hosted Agents with private clusters. Consider to use Self- hosted Agents. For customers that need to enable Azure Container Registry to work with private AKS, the Container Registry virtual network must be peered with the agent cluster virtual network.",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-1-implement-security-for-internal-traffic",
    "function_names": [
      "eks_cluster_control_plane_private_access_enabled",
      "eks_cluster_control_plane_ip_allowlist_restricted",
      "eks_cluster_control_plane_endpoint_public_access_disabled",
      "eks_cluster_control_plane_network_access_restricted",
      "eks_cluster_control_plane_private_link_enabled"
    ]
  },
  {
    "id": "5.4.2",
    "title": "Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled",
    "assessment": "Automated",
    "description": "Disable access to the Kubernetes API from outside the node network if it is not required.",
    "rationale": "In a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's wirtual network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's virtual network. Although Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's virtual network to perform any attack on the Kubernetes API.",
    "audit": "Check for the following to be 'enabled: false ' export CLUSTER_NAME=<your cluster name> export RESOURCE_GROUP=<your resource group name> az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"apiServerAccessProfile.enablePublicFqdn\" This command queries for the enablePublicFqdn property within the apiServerAccessProfile of your AKS cluster. The output will be true if endpointPublicAccess is enabled, allowing access to the AKS cluster API server from the internet. If it's false, endpointPublicAccess is disabled, meaning the API server is not accessible over the internet, which is a common configuration for private clusters. Check for the following to be 'enabled: true ' az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"apiServerAccessProfile.enablePrivateCluster\" This command queries the enablePrivateCluster property within the apiServerAccessProfile of your AKS cluster. If the output is true, it indicates that endpointPrivateAccess is enabled, and the AKS cluster API server is configured to be accessible only via a private endpoint. If the output is false, the cluster is not configured for private access only, and the API server might be accessible over the internet depending on other settings.",
    "remediation": "To use a private endpoint, create a new private endpoint in your virtual network then create a link between your virtual network and a new private DNS zone References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-2-connect-private-networks-together 2. https://learn.microsoft.com/en-us/azure/aks/private-clusters",
    "profile_applicability": "•  Level 2",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-2-connect-private-networks-together 2. https://learn.microsoft.com/en-us/azure/aks/private-clusters",
    "function_names": [
      "kubernetes_cluster_private_endpoint_enabled",
      "kubernetes_cluster_public_access_disabled",
      "kubernetes_cluster_api_private_only",
      "kubernetes_cluster_network_access_restricted",
      "kubernetes_cluster_external_access_disabled"
    ]
  },
  {
    "id": "5.4.3",
    "title": "Ensure clusters are created with Private Nodes",
    "assessment": "Automated",
    "description": "Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with no public IP addresses.",
    "rationale": "Disabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts. Impact: To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.",
    "audit": "Check for the following to be 'enabled: true' export CLUSTER_NAME=<your cluster name> export RESOURCE_GROUP=<your resource group name> az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"apiServerAccessProfile.enablePrivateCluster\"",
    "remediation": "az aks create \\ --resource-group <private-cluster-resource-group> \\ --name <private-cluster-name> \\ --load-balancer-sku standard \\ --enable-private-cluster \\ --network-plugin azure \\ --vnet-subnet-id <subnet-id> \\ --docker-bridge-address \\ --dns-service-ip \\ --service-cidr Where --enable-private-cluster is a mandatory flag for a private cluster. References: 1. https://learn.microsoft.com/en-us/azure/aks/private-clusters",
    "profile_applicability": "•  Level 1",
    "impact": "To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.",
    "references": "1. https://learn.microsoft.com/en-us/azure/aks/private-clusters",
    "function_names": [
      "gke_cluster_private_nodes_enabled",
      "gke_node_pool_public_ip_disabled",
      "gke_cluster_node_private_networking",
      "gke_node_no_public_ip_address",
      "gke_cluster_private_only_nodes",
      "gke_node_pool_private_ip_only",
      "gke_cluster_node_public_ip_disabled",
      "gke_node_private_networking_enabled"
    ]
  },
  {
    "id": "5.4.4",
    "title": "Ensure Network Policy is Enabled and set as appropriate",
    "assessment": "Automated",
    "description": "When you run modern, microservices-based applications in Kubernetes, you often want to control which components can communicate with each other. The principle of least privilege should be applied to how traffic can flow between pods in an Azure Kubernetes Service (AKS) cluster. Let's say you likely want to block traffic directly to back-end applications. The Network Policy feature in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster.",
    "rationale": "All pods in an AKS cluster can send and receive traffic without limitations, by default. To improve security, you can define rules that control the flow of traffic. Back-end applications are often only exposed to required front-end services, for example. Or, database components are only accessible to the application tiers that connect to them. Network Policy is a Kubernetes specification that defines access policies for communication between Pods. Using Network Policies, you define an ordered set of rules to send and receive traffic and apply them to a collection of pods that match one or more label selectors. These network policy rules are defined as YAML manifests. Network policies can be included as part of a wider manifest that also creates a deployment or service. Impact: Network Policy requires the Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an existing cluster, needs to be added prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block other operations on the cluster (including delete) until it has run to completion. If Network Policy is used, a cluster must have at least 2 nodes of type n1-standard-1 or higher. The recommended minimum size cluster to run Network Policy enforcement is 3 n1-standard-1 instances. Enabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory footprint of the kube-system process by approximately 128MB, and requires approximately 300 millicores of CPU.",
    "audit": "Check for the following is not null and set with appropriate group id: export CLUSTER_NAME=<your cluster name> az aks show --name ${CLUSTER_NAME} --resource-group ${RESOURCE_GROUP} --query \"networkProfile.networkPolicy\"",
    "remediation": "Utilize Calico or other network policy engine to segment and isolate your traffic. Default Value: By default, Network Policy is disabled. References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-2-connect-private-networks-together 2. https://docs.microsoft.com/en-us/azure/aks/use-network-policies",
    "profile_applicability": "•  Level 1",
    "impact": "Network Policy requires the Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an existing cluster, needs to be added prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block other operations on the cluster (including delete) until it has run to completion. If Network Policy is used, a cluster must have at least 2 nodes of type n1-standard-1 or higher. The recommended minimum size cluster to run Network Policy enforcement is 3 n1-standard-1 instances. Enabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory footprint of the kube-system process by approximately 128MB, and requires approximately 300 millicores of CPU.",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- network-security#ns-2-connect-private-networks-together 2. https://docs.microsoft.com/en-us/azure/aks/use-network-policies",
    "function_names": [
      "kubernetes_network_policy_enabled",
      "kubernetes_network_policy_ingress_restricted",
      "kubernetes_network_policy_egress_restricted",
      "kubernetes_network_policy_least_privilege",
      "kubernetes_network_policy_default_deny",
      "kubernetes_network_policy_pod_isolation",
      "kubernetes_network_policy_backend_protected",
      "kubernetes_network_policy_traffic_flow_controlled"
    ]
  },
  {
    "id": "5.4.5",
    "title": "Encrypt traffic to HTTPS load balancers with TLS certificates",
    "assessment": "Manual",
    "description": "Encrypt traffic to HTTPS load balancers using TLS certificates.",
    "rationale": "Encrypting traffic between users and your Kubernetes workload is fundamental to protecting data sent over the web.",
    "audit": "",
    "remediation": "References: 1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-4-encrypt-sensitive-information-in-transit",
    "profile_applicability": "•  Level 2",
    "references": "1. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data- protection#dp-4-encrypt-sensitive-information-in-transit",
    "function_names": [
      "cloud_cdn_load_balancer_tls_encryption_enabled",
      "cloud_cdn_load_balancer_ssl_certificate_attached",
      "cloud_cdn_load_balancer_https_required",
      "cloud_cdn_load_balancer_min_tls_1_2",
      "cloud_cdn_load_balancer_tls_termination_enabled",
      "cloud_cdn_load_balancer_certificate_valid",
      "cloud_cdn_load_balancer_tls_encryption_enforced",
      "cloud_cdn_load_balancer_secure_protocol_enabled"
    ]
  },
  {
    "id": "5.5.1",
    "title": "Manage Kubernetes RBAC users with Azure AD",
    "assessment": "Manual",
    "description": "Azure Kubernetes Service (AKS) can be configured to use Azure Active Directory (AD) for user authentication. In this configuration, you sign in to an AKS cluster using an Azure AD authentication token. You can also configure Kubernetes role-based access control (Kubernetes RBAC) to limit access to cluster resources based a user's identity or group membership.",
    "rationale": "Kubernetes RBAC and AKS help you secure your cluster access and provide only the minimum required permissions to developers and operators.",
    "audit": "",
    "remediation": "References: 1. https://docs.microsoft.com/en-us/azure/aks/azure-ad-rbac 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "profile_applicability": "•  Level 2",
    "references": "1. https://docs.microsoft.com/en-us/azure/aks/azure-ad-rbac 2. https://docs.microsoft.com/security/benchmark/azure/security-controls-v2- privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle",
    "function_names": [
      "aks_cluster_rbac_azure_ad_integration",
      "aks_cluster_user_authentication_azure_ad_enabled",
      "aks_cluster_rbac_managed_by_azure_ad",
      "aks_cluster_identity_provider_azure_ad_configured",
      "aks_cluster_authentication_azure_ad_required",
      "aks_cluster_rbac_group_membership_managed",
      "aks_cluster_user_access_azure_ad_restricted"
    ]
  },
  {
    "id": "5.5.2",
    "title": "Use Azure RBAC for Kubernetes Authorization",
    "assessment": "Manual",
    "description": "The ability to manage RBAC for Kubernetes resources from Azure gives you the choice to manage RBAC for the cluster resources either using Azure or native Kubernetes mechanisms. When enabled, Azure AD principals will be validated exclusively by Azure RBAC while regular Kubernetes users and service accounts are exclusively validated by Kubernetes RBAC. Azure role-based access control (RBAC) is an authorization system built on Azure Resource Manager that provides fine-grained access management of Azure resources. With Azure RBAC, you create a role definition that outlines the permissions to be applied. You then assign a user or group this role definition via a role assignment for a particular scope. The scope can be an individual resource, a resource group, or across the subscription.",
    "rationale": "Today you can already leverage integrated authentication between Azure Active Directory (Azure AD) and AKS. When enabled, this integration allows customers to use Azure AD users, groups, or service principals as subjects in Kubernetes RBAC. This feature frees you from having to separately manage user identities and credentials for Kubernetes. However, you still have to set up and manage Azure RBAC and Kubernetes RBAC separately. Azure RBAC for Kubernetes Authorization is an approach that allows for the unified management and access control across Azure Resources, AKS, and Kubernetes resources.",
    "audit": "",
    "remediation": "References: 1. https://docs.microsoft.com/en-us/azure/aks/manage-azure-rbac",
    "profile_applicability": "•  Level 2",
    "references": "1. https://docs.microsoft.com/en-us/azure/aks/manage-azure-rbac",
    "function_names": [
      "aks_cluster_azure_rbac_enabled",
      "aks_cluster_native_rbac_disabled",
      "aks_cluster_rbac_scope_validated",
      "aks_cluster_azure_ad_integration_enabled",
      "aks_cluster_role_definition_assigned",
      "aks_cluster_resource_scope_restricted",
      "aks_cluster_rbac_fine_grained_access_enabled",
      "aks_cluster_kubernetes_rbac_exclusive_validation",
      "aks_cluster_azure_rbac_role_assignment_valid",
      "aks_cluster_rbac_azure_ad_principal_validation_enabled"
    ]
  }
]