[
  {
    "id": "3.1.1",
    "title": "Ensure that the kubeconfig file permissions are set to 644 or more restrictive",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_kubeconfig_file_permissions_restrictive",
      "kubernetes_kubeconfig_file_permissions_644_or_stricter",
      "kubernetes_kubeconfig_file_permissions_secure",
      "kubernetes_kubeconfig_file_permissions_cis_compliant",
      "kubernetes_kubeconfig_file_permissions_min_644"
    ]
  },
  {
    "id": "3.1.2",
    "title": "Ensure that the kubelet kubeconfig file ownership is set to root:root",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_kubelet_kubeconfig_root_ownership",
      "kubernetes_kubeconfig_file_root_owner",
      "kubernetes_kubelet_config_root_group",
      "kubernetes_kubeconfig_root_ownership",
      "kubernetes_kubelet_file_root_permissions"
    ]
  },
  {
    "id": "3.1.3",
    "title": "Ensure that the kubelet configuration file has permissions set to 644",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_kubelet_config_file_permissions_644",
      "kubernetes_kubelet_config_file_read_only",
      "kubernetes_kubelet_config_file_restrictive_permissions",
      "kubernetes_kubelet_config_file_protected_access",
      "kubernetes_kubelet_config_file_secure_permissions"
    ],
    "references": "1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://cloud.google.com/kubernetes-engine/docs/concepts/cis-benchmarks CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 3.3 Configure Data Access Control Lists Configure data access control lists based on a user\u2019s need to know. Apply data access control lists, also known as access permissions, to local and remote file systems, databases, and applications. \u25cf \u25cf \u25cf v7 5.2 Maintain Secure Images Maintain secure images or templates for all systems in the enterprise based on the organization's approved configuration standards. Any new system deployment or existing system that becomes compromised should be imaged using one of those images or templates. \u25cf \u25cf"
  },
  {
    "id": "3.1.4",
    "title": "Ensure that the kubelet configuration file ownership is set to root:root",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_kubelet_config_file_ownership_root",
      "kubernetes_kubelet_config_file_permissions_root_only",
      "kubernetes_kubelet_config_file_secure_ownership",
      "kubernetes_kubelet_config_file_root_ownership_enforced",
      "kubernetes_kubelet_config_file_restrictive_ownership"
    ]
  },
  {
    "id": "4.1.1",
    "title": "Ensure that the cluster-admin role is only used where required",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_role_no_cluster_admin",
      "kubernetes_role_cluster_admin_restricted",
      "kubernetes_role_cluster_admin_minimal_usage",
      "kubernetes_role_cluster_admin_least_privilege",
      "kubernetes_role_cluster_admin_required_only"
    ]
  },
  {
    "id": "4.1.2",
    "title": "Minimize access to secrets",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "secrets_manager_secret_access_restricted",
      "secrets_manager_secret_no_public_access",
      "secrets_manager_secret_rotation_enabled",
      "secrets_manager_secret_encryption_enabled",
      "secrets_manager_secret_access_minimized",
      "secrets_manager_secret_policy_least_privilege",
      "secrets_manager_secret_no_wildcard_permissions",
      "secrets_manager_secret_access_logged",
      "secrets_manager_secret_no_anonymous_access",
      "secrets_manager_secret_vpc_endpoint_required"
    ]
  },
  {
    "id": "4.1.3",
    "title": "Minimize wildcard use in Roles and ClusterRoles",
    "assessment": "Automated",
    "remediation": "Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/ CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "kubernetes_role_wildcard_minimized",
      "kubernetes_clusterrole_wildcard_minimized",
      "kubernetes_role_no_wildcard_permissions",
      "kubernetes_clusterrole_no_wildcard_permissions",
      "kubernetes_role_restrict_wildcard_usage",
      "kubernetes_clusterrole_restrict_wildcard_usage",
      "kubernetes_role_wildcard_permissions_disallowed",
      "kubernetes_clusterrole_wildcard_permissions_disallowed"
    ]
  },
  {
    "id": "4.1.4",
    "title": "Ensure that default service accounts are not actively used",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "iam_service_account_no_active_default",
      "iam_service_account_default_unused",
      "compute_service_account_default_disabled",
      "compute_service_account_no_default_usage",
      "iam_service_account_default_no_active_credentials"
    ],
    "references": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 5.3 Disable Dormant Accounts Delete or disable any dormant accounts after a period of 45 days of inactivity, where supported. \u25cf \u25cf \u25cf v7 4.3 Ensure the Use of Dedicated Administrative Accounts Ensure that all users with administrative account access use a dedicated or secondary account for elevated activities. This account should only be used for administrative activities and not internet browsing, email, or similar activities. \u25cf \u25cf \u25cf v7 5.2 Maintain Secure Images Maintain secure images or templates for all systems in the enterprise based on the organization's approved configuration standards. Any new system deployment or existing system that becomes compromised should be imaged using one of those images or templates. \u25cf \u25cf v7 16.9 Disable Dormant Accounts Automatically disable dormant accounts after a set period of inactivity. \u25cf \u25cf \u25cf"
  },
  {
    "id": "4.1.5",
    "title": "Ensure that Service Account Tokens are only mounted where necessary",
    "assessment": "Automated",
    "remediation": "Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "kubernetes_service_account_token_mounted_necessary",
      "kubernetes_service_account_token_unnecessary_mounts",
      "kubernetes_service_account_token_mount_restricted",
      "kubernetes_service_account_token_mount_minimized",
      "kubernetes_service_account_token_mount_required_only"
    ]
  },
  {
    "id": "4.1.6",
    "title": "Avoid use of system:masters group",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "iam_group_system_masters_avoided",
      "iam_group_system_masters_unused",
      "iam_group_system_masters_restricted",
      "iam_group_system_masters_banned",
      "iam_group_system_masters_disabled"
    ],
    "references": "1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38 CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 5.4 Restrict Administrator Privileges to Dedicated Administrator Accounts Restrict administrator privileges to dedicated administrator accounts on enterprise assets. Conduct general computing activities, such as internet browsing, email, and productivity suite use, from the user\u2019s primary, non-privileged account. \u25cf \u25cf \u25cf v7 4 Controlled Use of Administrative Privileges Controlled Use of Administrative Privileges"
  },
  {
    "id": "4.1.7",
    "title": "Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_role_bind_permissions_restricted",
      "kubernetes_role_impersonate_permissions_restricted",
      "kubernetes_role_escalate_permissions_restricted",
      "kubernetes_cluster_bind_permissions_restricted",
      "kubernetes_cluster_impersonate_permissions_restricted",
      "kubernetes_cluster_escalate_permissions_restricted",
      "kubernetes_rbac_bind_permissions_restricted",
      "kubernetes_rbac_impersonate_permissions_restricted",
      "kubernetes_rbac_escalate_permissions_restricted"
    ]
  },
  {
    "id": "4.1.8",
    "title": "Avoid bindings to system:anonymous",
    "assessment": "Automated",
    "remediation": "Identify all clusterrolebindings and rolebindings to the user system:anonymous. Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation. Strongly consider replacing unsafe bindings with an authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles. If there are any unsafe bindings to the user system:anonymous, proceed to delete them after consideration for cluster operations with only necessary, safer bindings. kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME] kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE] Default Value: No clusterrolebindings nor rolebindings with user system:anonymous. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/#discovery-roles CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "iam_role_no_anonymous_bindings",
      "iam_service_account_no_anonymous_bindings",
      "iam_binding_no_anonymous_access",
      "iam_policy_no_anonymous_principal",
      "iam_member_no_anonymous_identity",
      "iam_principal_no_system_anonymous",
      "iam_permission_no_anonymous_grants"
    ]
  },
  {
    "id": "4.1.9",
    "title": "Avoid non-default bindings to system:unauthenticated",
    "assessment": "Automated",
    "remediation": "Identify all non-default clusterrolebindings and rolebindings to the group system:unauthenticated. Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation. Strongly consider replacing non-default, unsafe bindings with an authenticated, user- defined group. Where possible, bind to non-default, user-defined groups with least- privilege roles. If there are any non-default, unsafe bindings to the group system:unauthenticated, proceed to delete them after consideration for cluster operations with only necessary, safer bindings. kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME] kubectl delete rolebinding [ROLE_BINDING_NAME] -- namespace [ROLE_BINDING_NAMESPACE] Default Value: ClusterRoleBindings with group system:unauthenticated: \u2022 system:public-info-viewer No RoleBindings with the group system:unauthenticated. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/#discovery-roles CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "iam_role_no_unauthenticated_bindings",
      "iam_service_account_no_unauthenticated_bindings",
      "iam_policy_no_unauthenticated_access",
      "iam_binding_no_unauthenticated_principal",
      "iam_member_no_unauthenticated_identity",
      "iam_organization_policy_no_unauthenticated_bindings",
      "iam_workload_identity_no_unauthenticated_bindings",
      "iam_custom_role_no_unauthenticated_bindings"
    ],
    "references": "1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/#discovery-roles CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 5.5 Establish and Maintain an Inventory of Service Accounts Establish and maintain an inventory of service accounts. The inventory, at a minimum, must contain department owner, review date, and purpose. Perform service account reviews to validate that all active accounts are authorized, on a recurring schedule at a minimum quarterly, or more frequently. \u25cf \u25cf v7 16.8 Disable Any Unassociated Accounts Disable any account that cannot be associated with a business process or business owner. \u25cf \u25cf \u25cf"
  },
  {
    "id": "4.1.10",
    "title": "Avoid non-default bindings to system:authenticated",
    "assessment": "Automated",
    "remediation": "Identify all non-default clusterrolebindings and rolebindings to the group system:authenticated. Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation. Strongly consider replacing non-default, unsafe bindings with an authenticated, user- defined group. Where possible, bind to non-default, user-defined groups with least- privilege roles. If there are any non-default, unsafe bindings to the group system:authenticated, proceed to delete them after consideration for cluster operations with only necessary, safer bindings. kubectl delete clusterrolebinding [CLUSTER_ROLE_BINDING_NAME] kubectl delete rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE] Default Value: ClusterRoleBindings with group system:authenticated: \u2022 system:basic-user \u2022 system:discovery No RoleBindings with the group system:authenticated. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/#discovery-roles CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "iam_role_no_default_authenticated_bindings",
      "iam_service_account_no_default_authenticated_bindings",
      "iam_policy_no_default_authenticated_bindings",
      "iam_binding_no_default_authenticated_principal",
      "iam_member_no_default_authenticated_principal",
      "iam_organization_policy_no_default_authenticated_bindings",
      "iam_workload_identity_no_default_authenticated_bindings",
      "iam_custom_role_no_default_authenticated_bindings",
      "iam_predefined_role_no_default_authenticated_bindings",
      "iam_domain_restricted_no_default_authenticated_bindings"
    ]
  },
  {
    "id": "4.2.1",
    "title": "Ensure that the cluster enforces Pod Security Standard Baseline profile or stricter for all namespaces.",
    "assessment": "Manual",
    "remediation": "Ensure that Pod Security Admission is in place for every namespace which contains user workloads. Run the following command to enforce the Baseline profile in a namespace: kubectl label namespace <namespace-name> pod- security.kubernetes.io/enforce=baseline Default Value: By default, Pod Security Admission is enabled but no policies are in place. CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "kubernetes_namespace_pod_security_baseline_enabled",
      "kubernetes_namespace_pod_security_strict_enabled",
      "kubernetes_namespace_pod_security_profile_compliance",
      "kubernetes_pod_security_baseline_all_namespaces",
      "kubernetes_pod_security_strict_all_namespaces",
      "kubernetes_namespace_pod_security_min_baseline",
      "kubernetes_pod_security_profile_enforced"
    ]
  },
  {
    "id": "4.3.1",
    "title": "Ensure that the CNI in use supports Network Policies",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "compute_cluster_network_policy_supported",
      "kubernetes_cni_network_policy_enabled",
      "container_network_interface_policy_compliance",
      "k8s_cni_network_policy_required",
      "compute_cni_network_policy_validation",
      "container_network_policy_support_enabled",
      "kubernetes_network_policy_cni_compliance",
      "compute_network_policy_cni_compatible"
    ]
  },
  {
    "id": "4.3.2",
    "title": "Ensure that all Namespaces have Network Policies defined",
    "assessment": "Automated",
    "remediation": "Follow the documentation and create NetworkPolicy objects as needed. See: https://cloud.google.com/kubernetes-engine/docs/how-to/network- policy#creating_a_network_policy for more information. Default Value: By default, network policies are not created. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/network- policy#creating_a_network_policy 2. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 3. https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "kubernetes_namespace_network_policy_defined",
      "kubernetes_namespace_network_policy_required",
      "kubernetes_namespace_network_policy_exists",
      "kubernetes_namespace_network_policy_enforced",
      "kubernetes_namespace_network_policy_configured"
    ],
    "references": "1. https://cloud.google.com/kubernetes-engine/docs/how-to/network- policy#creating_a_network_policy 2. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 3. https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 13.4 Perform Traffic Filtering Between Network Segments Perform traffic filtering between network segments, where appropriate. \u25cf \u25cf v7 14.1 Segment the Network Based on Sensitivity Segment the network based on the label or classification level of the information stored on the servers, locate all sensitive information on separated Virtual Local Area Networks (VLANs). \u25cf \u25cf v7 14.2 Enable Firewall Filtering Between VLANs Enable firewall filtering between VLANs to ensure that only authorized systems are able to communicate with other systems necessary to fulfill their specific responsibilities. \u25cf \u25cf  4.4 Secrets Management"
  },
  {
    "id": "4.4.1",
    "title": "Prefer using secrets as files over secrets as environment variables",
    "assessment": "Automated",
    "remediation": "If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "compute_container_secrets_as_files",
      "compute_pod_secrets_as_files",
      "compute_workload_secrets_as_files",
      "compute_deployment_secrets_as_files",
      "compute_cronjob_secrets_as_files",
      "compute_statefulset_secrets_as_files",
      "compute_daemonset_secrets_as_files",
      "compute_job_secrets_as_files",
      "compute_replicaset_secrets_as_files",
      "compute_replicationcontroller_secrets_as_files"
    ]
  },
  {
    "id": "4.4.2",
    "title": "Consider external secret storage",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "secrets_manager_secret_external_storage_enabled",
      "secrets_manager_secret_rotation_enabled",
      "secrets_manager_secret_access_restricted",
      "secrets_manager_secret_encryption_enabled",
      "secrets_manager_secret_versioning_enabled",
      "secrets_manager_secret_audit_logging_enabled",
      "secrets_manager_secret_cross_account_access_restricted",
      "secrets_manager_secret_secure_transport_required",
      "secrets_manager_secret_immutable_after_creation",
      "secrets_manager_secret_automatic_deletion_disabled"
    ]
  },
  {
    "id": "4.5.1",
    "title": "Configure Image Provenance using ImagePolicyWebhook admission controller",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_image_provenance_required",
      "kubernetes_image_policy_webhook_enabled",
      "admission_controller_image_provenance_enforced",
      "kubernetes_image_webhook_validation_active",
      "image_policy_webhook_provenance_configured"
    ]
  },
  {
    "id": "4.6.1",
    "title": "Create administrative boundaries between resources using namespaces",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "compute_namespace_isolated_resources",
      "compute_namespace_admin_boundaries",
      "compute_namespace_resource_separation",
      "compute_namespace_secure_boundaries",
      "compute_namespace_access_controlled",
      "compute_namespace_resource_isolation",
      "compute_namespace_admin_partitioned",
      "compute_namespace_secure_segmentation"
    ]
  },
  {
    "id": "4.6.2",
    "title": "Ensure that the seccomp profile is set to RuntimeDefault in the pod definitions",
    "assessment": "Automated",
    "remediation": "Use security context to enable the RuntimeDefault seccomp profile in your pod definitions. An example is as below: { \"namespace\": \"kube-system\", \"name\": \"metrics-server-v0.7.0-dbcc8ddf6-gz7d4\", \"seccompProfile\": \"RuntimeDefault\" } Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/security/seccomp/ 2. https://cloud.google.com/kubernetes-engine/docs/concepts/seccomp-in-gke CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "kubernetes_pod_seccomp_profile_runtime_default",
      "kubernetes_pod_seccomp_profile_configured",
      "kubernetes_pod_security_profile_runtime_default",
      "kubernetes_pod_security_seccomp_enabled",
      "kubernetes_pod_security_profile_configured"
    ],
    "references": "1. https://kubernetes.io/docs/tutorials/security/seccomp/ 2. https://cloud.google.com/kubernetes-engine/docs/concepts/seccomp-in-gke CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 16.7 Use Standard Hardening Configuration Templates for Application Infrastructure Use standard, industry-recommended hardening configuration templates for application infrastructure components. This includes underlying servers, databases, and web servers, and applies to cloud containers, Platform as a Service (PaaS) components, and SaaS components. Do not allow in-house developed software to weaken configuration hardening. \u25cf \u25cf v7 5.2 Maintain Secure Images Maintain secure images or templates for all systems in the enterprise based on the organization's approved configuration standards. Any new system deployment or existing system that becomes compromised should be imaged using one of those images or templates. \u25cf \u25cf"
  },
  {
    "id": "4.6.3",
    "title": "Apply Security Context to Pods and Containers",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_pod_security_context_configured",
      "kubernetes_container_security_context_configured",
      "kubernetes_pod_read_only_root_filesystem_enabled",
      "kubernetes_container_read_only_root_filesystem_enabled",
      "kubernetes_pod_run_as_non_root_enabled",
      "kubernetes_container_run_as_non_root_enabled",
      "kubernetes_pod_capabilities_dropped",
      "kubernetes_container_capabilities_dropped",
      "kubernetes_pod_privilege_escalation_disabled",
      "kubernetes_container_privilege_escalation_disabled"
    ]
  },
  {
    "id": "4.6.4",
    "title": "The default namespace should not be used",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "kubernetes_namespace_default_not_used",
      "kubernetes_namespace_default_avoided",
      "kubernetes_namespace_non_default_required",
      "kubernetes_namespace_default_restricted",
      "kubernetes_namespace_default_prohibited"
    ]
  },
  {
    "id": "5.1.1",
    "title": "Ensure Image Vulnerability Scanning is enabled",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "compute_image_vulnerability_scanning_enabled",
      "container_registry_image_scanning_enabled",
      "container_image_vulnerability_scanning_enabled",
      "registry_image_scanning_enabled",
      "image_repository_vulnerability_scanning_enabled",
      "artifact_registry_image_scanning_enabled",
      "image_scanning_enabled",
      "image_vulnerability_scanning_active",
      "image_scanning_enabled_all_repositories",
      "image_scanning_enabled_all_regions"
    ]
  },
  {
    "id": "5.1.2",
    "title": "Minimize user access to Container Image repositories",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "container_registry_repository_user_access_restricted",
      "container_registry_repository_minimal_user_permissions",
      "container_registry_repository_read_only_access_default",
      "container_registry_repository_admin_access_denied",
      "container_registry_repository_anonymous_access_disabled",
      "container_registry_repository_user_access_audited",
      "container_registry_repository_access_policy_enforced",
      "container_registry_repository_user_roles_minimized"
    ],
    "references": "1. https://cloud.google.com/container-registry/docs/ 2. https://cloud.google.com/kubernetes-engine/docs/how-to/service-accounts 3. https://cloud.google.com/kubernetes-engine/docs/how-to/iam 4. https://cloud.google.com/artifact-registry/docs/access-control#grant CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 3.3 Configure Data Access Control Lists Configure data access control lists based on a user\u2019s need to know. Apply data access control lists, also known as access permissions, to local and remote file systems, databases, and applications. \u25cf \u25cf \u25cf v7 14.6 Protect Information through Access Control Lists Protect all information stored on systems with file system, network share, claims, application, or database specific access control lists. These controls will enforce the principle that only authorized individuals should have access to the information based on their need to access the information as a part of their responsibilities. \u25cf \u25cf \u25cf"
  },
  {
    "id": "5.1.3",
    "title": "Minimize cluster access to read-only for Container Image repositories",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "container_registry_image_repository_read_only_access",
      "container_registry_repository_minimize_access",
      "container_registry_repository_read_only_enforced",
      "container_registry_image_repository_access_restricted",
      "container_registry_repository_access_minimized",
      "container_registry_image_repository_read_only",
      "container_registry_repository_access_read_only",
      "container_registry_image_repository_access_minimized"
    ]
  },
  {
    "id": "5.1.4",
    "title": "Ensure only trusted container images are used",
    "assessment": "Manual",
    "remediation": "Using Google Cloud Console 1. Go to Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization. 2. Enable the Binary Authorization API (if disabled). 3. Create an appropriate policy for use with the cluster. See https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance. 4. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 5. Select the cluster for which Binary Authorization is disabled. 6. Under the details pane, within the Security section, click on the pencil icon named Edit Binary Authorization. 7. Check the box next to Enable Binary Authorization. 8. Choose Enforce policy and provide a directory for the policy to be used. 9. Click SAVE CHANGES. Using Command Line: Update the cluster to enable Binary Authorization: gcloud container cluster update <cluster_name> --zone <compute_zone> -- binauthz-evaluation-mode=<evaluation_mode> Example: gcloud container clusters update $CLUSTER_NAME --zone $COMPUTE_ZONE -- binauthz-evaluation-mode=PROJECT_SINGLETON_POLICY_ENFORCE See: https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#-- binauthz-evaluation-mode for more details around the evaluation modes available. Create a Binary Authorization Policy using the Binary Authorization Policy Reference: https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance. Import the policy file into Binary Authorization: gcloud container binauthz policy import <yaml_policy> Default Value: By default, Binary Authorization is disabled along with container registry allowlisting. References: 1. https://cloud.google.com/binary-authorization/docs/policy-yaml-reference 2. https://cloud.google.com/binary-authorization/docs/setting-up CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "container_image_trusted_source",
      "container_image_approved_registry",
      "container_image_vulnerability_scanned",
      "container_image_signed_verified",
      "container_image_immutable_tag",
      "container_image_minimal_base",
      "container_image_no_privileged",
      "container_image_no_root_user",
      "container_image_recently_updated",
      "container_image_no_high_severity_cves"
    ]
  },
  {
    "id": "5.2.1",
    "title": "Ensure GKE clusters are not running using the Compute Engine default service account",
    "assessment": "Automated",
    "remediation": "Using Google Cloud Console: To create a minimally privileged service account: 1. Go to Service Accounts by visiting: https://console.cloud.google.com/iam- admin/serviceaccounts. 2. Click on CREATE SERVICE ACCOUNT. 3. Enter Service Account Details. 4. Click CREATE AND CONTINUE. 5. Within Service Account permissions add the following roles: o Logs Writer. o Monitoring Metric Writer. o `Monitoring Viewer. 6. Click CONTINUE. 7. Grant users access to this service account and create keys as required. 8. Click DONE. To create a Node pool to use the Service account: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Click on the cluster name within which the Node pool will be launched. 3. Click on ADD NODE POOL. 4. Within the Node Pool details, select the Security subheading, and under `Identity defaults, select the minimally privileged service account from the Service Account drop-down. 5. Click `CREATE to launch the Node pool. Note: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation. Using Command Line: To create a minimally privileged service account: gcloud iam service-accounts create <node_sa_name> --display-name \"GKE Node Service Account\" export NODE_SA_EMAIL=gcloud iam service-accounts list --format='value(email)' --filter='displayName:GKE Node Service Account' Grant the following roles to the service account: export PROJECT_ID=gcloud config get-value project gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/monitoring.metricWriter gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/monitoring.viewer gcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/logging.logWriter To create a new Node pool using the Service account, run the following command: gcloud container node-pools create <node_pool> --service- account=<sa_name>@<project_id>.iam.gserviceaccount.com-- cluster=<cluster_name> --zone <compute_zone> Note: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation. Default Value: By default, nodes use the Compute Engine default service account when you create a new cluster. References: 1. https://cloud.google.com/compute/docs/access/service- accounts#compute_engine_default_service_account CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_cluster_default_service_account_not_used",
      "gke_cluster_custom_service_account_required",
      "compute_default_service_account_disabled_for_gke",
      "gke_cluster_service_account_non_default",
      "gke_cluster_service_account_compliance_check"
    ]
  },
  {
    "id": "5.2.2",
    "title": "Prefer using dedicated GCP Service Accounts and Workload Identity",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "iam_service_account_dedicated_usage",
      "iam_service_account_workload_identity_enabled",
      "compute_workload_identity_enabled",
      "iam_workload_identity_federation_enabled",
      "iam_service_account_no_user_impersonation",
      "iam_service_account_no_primitive_roles",
      "iam_service_account_least_privilege",
      "iam_workload_identity_provider_configured",
      "compute_instance_service_account_dedicated",
      "iam_service_account_no_broad_permissions"
    ]
  },
  {
    "id": "5.3.1",
    "title": "Ensure Kubernetes Secrets are encrypted using keys managed in Cloud KMS",
    "assessment": "Automated",
    "remediation": "To enable Application-layer Secrets Encryption, several configuration items are required. These include: \u2022 A key ring \u2022 A key \u2022 A GKE service account with Cloud KMS CryptoKey Encrypter/Decrypter role Once these are created, Application-layer Secrets Encryption can be enabled on an existing or new cluster. Using Google Cloud Console: To create a key 1. Go to Cloud KMS by visiting https://console.cloud.google.com/security/kms. 2. Select CREATE KEY RING. 3. Enter a Key ring name and the region where the keys will be stored. 4. Click CREATE. 5. Enter a Key name and appropriate rotation period within the Create key pane. 6. Click CREATE. To enable on a new cluster 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Click CREATE CLUSTER, and choose the required cluster mode. 3. Within the Security heading, under CLUSTER, check Encrypt secrets at the application layer checkbox. 4. Select the kms key as the customer-managed key and, if prompted, grant permissions to the GKE Service account. 5. Click CREATE. To enable on an existing cluster 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Select the cluster to be updated. 3. Under the Details pane, within the Security heading, click on the pencil named Application-layer secrets encryption. 4. Enable Encrypt secrets at the application layer and choose a kms key. 5. Click SAVE CHANGES. Using Command Line: To create a key: Create a key ring: gcloud kms keyrings create <ring_name> --location <location> --project <key_project_id> Create a key: gcloud kms keys create <key_name> --location <location> --keyring <ring_name> --purpose encryption --project <key_project_id> Grant the Kubernetes Engine Service Agent service account the Cloud KMS CryptoKey Encrypter/Decrypter role: gcloud kms keys add-iam-policy-binding <key_name> --location <location> -- keyring <ring_name> --member serviceAccount:<service_account_name> --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project <key_project_id> To create a new cluster with Application-layer Secrets Encryption: gcloud container clusters create <cluster_name> --cluster-version=latest -- zone <zone> --database-encryption-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKey s/<key_name> --project <cluster_project_id> To enable on an existing cluster: gcloud container clusters update <cluster_name> --zone <zone> --database- encryption-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKey s/<key_name> --project <cluster_project_id> Default Value: By default, Application-layer Secrets Encryption is disabled. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "kubernetes_secret_kms_encryption_enabled",
      "kubernetes_secret_kms_key_managed",
      "kubernetes_secret_kms_encryption_active",
      "kubernetes_secret_kms_key_specified",
      "kubernetes_secret_kms_encryption_enforced"
    ]
  },
  {
    "id": "5.4.1",
    "title": "Ensure the GKE Metadata Server is Enabled",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_cluster_metadata_server_enabled",
      "gke_node_metadata_server_enabled",
      "gke_metadata_server_auth_required",
      "gke_metadata_server_endpoint_secure",
      "gke_metadata_server_access_restricted",
      "gke_metadata_server_hop_limit_enabled",
      "gke_metadata_server_token_required",
      "gke_metadata_server_tls_enabled"
    ],
    "references": "1. https://cloud.google.com/kubernetes-engine/docs/how-to/protecting-cluster- metadata#concealment 2. https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity 3. https://cloud.google.com/kubernetes-engine/docs/concepts/workload-identity CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 16.7 Use Standard Hardening Configuration Templates for Application Infrastructure Use standard, industry-recommended hardening configuration templates for application infrastructure components. This includes underlying servers, databases, and web servers, and applies to cloud containers, Platform as a Service (PaaS) components, and SaaS components. Do not allow in-house developed software to weaken configuration hardening. \u25cf \u25cf v7 5.2 Maintain Secure Images Maintain secure images or templates for all systems in the enterprise based on the organization's approved configuration standards. Any new system deployment or existing system that becomes compromised should be imaged using one of those images or templates. \u25cf \u25cf  5.5 Node Configuration and Maintenance This section contains recommendations relating to node configurations in GKE."
  },
  {
    "id": "5.5.1",
    "title": "Ensure Container-Optimized OS (cos_containerd) is used for GKE Node images",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_node_image_cos_containerd_used",
      "gke_node_image_optimized_os_used",
      "gke_node_image_containerd_required",
      "gke_node_image_secure_os_used",
      "gke_node_image_default_os_used"
    ],
    "impact": "If modifying an existing cluster's Node pool to run COS, the upgrade operation used is long-running and will block other operations on the cluster (including delete) until it has run to completion. COS nodes also provide an option with containerd as the main container runtime directly integrated with Kubernetes instead of docker. Thus, on these nodes, Docker cannot view or access containers or images managed by Kubernetes. Applications should not interact with Docker directly. For general troubleshooting or debugging, use crictl instead. Audit: Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. From the list of clusters, select the cluster under test. 3. Under the 'Node pools' section, make sure that for each of the Node pools, 'Container-Optimized OS (cos_containerd)' is listed in the 'Image type' column. Using Command line: To check Node image type for an existing cluster's Node pool, first define 3 variables for Node Pool, Cluster Name and Zone, and then run the following command: : gcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME -- zone $COMPUTE_ZONE --format json | jq '.config.imageType' The output of the above command should return the following output \"config\": { .. \"imageType\": \"COS_CONTAINERD\", .. } if COS_CONTAINERD is used for Node images. Remediation: Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Select the Kubernetes cluster which does not use COS. 3. Under the Node pools heading, select the Node Pool that requires alteration. 4. Click EDIT. 5. Under the Image Type heading click CHANGE. 6. From the pop-up menu select Container-optimised OS with containerd (cos_containerd) (default) and click CHANGE 7. Repeat for all non-compliant Node pools. Using Command Line: To set the node image to cos for an existing cluster's Node pool: gcloud container clusters upgrade <cluster_name> --image-type cos_containerd --zone <compute_zone> --node-pool <node_pool_name> Default Value: Container-optimised OS with containerd (cos_containerd) (default) is the default option for a cluster node image. References: 1. https://cloud.google.com/kubernetes-engine/docs/concepts/using-containerd 2. https://cloud.google.com/kubernetes-engine/docs/concepts/node-images CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 2.5 Allowlist Authorized Software Use technical controls, such as application allowlisting, to ensure that only authorized software can execute or be accessed. Reassess bi-annually, or more frequently. \u25cf \u25cf v7 5.2 Maintain Secure Images Maintain secure images or templates for all systems in the enterprise based on the organization's approved configuration standards. Any new system deployment or existing system that becomes compromised should be imaged using one of those images or templates. \u25cf \u25cf"
  },
  {
    "id": "5.5.2",
    "title": "Ensure Node Auto-Repair is Enabled for GKE Nodes",
    "assessment": "Automated",
    "remediation": "Using Google Cloud Console 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list 2. Select the Kubernetes cluster containing the node pool for which auto-repair is disabled. 3. Select the Node pool by clicking on the name of the pool. 4. Navigate to the Node pool details pane and click EDIT. 5. Under the Management heading, check the Enable auto-repair box. 6. Click SAVE. 7. Repeat steps 2-6 for every cluster and node pool with auto-upgrade disabled. Using Command Line To enable node auto-repair for an existing cluster's Node pool: gcloud container node-pools update <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --enable-autorepair Default Value: Node auto-repair is enabled by default. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_node_auto_repair_enabled",
      "gke_node_auto_repair_enabled_all_clusters",
      "gke_node_auto_repair_enabled_all_regions",
      "gke_node_auto_repair_enabled_default_pool",
      "gke_node_auto_repair_enabled_custom_pool"
    ]
  },
  {
    "id": "5.5.3",
    "title": "Ensure Node Auto-Upgrade is Enabled for GKE Nodes",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_node_auto_upgrade_enabled",
      "gke_node_auto_upgrade_enabled_all_clusters",
      "gke_node_auto_upgrade_enabled_min_version",
      "gke_node_auto_upgrade_enabled_security_patches",
      "gke_node_auto_upgrade_enabled_latest_stable"
    ],
    "references": "1. https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-upgrades 2. https://cloud.google.com/kubernetes-engine/docs/how-to/maintenance-windows- and-exclusions 3. https://cloud.google.com/kubernetes-engine/docs/concepts/node-images 4. https://cloud.google.com/kubernetes-engine/docs/concepts/node-images Additional Information: Node auto-upgrades is not available for Alpha Clusters. CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 7.3 Perform Automated Operating System Patch Management Perform operating system updates on enterprise assets through automated patch management on a monthly, or more frequent, basis. \u25cf \u25cf \u25cf v7 2.2 Ensure Software is Supported by Vendor Ensure that only software applications or operating systems currently supported by the software's vendor are added to the organization's authorized software inventory. Unsupported software should be tagged as unsupported in the inventory system. \u25cf \u25cf \u25cf v7 3.4 Deploy Automated Operating System Patch Management Tools Deploy automated software update tools in order to ensure that the operating systems are running the most recent security updates provided by the software vendor. \u25cf \u25cf \u25cf v7 3.5 Deploy Automated Software Patch Management Tools Deploy automated software update tools in order to ensure that third-party software on all systems is running the most recent security updates provided by the software vendor. \u25cf \u25cf \u25cf"
  },
  {
    "id": "5.5.4",
    "title": "When creating New Clusters - Automate GKE version management using Release Channels",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_cluster_release_channel_enabled",
      "gke_cluster_auto_version_management_enabled",
      "gke_cluster_release_channel_configured",
      "gke_cluster_version_management_automated",
      "gke_cluster_release_channel_used"
    ]
  },
  {
    "id": "5.5.5",
    "title": "Ensure Shielded GKE Nodes are Enabled",
    "assessment": "Automated",
    "remediation": "Note: From version 1.18, clusters will have Shielded GKE nodes enabled by default. Using Google Cloud Console: To update an existing cluster to use Shielded GKE nodes: 1. Navigate to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Select the cluster which for which Shielded GKE Nodes is to be enabled. 3. With in the Details pane, under the Security heading, click on the pencil icon named Edit Shields GKE nodes. 4. Check the box named Enable Shield GKE nodes. 5. Click SAVE CHANGES. Using Command Line: To migrate an existing cluster, the flag --enable-shielded-nodes needs to be specified in the cluster update command: gcloud container clusters update <cluster_name> --zone <cluster_zone> -- enable-shielded-nodes Default Value: Clusters will have Shielded GKE nodes enabled by default, as of version v1.18 References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_node_shielded_enabled",
      "gke_node_shielded_integrity_monitoring_enabled",
      "gke_node_shielded_secure_boot_enabled",
      "gke_node_shielded_vtpm_enabled",
      "gke_node_shielded_all_features_enabled"
    ]
  },
  {
    "id": "5.5.6",
    "title": "Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled",
    "assessment": "Automated",
    "remediation": "Once a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. New Node pools must be created within the cluster with Integrity Monitoring enabled. Using Google Cloud Console 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list 2. From the list of clusters, click on the cluster requiring the update and click ADD NODE POOL. 3. Ensure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' Heading. 4. Click SAVE. Workloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation Using Command Line To create a Node pool within the cluster with Integrity Monitoring enabled, run the following command: gcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-integrity-monitoring Workloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation Default Value: Integrity Monitoring is disabled by default on GKE clusters. Integrity Monitoring is enabled by default for Shielded GKE Nodes; however, if Secure Boot is enabled at creation time, Integrity Monitoring is disabled. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes 2. https://cloud.google.com/compute/shielded-vm/docs/integrity-monitoring CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_node_shielded_integrity_monitoring_enabled",
      "gke_node_integrity_monitoring_enabled",
      "compute_shielded_node_integrity_monitoring_enabled",
      "gke_shielded_node_integrity_monitoring_enabled",
      "gke_node_integrity_monitoring_active",
      "gke_shielded_node_integrity_monitoring_active",
      "compute_node_integrity_monitoring_enabled",
      "gke_node_shielded_integrity_monitoring_active"
    ]
  },
  {
    "id": "5.5.7",
    "title": "Ensure Secure Boot for Shielded GKE Nodes is Enabled",
    "assessment": "Automated",
    "remediation": "Once a Node pool is provisioned, it cannot be updated to enable Secure Boot. New Node pools must be created within the cluster with Secure Boot enabled. Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list 2. From the list of clusters, click on the cluster requiring the update and click ADD NODE POOL. 3. Ensure that the Secure boot checkbox is checked under the Shielded options Heading. 4. Click SAVE. Workloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools. Using Command Line: To create a Node pool within the cluster with Secure Boot enabled, run the following command: gcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-secure-boot Workloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools. Default Value: By default, Secure Boot is disabled in GKE clusters. By default, Secure Boot is disabled when Shielded GKE Nodes is enabled. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke- nodes#secure_boot 2. https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_node_secure_boot_enabled",
      "gke_shielded_node_secure_boot_enabled",
      "compute_node_secure_boot_enabled",
      "compute_shielded_node_secure_boot_enabled",
      "gke_node_shielded_secure_boot_enabled",
      "gke_shielded_node_boot_integrity_enabled",
      "compute_shielded_node_boot_integrity_enabled"
    ]
  },
  {
    "id": "5.6.1",
    "title": "Enable VPC Flow Logs and Intranode Visibility",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "vpc_flow_logs_enabled",
      "vpc_flow_logs_intranode_visibility_enabled",
      "vpc_flow_logs_all_regions_enabled",
      "vpc_flow_logs_retention_period_over_90d",
      "vpc_flow_logs_encryption_enabled",
      "vpc_flow_logs_destination_s3_bucket_encrypted",
      "vpc_flow_logs_destination_cloudwatch_logs_encrypted",
      "vpc_flow_logs_destination_s3_bucket_logging_enabled",
      "vpc_flow_logs_destination_s3_bucket_versioning_enabled",
      "vpc_flow_logs_destination_s3_bucket_public_access_blocked"
    ]
  },
  {
    "id": "5.6.2",
    "title": "Ensure use of VPC-native clusters",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_cluster_vpc_native_enabled",
      "gke_cluster_non_native_vpc_disabled",
      "gke_cluster_vpc_native_required",
      "gke_cluster_vpc_native_only",
      "gke_cluster_vpc_native_enforced"
    ]
  },
  {
    "id": "5.6.3",
    "title": "Ensure Control Plane Authorized Networks is Enabled",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_cluster_control_plane_authorized_networks_enabled",
      "gke_cluster_control_plane_authorized_networks_restricted",
      "gke_cluster_control_plane_public_access_disabled",
      "gke_cluster_control_plane_private_endpoint_enabled",
      "gke_cluster_control_plane_ip_whitelisting_enabled",
      "gke_cluster_control_plane_network_policy_enabled",
      "gke_cluster_control_plane_authorized_cidr_restricted",
      "gke_cluster_control_plane_external_access_disabled"
    ],
    "impact": "When implementing Control Plane Authorized Networks, be careful to ensure all desired networks are on the allowlist to prevent inadvertently blocking external access to your cluster's control plane. Audit: Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list 2. From the list of clusters, click on the cluster to open the Details page and make sure 'Master authorized networks' is set to 'Enabled'. Using Command Line: To check Master Authorized Networks status for an existing cluster, first define 2 variables Cluster Name and Zone and then run the following command: gcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE -- format json | jq '.masterAuthorizedNetworksConfig' The output should return { \"gcpPublicCidrsAccessEnabled\": true } if Control Plane Authorized Networks is enabled. If Master Authorized Networks is disabled, the above command will return null ({ }). Remediation: Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list 2. Select Kubernetes clusters for which Control Plane Authorized Networks is disabled 3. Within the Details pane, under the Networking heading, click on the pencil icon named Edit control plane authorised networks. 4. Check the box next to Enable control plane authorised networks. 5. Click SAVE CHANGES. Using Command Line: To enable Control Plane Authorized Networks for an existing cluster, run the following command: gcloud container clusters update <cluster_name> --zone <compute_zone> -- enable-master-authorized-networks Along with this, you can list authorized networks using the --master-authorized- networks flag which contains a list of up to 20 external networks that are allowed to connect to your cluster's control plane through HTTPS. You provide these networks as a comma-separated list of addresses in CIDR notation (such as 90.90.100.0/24). Default Value: By default, Control Plane Authorized Networks is disabled. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/authorized-networks 2. https://cloud.google.com/kubernetes-engine/docs/how-to/latest/network-isolation 3. https://cloud.google.com/kubernetes-engine/docs/how-to/latest/network-isolation CIS Controls: Controls Version Control IG 1 IG 2 IG 3 v8 3.3 Configure Data Access Control Lists Configure data access control lists based on a user\u2019s need to know. Apply data access control lists, also known as access permissions, to local and remote file systems, databases, and applications. \u25cf \u25cf \u25cf v7 14.6 Protect Information through Access Control Lists Protect all information stored on systems with file system, network share, claims, application, or database specific access control lists. These controls will enforce the principle that only authorized individuals should have access to the information based on their need to access the information as a part of their responsibilities. \u25cf \u25cf \u25cf"
  },
  {
    "id": "5.6.4",
    "title": "Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_cluster_private_endpoint_enabled",
      "gke_cluster_public_access_disabled",
      "gke_cluster_private_network_only",
      "gke_cluster_endpoint_restricted",
      "gke_cluster_public_access_blocked",
      "gke_cluster_private_endpoint_only",
      "gke_cluster_network_isolation_enabled",
      "gke_cluster_external_access_disabled"
    ]
  },
  {
    "id": "5.6.5",
    "title": "Ensure clusters are created with Private Nodes",
    "assessment": "Automated",
    "remediation": "Once a cluster is created without enabling Private Nodes, it cannot be remediated. Rather the cluster must be recreated. Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Click CREATE CLUSTER. 3. Configure the cluster as required then click Networking under CLUSTER in the navigation pane. 4. Under IPv4 network access, click the Private cluster radio button. 5. Configure the other settings as required, and click CREATE. Using Command Line: To create a cluster with Private Nodes enabled, include the --enable-private-nodes flag within the cluster create command: gcloud container clusters create <cluster_name> --enable-private-nodes Setting this flag also requires the setting of --enable-ip-alias and --master-ipv4- cidr=<master_cidr_range>. Default Value: By default, Private Nodes are disabled. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/latest/network-isolation CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_cluster_private_nodes_enabled",
      "gke_cluster_private_endpoint_enabled",
      "gke_cluster_public_access_disabled",
      "gke_cluster_private_ip_only",
      "gke_cluster_private_network_enabled",
      "gke_cluster_public_ip_disabled",
      "gke_cluster_private_service_connect_enabled",
      "gke_cluster_private_control_plane_enabled"
    ]
  },
  {
    "id": "5.6.6",
    "title": "Consider firewalling GKE worker nodes",
    "assessment": "Manual",
    "remediation": "Using Google Cloud Console: 1. Go to Firewall Rules by visiting: https://console.cloud.google.com/networking/firewalls/list 2. Click CREATE FIREWALL RULE. 3. Configure the firewall rule as required. Ensure the firewall targets the nodes correctly, either selecting the nodes using tags (under Targets, select Specified target tags, and set Target tags to <tag>), or using the Service account associated with node (under Targets, select Specified service account, set Service account scope as appropriate, and Target service account to <service_account>). 4. Click CREATE. Using Command Line: Use the following command to generate firewall rules, setting the variables as appropriate: gcloud compute firewall-rules create <firewall_rule_name> --network <network> --priority <priority> --direction <direction> --action <action> --target-tags <tag> --target-service-accounts <service_account> --source-ranges <source_cidr_range> --source-tags <source_tags> --source-service-accounts <source_service_account> --destination-ranges <destination_cidr_range> -- rules <rules> Default Value: Every VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console: \u2022 The implied allow egress rule: An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets any instance send traffic to any destination, except for traffic blocked by GCP. Outbound access may be restricted by a higher priority firewall rule. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a NAT instance. \u2022 The implied deny ingress rule: An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects all instances by blocking incoming traffic to them. Incoming access may be allowed by a higher priority rule. Note that the default network includes some additional rules that override this one, allowing certain types of incoming traffic. The implied rules cannot be removed, but they have the lowest possible priorities. References: 1. https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture 2. https://cloud.google.com/vpc/docs/using-firewalls 3. https://cloud.google.com/kubernetes-engine/docs/how-to/tags-firewall-policies CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_node_firewall_enabled",
      "gke_node_network_restricted",
      "gke_node_inbound_traffic_limited",
      "gke_node_public_ip_disabled",
      "gke_node_private_network_only",
      "gke_node_firewall_rules_applied",
      "gke_node_network_policy_enforced",
      "gke_node_external_access_blocked"
    ]
  },
  {
    "id": "5.6.7",
    "title": "Ensure use of Google-managed SSL Certificates",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "cloud_cdn_ssl_certificate_google_managed",
      "cloud_cdn_ssl_certificate_managed_by_google",
      "cloud_cdn_ssl_certificate_no_self_managed",
      "cloud_cdn_ssl_certificate_google_managed_only",
      "cloud_cdn_ssl_certificate_automated_management"
    ]
  },
  {
    "id": "5.7.1",
    "title": "Ensure Logging and Cloud Monitoring is Enabled",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "cloudtrail_trail_logging_enabled",
      "cloudtrail_trail_cloud_monitoring_enabled",
      "cloudtrail_trail_logging_and_monitoring_enabled",
      "cloudtrail_trail_logging_enabled_all_regions",
      "cloudtrail_trail_cloud_monitoring_enabled_all_regions",
      "cloudtrail_trail_logging_and_monitoring_enabled_all_regions",
      "cloudtrail_trail_logging_enabled_over_90d",
      "cloudtrail_trail_cloud_monitoring_enabled_over_90d",
      "cloudtrail_trail_logging_and_monitoring_enabled_over_90d",
      "cloudtrail_trail_logging_enabled_min_tls_1_2"
    ]
  },
  {
    "id": "5.7.2",
    "title": "Enable Linux auditd logging",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "compute_instance_auditd_logging_enabled",
      "linux_auditd_logging_enabled",
      "compute_auditd_logging_enabled",
      "linux_auditd_config_enabled",
      "compute_auditd_config_enabled",
      "linux_auditd_logging_active",
      "compute_auditd_logging_active"
    ]
  },
  {
    "id": "5.8.1",
    "title": "Ensure authentication using Client Certificates is Disabled",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "compute_instance_client_certificate_auth_disabled",
      "compute_vm_client_certificate_auth_disabled",
      "cloud_cdn_client_certificate_auth_disabled",
      "load_balancer_client_certificate_auth_disabled",
      "api_gateway_client_certificate_auth_disabled",
      "app_service_client_certificate_auth_disabled",
      "container_service_client_certificate_auth_disabled",
      "kubernetes_client_certificate_auth_disabled",
      "network_client_certificate_auth_disabled",
      "security_group_client_certificate_auth_disabled"
    ]
  },
  {
    "id": "5.8.2",
    "title": "Manage Kubernetes RBAC users with Google Groups for GKE",
    "assessment": "Manual",
    "remediation": "Follow the G Suite Groups instructions at: https://cloud.google.com/kubernetes- engine/docs/how-to/role-based-access-control#google-groups-for-gke. Then, create a cluster with: gcloud container clusters create <cluster_name> --security-group <security_group_name> Finally create Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings that reference the G Suite Groups. Default Value: Google Groups for GKE is disabled by default. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/google-groups-rbac 2. https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access- control CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_cluster_rbac_google_groups_managed",
      "gke_cluster_user_google_groups_required",
      "gke_rbac_google_groups_integration_enabled",
      "gke_cluster_iam_google_groups_enforced",
      "gke_rbac_user_google_groups_only",
      "gke_cluster_access_google_groups_restricted",
      "gke_iam_google_groups_sync_enabled",
      "gke_rbac_google_groups_mapping_required"
    ]
  },
  {
    "id": "5.8.3",
    "title": "Ensure Legacy Authorization (ABAC) is Disabled",
    "assessment": "Automated",
    "remediation": "Using Google Cloud Console: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list. 2. Select Kubernetes clusters for which Legacy Authorization is enabled. 3. Click EDIT. 4. Set 'Legacy Authorization' to 'Disabled'. 5. Click SAVE. Using Command Line: To disable Legacy Authorization for an existing cluster, run the following command: gcloud container clusters update <cluster_name> --zone <compute_zone> --no- enable-legacy-authorization Default Value: Kubernetes Engine clusters running GKE version 1.8 and later disable the legacy authorization system by default, and thus role-based access control permissions take effect with no special action required. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access- control 2. https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your- cluster#leave_abac_disabled_default_for_110 Additional Information: On clusters running GKE 1.6 or 1.7, Kubernetes Service accounts have full permissions on the Kubernetes API by default. To ensure that the role-based access control permissions take effect for a Kubernetes service account, the cluster must be created or updated with the option --no-enable-legacy-authorization. This requirement is removed for clusters running GKE version 1.8 or higher. CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "iam_policy_abac_disabled",
      "iam_role_abac_disabled",
      "iam_user_abac_disabled",
      "iam_group_abac_disabled",
      "iam_permission_abac_disabled",
      "iam_legacy_auth_disabled",
      "iam_abac_policy_disabled",
      "iam_attribute_based_access_disabled"
    ]
  },
  {
    "id": "5.9.1",
    "title": "Enable Customer-Managed Encryption Keys (CMEK) for GKE Persistent Disks (PD)",
    "assessment": "Manual",
    "remediation": "This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster created. Using Google Cloud Console: This is not possible using Google Cloud Console. Using Command Line: Follow the instructions detailed at: https://cloud.google.com/kubernetes- engine/docs/how-to/using-cmek. Default Value: Persistent disks are encrypted at rest by default, but are not encrypted using Customer- Managed Encryption Keys by default. By default, the Compute Engine Persistent Disk CSI Driver is not provisioned within the cluster. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek 2. https://cloud.google.com/compute/docs/disks/customer-managed-encryption 3. https://cloud.google.com/security/encryption-at-rest/default-encryption/ 4. https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes 5. https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "gke_persistent_disk_cmek_enabled",
      "gke_persistent_disk_customer_managed_key_used",
      "gke_persistent_disk_encryption_key_specified",
      "gke_persistent_disk_default_encryption_disabled",
      "gke_persistent_disk_cmek_compliance_checked"
    ]
  },
  {
    "id": "5.9.2",
    "title": "Enable Customer-Managed Encryption Keys (CMEK) for Boot Disks",
    "assessment": "Automated",
    "remediation": "This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster created. Using Google Cloud Console: To create a new node pool: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list 2. Select Kubernetes clusters for which node boot disk CMEK is disabled. 3. Click ADD NODE POOL. 4. In the Nodes section, under machine configuration, ensure Boot disk type is Standard persistent disk or SSD persistent disk. 5. Select Enable customer-managed encryption for Boot Disk and select the Cloud KMS encryption key to be used. 6. Click CREATE. To create a new cluster: 1. Go to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list 2. Click CREATE and click `CONFIGURE for the required cluster mode. 3. Under NODE POOLS, expand the default-pool list and click Nodes. 4. In the Configure node settings pane, select Standard persistent disk or SSD Persistent Disk as the Boot disk type. 5. Select Enable customer-managed encryption for Boot Disk check box and choose the Cloud KMS encryption key to be used. 6. Configure the rest of the cluster settings as required. 7. Click CREATE. Using Command Line: Create a new node pool using customer-managed encryption keys for the node boot disk, of <disk_type> either pd-standard or pd-ssd: gcloud container node-pools create <cluster_name> --disk-type <disk_type> -- boot-disk-kms-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKey s/<key_name> Create a cluster using customer-managed encryption keys for the node boot disk, of <disk_type> either pd-standard or pd-ssd: gcloud container clusters create <cluster_name> --disk-type <disk_type> -- boot-disk-kms-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKey s/<key_name> Default Value: Persistent disks are encrypted at rest by default, but are not encrypted using Customer- Managed Encryption Keys by default. By default, the Compute Engine Persistent Disk CSI Driver is not provisioned within the cluster. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek 2. https://cloud.google.com/compute/docs/disks/customer-managed-encryption 3. https://cloud.google.com/security/encryption-at-rest/default-encryption/ 4. https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes 5. https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "compute_disk_encryption_enabled",
      "compute_disk_cmek_enabled",
      "compute_boot_disk_cmek_enabled",
      "compute_disk_customer_key_required",
      "compute_boot_disk_customer_managed_key",
      "compute_disk_encryption_key_customer_managed",
      "compute_boot_disk_encryption_key_cmek",
      "compute_disk_encryption_key_not_default"
    ]
  },
  {
    "id": "5.10.1",
    "title": "Ensure Kubernetes Web UI is Disabled",
    "assessment": "Automated",
    "remediation": "Using Google Cloud Console: Currently not possible, due to the add-on having been removed. Must use the command line. Using Command Line: To disable the Kubernetes Dashboard on an existing cluster, run the following command: gcloud container clusters update <cluster_name> --zone <zone> --update- addons=KubernetesDashboard=DISABLED Default Value: The Kubernetes web UI (Dashboard) does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI add-on KubernetesDashboard is no longer supported as a managed add-on. References: 1. https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your- cluster#disable_kubernetes_dashboard CIS Controls:",
    "description": "",
    "rationale": "",
    "audit": "",
    "function_names": [
      "compute_kubernetes_web_ui_disabled",
      "kubernetes_dashboard_disabled",
      "compute_kubernetes_ui_access_restricted",
      "kubernetes_web_ui_not_installed",
      "compute_kubernetes_dashboard_deployment_removed"
    ]
  },
  {
    "id": "5.10.2",
    "title": "Ensure that Alpha clusters are not used for production workloads",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "compute_cluster_no_production_workloads",
      "compute_cluster_production_restricted",
      "compute_alpha_cluster_production_disabled",
      "compute_cluster_production_environment_compliance",
      "compute_alpha_cluster_non_production_only"
    ]
  },
  {
    "id": "5.10.3",
    "title": "Consider GKE Sandbox for running untrusted workloads",
    "assessment": "Automated",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "gke_cluster_sandbox_enabled",
      "gke_workload_untrusted_sandbox_required",
      "gke_pod_sandbox_mode_enabled",
      "gke_node_pool_sandbox_isolation_enabled",
      "gke_container_runtime_sandbox_enabled",
      "gke_workload_sandbox_policy_enforced",
      "gke_untrusted_workload_sandbox_required"
    ]
  },
  {
    "id": "5.10.4",
    "title": "Enable Security Posture",
    "assessment": "Manual",
    "description": "",
    "rationale": "",
    "audit": "",
    "remediation": null,
    "function_names": [
      "security_center_security_posture_enabled",
      "security_center_security_posture_monitoring_enabled",
      "security_center_security_posture_continuous_enabled",
      "security_center_security_posture_alerting_enabled",
      "security_center_security_posture_baseline_enabled",
      "security_center_security_posture_compliance_enabled",
      "security_center_security_posture_automated_enabled",
      "security_center_security_posture_realtime_enabled",
      "security_center_security_posture_standards_enabled",
      "security_center_security_posture_remediation_enabled"
    ]
  }
]