[
  {
    "id": "1.1.1",
    "title": "Ensure NGINX is installed",
    "assessment": "Automated",
    "description": "The CIS NGINX Benchmark recommends using the NGINX binary provided by your vendor for most situations. As an alternative, packages from nginx.org are available for a variety of platforms, including Linux and FreeBSD.",
    "rationale": "The main benefits of using NGINX packages from your vendor are: \u2022 Ease of installation \u2022 Dependency resolution \u2022 Increased effectiveness of maintenance and security patches \u2022 Q&A procedures carried out by your vendor",
    "audit": "To check if nginx is installed on your server, run the following command: nginx -v The command output should return the version of nginx that is installed on the server. If there is no output, nginx is not installed.",
    "remediation": "1. Configure and setup Nginx  sudo su dnf update -y && dnf install dnf-utils -y cat << EOF > /etc/yum.repos.d/nginx.repo [nginx-stable] name=nginx stable repo baseurl=http://nginx.org/packages/rhel/8/\\$basearch/ gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true EOF dnf install nginx -y Default Value: NGINX is not installed by default. References: 1. http://nginx.org/en/docs/install.html 2. http://nginx.org/en/linux_packages.html",
    "function_names": [
      "compute_nginx_installed",
      "compute_nginx_vendor_package_used",
      "compute_nginx_org_package_used",
      "compute_nginx_latest_version_installed"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/install.html 2. http://nginx.org/en/linux_packages.html"
  },
  {
    "id": "1.1.2",
    "title": "Ensure NGINX is installed from source",
    "assessment": "Manual",
    "description": "Installing NGINX directly from source allows you to install NGINX without the use of a package manager.",
    "rationale": "Installing NGINX from source allows you to harden your instance of NGINX by minimizing modules. NGINX is unable to remove modules when installed using a package manager. By installing from source, you are able to minimize modules, however, some additional configuration will be required and updates will not be automated out of the box for you. Impact: By installing NGINX from source, you will have to manually upgrade NGINX or automate upgrades yourself. The default values for NGINX may also vary from this guide using this method.",
    "audit": "To check if nginx is installed on your server, run the following command: nginx -v The command output should return the version of nginx that is installed on the server. If there is no output, nginx is not installed.",
    "remediation": "Installation depends on the operating system platform. For a source build, consult the NGINX documentation \"Building nginx from Sources\". Default Value: NGINX is not installed by default. References: 1. https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open- source/",
    "function_names": [
      "compute_nginx_source_installation",
      "compute_nginx_no_package_manager",
      "compute_nginx_source_origin",
      "compute_nginx_direct_installation",
      "compute_nginx_manual_installation"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "impact": "By installing NGINX from source, you will have to manually upgrade NGINX or automate upgrades yourself. The default values for NGINX may also vary from this guide using this method.",
    "references": "1. https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open- source/"
  },
  {
    "id": "1.2.1",
    "title": "Ensure package manager repositories are properly configured",
    "assessment": "Manual",
    "description": "Systems need to have package manager repositories properly configured to ensure they receive the latest patches and updates.",
    "rationale": "If a system's package manager repositories are misconfigured, important patches may not be identified, or a rogue repository could introduce compromised software.",
    "audit": "To verify package manager repositories are configured correctly, run the following commands: Redhat: dnf repolist -v nginx-stable",
    "remediation": "Configure your package manager repositories according to your vendor. As an alternative, package manager repositories from nginx.org are available for a variety of Linux platforms. References: 1. http://nginx.org/en/linux_packages.html Additional Information: Package update and installation commands are based on Red Hat Enterprise Linux 8. If using a different Linux distribution, please substitute with the appropriate command(s).",
    "function_names": [
      "package_manager_repositories_properly_configured",
      "package_manager_repositories_secure_sources",
      "package_manager_repositories_signed_packages",
      "package_manager_repositories_latest_patches",
      "package_manager_repositories_no_insecure_sources",
      "package_manager_repositories_verified_sources",
      "package_manager_repositories_update_enabled",
      "package_manager_repositories_secure_configuration"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/linux_packages.html Additional Information: Package update and installation commands are based on Red Hat Enterprise Linux 8. If using a different Linux distribution, please substitute with the appropriate command(s)."
  },
  {
    "id": "1.2.2",
    "title": "Ensure the latest software package is installed",
    "assessment": "Manual",
    "description": "As new security vulnerabilities are discovered, the corresponding fixes are implemented by your NGINX software package provider. Installing the latest software version ensures these fixes are available on your system.",
    "rationale": "Up-to-date software provides the best possible protection against exploitation of security vulnerabilities, such as the execution of malicious code.",
    "audit": "To verify your NGINX package is up to date, run the following command: Redhat: dnf info nginx",
    "remediation": "To install the latest NGINX package, run the following command: Redhat: dnf update nginx -y References: 1. http://nginx.org/en/linux_packages.html Additional Information: Package update and installation commands are based on Red Hat Enterprise Linux 8. If using a different Linux distribution, please substitute with the appropriate command(s).",
    "function_names": [
      "compute_instance_latest_package_installed",
      "compute_image_latest_package_installed",
      "compute_software_package_up_to_date",
      "compute_system_package_latest_version",
      "compute_vm_latest_patches_installed",
      "compute_os_package_current_version",
      "compute_server_latest_software_installed",
      "compute_instance_package_version_current",
      "compute_software_latest_security_patches",
      "compute_system_latest_package_version"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/linux_packages.html Additional Information: Package update and installation commands are based on Red Hat Enterprise Linux 8. If using a different Linux distribution, please substitute with the appropriate command(s)."
  },
  {
    "id": "2.1.1",
    "title": "Ensure only required modules are installed",
    "assessment": "Manual",
    "description": "This NGINX installation comes with several modules out of the box. These modules are not all always needed. Installations of NGINX should be hardened to ensure only the necessary modules are installed.",
    "rationale": "Minimizing features and functionality built into NGINX can help to reduce the number of vulnerabilities your server has, which reduces the likelihood of a successful compromise by attackers.",
    "audit": "Audit the modules used in your current NGINX build by using the nginx verification command: nginx -V",
    "remediation": "Consult the NGINX module documentation to determine which modules are needed for your specific installation. Modules may be removed using the configure command. References: 1. http://nginx.org/en/docs/configure.html Additional Information: NOTE: NGINX does not support the removal of modules using the dnf method of installation. In order to remove modules from NGINX, you will need to compile it from source. Consider the tradeoff between security hardening and ease of upgrade prior to attempting to build from source.",
    "function_names": [
      "nginx_module_unnecessary_removed",
      "nginx_module_required_only",
      "nginx_module_hardened_config",
      "nginx_module_default_disabled",
      "nginx_module_minimal_installation"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/configure.html Additional Information: NOTE: NGINX does not support the removal of modules using the dnf method of installation. In order to remove modules from NGINX, you will need to compile it from source. Consider the tradeoff between security hardening and ease of upgrade prior to attempting to build from source."
  },
  {
    "id": "2.1.2",
    "title": "Ensure HTTP WebDAV module is not installed",
    "assessment": "Automated",
    "description": "The http_dav_module enables HTTP Extensions for Web Distributed Authoring and Versioning (WebDAV) as defined by RFC 4918. This enables file-based operations on your web server, such as the ability to create, delete, change and move files on your server. Most modern architectures have replaced this functionality with cloud-based object storage, in which case the module should not be installed.",
    "rationale": "WebDAV functionality opens up an unnecessary path for exploiting your web server. Through misconfigurations of WebDAV operations, an attacker may be able to access and manipulate files on the server.",
    "audit": "Run the following command to ensure the http_dav_module is not installed: nginx -V 2>&1 | grep http_dav_module Ensure the output of the command is empty.",
    "remediation": "To remove the http_dav_module, recompile nginx from source without the --with- http_dav_module flag. Default Value: The HTTP WebDAV module is not installed by default when installing from source. It does come by default when installed using dnf. References: 1. http://nginx.org/en/docs/configure.html 2. https://tools.ietf.org/html/rfc4918 Additional Information: NGINX does not support the removal of modules using the dnf method of installation. In order to remove modules from NGINX, you will need to compile from source.",
    "function_names": [
      "compute_http_webdav_module_not_installed",
      "web_server_http_dav_module_disabled",
      "apache_http_dav_module_removed",
      "nginx_http_webdav_extension_uninstalled",
      "server_http_dav_rfc_4918_compliance",
      "webdav_module_http_disabled",
      "http_webdav_module_absent",
      "server_webdav_functionality_disabled"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/configure.html 2. https://tools.ietf.org/html/rfc4918 Additional Information: NGINX does not support the removal of modules using the dnf method of installation. In order to remove modules from NGINX, you will need to compile from source."
  },
  {
    "id": "2.1.3",
    "title": "Ensure modules with gzip functionality are disabled",
    "assessment": "Automated",
    "description": "gzip is used for compression. Compression functionality should be disabled to prevent certain types of attacks from being performed successfully.",
    "rationale": "Compression has been linked with the Breach attack and others. While the Breach attack has been mitigated with modern usages of the HTTP protocol, disabling the use of compression is considered a defense-in-depth strategy to mitigate other attacks.",
    "audit": "Run the following command to ensure gzip modules are not installed: nginx -V 2>&1 | grep -E '(http_gzip_module|http_gzip_static_module)' Ensure the output of the command is empty.",
    "remediation": "In order to disable the http_gzip_module and the http_gzip_static_module, NGINX must be recompiled from source. This can be accomplished using the below command in the folder you used during your original compilation. This must be done without the --with- http_gzip_static_module or --with-http_gzip_module configuration directives. ./configure --without-http_gzip_module --without-http_gzip_static_module Default Value: The http_gzip_module is enabled by default in the source build, and the http_gzip_static_module is not. Only the http_gzip_static_module is enabled by default in the dnf package. References: 1. http://nginx.org/en/docs/configure.html 2. http://nginx.org/en/docs/configure.html 3. http://nginx.org/en/docs/http/ngx_http_gzip_module.html 4. http://nginx.org/en/docs/http/ngx_http_gzip_static_module.html  Additional Information: NGINX does not support the removal of modules using the yum method of installation. In order to remove modules from NGINX, you will need to compile from source.",
    "function_names": [
      "cloud_cdn_module_gzip_disabled",
      "cloud_cdn_module_compression_disabled",
      "cloud_cdn_module_gzip_functionality_disabled",
      "cloud_cdn_module_no_gzip_enabled",
      "cloud_cdn_module_compression_attack_protection_enabled"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/configure.html 2. http://nginx.org/en/docs/configure.html 3. http://nginx.org/en/docs/http/ngx_http_gzip_module.html 4. http://nginx.org/en/docs/http/ngx_http_gzip_static_module.html  Additional Information: NGINX does not support the removal of modules using the yum method of installation. In order to remove modules from NGINX, you will need to compile from source."
  },
  {
    "id": "2.1.4",
    "title": "Ensure the autoindex module is disabled",
    "assessment": "Automated",
    "description": "The autoindex module processes requests ending with the slash character. This feature enables directory listing, which could be useful in attacker reconnaissance, so it should be disabled.",
    "rationale": "Automated directory listings may reveal information helpful to an attacker, such as naming conventions and directory paths. Directory listings may also reveal files that were not intended to be revealed.",
    "audit": "To determine if the autoindex module is disabled, search the NGINX configuration files (nginx.conf and any included configuration files) for autoindex directives: egrep -i '^\\s*autoindex\\s+' /etc/nginx/nginx.conf egrep -i '^\\s*autoindex\\s+' /etc/nginx/conf.d/* Ensure there are no autoindex on directives present.",
    "remediation": "Perform the following to disable the autoindex module: 1. Search the NGINX configuration files (nginx.conf and any included configuration files) to find autoindex directives. egrep -i '^\\s*autoindex\\s+' /etc/nginx/nginx.conf egrep -i '^\\s*autoindex\\s+' /etc/nginx/conf.d/*  2. Set the value for all autoindex directives to off, or remove those directives. Default Value: This module is not enabled by default. References: 1. http://nginx.org/en/docs/http/ngx_http_autoindex_module.html",
    "function_names": [
      "cloud_cdn_distribution_autoindex_disabled",
      "cloud_cdn_cache_autoindex_disabled",
      "cloud_cdn_origin_autoindex_disabled",
      "cloud_cdn_behavior_autoindex_disabled",
      "cloud_cdn_response_autoindex_disabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver",
    "references": "1. http://nginx.org/en/docs/http/ngx_http_autoindex_module.html"
  },
  {
    "id": "2.2.1",
    "title": "Ensure that NGINX is run using a non-privileged, dedicated service account",
    "assessment": "Automated",
    "description": "The nginx user directive designates which user account nginx worker processes run under. Ensuring a non-privileged, dedicated service account is used is a defense in depth measure to limit what an attacker who compromises the account can do.",
    "rationale": "Running a web server under a non-privileged, dedicated service account helps mitigate the risk of lateral movement to other services or processes in the event the user account running the web services is compromised. The default user nobody is typically used for several processes, and if this is compromised, it could allow an attacker to have access to all processes running as that user.",
    "audit": "Run the following to verify nginx is being run by a dedicated non-privileged user account: Step 1: Verify nginx is being run as a dedicated user: grep -Pi -- '^\\h*user\\h+[^;\\n\\r]+\\h*;.*$' /etc/nginx/nginx.conf If a user directive similar to the below is not found, this is not a dedicated user. If a user is found similar to the output shown below, continue to step 2. If the user does not exist, a user will need to be added. user  nginx; Step 2: Verify the nginx dedicated user is not privileged: Run the below command, replacing nginx with any designated user you may have assigned: sudo -l -U nginx The output should look similar to the below if this user is not privileged: User nginx is not allowed to run sudo Step 3: Verify the nginx dedicated user is not part of any unexpected groups: Run the below command, replacing nginx with any designated user you may have assigned:  groups nginx The output should look similar to the below if this user is not part of any other groups than the primary group: nginx : nginx",
    "remediation": "Add a system account for the nginx user with a home directory of /var/cache/nginx and a shell of /sbin/nologin so it does not have the ability to log in, then add the nginx user to be used by nginx: useradd nginx -r -g nginx -d /var/cache/nginx -s /sbin/nologin Then add the nginx user to /etc/nginx/nginx.conf by adding the user directive as shown below: user nginx; Default Value: By default, if nginx is compiled from source, the user and group are nobody. If downloaded from dnf, the user and group nginx and the account are not privileged. References: 1. http://nginx.org/en/docs/ngx_core_module.html#user",
    "function_names": [
      "compute_nginx_non_privileged_account",
      "compute_nginx_dedicated_service_account",
      "compute_nginx_worker_user_restricted",
      "compute_nginx_user_directive_secure",
      "compute_nginx_account_defense_in_depth"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/ngx_core_module.html#user"
  },
  {
    "id": "2.2.2",
    "title": "Ensure the NGINX service account is locked",
    "assessment": "Automated",
    "description": "The nginx user account should have a valid password, but the account should be locked. NOTE: If a different account is used to run nginx, that account's name should be substituted for nginx in the audit and remediation procedures.",
    "rationale": "As a defense-in-depth measure, the nginx user account should be locked to prevent logins and to prevent someone from switching users to nginx using the password. In general, there shouldn't be a need for anyone to have to su as nginx, and when there is a need, sudo should be used instead, which would not require the nginx account password. Impact: This ensures the nginx user account may not be used by a human user.",
    "audit": "Verify the nginx service account is locked by running this command: passwd -S \"$(awk '$1~/^\\s*user\\s*$/ {print $2}' /etc/nginx/nginx.conf | sed - r 's/;.*//g')\" The result should be similar to the following: nginx LK 2022-09-06 -1 -1 -1 -1 (Password locked.)",
    "remediation": "Use the passwd command to lock the nginx service account: passwd -l \"$(awk '$1~/^\\s*user\\s*$/ {print $2}' /etc/nginx/nginx.conf | sed - r 's/;.*//g')\" Default Value: The nginx user is locked by default.",
    "function_names": [
      "compute_service_account_locked",
      "compute_nginx_account_locked",
      "compute_service_account_password_set",
      "compute_nginx_account_password_set",
      "compute_service_account_no_login",
      "compute_nginx_account_no_login",
      "compute_service_account_secure",
      "compute_nginx_account_secure"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "impact": "This ensures the nginx user account may not be used by a human user."
  },
  {
    "id": "2.2.3",
    "title": "Ensure the NGINX service account has an invalid shell",
    "assessment": "Automated",
    "description": "The nginx account should not have the ability to log in, so the /sbin/nologin shell should be set for the account.",
    "rationale": "The account used for nginx should only be used for the nginx service and does not need to have the ability to log in. This prevents an attacker who compromises the account to log in with it.",
    "audit": "Run the following script to verify the nginx service account has an invalid shell:  #!/usr/bin/env bash { l_output=\"\" l_output2=\"\" l_out=\"\" if [ -f /etc/nginx/nginx.conf ]; then l_user=\"$(awk '$1~/^\\s*user\\s*$/ {print $2}' /etc/nginx/nginx.conf | sed -r 's/;.*//g')\" l_valid_shells=\"^($( sed -rn '/^\\//{s,/,\\\\\\\\/,g;p}' /etc/shells | paste -s -d '|' - ))$\" l_out=\"$(awk -v pat=\"$l_valid_shells\" -v ngusr=\"$l_user\" -F: '($(NF) ~ pat && $1==ngusr) { $(NF-1) }' /etc/passwd)\" if [ -z \"$l_out\" ]; then l_output=\" - NGINX user account: \\\"$l_user\\\" has an invalid shell\" else l_output2=\" - NGINX user account: \\\"$l_user\\\" has a valid shell: \\\"$l_out\\\"\" fi else l_output2=\" - NGINX user account can not be determined.\\n - file: \\\"/etc/nginx/nginx.conf\\\" is missing\" fi if [ -z \"$l_output2\" ]; then echo -e \"\\n- Audit Result:\\n  ** PASS **\\n$l_output\\n\" else echo -e \"\\n- Audit Result:\\n  ** FAIL **\\n - Reason(s) for audit failure:\\n$l_output2\\n\" fi }",
    "remediation": "Change the login shell for the nginx account to /sbin/nologin by using the following command: usermod -s /sbin/nologin nginx Default Value: The nginx user has a shell of /sbin/nologin by default on RHEL systems.",
    "function_names": [
      "compute_service_account_invalid_shell",
      "compute_nginx_account_nologin_shell",
      "compute_service_account_nologin_shell",
      "compute_nginx_account_invalid_shell",
      "compute_account_shell_restricted"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "2.3.1",
    "title": "Ensure NGINX directories and files are owned by root",
    "assessment": "Automated",
    "description": "The owner and group of the /etc/nginx directory and its files should be root.",
    "rationale": "Setting ownership to only those users in the root group and the root user will reduce the likelihood of unauthorized modifications to the nginx configuration files.",
    "audit": "Run the following command to verify the ownership of the nginx configuration files: stat /etc/nginx The output should show the ownership and group as root, similar to the output below: Access: (0755/drwxr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)",
    "remediation": "Run the following command to ensure ownership and group ownership is set to root: chown -R root:root /etc/nginx Default Value: The default ownership and group for nginx is root.",
    "function_names": [
      "compute_nginx_directory_root_owned",
      "compute_nginx_file_root_owned",
      "compute_nginx_config_root_owned",
      "compute_nginx_directory_root_group",
      "compute_nginx_file_root_group",
      "compute_nginx_config_root_group"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "2.3.2",
    "title": "Ensure access to NGINX directories and files is restricted",
    "assessment": "Automated",
    "description": "Permissions on the /etc/nginx directory should enforce the principle of least privilege.",
    "rationale": "This ensures that only users who need access to configuration files are able to view them, thus preventing unauthorized access. Other users will need to use sudo in order to access these files.",
    "audit": "To verify the nginx directory has other write permissions revoked, look at the permissions by running the below command: find /etc/nginx -type d -exec stat -Lc \"%n %a\" {} + Verify The output directories are mode 755 or more restrictive: Example /etc/nginx 755 To verify the nginx configuration files have other read, write and execute permissions revoked, look at the permissions by running the below command: find /etc/nginx -type f -exec stat -Lc \"%n %a\" {} + Verify The output files are mode 660 or more restrictive: Example: /etc/nginx/nginx.conf 660",
    "remediation": "Permissions are set with the ability to read as other by default on all configuration files: - rw-r--r-- Permissions are set with the ability to read and execute as other by default on all directories: drwxr-xr-x To set permissions to least privilege on the nginx configuration files, issue these commands:  find /etc/nginx -type d -exec chmod go-w {} + find /etc/nginx -type f -exec chmod ug-x,o-rwx {} + Default Value: Permissions are set with the ability to read as other by default on all configuration files: - rw-r--r-- Permissions are set with the ability to read and execute as other by default on all directories: drwxr-xr-x References: 1. https://dev-sec.io/baselines/nginx/ Additional Information: Note: You should always check your private key permissions after implementing this recommendation. This recommendation assumes the private key has not yet been created and is not in the /etc/nginx directory.",
    "function_names": [
      "nginx_directory_restricted_permissions",
      "nginx_file_restricted_permissions",
      "nginx_directory_least_privilege",
      "nginx_file_least_privilege",
      "nginx_config_directory_secure_permissions",
      "nginx_config_file_secure_permissions",
      "nginx_directory_owner_valid",
      "nginx_file_owner_valid",
      "nginx_directory_group_valid",
      "nginx_file_group_valid"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://dev-sec.io/baselines/nginx/ Additional Information: Note: You should always check your private key permissions after implementing this recommendation. This recommendation assumes the private key has not yet been created and is not in the /etc/nginx directory."
  },
  {
    "id": "2.3.3",
    "title": "Ensure the NGINX process ID (PID) file is secured",
    "assessment": "Automated",
    "description": "The PID file stores the main process ID of the nginx process. This file should be protected from unauthorized modification.",
    "rationale": "The PID file should be owned by root and the group root. It should also be readable to everyone, but only writable by root (permissions 644). This will prevent unauthorized modification of the PID file, which could cause a denial of service.",
    "audit": "Run this command to verify the ownership and permissions of the nginx PID file: stat -L -c \"%U:%G\"  /var/run/nginx.pid && stat -L -c \"%a\" /var/run/nginx.pid The output should show that the PID file is owned by root and has the group root and that the permissions are 644 as shown below: root:root 644",
    "remediation": "If the PID file is not owned by root, issue this command: chown root:root /var/run/nginx.pid If the PID file has permissions greater than 644, issue this command: chmod u-x,go-wx /var/run/nginx.pid Default Value: The PID file is owned by root and has permissions 644 by default when building using dnf.",
    "function_names": [
      "compute_nginx_pid_file_secure_permissions",
      "compute_nginx_pid_file_secure_ownership",
      "compute_nginx_pid_file_secure_location",
      "compute_nginx_pid_file_restricted_access",
      "compute_nginx_pid_file_no_world_writable"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "2.3.4",
    "title": "Ensure the core dump directory is secured",
    "assessment": "Manual",
    "description": "Core dumps are snapshots of memory. The working_directory directive is used to specify the directory NGINX attempts to create core dumps in. Core dumps will be disabled if the directory is not writable by the NGINX user. It is recommended that the working_directory directive be set to a directory that is owned by the root user and the group the NGINX process executes as, and is inaccessible to other users. Usually, production systems should not have this enabled.",
    "rationale": "Core dumps may contain sensitive information that should not be accessible by other accounts on the system.",
    "audit": "Run the following procedure to verify the core dump configuration is secured: Step 1 : Check to see if the working_directory directive is configured: grep working_directory /etc/nginx/nginx.conf Step 2 : If the working_directory directive is enabled, it needs to meet the following requirements: 1. It is not within the NGINX web document root. 2. It is owned by root and has a group ownership of the NGINX group. 3. It has no read-write-search access permission for other users (e.g. o=rwx).",
    "remediation": "Either remove the working_directory directive from the NGINX configuration files or ensure that the configured directory meets the following requirements: 1. It is not within the NGINX web document root. 2. It is owned by root and has a group ownership of the NGINX group: chown root:nginx /var/log/nginx  3. It has no read-write-search access permission for other users:  chmod o-rwx /var/log/nginx Default Value: The working_directory value is not set by default. References: 1. https://www.nginx.com/resources/wiki/start/topics/tutorials/debugging/#core- dump",
    "function_names": [
      "compute_core_dump_directory_secured",
      "compute_core_dump_directory_root_owned",
      "compute_core_dump_directory_restricted_access",
      "compute_core_dump_directory_writable_by_nginx",
      "compute_core_dump_directory_disabled_in_production"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver",
    "references": "1. https://www.nginx.com/resources/wiki/start/topics/tutorials/debugging/#core- dump"
  },
  {
    "id": "2.4.1",
    "title": "Ensure NGINX only listens for network connections on authorized ports",
    "assessment": "Manual",
    "description": "NGINX can be configured to listen on any port, but it should be configured to listen on authorized ports only.",
    "rationale": "Limiting the listening ports to only those that are authorized helps to ensure no unauthorized services are running through the use of NGINX.",
    "audit": "Use this command to audit all listening ports on the server: grep -ir \"listen[^;]*;\"  /etc/nginx The ports being used should immediately follow the listen directive in the output. Ensure all ports that are actively listening and not commented out are authorized for use on the server. The output should look similar to this example: /etc/nginx/nginx.conf.rpmsave:    listen       80; /etc/nginx/nginx.conf.rpmsave:    listen 443 ssl http2; /etc/nginx/conf.d/default.conf:    listen       80; Any files included in the include directive of the nginx configuration file or the nginx configuration file itself should be checked. To check which files are included in the nginx configuration file run the following command: grep include  /etc/nginx/nginx.conf The output below shows that any file matching the pattern /etc/nginx/conf.d/*.conf/ in the above audit check should be considered an auditable port.",
    "remediation": "If any ports are listening that are not authorized, comment out or delete the associated configuration for that listener.  Default Value: Only port 80 is listening by default.",
    "function_names": [
      "compute_nginx_authorized_ports_only",
      "compute_nginx_listener_restricted_ports",
      "nginx_listener_authorized_ports",
      "nginx_listener_restricted_ports",
      "compute_nginx_secure_listener_ports",
      "nginx_secure_listener_ports",
      "compute_nginx_port_restriction",
      "nginx_port_restriction"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "2.4.2",
    "title": "Ensure requests for unknown host names are rejected",
    "assessment": "Automated",
    "description": "Your host header should be part of a predefined allowlist of known good hosts, which enables blocking access to other hosts. You should treat the host header as another input to be validated, as it is defined by the user agent.",
    "rationale": "Allowlisting specific hosts and blocking access to all other hosts, you help to mitigate host header injection attacks against your server. Such attacks could be used by an attacker to redirect you to a rogue host and execute scripts or get you to input credentials. Impact: If you are in an environment such as the cloud, you should not put an IP address or default hostname as your server_name because these addresses are often ephemeral in nature. Additionally, you will be blocked from accessing your site if you use a means of access that does not directly reference names in the server_name directive. You should reserve a DNS name to use for implementing this recommendation.",
    "audit": "Run the following comment to verify this is configured: curl -k -v https://127.0.0.1 -H 'Host: invalid.host.com' If you do not receive a 400 series response, this recommendation is not implemented.",
    "remediation": "Ensure your first server block mirrors the below in your nginx configuration, either at /etc/nginx/nginx.conf or any included file within your nginx config:  server { return 404; } Then investigate each server block to ensure the server_name directive is explicitly defined. Each server block should look similar to the below with the defined hostname of the associated server block in the server_name directive. For example, if your server is cisecurity.org, the configuration should look like the below example: server { listen       443; server_name  cisecurity.org; ..... } Default Value: This is not set by default. References: 1. https://www.acunetix.com/blog/articles/automated-detection-of-host-header- attacks/ 2. https://hackerone.com/reports/94637 3. https://stackoverflow.com/questions/9824328/why-is-nginx-responding-to-any- domain-name",
    "function_names": [
      "cloud_cdn_distribution_host_header_allowlist_enabled",
      "cloud_cdn_distribution_unknown_host_rejection_enabled",
      "cloud_cdn_distribution_host_validation_enabled",
      "cloud_cdn_distribution_known_hosts_allowlist_enforced",
      "cloud_cdn_distribution_host_header_restriction_enabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "impact": "If you are in an environment such as the cloud, you should not put an IP address or default hostname as your server_name because these addresses are often ephemeral in nature. Additionally, you will be blocked from accessing your site if you use a means of access that does not directly reference names in the server_name directive. You should reserve a DNS name to use for implementing this recommendation.",
    "references": "1. https://www.acunetix.com/blog/articles/automated-detection-of-host-header- attacks/ 2. https://hackerone.com/reports/94637 3. https://stackoverflow.com/questions/9824328/why-is-nginx-responding-to-any- domain-name"
  },
  {
    "id": "2.4.3",
    "title": "Ensure keepalive_timeout is 10 seconds or less, but not 0",
    "assessment": "Automated",
    "description": "Persistent connections are leveraged by all modern browsers to facilitate greater web performance. The keep-alive timeout limits the time a persistent connection may remain open. Setting the keep-alive timeout allows this timeout to be controlled on the server side.",
    "rationale": "Setting a keep-alive timeout on the server side helps mitigate denial of service attacks that establish too many persistent connections, exhausting server resources.",
    "audit": "To check the current setting for the keepalive_timeout directive, issue the below command. You should also manually check your nginx configuration for include statements that may be located outside the /etc/nginx directory. If none of these are present, the value is set at the default. grep -ir keepalive_timeout /etc/nginx The output of the command should contain something similar to the following: keepalive_timeout 10;",
    "remediation": "Find the HTTP or server block of your nginx configuration, and add the keepalive_timeout directive. Set it to 10 seconds or less, but not 0. This example command sets it to 10 seconds: keepalive_timeout 10; Default Value: By default, this timeout is dictated by the user agent and varies. It is not set on the server side by default.  References: 1. http://nginx.org/en/docs/http/ngx_http_core_module.html#keepalive_timeout",
    "function_names": [
      "cloud_cdn_distribution_keepalive_timeout_within_limits",
      "cloud_cdn_distribution_keepalive_timeout_configured",
      "cloud_cdn_distribution_keepalive_timeout_valid_range",
      "cloud_cdn_distribution_keepalive_timeout_not_zero",
      "cloud_cdn_distribution_keepalive_timeout_max_10s"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/http/ngx_http_core_module.html#keepalive_timeout"
  },
  {
    "id": "2.4.4",
    "title": "Ensure send_timeout is set to 10 seconds or less, but not 0",
    "assessment": "Automated",
    "description": "The send_timeout directive sets a timeout for transmitting a response to the client between two successive write operations.",
    "rationale": "Setting the send_timeout directive on the server side helps mitigate slow HTTP denial of service attacks by ensuring write operations taking up large amounts of time are closed.",
    "audit": "To check the current setting for the send_timeout directive, issue the below command. You should also manually check your nginx configuration for include statements that may be located outside the /etc/nginx directory. If none of these are present, the value is set at the default. grep -ir send_timeout /etc/nginx The output of the command should be similar to the following: send_timeout  10;",
    "remediation": "Find the HTTP or server block of your nginx configuration, and add the send_timeout directive. Set it to 10 seconds or less, but not 0. send_timeout   10; Default Value: send_timeout 60s; References: 1. https://www.owasp.org/index.php/SCG_WS_nginx 2. http://nginx.org/en/docs/http/ngx_http_core_module.html#send_timeout",
    "function_names": [
      "cloud_cdn_load_balancer_send_timeout_within_10s",
      "cloud_cdn_load_balancer_send_timeout_not_zero",
      "cloud_cdn_load_balancer_send_timeout_configured",
      "cloud_cdn_load_balancer_send_timeout_valid_range",
      "cloud_cdn_load_balancer_send_timeout_less_than_or_equal_10s"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.owasp.org/index.php/SCG_WS_nginx 2. http://nginx.org/en/docs/http/ngx_http_core_module.html#send_timeout"
  },
  {
    "id": "2.5.1",
    "title": "Ensure server_tokens directive is set to `off`",
    "assessment": "Automated",
    "description": "The server_tokens directive is responsible for displaying the NGINX version number and operating system version on error pages and in the Server HTTP response header field. This information should not be displayed.",
    "rationale": "Attackers can conduct reconnaissance on a website using these response headers, then target attacks for specific known vulnerabilities associated with the underlying technologies. Hiding the version will slow down and deter some potential attackers.",
    "audit": "In the NGINX configuration file nginx.conf, verify the server_tokens directive is set to off. To do this, check the response headers for the server header by issuing this command: curl -I 127.0.0.1 | grep -i server The output should not contain the server header providing your server version, such as the below: Server: nginx/1.22.0",
    "remediation": "To disable the server_tokens directive, set it to off inside of every server block in your nginx.conf or in the http block: server { ... server_tokens        off; ... } Default Value: The default value of server_tokens is on.",
    "function_names": [
      "nginx_server_server_tokens_off",
      "nginx_server_version_hidden",
      "nginx_server_header_info_suppressed",
      "nginx_server_error_page_tokens_disabled",
      "nginx_server_http_response_sanitized"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver"
  },
  {
    "id": "2.5.2",
    "title": "Ensure default error and index.html pages do not reference NGINX",
    "assessment": "Automated",
    "description": "The default error and index.html pages for NGINX reveal that the server is NGINX. These default pages should be removed or modified so they do not advertise the underlying infrastructure of the server.",
    "rationale": "By gathering information about the server, attackers can target attacks against its known vulnerabilities. Removing pages that disclose the server runs NGINX helps reduce targeted attacks on the server.",
    "audit": "Locate the error page and index directives in the location block of your server configuration. The default index and error pages in nginx are located at /usr/share/nginx/html/. Open these files and verify there are no references to NGINX. Issue the following commands to check the default pages and verify no results are returned: grep -i nginx /usr/share/nginx/html/index.html grep -i nginx /usr/share/nginx/html/50x.html",
    "remediation": "Edit /usr/share/nginx/html/index.html and usr/share/nginx/html/50x.html and remove any lines that reference NGINX.",
    "function_names": [
      "cloud_cdn_distribution_default_pages_removed",
      "cloud_cdn_distribution_error_page_customized",
      "cloud_cdn_distribution_index_page_customized",
      "cloud_cdn_distribution_nginx_reference_removed",
      "cloud_cdn_distribution_default_content_removed",
      "cloud_cdn_distribution_server_info_hidden",
      "cloud_cdn_distribution_custom_error_pages",
      "cloud_cdn_distribution_custom_index_pages"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "2.5.3",
    "title": "Ensure hidden file serving is disabled",
    "assessment": "Manual",
    "description": "Disabling hidden files is a defense-in-depth mechanism to help prevent accidentally exposing sensitive information.",
    "rationale": "Disabling hidden files prevents an attacker from being able to reference a hidden file that may be put in your location and have sensitive information, like .git files. Impact: This may break well-known hidden files that are needed for functionality. For example, it may prevent functionality used by LetsEncrypt. To enable, configure a location exception like that shown below: location ~ /\\.well-known\\/acme-challenge { allow all; }",
    "audit": "To verify hidden files are disabled, open your nginx configuration file and search for the below string or another regex pattern that denies access to files with a dot as the first character in the file path. Run the following command: grep location /etc/nginx/nginx.conf Verify the output is: location ~ /\\.  { deny all; return 404; }",
    "remediation": "Edit the nginx.conf file and add the following line: location ~ /\\.  { deny all; return 404; } Default Value: This is not set by default. References: 1. https://programming-review.com/nginx-disable-access-to-htaccess-file/",
    "function_names": [
      "cloud_cdn_distribution_hidden_file_serving_disabled",
      "cloud_cdn_distribution_hidden_files_blocked",
      "cloud_cdn_distribution_sensitive_file_serving_disabled",
      "cloud_cdn_distribution_hidden_content_blocked",
      "cloud_cdn_distribution_file_serving_restricted"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver",
    "impact": "This may break well-known hidden files that are needed for functionality. For example, it may prevent functionality used by LetsEncrypt. To enable, configure a location exception like that shown below: location ~ /\\.well-known\\/acme-challenge { allow all; }",
    "references": "1. https://programming-review.com/nginx-disable-access-to-htaccess-file/"
  },
  {
    "id": "2.5.4",
    "title": "Ensure the NGINX reverse proxy does not enable information disclosure",
    "assessment": "Automated",
    "description": "The server and x-powered-by header may specify the underlying technology used by an application. The NGINX reverse proxy may pass these headers if not explicitly directed to remove them.",
    "rationale": "Attackers can conduct reconnaissance on a website using these response headers, then target attacks for specific known vulnerabilities associated with the underlying technologies. Removing these headers will reduce the likelihood of targeted attacks.",
    "audit": "Confirm that the headers are denied as part of the location block of the nginx configuration. You may also have to check included files as part of this configuration. Run this command: grep proxy_hide_header /etc/nginx/nginx.conf The output should read: proxy_hide_header X-Powered-By; proxy_hide_header Server;",
    "remediation": "Implement the below directives as part of your location block. Edit /etc/nginx/nginx.conf and add the following: location /docs { .... proxy_hide_header X-Powered-By; proxy_hide_header Server; .... } Default Value: This is not implemented by default.  References: 1. http://nginx.org/en/docs/http/ngx_http_proxy_module.html",
    "profile_applicability": "\u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/http/ngx_http_proxy_module.html"
  },
  {
    "id": "3.1",
    "title": "Ensure detailed logging is enabled",
    "assessment": "Manual",
    "description": "System logging should be configured to meet your organizational security and privacy policies. Enabling detailed logging to include information about events, event sources, timestamps, and users may assist in incident response activities. NOTE: Aim to keep sensitive information out of logs. For example, keep sensitive information out of query strings and URIs to avoid this.",
    "rationale": "Performing detailed logging ensures that incident responders, auditors, and others are able to clearly view the activity that has occurred on your server. CIS control 8.5: \u201cCollect Detailed Audit Logs\u201d recommends that you configure detailed audit logging for enterprise assets containing sensitive data. It further recommends you include event source, date, username, timestamp, source addresses, destination addresses, and other useful elements that could assist in a forensic investigation.",
    "audit": "Verify your log format meets your organizational security and privacy policies. All necessary logging variables should contain descriptive definitions at /etc/nginx/nginx.conf. Edit the log format directive in /etc/nginx/nginx.conf so it logs everything needed to meet your organizational policies. The final format should resemble something like the below example:  log_format  main   'Event Source Information - Server Name: \"$server_name\" Server Protocol: \"$server_protocol\" Hostname: \"$hostname\" Host: \"$host\" ' 'Date and Timestamp Information - Local Time: \"$time_local\" ISO8601 Time: \"$time_iso8601\" ' 'Username Information -  Basic Authentication User: \"$remote_user\" ' 'Source Address Information - Source Address:Port: \"$remote_addr:$remote_port\" ' 'Destination Address Information - Desination Address:Port \"$server_addr:$server_port \" ' 'Request Information - Request: \"$request\" HTTP Response Status: \"$status\" Referer: \"$http_referer\" Content Type: \"$content_type\" Body Bytes Sent: \"$body_bytes_sent\" User Agent: \"$http_user_agent\" '; The following variables may be considered as useful examples to include in your log_format with descriptive logging. You should consult the NGINX documentation and your organizational policy to ensure you are logging sufficient information and removing sensitive information where needed. The NGINX documentation on supported log variables may be found here: http://nginx.org/en/docs/varindex.html Please review the NGINX documentation for log variables to include in your logs in alignment with your use case and logging policy. Below are some relevant log variables to help focus your review: Event Source Variables: The event source is the name of the software that logs the event. It may frequently be the name of the application or application component that receives the request. $server_name  - Logs the name of the server which accepted a request $server_protocol - Logs the request protocol, usually \u201cHTTP/1.0\u201d, \u201cHTTP/1.1\u201d, or \u201cHTTP/2.0\u201d $hostname -  Logs the hostname $host - Logs the host name from the request line, or host name from the \u201cHost\u201d request header field, or the server name matching a request in that order of precedence. Date & Timestamp Variables: The date and timestamp of the HTTP request used for forensic investigations. $time_local - Logs the local time in the Common Log Format. $time_iso8601 - Logs the local time in the ISO 8601 standard format. Username Variables: $remote_user - Logs the user name supplied with basic authentication if basic authentication is used by NGINX. Nothing will be logged if the request is not part of an application using basic authentication with NGINX. Source Address Variables: Identify the source of the HTTP request $remote_addr - Logs the client IP address of the HTTP request $remote_port - Logs the client port used in the HTTP requests Destination Address Variables: Identify the destination of the HTTP request.  $server_addr - Logs an address of the server which accepted a request $server_port - Logs the port of the server which accepted a request Other useful elements that could assist in a forensic investigation: $request - Logs an address of the server which accepted a request, which may be useful in forensic investigations $status - Logs the response status code of the HTTP request, which may be useful in forensic investigations $http_referer - Logs the response status code of the HTTP request, which may be useful in forensic investigations $http_user_agent  - Logs the user agent of the client behind the request, which may be useful in forensic investigations $pid - Logs the process ID of the worker process of NGINX.",
    "remediation": "Edit the log format directive in /etc/nginx/nginx.conf so it logs everything needed to meet your organizational policies. Default Value: log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; References: 1. http://nginx.org/en/docs/http/ngx_http_log_module.html#log_format 2. http://nginx.org/en/docs/varindex.html 3. https://workbench.cisecurity.org/sections/698300/recommendations/1142990 Additional Information: Note: Load balancers are not source IP transparent. We must configure the X- Forwarded-For Header on the proxy and in the logs to show where the request is coming from.",
    "function_names": [
      "cloudtrail_trail_logging_enabled",
      "cloudtrail_trail_logging_all_regions",
      "cloudtrail_trail_logging_sensitive_data_excluded",
      "cloudtrail_trail_logging_event_details_included",
      "cloudtrail_trail_logging_timestamp_enabled",
      "cloudtrail_trail_logging_user_activity_enabled",
      "cloudtrail_trail_logging_retention_policy_set",
      "cloudtrail_trail_logging_encryption_enabled",
      "cloudtrail_trail_logging_integrity_validation_enabled",
      "cloudtrail_trail_logging_immutable_storage_enabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/http/ngx_http_log_module.html#log_format 2. http://nginx.org/en/docs/varindex.html 3. https://workbench.cisecurity.org/sections/698300/recommendations/1142990 Additional Information: Note: Load balancers are not source IP transparent. We must configure the X- Forwarded-For Header on the proxy and in the logs to show where the request is coming from."
  },
  {
    "id": "3.2",
    "title": "Ensure access logging is enabled",
    "assessment": "Manual",
    "description": "The access_log directive should be on for every core site. It is enabled by default.",
    "rationale": "Access logging allows incident responders and auditors to investigate access to a system in the event of an incident.",
    "audit": "Run the following to verify access logging is enabled: grep -ir access_log /etc/nginx The output should show an access log configured and not disabled. If the output is similar to the below, the nginx configuration file should be manually inspected to ensure you are logging access to all core sites and proxies. access_log off;",
    "remediation": "Ensure the access_log directive is configured for every core site your organization requires logging for. This should look similar to the below configuration snippet. You may use different log file locations based on your needs. access_log  /var/log/nginx/host.access.log  main; Default Value: The access log is enabled by default. References: 1. http://nginx.org/en/docs/http/ngx_http_log_module.html#log_format",
    "function_names": [
      "cloud_cdn_site_access_logging_enabled",
      "cloud_cdn_core_site_access_logging_enabled",
      "cloud_cdn_site_access_logging_enabled_all_regions",
      "cloud_cdn_core_site_access_logging_enabled_all_regions"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/http/ngx_http_log_module.html#log_format"
  },
  {
    "id": "3.3",
    "title": "Ensure error logging is enabled and set to the info logging level",
    "assessment": "Automated",
    "description": "All errors for applications should be logged.",
    "rationale": "Error logging can be useful in identifying an attacker attempting to exploit a system and recreating an attacker's steps. Error logging also helps with identifying possible issues with an application.",
    "audit": "Run the following to verify the error logging configuration in /etc/nginx/nginx.conf: grep error_log /etc/nginx/nginx.conf If there is no output, the output is commented out, or the logging level is set to anything other than info, this recommendation is not implemented.",
    "remediation": "Edit /etc/nginx/nginx.conf so the error_log directive is present and not commented out. The error_log should be configured to the logging location of your choice. The configuration should look similar to the below: error_log  /var/log/nginx/error_log.log  info;",
    "function_names": [
      "cloudtrail_trail_error_logging_enabled",
      "cloudtrail_trail_info_logging_level",
      "cloudtrail_trail_logging_enabled_info_level",
      "cloudtrail_trail_logging_level_info",
      "cloudtrail_trail_error_logging_info_level",
      "cloudtrail_trail_logging_enabled",
      "cloudtrail_trail_logging_level_set",
      "cloudtrail_trail_logging_level_info_enabled",
      "cloudtrail_trail_error_logging_level_info",
      "cloudtrail_trail_logging_info_level_enabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "3.4",
    "title": "Ensure log files are rotated",
    "assessment": "Automated",
    "description": "Log rotation ensures log files do not consume excessive disk space, potentially causing a denial of service.",
    "rationale": "Log files are important to track activity that occurs on your server, but they take up significant amounts of space. Log rotation should be configured in order to ensure the logs do not consume so much disk space that logging becomes unavailable.",
    "audit": "Run the below commands to verify the log rotation configuration. They should show that log compression occurs weekly and log rotation occurs every 13 weeks. cat /etc/logrotate.d/nginx | grep weekly cat /etc/logrotate.d/nginx | grep rotate",
    "remediation": "Follow the below procedure to change the default configuration to the recommended log rotation configuration. You may need to manually edit or change the below command if the configuration is not the default. To change log compression from daily to weekly: sed -i \"s/daily/weekly/\" /etc/logrotate.d/nginx To change log rotation from every year to every 13 weeks:  sed -i \"s/rotate 52/rotate 13/\" /etc/logrotate.d/nginx Default Value: cat /etc/logrotate.d/nginx /var/log/nginx/*.log { daily missingok rotate 52 compress delaycompress notifempty create 640 nginx adm sharedscripts postrotate if [ -f /var/run/nginx.pid ]; then kill -USR1 `cat /var/run/nginx.pid` fi endscript } Additional Information: You should always comply with your organizational log retention policy.",
    "function_names": [
      "compute_instance_log_rotation_enabled",
      "compute_instance_log_rotation_size_based",
      "compute_instance_log_rotation_time_based",
      "cloudtrail_log_rotation_enabled",
      "cloudtrail_log_rotation_retention_set",
      "cloudtrail_log_rotation_size_based",
      "cloudtrail_log_rotation_time_based",
      "cloud_cdn_log_rotation_enabled",
      "cloud_cdn_log_rotation_retention_set",
      "cloud_cdn_log_rotation_size_based",
      "cloud_cdn_log_rotation_time_based",
      "load_balancer_log_rotation_enabled",
      "load_balancer_log_rotation_retention_set",
      "load_balancer_log_rotation_size_based",
      "load_balancer_log_rotation_time_based"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "3.5",
    "title": "Ensure error logs are sent to a remote syslog server",
    "assessment": "Manual",
    "description": "Centralized log management helps ensure logs are forensically sound and are available at a central location for auditing and incident investigation.",
    "rationale": "A centralized logging solution aggregates logs from multiple systems to ensure logs can be referenced in the event systems are thought to be compromised. Centralized log servers are also often used to correlate logs for potential patterns of attack. If a centralized logging solution is not used and systems (and their logs) are believed to be compromised, then logs may not be permitted to be used as evidence.",
    "audit": "Use this command to verify your server is configured for central logging: grep -ir syslog /etc/nginx The output should show the error logs being sent to a central server, similar to the output of the command below. 192.168.2.1 should be replaced with your central log server, and the logging level should be set to info. error_log syslog:server=192.168.2.1 info;",
    "remediation": "To enable central logging for your error logs, add the below line to your server block in your server configuration file. 192.168.2.1 should be replaced with the location of your central log server. error_log syslog:server=192.168.2.1 info; Default Value: Syslog is not configured by default. References: 1. http://nginx.org/en/docs/syslog.html",
    "function_names": [
      "cloudtrail_trail_remote_syslog_enabled",
      "cloudtrail_trail_error_logs_remote_exported",
      "cloudtrail_trail_logs_centralized_syslog",
      "cloudtrail_trail_error_logs_remote_destination",
      "cloudtrail_trail_syslog_server_configured",
      "cloudtrail_trail_logs_remote_syslog_forwarding",
      "cloudtrail_trail_error_logs_external_syslog",
      "cloudtrail_trail_centralized_logging_enabled"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/syslog.html"
  },
  {
    "id": "3.6",
    "title": "Ensure access logs are sent to a remote syslog server",
    "assessment": "Manual",
    "description": "Centralized log management helps ensure logs are forensically sound and are available at a central location for auditing and incident investigation.",
    "rationale": "A centralized logging solution aggregates logs from multiple systems to ensure logs can be referenced in the event systems are thought to be compromised. Centralized log servers are also often used to correlate logs for potential patterns of attack. If a centralized logging solution is not used and systems (and their logs) are believed to be compromised, then logs may not be permitted to be used as evidence.",
    "audit": "Use this command to verify your server is configured for central logging: grep -ir syslog /etc/nginx The output should show the access logs being sent to a central server, similar to the output of the command below. 192.168.2.1 should be replaced with your central log server. The local logging facility may be any unconfigured facility on your server. access_log syslog:server=192.168.2.1,facility=local7,tag=nginx,severity=info combined;",
    "remediation": "To enable central logging for your access logs, add the below line to your server block in your server configuration file. 192.168.2.1 should be replaced with the location of your central log server. The local logging facility may be changed to any unconfigured facility on your server. access_log syslog:server=192.168.2.1,facility=local7,tag=nginx,severity=info combined; Default Value: Syslog is not set up by default.  References: 1. http://nginx.org/en/docs/syslog.html",
    "function_names": [
      "cloudtrail_trail_remote_syslog_enabled",
      "cloudtrail_trail_logs_centralized",
      "cloudtrail_trail_syslog_server_configured",
      "cloudtrail_trail_external_logging_enabled",
      "cloudtrail_trail_logs_forwarded_to_syslog",
      "cloudtrail_trail_remote_logging_enabled",
      "cloudtrail_trail_syslog_export_enabled",
      "cloudtrail_trail_logs_sent_to_remote_server"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/syslog.html"
  },
  {
    "id": "3.7",
    "title": "Ensure proxies pass source IP information",
    "assessment": "Manual",
    "description": "The x-forwarded-for and remote address headers help identify and separate the originating client IP address of the user agent and the proxy IP address. The two types of addresses are the same, and one should always be present.",
    "rationale": "Being able to identify the originating client IP address can help auditors or incident responders identify where the corresponding user came from. This may be useful in the event of an attack to analyze if the IP address is a good candidate for blocking. It may also be useful to correlate an attacker's actions.",
    "audit": "Open the nginx configuration file and the associated included files in that configuration. Check all location blocks for the presence of the proxy_pass directive. The proxy_pass directive should be followed by one of the below two directives to ensure the client IP address is passed to the endpoint the proxy is serving traffic to. proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;",
    "remediation": "To ensure your proxy or load balancer will forward information about the client and the proxy to the application, you must set the below headers in your location block. Edit your location block so it shows the proxy_set_header directives for the client and the proxy as shown below. These headers are the exact same and there is no need to have both present. server { ... location / { proxy_pass (Insert Application URL here); proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } Default Value: This is not set by default.  References: 1. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For 2. http://nginx.org/en/docs/http/ngx_http_proxy_module.html Additional Information: Users' privacy should be kept in mind when deploying this header. If it is deployed, you should ensure your privacy policy includes that you collect IP address information about your users.",
    "function_names": [
      "cloud_cdn_distribution_x_forwarded_for_enabled",
      "cloud_cdn_distribution_remote_address_enabled",
      "cloud_cdn_distribution_source_ip_headers_enabled",
      "cloud_cdn_distribution_client_ip_forwarding_enabled",
      "cloud_cdn_distribution_proxy_headers_configured",
      "cloud_cdn_distribution_origin_ip_preservation_enabled",
      "cloud_cdn_distribution_forwarded_headers_required",
      "cloud_cdn_distribution_ip_headers_passed"
    ],
    "profile_applicability": "\u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For 2. http://nginx.org/en/docs/http/ngx_http_proxy_module.html Additional Information: Users' privacy should be kept in mind when deploying this header. If it is deployed, you should ensure your privacy policy includes that you collect IP address information about your users."
  },
  {
    "id": "4.1.1",
    "title": "Ensure HTTP is redirected to HTTPS",
    "assessment": "Manual",
    "description": "Browsers and clients establish encrypted connections with servers by leveraging HTTPS. Requests leveraging HTTP are unencrypted. Unencrypted requests should be redirected so they are encrypted. Any listening HTTP port on your web server should redirect to a server profile that uses encryption. The default HTTP (unencrypted) port is 80.",
    "rationale": "Redirecting user agent traffic to HTTPS helps to ensure all user traffic is encrypted. Modern browsers alert users that your website is insecure when HTTPS is not used. This can decrease user trust in your website and ultimately result in decreased use of your web services. Redirection from HTTP to HTTPS couples security with usability; users are able to access your website even if they lack the security awareness to use HTTPS over HTTP when requesting your website. Impact: Use of HTTPS does result in a performance reduction in traffic to your website, however, due to the increased value of the security, many businesses consider this to be a cost of doing business.",
    "audit": "To verify your server listening configuration, check your web server or proxy configuration file. The default web server configuration file is /etc/nginx/conf.d/default.conf, and the default proxy configuration file is /etc/nginx/nginx.conf. The configuration file should return a statement redirecting to HTTPS. This should be similar to the code below, where cisecurity.org is used as an example.  server { listen 80; server_name cisecurity.org; return 301 https://$host$request_uri; }",
    "remediation": "Edit your web server or proxy configuration file to redirect all unencrypted listening ports, such as port 80, using a redirection through the return directive (cisecurity.org is used as an example server name). server { listen 80; server_name cisecurity.org; return 301 https://$host$request_uri; } Default Value: NGINX is not configured to use HTTPS or redirect to it by default. References: 1. https://serversforhackers.com/c/redirect-http-to-https-nginx",
    "function_names": [
      "cloud_cdn_distribution_https_redirect_enabled",
      "compute_load_balancer_https_redirect_enabled",
      "compute_instance_https_redirect_enabled",
      "cloud_storage_bucket_https_redirect_enabled",
      "web_server_https_redirect_enabled",
      "cloud_front_distribution_https_redirect_enabled",
      "api_gateway_https_redirect_enabled",
      "app_engine_https_redirect_enabled",
      "cloud_run_https_redirect_enabled",
      "cloud_functions_https_redirect_enabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "impact": "Use of HTTPS does result in a performance reduction in traffic to your website, however, due to the increased value of the security, many businesses consider this to be a cost of doing business.",
    "references": "1. https://serversforhackers.com/c/redirect-http-to-https-nginx"
  },
  {
    "id": "4.1.2",
    "title": "Ensure a trusted certificate and trust chain is installed",
    "assessment": "Manual",
    "description": "Certificates and their trust chains are needed to establish the identity of a web server as legitimate and trusted. Certificate authorities validate a web server's identity and that you are the owner of that web server domain name.",
    "rationale": "Without a certificate and full trust chain installed on your web server, modern browsers will flag your web server as untrusted.",
    "audit": "Run this command to find the file location of your certificate: grep -ir ssl_certificate /etc/nginx/ The output of your command should look similar to the below output. If there is no output, you do not have a certificate installed. Web Server: /etc/nginx/nginx.conf:    ssl_certificate /etc/nginx/cert.pem; /etc/nginx/nginx.conf:    ssl_certificate_key /etc/nginx/nginx.key; Open the file to the right of the ssl_certificate directive using the following command: cat /etc/nginx/cert.pem The output of your command should look similar to the below. It should include the full certificate chain.  -----BEGIN CERTIFICATE----- Insert Your Web Server Certificate -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- Insert Your Certificate Authority Intermediate Certificate -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- Insert Your Certificate Authority Root Certificate -----END CERTIFICATE-----",
    "remediation": "Use the following procedure to install a certificate and its signing certificate chain onto your web server, load balancer, or proxy. Step 1: Create the server's private key and a certificate signing request. The following command will create your certificate's private key with 2048-bit key strength. Optionally, this parameter may be changed to 4096 for greater security. It will also output your certificate signing request to the nginx.csr file in your present working directory. openssl req -new -newkey rsa:2048 -keyout nginx.key -out nginx.csr Enter the below information about your private key: Country Name (2 letter code) [XX]: Your Country State or Province Name (full name) []: Your State Locality Name (eg, city) [Default City]: Your City Organization Name (eg, company) [Default Company Ltd]: Your City Organizational Unit Name (eg, section) []: Your Organizational Unit Common Name (eg, your name or your server's hostname) []: Your server's DNS name Email Address []: Your email address Step 2: Obtain a signed certificate from your certificate authority. Provide your chosen certificate authority with your certificate signing request. Follow your certificate authority's signing procedures in order to obtain a certificate and the certificate's trust chain. A full trust chain is typically delivered in .pem format. Step 3: Install certificate and signing certificate chain on your web server. Place the .pem file from your certificate authority into the directory of your choice. Locate your created key file from the command you used to generate your certificate signing request. Open your website configuration file and edit your encrypted listener to leverage the ssl_certificate and ssl_certificate_key directives for a web server as shown below. You should also inspect include files inside your nginx.conf. This should be part of the server block.  server { listen 443 ssl http2; listen [::]:443 ssl http2; ssl_certificate /etc/nginx/cert.crt; ssl_certificate_key /etc/nginx/nginx.key; ... } After editing this file, you must recycle nginx services for these changes to take effect. This can be done with the following command: sudo systemctl restart nginx Default Value: No certificate is installed by default. References: 1. http://nginx.org/en/docs/http/configuring_https_servers.html#chains 2. https://www.digicert.com/csr-ssl-installation/nginx-openssl.htm 3. https://support.globalsign.com/customer/portal/articles/1290470-install-certificate- --nginx",
    "function_names": [
      "cloud_cdn_ssl_certificate_trusted_chain_installed",
      "compute_load_balancer_ssl_certificate_trusted_chain_installed",
      "cloudfront_distribution_ssl_certificate_trusted_chain_installed",
      "api_gateway_domain_ssl_certificate_trusted_chain_installed",
      "elastic_load_balancer_ssl_certificate_trusted_chain_installed",
      "cloud_run_service_ssl_certificate_trusted_chain_installed",
      "app_engine_domain_ssl_certificate_trusted_chain_installed",
      "cloud_load_balancer_ssl_certificate_trusted_chain_installed"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. http://nginx.org/en/docs/http/configuring_https_servers.html#chains 2. https://www.digicert.com/csr-ssl-installation/nginx-openssl.htm 3. https://support.globalsign.com/customer/portal/articles/1290470-install-certificate- --nginx"
  },
  {
    "id": "4.1.3",
    "title": "Ensure private key permissions are restricted",
    "assessment": "Automated",
    "description": "The server's private key should be protected from unauthorized access by limiting access based on the principle of least privilege.",
    "rationale": "A server's private key file should be restricted to 400 permissions. This ensures only the owner of the private key file can access it. This is the minimum necessary permissions for the server to operate. If the private key file is not protected, an unauthorized user with access to the server may be able to find the private key file and use it to decrypt traffic sent to your server.",
    "audit": "Verify the permissions on the key file are 400. This can be found by running the following command. You should replace /etc/nginx/nginx.key with the location of your key file. find /etc/nginx/ -name '*.key' -exec stat -Lc \"%n %a\" {} + The output should show mode 400 or more restrictive Example: /etc/nginx/nginx.key 400",
    "remediation": "Run the following command to remove excessive permissions on key files in the `/etc/nginx/ directory. Note: The directory /etc/nginx/ should be replaced with the location of your key file. find /etc/nginx/ -name '*.key' -exec chmod u-wx,go-rwx {} + Default Value: The default permissions on the server's private key are 644 or -rw-r--r--.  Additional Information: Important Note: This recommendation should be applied to both the keys of your server certificate and the key of your client certificate if you are looking to mutually authenticate a proxy server.",
    "function_names": [
      "compute_private_key_permissions_restricted",
      "compute_private_key_least_privilege_access",
      "compute_private_key_unauthorized_access_denied",
      "compute_private_key_secure_permissions",
      "compute_private_key_access_restricted"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer"
  },
  {
    "id": "4.1.4",
    "title": "Ensure only modern TLS protocols are used",
    "assessment": "Automated",
    "description": "Only modern TLS protocols should be enabled in NGINX for all client connections and upstream connections. Removing legacy TLS and SSL protocols (SSL 3.0, TLS 1.0 and 1.1), and enabling emerging and stable TLS protocols (TLS 1.2, and TLS 1.3), ensures users are able to take advantage of strong security capabilities and protects them from insecure legacy protocols.",
    "rationale": "Why disable SSL 3.0: The POODLE Vulnerability allowed attackers to exploit SSL 3.0 to obtain cleartext information by exploiting weaknesses in CBC in 2014. SSL 3.0 is also no longer FIPS 140-2 compliant. Why disable TLS 1.0: TLS 1.0 was deprecated from use when PCI DSS Compliance mandated that it not be used for any applications processing credit card numbers in June 2018. TLS 1.0 does not make use of modern protections, and almost all user agents that do not support TLS 1.2 or higher are no longer supported by their vendor. Why disable TLS 1.1: Because of the increased security associated with higher versions of TLS, TLS 1.0 should be disabled. Modern browsers will begin to flag TLS 1.1 as deprecated in early 2019. Why enable TLS 1.2: TLS 1.2 takes advantage of several security features including modern cipher suites, perfect forward security, and authenticated encryption. Why enable TLS 1.3: TLS 1.3 improves security by removing several insecure cipher suites by default and adding several more secure algorithms. All public-key exchange mechanisms support perfect forward secrecy in this version of TLS. Additionally, TLS 1.3 makes drastic performance improvements by removing a full round trip in the TLS handshake. Impact: Disabling certain TLS may not allow legacy user agents to connect to your server. Disabling negotiation of specific protocols with your backend server may also limit your ability to connect with legacy servers. You should always consider if you need to support legacy user agents or servers when selecting your TLS protocols.",
    "audit": "You can verify which SSL/TLS protocols your server uses by issuing the below command to see the configured cipher suites on the server. If anything older than TLS 1.2 is implemented or nothing appears, this recommendation is not implemented. grep -ir ssl_protocol /etc/nginx Note: Depending on your configuration, you may see different results. The directive ssl_protocols should always be part of your server block. If your NGINX server is also a proxy or load balancer, you should also check for the presence of the proxy_ssl_protocols directive as part of the location block of your nginx configuration. This ensures your proxy follows a specific set of negotiation rules for encrypting traffic with your upstream server.",
    "remediation": "Run the following commands to change your ssl_protocols if they are already configured. This remediation advice assumes your nginx configuration file does not include server configuration outside of /etc/nginx/nginx.conf. You may have to also inspect the include files in your nginx.conf to ensure this is properly implemented. Web Server: sed -i \"s/ssl_protocols[^;]*;/ssl_protocols TLSv1.2 TLSv1.3;/\" /etc/nginx/nginx.conf Proxy: sed -i \"s/proxy_ssl_protocols[^;]*;/proxy_ssl_protocols TLSv1.2 TLSv1.3;/\" /etc/nginx/nginx.conf If your ssl_protocols are not already configured, this can be accomplished manually by opening your web server or proxy server configuration file and manually adding the directives. Web Server: server { ssl_protocols TLSv1.2 TLSv1.3; } Proxy: location / { proxy_pass cisecurity.org; proxy_ssl_protocols TLSv1.2 TLSv1.3; } Default Value: By default, NGINX does not specify the TLS protocol and accepts all TLS versions, except for TLS 1.3, which must be enabled by an administrator to take effect. Defaults: ssl_protocols TLSv1.0 TLSv1.1 TLSv1.2 proxy_ssl_protocols TLSv1.0 TLSv1.1 TLSv1.2  References: 1. https://webkit.org/blog/8462/deprecation-of-legacy-tls-1-0-and-1-1-versions/ 2. https://www.cloudflare.com/learning/ssl/why-use-tls-1.3/ 3. https://tools.ietf.org/html/rfc8446 Additional Information: Note: TLS configuration should always be set to your organizational policy. This recommendation is designed to meet best practices. Compatibility Note: TLS 1.3 will only be available if a version of OpenSSL greater than 1.1.0 is installed on your server. You can check the version of OpenSSL you are using by leveraging the following command: openssl version",
    "function_names": [
      "nginx_client_tls_min_version",
      "nginx_upstream_tls_min_version",
      "nginx_tls_legacy_protocols_disabled",
      "nginx_tls_1_2_enabled",
      "nginx_tls_1_3_enabled",
      "nginx_ssl_3_0_disabled",
      "nginx_tls_1_0_disabled",
      "nginx_tls_1_1_disabled",
      "nginx_tls_secure_protocols_only",
      "nginx_tls_protocol_compliance"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "impact": "Disabling certain TLS may not allow legacy user agents to connect to your server. Disabling negotiation of specific protocols with your backend server may also limit your ability to connect with legacy servers. You should always consider if you need to support legacy user agents or servers when selecting your TLS protocols.",
    "references": "1. https://webkit.org/blog/8462/deprecation-of-legacy-tls-1-0-and-1-1-versions/ 2. https://www.cloudflare.com/learning/ssl/why-use-tls-1.3/ 3. https://tools.ietf.org/html/rfc8446 Additional Information: Note: TLS configuration should always be set to your organizational policy. This recommendation is designed to meet best practices. Compatibility Note: TLS 1.3 will only be available if a version of OpenSSL greater than 1.1.0 is installed on your server. You can check the version of OpenSSL you are using by leveraging the following command: openssl version"
  },
  {
    "id": "4.1.5",
    "title": "Disable weak ciphers",
    "assessment": "Manual",
    "description": "The ssl_ciphers directive should be used to configure the available ciphers on your web server, and the proxy_ssl_ciphers directive should be used to configure the available ciphers for your proxy. Weak ciphers should be disabled based on your company's policy or an industry best practice compliance profile. The ssl_prefer_server_ciphers should be used to ensure the user agent respects the server's preferred cipher order and does not set its own. If you are using a proxy or load balancer, you should use the proxy_ssl_ciphers directive to ensure your upstream connections are negotiated using secure ciphers.",
    "rationale": "The use of strong ciphers is critical to maintaining strong encryption on your web server, load balancer, or proxy. Weak ciphers may compromise the security of your site or your users by allowing legacy user agents to connect to your site in a vulnerable way. You may also meet compliance concerns by ensuring that your upstream connections meet the same level of security if using a proxy or load balancer. The server should enforce the cipher preference on the server side to protect users from malicious actors on the client side. Impact: Strong cipher configurations may not allow legacy user agents or user agents with weak configurations to connect to your site. If your server must also pass to a legacy upstream server, this may prevent it from being able to negotiate a cipher upstream.",
    "audit": "Use the following procedure to verify the ssl_cipher and proxy_ssl_cipher directives meet your company's policy.  grep -ir ssl_ciphers /etc/nginx/ grep -ir proxy_ssl_ciphers /etc/nginx This output will show the server's configured ciphers and cipher preference policy. If you have multiple server blocks or proxy passes, you should ensure the directive or directives appear for each. In your environment, you may have to check all include files in your nginx configuration or the nginx configuration itself manually. The server ciphers may be located as part of the server block, and the proxy ciphers may be located as part of the location block for your upstream traffic. OpenSSL may also be used to check compatible ciphers following the procedure found at OWASP: https://www.owasp.org/index.php/Testing_for_SSL-TLS_%28OWASP-CM-001%29",
    "remediation": "The following procedures may be used to implement industry standard cipher profiles if you have an existing profile defined. These profiles may be modified to meet the requirements defined in your company's policy. This procedure assumes that all server blocks will be in /etc/nginx/nginx.conf and not inside any included files in the configuration. Set the ssl_cipher directive as part of your server block, and set the proxy_ssl_ciphers directive as part of the location block for your upstream server. This should look similar to the below examples: Server block configuration for client connectivity to web server, proxy, or load balancer: server { ssl_ciphers ALL:!EXP:!NULL:!ADH:!LOW:!SSLv2:!SSLv3:!MD5:!RC4; } Proxy or load balancer configuration for defined upstream negotiation: location / { proxy_pass https://cisecurity.org; proxy_ssl_ciphers ALL:!EXP:!NULL:!ADH:!LOW:!SSLv2:!SSLv3:!MD5:!RC4; } The below procedure assumes the default configuration profile. If you do not have ssl_ciphers or proxy_ssl_ciphers defined, add the directives to your proxy or web server configuration profile, then run the below commands to configure them to your selected profile. No weak ciphers SSLLABS proxy configuration sed -i \"s/proxy_ssl_ciphers[^;]*;/proxy_ssl_ciphers ECDHE-ECDSA-AES128-GCM- SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA- AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20- POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;/\" /etc/nginx/nginx.conf No weak ciphers SSLLABS web server configuration:  sed -i \"s/ssl_ciphers[^;]*;/ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE- RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM- SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA- AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384;/\" /etc/nginx/nginx.conf For changes to take effect, you must recycle nginx: systemctl restart nginx Default Value: These directives are not specified by default and are set to the default of HIGH:!aNULL:!MD5. References: 1. https://ssllabs.com 2. https://mozilla.github.io/server-side-tls/ssl-config-generator/ 3. http://nginx.org/en/docs/http/ngx_http_ssl_module.html 4. https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/ 5. https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/ Additional Information: Note: TLS configuration should always be set to your organizational policy. This recommendation is designed to meet best practices and offers several profiles your organization may align to.",
    "function_names": [
      "cloud_cdn_ssl_ciphers_weak_disabled",
      "cloud_cdn_proxy_ssl_ciphers_weak_disabled",
      "cloud_cdn_ssl_prefer_server_ciphers_enabled",
      "cloud_cdn_proxy_ssl_ciphers_secure_configured",
      "compute_load_balancer_ssl_ciphers_weak_disabled",
      "compute_load_balancer_proxy_ssl_ciphers_weak_disabled",
      "compute_load_balancer_ssl_prefer_server_ciphers_enabled",
      "compute_load_balancer_ssl_ciphers_secure_configured"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "impact": "Strong cipher configurations may not allow legacy user agents or user agents with weak configurations to connect to your site. If your server must also pass to a legacy upstream server, this may prevent it from being able to negotiate a cipher upstream.",
    "references": "1. https://ssllabs.com 2. https://mozilla.github.io/server-side-tls/ssl-config-generator/ 3. http://nginx.org/en/docs/http/ngx_http_ssl_module.html 4. https://www.acunetix.com/blog/articles/tls-vulnerabilities-attacks-final-part/ 5. https://www.gracefulsecurity.com/tls-ssl-vulnerabilities/ Additional Information: Note: TLS configuration should always be set to your organizational policy. This recommendation is designed to meet best practices and offers several profiles your organization may align to."
  },
  {
    "id": "4.1.6",
    "title": "Ensure custom Diffie-Hellman parameters are used",
    "assessment": "Automated",
    "description": "Custom Diffie-Hellman (DH) key exchange parameters should be used. DH Ephemeral (DHE) parameters with at least 2048 bits should be generated.",
    "rationale": "Backward-compatible Perfect Forward Secrecy (PFS) ciphers (e.g. DHE-RSA-AES128- SHA256) should use strong and unique parameters. By default, NGINX will generate 1024-bit RSA keys for PFS ciphers; stronger alternatives should be used instead to provide better protection for data protected by encryption.",
    "audit": "Verify the option ssl_dhparam is explicitly provided: grep ssl_dhparam /etc/nginx/nginx.conf",
    "remediation": "Generate strong DHE (Ephemeral Diffie-Hellman) parameters using the following commands: mkdir /etc/nginx/ssl openssl dhparam -out /etc/nginx/ssl/dhparam.pem 2048 chmod 400 /etc/nginx/ssl/dhparam.pem Alter the server configuration to use the new parameters: http { server { ssl_dhparam /etc/nginx/ssl/dhparam.pem; } } References: 1. https://weakdh.org/sysadmin.html",
    "function_names": [
      "compute_ssl_certificate_custom_dh_parameters",
      "compute_ssl_certificate_dhe_min_2048_bits",
      "compute_ssl_certificate_no_default_dh_parameters",
      "compute_ssl_certificate_strong_dh_parameters",
      "compute_ssl_certificate_dh_ephemeral_enabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://weakdh.org/sysadmin.html"
  },
  {
    "id": "4.1.7",
    "title": "Ensure Online Certificate Status Protocol (OCSP) stapling is enabled",
    "assessment": "Automated",
    "description": "OCSP allows a user's browser or another user agent to verify the certificate it is seeing is not revoked. OCSP stapling ensures your server presents this information to the user's browser in a way that best meets the performance and security needs of your website. It polls the Certificate Authority's (CA) OCSP server at regular intervals to ensure it is continuously kept up to date. OCSP stapling helps improve performance and security, so it should be enabled.",
    "rationale": "OCSP stapling protects your users from accessing a website where a private key is believed to be compromised. If a private key is compromised, an attacker may be able to obtain unauthorized access to the encrypted data transmitted by a user. Note: OCSP stapling, while a step forward from the older certificate revocation list model, does share similar risks. Between the time a certificate is revoked and the point where a new signed OCSP profile is requested, if a server's certificate has been revoked a user agent may not be informed.",
    "audit": "Run the following command to verify OCSP stapling is enabled: grep -ir ssl_stapling /etc/nginx If the output does not contain your proxy or web server configuration file with the below two directives, this recommendation is not implemented. ssl_stapling on; ssl_stapling_verify on;",
    "remediation": "Follow this procedure to enable OCSP validation: Step 1: Ensure your NGINX server has access to your CA's OCSP server. Your CA's OCSP server may be found on your CA's website and will vary depending on your CA vendor. Issue the following command in order to check your connectivity to their site:  curl -I \"insert certificate authority ocsp server here\" If you get a 200 code response, your server has access. Step 2: Enable OCSP on nginx. Implement the ssl_stapling and ssl_stapling_verify directives. The directive ssl_stapling enables OCSP stapling, and the directive ssl_stapling_verify enables verification of the OCSP responses on nginx. server { ssl_stapling on; ssl_stapling_verify on; } Default Value: OCSP stapling is not enabled by default. References: 1. https://www.digicert.com/ssl-support/nginx-enable-ocsp-stapling-on-server.htm",
    "function_names": [
      "cloud_cdn_ssl_certificate_ocsp_stapling_enabled",
      "cloud_cdn_load_balancer_ocsp_stapling_enabled",
      "compute_ssl_certificate_ocsp_stapling_enabled",
      "compute_load_balancer_ocsp_stapling_enabled",
      "cloud_cdn_ssl_certificate_ocsp_stapling_enabled_all_regions",
      "compute_ssl_certificate_ocsp_stapling_enabled_all_regions"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.digicert.com/ssl-support/nginx-enable-ocsp-stapling-on-server.htm"
  },
  {
    "id": "4.1.8",
    "title": "Ensure HTTP Strict Transport Security (HSTS) is enabled",
    "assessment": "Automated",
    "description": "HTTP Strict Transport Security (HSTS) headers instruct a user agent on how to communicate with a web server. HSTS headers ensure the strict transport security policies built into browsers and other user agents are informed only to communicate over HTTPS. HSTS with long validity periods should be used to most effectively secure your user population. Strict-Transport-Security should have a long max-age, which is recommended to be at least six months in length. This ensures the browser remembers your website should only be accessible via HTTPS for this amount of time.",
    "rationale": "HSTS headers help protect a server's users from accessing the server over unencrypted protocols. This header helps to prevent HTTP downgrade attacks.",
    "audit": "Issue this command to check for HSTS headers on your website: grep -ir Strict-Transport-Security /etc/nginx If your output does not include the following directive associated with your server configuration file, this recommendation is not implemented. The header should also include the max-age directive with 15768000 seconds (six months) or longer configured. add_header Strict-Transport-Security \"max-age=15768000;\" always;",
    "remediation": "Ensure the below snippet of code can be found in your server configuration for your proxy or web server. This will ensure the HSTS header is set with a validity period of six months, or 15768000 seconds.  server { add_header Strict-Transport-Security \"max-age=15768000;\" always; } Default Value: HSTS headers are not set by default. References: 1. https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it/ 2. https://mozilla.github.io/server-side-tls/ssl-config-generator/ 3. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport- Security#Preloading_Strict_Transport_Security 4. https://hstspreload.org 5. https://tools.ietf.org/html/rfc6797",
    "function_names": [
      "cloud_cdn_distribution_hsts_enabled",
      "cloud_cdn_distribution_hsts_max_age_long",
      "cloud_cdn_distribution_hsts_min_6_months",
      "cloud_cdn_distribution_hsts_secure_headers",
      "cloud_cdn_distribution_hsts_https_only",
      "cloud_cdn_distribution_hsts_strict_transport_security",
      "cloud_cdn_distribution_hsts_validity_long",
      "cloud_cdn_distribution_hsts_browser_compliance",
      "cloud_cdn_distribution_hsts_user_agent_enforced",
      "cloud_cdn_distribution_hsts_secure_communication"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it/ 2. https://mozilla.github.io/server-side-tls/ssl-config-generator/ 3. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport- Security#Preloading_Strict_Transport_Security 4. https://hstspreload.org 5. https://tools.ietf.org/html/rfc6797"
  },
  {
    "id": "4.1.9",
    "title": "Ensure upstream server traffic is authenticated with a client certificate",
    "assessment": "Automated",
    "description": "Client certificate validation allows the upstream server to authenticate the identity of the client connecting to it. This assists in the establishment of mutual authentication between the client and the server.",
    "rationale": "Using client certificate validation allows you to establish a trusted proxy server.",
    "audit": "To verify this recommendation, validate that the below configuration is in the location block of your nginx configuration that is sending traffic to an upstream location. The command below may be helpful in determining if this is set up: grep -ir proxy_ssl_certificate /etc/nginx You should see output similar to the below. You may need to manually ensure this is configured properly by investigating your location block for the output as well. proxy_ssl_certificate /etc/nginx/ssl/nginx.pem; proxy_ssl_certificate_key /etc/nginx/ssl/nginx.key;",
    "remediation": "In order to implement this recommendation, you must create a client certificate to be authenticated against and have it signed. Once you have a signed certificate, place the certificate in a location of your choice. In the below example, we use /etc/nginx/ssl/cert.pem. Implement the configuration as part of the location block: proxy_ssl_certificate /etc/nginx/ssl/nginx.pem; proxy_ssl_certificate_key /etc/nginx/ssl/nginx.key; Default Value: This is not authenticated by default. References: 1. https://docs.nginx.com/nginx/admin-guide/security-controls/securing-http-traffic- upstream/  2. http://www.staticshin.com/programming/proxy-ssl-cert-in-nginx.html 3. http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_ssl_certificate Additional Information: Your upstream server must also be configured to verify the client certificate. If your upstream web server is an NGINX web server, you may accomplish this by adding the client certificate into a file referenced by the directive ssl_client_certificate. This should be part of your server block. An example is below. ssl_client_certificate      /etc/nginx/ssl/ca.cert; ssl_verify_client     on;",
    "function_names": [
      "cloud_cdn_upstream_server_client_certificate_authentication_enabled",
      "cloud_cdn_upstream_server_mtls_authentication_enabled",
      "cloud_cdn_upstream_server_client_cert_validation_enabled",
      "cloud_cdn_upstream_server_mutual_tls_required",
      "cloud_cdn_upstream_server_client_auth_enabled"
    ],
    "profile_applicability": "\u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://docs.nginx.com/nginx/admin-guide/security-controls/securing-http-traffic- upstream/  2. http://www.staticshin.com/programming/proxy-ssl-cert-in-nginx.html 3. http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_ssl_certificate Additional Information: Your upstream server must also be configured to verify the client certificate. If your upstream web server is an NGINX web server, you may accomplish this by adding the client certificate into a file referenced by the directive ssl_client_certificate. This should be part of your server block. An example is below. ssl_client_certificate      /etc/nginx/ssl/ca.cert; ssl_verify_client     on;"
  },
  {
    "id": "4.1.10",
    "title": "Ensure the upstream traffic server certificate is trusted",
    "assessment": "Manual",
    "description": "The NGINX server should be configured to validate the identity of the upstream server it is sending information to.",
    "rationale": "Configuring NGINX to validate the identity of the upstream server helps mitigate the risk of a man in the middle attack occurring against your server.",
    "audit": "To verify the configuration, follow this procedure: Step 1: Verify your nginx proxy or load balancer is set up to validate server identity. You should check for the presence of the below directives as part of the location block you are using to send traffic to your upstream server. The two commands should help you to identify if this is implemented; however, you may also want to manually check through include files as well that can be found in your nginx configuration. As part of this configuration check, you should also ensure that the proxy_ssl_verify directive is set to on. grep -ir proxy_ssl_trusted_certificate /etc/nginx grep -ir proxy_ssl_verify /etc/nginx The output and directives you should look for are: proxy_ssl_trusted_certificate /etc/nginx/trusted_ca_cert.crt; proxy_ssl_verify        on; Step 2: Verify the certificate trust chains for upstream servers are installed properly. Verify the certificates installed in the location referenced by the proxy_ssl_trusted_certificate directive are valid.",
    "remediation": "Obtain the full certificate chain of the upstream server in .pem format. Then reference that file in the location block as part of the proxy_ssl_trusted_certificate directive. Implement the proxy_ssl_trusted_certificate and proxy_ssl_verify directives as shown below as part of the location block you are using to send traffic to your upstream server.  proxy_ssl_trusted_certificate /etc/nginx/trusted_ca_cert.crt; proxy_ssl_verify        on; Default Value: This is not set up by default. References: 1. https://docs.nginx.com/nginx/admin-guide/security-controls/securing-http-traffic- upstream/ 2. http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_ssl_trusted_cer tificate",
    "function_names": [
      "compute_upstream_server_certificate_trusted",
      "compute_upstream_server_certificate_valid",
      "compute_upstream_server_identity_verified",
      "compute_upstream_tls_certificate_trusted",
      "compute_upstream_connection_certificate_validated"
    ],
    "profile_applicability": "\u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. https://docs.nginx.com/nginx/admin-guide/security-controls/securing-http-traffic- upstream/ 2. http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_ssl_trusted_cer tificate"
  },
  {
    "id": "4.1.11",
    "title": "Ensure your domain is preloaded",
    "assessment": "Manual",
    "description": "Preloading your domain hardcodes it as only being accessible through HTTPS by browsers. Note: Preloading should only be done with careful consideration! Your website and all its subdomains will be forced over HTTPS. If your website or any of its subdomains are not able to support preloading, you should not preload your site. Preloading should be opt-in only, and if done, may impact more sites than the nginx instance you are working on. Removing preloading can be slow and painful, and should only be done with careful consideration according to https://hstspreload.org.",
    "rationale": "Preloading your domain helps prevent HTTP downgrade attacks and increases trust.",
    "audit": "Visit https://hstspreload.org/ and type in your top-level domain name to verify it is preloaded.",
    "remediation": "In order to successfully preload your website, you must meet the below criteria: 1. Serve a valid certificate. This may be accomplished by following recommendation 4.1.2. 2. Redirect from HTTP to HTTPS if using port 80. This may be accomplished by following recommendation 4.1.1. 3. Configure all subdomains to support HTTPS only.  This will require you to configure all subdomains for HTTPS only. For example, a subdomain of cissecurity.org is workbench.cissecurity.org and would need to be configured for HTTPS only.  4. Configure an HSTS header on your base domain, as shown below for nginx. If your base domain is nginx, you may accomplish this with several modifications from the HSTS recommendation. Change your header to include the preload directive and the includesubdomains directive, and make your max-length one year or longer. The header should be modified similar to the below snippet. add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\"; After you have met these requirements, add your site to the list by following the instructions at https://hstspreload.org/. Default Value: Your website is not preloaded by default. References: 1. https://hstspreload.org/",
    "function_names": [
      "cloud_cdn_domain_preloaded",
      "cloud_cdn_domain_https_enforced",
      "cloud_cdn_domain_hsts_preloaded",
      "cloud_cdn_domain_secure_transport_required",
      "cloud_cdn_domain_preload_opt_in",
      "cloud_cdn_domain_preload_considered",
      "cloud_cdn_domain_subdomains_https_compatible",
      "cloud_cdn_domain_preload_removal_aware"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. https://hstspreload.org/"
  },
  {
    "id": "4.1.12",
    "title": "Ensure session resumption is disabled to enable perfect forward security",
    "assessment": "Automated",
    "description": "Session resumption for HTTPS sessions should be disabled so perfect forward secrecy can be achieved.",
    "rationale": "Perfect forward secrecy is an encryption mechanism that enables past session keys to not be compromised even if the server's private key is compromised. If an attacker recorded all traffic to a server and stored it and then obtained the private key without perfect forward secrecy, all communications would be compromised. With perfect forward secrecy, session keys are generated using Diffie-Hellman for every session a user initiates, which isolates session compromise to only that communication session. Allowing session resumption breaks perfect forward secrecy; this expands the surface area for an attacker to compromise past sessions and communications with a server if they are able to compromise the session.",
    "audit": "Run the following command to verify the ssl_session_tickets directive is turned off. You should always investigate your nginx configuration file for included file locations outside of /etc/nginx to ensure you are properly investigating each server block for the presence of the ssl_session_tickets directive being turned off. grep -ir ssl_session_tickets /etc/nginx The output should contain the following: ssl_session_tickets off;",
    "remediation": "Turn off the ssl_session_tickets directive as part of any server block in your nginx configuration: ssl_session_tickets off; Default Value: Perfect forward security is not enabled by default.  References: 1. https://www.imperialviolet.org/2013/06/27/botchingpfs.html 2. https://scotthelme.co.uk/perfect-forward-secrecy/",
    "function_names": [
      "cloud_cdn_load_balancer_session_resumption_disabled",
      "cloud_cdn_load_balancer_perfect_forward_secrecy_enabled",
      "cloud_cdn_ssl_certificate_session_resumption_disabled",
      "cloud_cdn_ssl_certificate_perfect_forward_secrecy_enabled",
      "compute_load_balancer_session_resumption_disabled",
      "compute_load_balancer_perfect_forward_secrecy_enabled",
      "compute_ssl_certificate_session_resumption_disabled",
      "compute_ssl_certificate_perfect_forward_secrecy_enabled"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. https://www.imperialviolet.org/2013/06/27/botchingpfs.html 2. https://scotthelme.co.uk/perfect-forward-secrecy/"
  },
  {
    "id": "4.1.13",
    "title": "Ensure HTTP/2.0 is used",
    "assessment": "Automated",
    "description": "HTTP/2.0 is an optimized and more secure version of the HTTP protocol. It should be enabled so users can take advantage of it. Note: Legacy user agents may not be able to connect to a server using HTTP/2.0.",
    "rationale": "HTTP/2.0 introduces both performance benefits through full multiplexing and several security benefits. HTTP/2.0 has improved cipher suite requirements and denylists. It also disables session renegotiation and TLS compression. This helps protect against vulnerabilities like CRIME and ensures we have stronger encryption.",
    "audit": "Verify that listening ports on the web server leverage HTTP/2.0 by running this command: grep -ir http2 /etc/nginx If there is no output, the output does not cover all running ports on the server that are not redirecting to another port, or the output is commented out, this recommendation is not implemented.",
    "remediation": "Open the nginx server configuration file and configure all listening ports with http2, similar to that of this example: server { listen 443 ssl http2; } Default Value: By default, HTTP/1.1 is used. References: 1. https://mozilla.github.io/server-side-tls/ssl-config-generator/ 2. https://http2.github.io/  Additional Information: 1. HTTP/2.0 is not supported for proxies at the time of the writing of this recommendation. 2. Versions of NGINX prior to 1.17.2 are affected by several HTTP/2 vulnerabilities. More information on these on these vulnerabilities can be found at: https://www.nginx.com/blog/nginx-updates-mitigate-august-2019-http-2- vulnerabilities/ and http://nginx.org/en/security_advisories.html",
    "function_names": [
      "cloud_cdn_distribution_http2_enabled",
      "cloud_cdn_endpoint_http2_enabled",
      "compute_load_balancer_http2_enabled",
      "compute_load_balancer_http2_min_tls_1_2",
      "cloud_cdn_distribution_http2_min_tls_1_2",
      "cloud_cdn_endpoint_http2_min_tls_1_2"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver",
    "references": "1. https://mozilla.github.io/server-side-tls/ssl-config-generator/ 2. https://http2.github.io/  Additional Information: 1. HTTP/2.0 is not supported for proxies at the time of the writing of this recommendation. 2. Versions of NGINX prior to 1.17.2 are affected by several HTTP/2 vulnerabilities. More information on these on these vulnerabilities can be found at: https://www.nginx.com/blog/nginx-updates-mitigate-august-2019-http-2- vulnerabilities/ and http://nginx.org/en/security_advisories.html"
  },
  {
    "id": "4.1.14",
    "title": "Ensure only Perfect Forward Secrecy Ciphers are Leveraged",
    "assessment": "Manual",
    "description": "Perfect forward secrecy protects users of your website by ensuring that even if your private key is compromised that your user's sessions are not able to be compromised. This improves upon other ciphers where if your private key was compromised all user sessions can also be compromised retroactively.",
    "rationale": "Perfect Forward Secrecy (PFS) helps to reduce the impact of a private key compromise.",
    "audit": "To check that only PFS ciphers are used to ensure that only ciphers that are ECDHE/EECDH ciphers and DHE/EDH ciphers are enabled. To audit the ciphers setup on NGINX run the below two commands: grep -ir ssl_ciphers /etc/nginx/ grep -ir proxy_ssl_ciphers /etc/nginx",
    "remediation": "Ensure that only ciphers that are compatible with perfect forward secrecy are used. ECDHE/EECDH ciphers and DHE/EDH ciphers support this capability. Its recommended to leverage ECDHE ciphers unless you need to support legacy clients because they are considered stronger and faster. An example configuration that may be used is: \"EECDH:EDH:!NULL:!SSLv2:!RC4:!aNULL:!3DES:!IDEA\". The below configuration will only enable ciphers compatible with perfect forward secrecy. Web Server: ssl_ciphers EECDH:EDH:!NULL:!SSLv2:!RC4:!aNULL:!3DES:!IDEA; Proxy: proxy_ssl_ciphers EECDH:EDH:!NULL:!SSLv2:!RC4:!aNULL:!3DES:!IDEA; Default Value: Perfect Forward Secrecy is not the only default negotiable cipher suite.  References: 1. https://scotthelme.co.uk/perfect-forward-secrecy/",
    "function_names": [
      "cloud_cdn_load_balancer_pfs_ciphers_enabled",
      "cloud_cdn_ssl_certificate_pfs_ciphers_enabled",
      "cloud_cdn_ssl_policy_pfs_ciphers_only",
      "compute_load_balancer_pfs_ciphers_enabled",
      "compute_ssl_certificate_pfs_ciphers_only",
      "compute_ssl_policy_pfs_ciphers_only",
      "network_load_balancer_pfs_ciphers_enabled",
      "network_ssl_policy_pfs_ciphers_only"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. https://scotthelme.co.uk/perfect-forward-secrecy/"
  },
  {
    "id": "5.1.1",
    "title": "Ensure allow and deny filters limit access to specific IP addresses",
    "assessment": "Manual",
    "description": "IP-based restrictions act as a defense in depth mechanism. They allow you to allowlist legitimate paths to your applications and explicitly deny IP addresses you believe to be malicious.",
    "rationale": "IP restrictions help you to only allow traffic based on the concept of least privilege. You may specify vlans, countries, or specific servers that may be allowed or denied on your site. It is recommended that you implicitly deny all traffic and only allow those with a legitimate use case to access your website if choosing to take this approach. This allows you to limit the surface area an attack may come from.",
    "audit": "To verify IP-based restrictions are limiting access correctly, perform the following steps: Step 1: Open your nginx config file and any files that are appended in an include statement. Step 2: Check the location context of your server block for the allow and deny directives. The output should look similar to the below snippet and may be expressed in CIDR notation or single addresses. location / { allow 10.1.1.1; deny all; } Step 3: Ensure the allowed IP addresses are not too permissive for your use case. For example, in the above snippet, 10.1.1.1 may be a load balancer connecting to your proxy, and you only want user traffic to come from the load balancer.",
    "remediation": "Compile a list of network ranges or IP addresses you would want to access your web server or proxy. Then add these ranges with the allow directive. The deny directive should be included with all IP addresses implicitly denied.  location / { allow 10.1.1.1; deny all; } Default Value: This is not configured by default. References: 1. https://help.dreamhost.com/hc/en-us/articles/222784068-The-most-important- steps-to-take-to-make-an-nginx-server-more-secure 2. http://nginx.org/en/docs/http/ngx_http_access_module.html Additional Information: Note: If you do not want to restrict this to a specific network range, this recommendation may not fit your use case.",
    "function_names": [
      "cloud_cdn_distribution_ip_restrictions_enabled",
      "cloud_cdn_distribution_allowlist_configured",
      "cloud_cdn_distribution_denylist_configured",
      "cloud_cdn_distribution_ip_filters_applied",
      "cloud_cdn_distribution_specific_ip_access_only",
      "cloud_cdn_distribution_restrictive_ip_policy",
      "cloud_cdn_distribution_ip_based_access_control",
      "cloud_cdn_distribution_allowlist_minimal_ips",
      "cloud_cdn_distribution_denylist_malicious_ips",
      "cloud_cdn_distribution_ip_restrictions_strict"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "references": "1. https://help.dreamhost.com/hc/en-us/articles/222784068-The-most-important- steps-to-take-to-make-an-nginx-server-more-secure 2. http://nginx.org/en/docs/http/ngx_http_access_module.html Additional Information: Note: If you do not want to restrict this to a specific network range, this recommendation may not fit your use case."
  },
  {
    "id": "5.1.2",
    "title": "Ensure only approved HTTP methods are allowed",
    "assessment": "Manual",
    "description": "HTTP methods (also known as verbs) allow different actions to be requested from the web server at a specified path. Only the necessary methods should be enabled.",
    "rationale": "Most websites only require the methods GET, POST and HEAD to function correctly. Web applications may also require other verbs (e.g. DELETE). In order to narrow vectors of attack, it is recommended to only enable the required verbs.",
    "audit": "To verify this, use a tool like curl to send a request with each method which should not be supported (e.g. DELETE) and compare the output to a supported method (e.g. GET). # curl -X DELETE http://localhost/index.html curl: (52) Empty reply from server # curl -X GET http://localhost/index.html",
    "remediation": "To remove unneeded methods and only allow required methods, add the following into a server or location block in your nginx.conf. The below snippet assumes only the methods GET, HEAD and POST are required for an application. The reason for 444 as a response is because it contains no information and can help mitigate automated attacks. if ($request_method !~ ^(GET|HEAD|POST)$) { return 444; } Default Value: All methods are allowed. References: 1. https://www.acunetix.com/blog/articles/nginx-server-security-hardening- configuration-1/",
    "function_names": [
      "cloud_cdn_distribution_approved_http_methods",
      "cloud_cdn_distribution_http_methods_restricted",
      "cloud_cdn_distribution_allowed_methods_configured",
      "cloud_cdn_distribution_methods_whitelisted",
      "cloud_cdn_distribution_http_methods_limited",
      "cloud_cdn_distribution_secure_methods_enabled",
      "cloud_cdn_distribution_methods_restricted_to_approved",
      "cloud_cdn_distribution_http_methods_compliance",
      "cloud_cdn_distribution_allowed_methods_enforced",
      "cloud_cdn_distribution_methods_approved_only"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.acunetix.com/blog/articles/nginx-server-security-hardening- configuration-1/"
  },
  {
    "id": "5.2.1",
    "title": "Ensure timeout values for reading the client header and body are set correctly",
    "assessment": "Automated",
    "description": "The client_header_timeout and client_body_timeout directives define the time the server will wait for the header or body to be sent from the client. If the client does not send the entire header in this predefined timeframe, the server will send back a 408 request timeout error.",
    "rationale": "Setting the client header and body timeouts help your server mitigate possible denial of service attacks. By timing out a request, the server is able to free up resources that may be waiting for the body or header.",
    "audit": "To verify the current settings for the client_body_timeout and client_header_timeout directives, issue the below command. You should also manually check your nginx configuration for include statements that may be located outside of the /etc/nginx directory. If this is not present, the value is set at the default. grep -ir timeout /etc/nginx The output should contain the following: client_body_timeout   10; client_header_timeout 10;",
    "remediation": "Find the HTTP or server block of your nginx configuration and add the client_header_timeout and client_body_timeout directives set to the configuration. The below example sets the timeouts to 10 seconds. client_body_timeout   10; client_header_timeout 10; Default Value: client_header_timeout 60; client_body_timeout 60;  References: 1. https://www.owasp.org/index.php/SCG_WS_nginx 2. https://blog.qualys.com/securitylabs/2011/11/02/how-to-protect-against-slow-http- attacks 3. http://nginx.org/en/docs/http/ngx_http_core_module.html#client_header_timeout",
    "function_names": [
      "cloud_cdn_distribution_client_header_timeout_set",
      "cloud_cdn_distribution_client_body_timeout_set",
      "cloud_cdn_distribution_timeout_values_configured",
      "cloud_cdn_distribution_client_header_timeout_within_limit",
      "cloud_cdn_distribution_client_body_timeout_within_limit"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.owasp.org/index.php/SCG_WS_nginx 2. https://blog.qualys.com/securitylabs/2011/11/02/how-to-protect-against-slow-http- attacks 3. http://nginx.org/en/docs/http/ngx_http_core_module.html#client_header_timeout"
  },
  {
    "id": "5.2.2",
    "title": "Ensure the maximum request body size is set correctly",
    "assessment": "Automated",
    "description": "The client_max_body_size directive sets the size of the request body that is allowed to read a client request. This defines the number of bytes allowed in a request and is equivalent to the Content-Length request header field.",
    "rationale": "Limiting the size of the request body helps prevent unexpectedly long or large client requests from being passed to an application to perform buffer overflow attacks. This value should be set low enough to protect the application but high enough not to interfere with functionality and block legitimate request bodies.",
    "audit": "To verify the current setting for the client_max_body_size directive, issue the below command. You should also manually check your nginx configuration for include statements that may be located outside of the /etc/nginx directory. If this is not present, the value is set at the default. grep -ir client_max_body_size /etc/nginx",
    "remediation": "Find the HTTP or server block of your nginx configuration and add the client_max_body_size set to 100K in this block. The appropriate value may be different based on your application's needs. client_max_body_size 100K; Default Value: client_max_body_size 1m; References: 1. https://www.cyberciti.biz/tips/linux-unix-bsd-nginx-webserver-security.html 2. http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_temp_path  3. https://www.acunetix.com/blog/articles/nginx-server-security-hardening- configuration-1/ 4. https://www.tecmint.com/nginx-web-server-security-hardening-and-performance- tips/ 5. http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size",
    "function_names": [
      "cloud_cdn_distribution_max_body_size_configured",
      "cloud_cdn_distribution_request_body_size_limited",
      "cloud_cdn_distribution_client_max_body_size_set",
      "cloud_cdn_distribution_body_size_restricted",
      "cloud_cdn_distribution_request_size_limited"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.cyberciti.biz/tips/linux-unix-bsd-nginx-webserver-security.html 2. http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_temp_path  3. https://www.acunetix.com/blog/articles/nginx-server-security-hardening- configuration-1/ 4. https://www.tecmint.com/nginx-web-server-security-hardening-and-performance- tips/ 5. http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size"
  },
  {
    "id": "5.2.3",
    "title": "Ensure the maximum buffer size for URIs is defined",
    "assessment": "Automated",
    "description": "The large_client_header_buffers directive defines the number and size of buffers used within the URI. A request cannot exceed the size of this buffer when this directive is configured. The large_client_header_buffers directive should be set to restrict buffer usage. The number of buffers should generally set to two and the length be set to 1K; however, this may not be a good fit for your application and may need to be set differently.",
    "rationale": "The large_client_header_buffers directive may assist in preventing buffer overflow attacks that leverage long URI query parameters.",
    "audit": "Run this command to verify that the large_client_header_buffers directive is configured properly: grep -ir large_client_header_buffers /etc/nginx/ The output should be similar to the below: large_client_header_buffers 2 1k;",
    "remediation": "Open your nginx.conf file and locate your server or HTTP blocks. This may be added to the HTTP block for all configurations or the server block for more specific configurations to meet your needs. Add the below line to implement this recommendation: large_client_header_buffers 2 1k; Default Value: large_client_header_buffers 4 8k; References: 1. https://www.cyberciti.biz/tips/linux-unix-bsd-nginx-webserver-security.html  2. https://www.owasp.org/index.php/Denial_of_Service_Cheat_Sheet 3. http://nginx.org/en/docs/http/ngx_http_core_module.html#large_client_header_bu ffers Additional Information: If this directive is not set correctly, users may receive a 414 Request-URI Too Large error.",
    "function_names": [
      "cloud_cdn_load_balancer_max_uri_buffer_size_defined",
      "cloud_cdn_load_balancer_large_client_header_buffers_configured",
      "cloud_cdn_load_balancer_uri_buffer_size_restricted",
      "cloud_cdn_load_balancer_header_buffer_limits_set",
      "cloud_cdn_load_balancer_buffer_size_and_count_defined"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver \u2022  Level 1 - Proxy \u2022  Level 1 - Loadbalancer",
    "references": "1. https://www.cyberciti.biz/tips/linux-unix-bsd-nginx-webserver-security.html  2. https://www.owasp.org/index.php/Denial_of_Service_Cheat_Sheet 3. http://nginx.org/en/docs/http/ngx_http_core_module.html#large_client_header_bu ffers Additional Information: If this directive is not set correctly, users may receive a 414 Request-URI Too Large error."
  },
  {
    "id": "5.2.4",
    "title": "Ensure the number of connections per IP address is limited",
    "assessment": "Manual",
    "description": "The maximum number of simultaneous connections allowed from a single IP address to your server should be limited. It should be set to a value that meets your organizational policies.",
    "rationale": "Limiting the number of simultaneous connections is an effective way to prevent slow denial of service attacks that try to use as many server resources as possible. This can also help prevent brute force attacks on a login page. Impact: Users of your system that are behind a corporate web proxy using network address translation or a proxy service such as tor may have an increased chance of being blocked due to this configuration. This is because multiple users in these scenarios come from the same IP address. You should always consider your user base when setting a connection limit.",
    "audit": "Verify the HTTP block and server block contain the below configuration. You may also need to check files included in include statements within your nginx config. http { limit_conn_zone $binary_remote_addr zone=limitperip:10m; server { limit_conn limitperip 10; } }",
    "remediation": "Implement the below directives under the HTTP and server blocks of your nginx configuration or any include files. The below configuration creates a memory zone of 10 megabytes called limitperip. It will limit the number of connections per IP address to 10 simultaneous connections. The number of simultaneous connections to allow may be different depending on your organization's policies and use cases.  http { limit_conn_zone $binary_remote_addr zone=limitperip:10m; server { limit_conn limitperip 10; } } Default Value: This value is not set by default. References: 1. https://www.nginx.com/resources/library/complete-nginx-cookbook/ 2. http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html 3. https://scotthelme.co.uk/mitigating-http-get-dos-attack/",
    "function_names": [
      "cloud_cdn_distribution_connection_limit_per_ip",
      "compute_load_balancer_connection_limit_per_ip",
      "network_load_balancer_connection_limit_per_ip",
      "application_load_balancer_connection_limit_per_ip",
      "cloudfront_distribution_connection_limit_per_ip",
      "api_gateway_connection_limit_per_ip",
      "cloud_run_service_connection_limit_per_ip",
      "cloud_functions_connection_limit_per_ip",
      "app_engine_connection_limit_per_ip",
      "cloud_sql_connection_limit_per_ip"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "impact": "Users of your system that are behind a corporate web proxy using network address translation or a proxy service such as tor may have an increased chance of being blocked due to this configuration. This is because multiple users in these scenarios come from the same IP address. You should always consider your user base when setting a connection limit.",
    "references": "1. https://www.nginx.com/resources/library/complete-nginx-cookbook/ 2. http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html 3. https://scotthelme.co.uk/mitigating-http-get-dos-attack/"
  },
  {
    "id": "5.2.5",
    "title": "Ensure rate limits by IP address are set",
    "assessment": "Manual",
    "description": "Rate limiting should be enabled to limit the number of requests an IP address may make to a server in a given period of time. The configuration values should be set based on your application's needs and your organizational policy.",
    "rationale": "Rate limiting allows you to mitigate potential denial of service attacks as a defense in depth mechanism. Impact: If you serve a high traffic API, this may prevent users from being able to call your website. You may also limit users behind a corporate web proxy or a proxy service such as tor if they use your website heavily.",
    "audit": "Verify the HTTP block and server block contains the below configuration. You may also need to check files included in include statements within your nginx config. http { limit_req_zone $binary_remote_addr zone=ratelimit:10m rate=5r/s; server { location / { limit_req zone=ratelimit burst=10 nodelay; } } }",
    "remediation": "Implement the below directives under the HTTP and server blocks of your nginx configuration or any include files. The below configuration creates a memory zone of 10 megabytes called \"ratelimit\" and sets the number of requests per second that can be sent by any given IP address to 5. Further, this configuration sets a burst of 10 to ensure that requests may come more frequently and sets no delay to ensure that the bursting may be all at once and not queued.  http { limit_req_zone $binary_remote_addr zone=ratelimit:10m rate=5r/s; server { location / { limit_req zone=ratelimit burst=10 nodelay; } } } Default Value: This is not set by default. References: 1. https://scotthelme.co.uk/mitigating-http-get-dos-attack/ 2. https://www.nginx.com/blog/rate-limiting-nginx/",
    "function_names": [
      "cloud_cdn_distribution_rate_limiting_enabled",
      "cloud_cdn_distribution_ip_rate_limiting_configured",
      "cloud_cdn_distribution_request_rate_limited",
      "cloud_cdn_distribution_per_ip_rate_limits_set",
      "cloud_cdn_distribution_rate_limits_by_ip_enabled"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver \u2022  Level 2 - Proxy \u2022  Level 2 - Loadbalancer",
    "impact": "If you serve a high traffic API, this may prevent users from being able to call your website. You may also limit users behind a corporate web proxy or a proxy service such as tor if they use your website heavily.",
    "references": "1. https://scotthelme.co.uk/mitigating-http-get-dos-attack/ 2. https://www.nginx.com/blog/rate-limiting-nginx/"
  },
  {
    "id": "5.3.1",
    "title": "Ensure X-Frame-Options header is configured and enabled",
    "assessment": "Automated",
    "description": "The X-Frame-Options header should be set to allow specific websites or no sites at all to embed your website as an object within their own, depending on your organizational policy and application needs.",
    "rationale": "The X-Frame-Options header allows you to mitigate the risk of clickjacking attacks. Impact: Implementing this may block legitimate partner sites from embedding your website if this header is not configured properly.",
    "audit": "Run the following to verify this header is implemented on your site: grep -ir X-Frame-Options /etc/nginx The output should look similar to the below, but customized to your use case. If there is no output from this command, this recommendation is not implemented. add_header X-Frame-Options \"SAMEORIGIN\" always;",
    "remediation": "Add the below to your server blocks in your nginx configuration. The policy should be configured to meet your organization's needs. add_header X-Frame-Options \"SAMEORIGIN\" always; Default Value: This is not configured by default. References: 1. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Frame-Options 2. https://scotthelme.co.uk/hardening-your-http-response-headers/ 3. https://www.veracode.com/blog/2014/03/guidelines-for-setting-security-headers 4. https://www.owasp.org/index.php/OWASP_Secure_Headers_Project  Additional Information: Important Note: The configuration in this recommendation recommends that this header is set to the same origin; however, this does not mean that if it is not set so, that this is done incorrectly. This header may also be configured to allow from specific origins or deny from all origins. The always parameter ensures that this header is applied no matter what the http response code is.",
    "function_names": [
      "cloud_cdn_response_header_x_frame_options_configured",
      "cloud_cdn_response_header_x_frame_options_enabled",
      "cloud_cdn_response_header_x_frame_options_deny_or_sameorigin",
      "cloud_cdn_response_header_x_frame_options_no_allow_from",
      "cloud_cdn_response_header_x_frame_options_valid_value",
      "cloud_cdn_response_header_x_frame_options_not_disabled",
      "cloud_cdn_response_header_x_frame_options_not_missing",
      "cloud_cdn_response_header_x_frame_options_properly_set"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver",
    "impact": "Implementing this may block legitimate partner sites from embedding your website if this header is not configured properly.",
    "references": "1. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Frame-Options 2. https://scotthelme.co.uk/hardening-your-http-response-headers/ 3. https://www.veracode.com/blog/2014/03/guidelines-for-setting-security-headers 4. https://www.owasp.org/index.php/OWASP_Secure_Headers_Project  Additional Information: Important Note: The configuration in this recommendation recommends that this header is set to the same origin; however, this does not mean that if it is not set so, that this is done incorrectly. This header may also be configured to allow from specific origins or deny from all origins. The always parameter ensures that this header is applied no matter what the http response code is."
  },
  {
    "id": "5.3.2",
    "title": "Ensure X-Content-Type-Options header is configured and enabled",
    "assessment": "Automated",
    "description": "The X-Content-Type-Options header should be used to force supported user agents to check an HTTP response's content type header with what is expected from the destination of the request.",
    "rationale": "Implementing the X-Content-Type-Options header with the \"nosniff\" directive helps to prevent drive-by download attacks where a user agent is sniffing content types in responses.",
    "audit": "Run this command to verify the X-Content-Type-Options Header is enabled and set to not allow content type sniffing: grep -ir X-Content-Type-Options /etc/nginx The below should be part of the output. If it is not, this recommendation is not implemented. This should be implemented on every server block. If there are multiple server blocks on the system, each should be checked. add_header X-Content-Type-Options \"nosniff\" always;",
    "remediation": "Open the nginx configuration file that contains your server blocks. Add the below line into your server block to add X-Content-Type-Options header and direct your user agent to not sniff content types. add_header X-Content-Type-Options \"nosniff\" always; Default Value: This header is not implemented by default. References: 1. https://scotthelme.co.uk/hardening-your-http-response-headers/ 2. https://www.veracode.com/blog/2014/03/guidelines-for-setting-security-headers 3. https://www.owasp.org/index.php/OWASP_Secure_Headers_Project 4. https://fetch.spec.whatwg.org/#x-content-type-options-header  5. https://www.iana.org/assignments/message-headers/message- headers.xml#perm-headers Additional Information: The always parameter ensures that the header is applied to the HTTP response no matter what the HTTP response code is.",
    "function_names": [
      "cloud_cdn_header_x_content_type_options_enabled",
      "cloud_cdn_response_header_content_type_options_configured",
      "cloud_cdn_security_header_x_content_type_options_set",
      "cloud_cdn_http_header_x_content_type_options_enabled",
      "cloud_cdn_content_type_options_header_enforced"
    ],
    "profile_applicability": "\u2022  Level 1 - Webserver",
    "references": "1. https://scotthelme.co.uk/hardening-your-http-response-headers/ 2. https://www.veracode.com/blog/2014/03/guidelines-for-setting-security-headers 3. https://www.owasp.org/index.php/OWASP_Secure_Headers_Project 4. https://fetch.spec.whatwg.org/#x-content-type-options-header  5. https://www.iana.org/assignments/message-headers/message- headers.xml#perm-headers Additional Information: The always parameter ensures that the header is applied to the HTTP response no matter what the HTTP response code is."
  },
  {
    "id": "5.3.3",
    "title": "Ensure that Content Security Policy (CSP) is enabled and configured properly",
    "assessment": "Manual",
    "description": "Content Security Policy allows administrators to specify the locations from which allowable scripts may be executed, or if scripts may be executed at all. Content Security Policy should be used to improve user trust of your website.",
    "rationale": "Content Security Policies assist organizations in mitigating and reporting cross-site scripting (XSS) attacks.",
    "audit": "Run this command to verify the Content Security Policies header is enabled: grep -ir Content-Security-Policy /etc/nginx Output similar to the below shows this recommendation is implemented. It should be implemented on every server block. If there are multiple server blocks on the system, each should be checked. add_header Content-Security-Policy \"default-src 'self'\" always;",
    "remediation": "Open your nginx configuration file that contains your server blocks. Add the below line into your server block to add Content-Security-Policy and direct your user agent to accept documents from only specific origins. add_header Content-Security-Policy \"default-src 'self'\" always; Default Value: This is not enabled by default. References: 1. https://scotthelme.co.uk/hardening-your-http-response-headers/ 2. https://www.owasp.org/index.php/OWASP_Secure_Headers_Project 3. https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP 4. https://www.w3.org/TR/CSP3/  Additional Information: Important Note: Content Security Policy may be customized for a significant number of use cases, including upgrading insecure requests, directing the origin of executables, and reporting violations of the policy. This is a simplistic version that does not go into the depth of what a CSP may do and is not representative of how the policy should look for your site. Organizations should ensure that their CSP will meet their specific use case and needs. The always parameter ensures that the header is returned in the response no matter what the HTTP response code is. Note: If your CSP is not continuously updated as your application adds resources that come from different origins or if the CSP is not correct the first time, you may block execution from legitimate origins.",
    "function_names": [
      "cloud_cdn_distribution_csp_enabled",
      "cloud_cdn_distribution_csp_configured_properly",
      "cloud_cdn_distribution_csp_script_sources_restricted",
      "cloud_cdn_distribution_csp_inline_scripts_blocked",
      "cloud_cdn_distribution_csp_default_src_restricted",
      "cloud_cdn_distribution_csp_report_uri_configured",
      "cloud_cdn_distribution_csp_frame_ancestors_restricted",
      "cloud_cdn_distribution_csp_object_src_restricted",
      "cloud_cdn_distribution_csp_base_uri_restricted",
      "cloud_cdn_distribution_csp_form_action_restricted"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver",
    "references": "1. https://scotthelme.co.uk/hardening-your-http-response-headers/ 2. https://www.owasp.org/index.php/OWASP_Secure_Headers_Project 3. https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP 4. https://www.w3.org/TR/CSP3/  Additional Information: Important Note: Content Security Policy may be customized for a significant number of use cases, including upgrading insecure requests, directing the origin of executables, and reporting violations of the policy. This is a simplistic version that does not go into the depth of what a CSP may do and is not representative of how the policy should look for your site. Organizations should ensure that their CSP will meet their specific use case and needs. The always parameter ensures that the header is returned in the response no matter what the HTTP response code is. Note: If your CSP is not continuously updated as your application adds resources that come from different origins or if the CSP is not correct the first time, you may block execution from legitimate origins."
  },
  {
    "id": "5.3.4",
    "title": "Ensure the Referrer Policy is enabled and configured properly",
    "assessment": "Manual",
    "description": "When an origin site directs a user to another site, a referrer is sent that identifies the URL the user came from. Depending on your site's specific use, this may present a privacy concern to your users. The Referrer Policy enables organizations to define what sites should see that a referral came from your site, which helps protect user privacy.",
    "rationale": "A Referrer header may expose sensitive data in another web server's log if you use sensitive data in your URL parameters, such as personal information, username, and password or persistent sessions. Ultimately, depending on your application design, not using a properly configured Referrer Policy may allow session hijacking, credential gathering, or sensitive data exposure in a third party's logs.",
    "audit": "Verify your referrer policy is enabled and configured properly by running the following command. You should check to ensure that the header is part of the server block of all sites. grep -r Referrer-Policy /etc/nginx The output should look similar to the below. The policy may differ depending on your organization's needs. add_header Referrer-Policy \"no-referrer\";",
    "remediation": "Add the below line to the server blocks within your nginx configuration. The policy should be customized for your specific organization's needs. The below policy will ensure your website is never allowed in a referrer. add_header Referrer-Policy \"no-referrer\"; Default Value: This policy is not set by default. References: 1. https://scotthelme.co.uk/a-new-security-header-referrer-policy/ 2. https://www.w3.org/TR/referrer-policy/  Additional Information: Important Note: This header should be customized to your application and \"no- referrer\" is not always appropriate. This may vary depending on your circumstances. The always parameter ensures that the header is returned in the response no matter what the HTTP response code is.",
    "function_names": [
      "cloud_cdn_origin_referrer_policy_enabled",
      "cloud_cdn_origin_referrer_policy_configured_properly",
      "cloud_cdn_origin_referrer_policy_no_referrer_when_downgrade",
      "cloud_cdn_origin_referrer_policy_same_origin",
      "cloud_cdn_origin_referrer_policy_strict_origin_when_cross_origin",
      "cloud_cdn_origin_referrer_policy_unsafe_url",
      "cloud_cdn_origin_referrer_policy_no_referrer",
      "cloud_cdn_origin_referrer_policy_strict_origin",
      "cloud_cdn_origin_referrer_policy_origin_when_cross_origin",
      "cloud_cdn_origin_referrer_policy_origin"
    ],
    "profile_applicability": "\u2022  Level 2 - Webserver",
    "references": "1. https://scotthelme.co.uk/a-new-security-header-referrer-policy/ 2. https://www.w3.org/TR/referrer-policy/  Additional Information: Important Note: This header should be customized to your application and \"no- referrer\" is not always appropriate. This may vary depending on your circumstances. The always parameter ensures that the header is returned in the response no matter what the HTTP response code is."
  }
]