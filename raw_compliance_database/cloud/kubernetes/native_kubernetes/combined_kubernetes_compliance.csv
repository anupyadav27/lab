id,title,description,rationale,audit,remediation,profile_applicability,impact,references,source_file,original_filename
1.1.1,Ensure that the API server pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the API server pod specification file has permissions of 600 or more restrictive.,The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_api_server_pod_spec_file_permissions_restricted; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_permissions_strict; kubernetes_api_server_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.2,Ensure that the API server pod specification file ownership is set to root:root,Automated,Ensure that the API server pod specification file ownership is set to root:root.,The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_owner_root; kubernetes_api_server_pod_spec_file_group_root; kubernetes_api_server_pod_spec_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.3,Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the controller manager pod specification file has permissions of 600 or more restrictive.,The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, the kube-controller-manager.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_controller_manager_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_restrictive; kubernetes_controller_manager_file_permissions_secure; kubernetes_controller_manager_pod_spec_file_permissions_compliant; kubernetes_controller_manager_file_permissions_600_or_stricter,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.4,Ensure that the controller manager pod specification file ownership is set to root:root,Automated,Ensure that the controller manager pod specification file ownership is set to root:root.,The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, kube-controller-manager.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_pod_spec_ownership_root; kubernetes_controller_manager_file_permissions_root; kubernetes_controller_manager_spec_file_ownership_root; kubernetes_controller_manager_pod_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager
1.1.5,Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler pod specification file has permissions of 600 or more restrictive.,The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_spec_file_permissions_600_or_stricter; kubernetes_scheduler_pod_spec_file_permissions_restrictive; kubernetes_scheduler_pod_spec_file_permissions_secure; kubernetes_scheduler_pod_spec_file_permissions_compliant; kubernetes_scheduler_pod_spec_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.6,Ensure that the scheduler pod specification file ownership is set to root:root,Automated,Ensure that the scheduler pod specification file ownership is set to root:root.,The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_file_ownership_root; kubernetes_scheduler_pod_spec_root_owned; kubernetes_scheduler_pod_file_root_root; kubernetes_scheduler_spec_file_ownership_root; kubernetes_scheduler_pod_root_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.7,Ensure that the etcd pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file has permissions of 600 or more restrictive.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/etcd.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_pod_spec_file_permissions_restrictive; kubernetes_etcd_manifest_file_permissions_600; kubernetes_etcd_yaml_file_permissions_secure; kubernetes_manifests_etcd_file_permissions_restricted; kubernetes_etcd_spec_file_permissions_600_or_stricter,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.8,Ensure that the etcd pod specification file ownership is set to root:root,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_manifest_file_ownership_root; kubernetes_etcd_pod_spec_file_ownership_root; kubernetes_etcd_yaml_file_ownership_root; kubernetes_manifest_etcd_file_ownership_root; kubernetes_etcd_config_file_ownership_root,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.9,Ensure that the Container Network Interface file permissions are set to 600 or more restrictive,Manual,Ensure that the Container Network Interface files have permissions of 600 or more restrictive.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a <path/to/cni/files> Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",compute_container_network_interface_file_permissions_600_or_more_restrictive; compute_cni_file_permissions_600_or_more_restrictive; container_network_interface_file_permissions_secure; cni_file_permissions_600_or_stricter,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.10,Ensure that the Container Network Interface file ownership is set to root:root,Manual,Ensure that the Container Network Interface files have ownership set to root:root.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G <path/to/cni/files> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_ownership_root; container_network_interface_file_owner_root; container_network_interface_file_group_root; container_network_interface_file_permissions_root_only; container_network_interface_file_access_root_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Automated,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd Default Value: By default, etcd data directory has permissions of 755. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_700_or_more_restrictive; etcd_data_directory_permissions_restrictive; etcd_directory_permissions_secure; etcd_data_directory_permissions_compliant; etcd_directory_permissions_700_or_stricter,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Automated,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd Default Value: By default, etcd data directory ownership is set to etcd:etcd. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_ownership_etcd_etcd; etcd_directory_permissions_etcd_etcd; etcd_data_directory_secure_ownership; etcd_data_directory_correct_ownership; etcd_directory_ownership_valid,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.13,Ensure that the default administrative credential file permissions are set to 600,Automated,"Ensure that the admin.conf file (and super-admin.conf file, where it exists) have permissions of 600.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should restrict their file permissions to maintain the integrity and confidentiality of the file(s). The file(s) should be readable and writable by only the administrators on the system. Impact: None.","Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %a /etc/kubernetes/super-admin.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chmod 600 /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf have permissions of 600. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",kubernetes_admin_credential_file_permissions_600; kubernetes_admin_conf_file_permissions_600; kubernetes_super_admin_conf_file_permissions_600; kubernetes_credential_file_permissions_restricted; kubernetes_admin_file_permissions_secure,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.14,Ensure that the default administrative credential file ownership is set to root:root,Automated,"Ensure that the admin.conf (and super-admin.conf file, where it exists) file ownership is set to root:root.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should set their file ownership to maintain the integrity and confidentiality of the file. The file(s) should be owned by root:root. Impact: None.","Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %a /etc/kubernetes/super-admin.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chown root:root /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",compute_admin_credential_file_owner_root; compute_admin_credential_file_group_root; compute_super_admin_credential_file_owner_root; compute_super_admin_credential_file_group_root,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.15,Ensure that the scheduler.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler.conf file has permissions of 600 or more restrictive.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/scheduler.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",scheduler_file_permissions_restrictive; scheduler_conf_file_permissions_600; scheduler_conf_file_permissions_restricted; scheduler_file_permissions_secure; scheduler_conf_file_permissions_secure,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.16,Ensure that the scheduler.conf file ownership is set to root:root,Automated,Ensure that the scheduler.conf file ownership is set to root:root.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/scheduler.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",scheduler_file_root_ownership; scheduler_conf_root_ownership; scheduler_config_root_ownership; scheduler_file_ownership_root; scheduler_conf_ownership_root; scheduler_config_ownership_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.17,Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the controller-manager.conf file has permissions of 600 or more restrictive.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/controller-manager.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_permissions_600_or_stricter; kubernetes_controller_manager_conf_file_permissions_restrictive; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_permissions_compliant; kubernetes_controller_manager_conf_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.18,Ensure that the controller-manager.conf file ownership is set to root:root,Automated,Ensure that the controller-manager.conf file ownership is set to root:root.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/controller-manager.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_ownership_root; kubernetes_controller_manager_conf_file_group_ownership_root; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_ownership_root_root; kubernetes_controller_manager_conf_file_ownership_correct,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.19,Ensure that the Kubernetes PKI directory and file ownership is set to root:root,Automated,Ensure that the Kubernetes PKI directory and file ownership is set to root:root.,Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/ Verify that the ownership of all files and directories in this hierarchy is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown -R root:root /etc/kubernetes/pki/ Default Value: By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained within it, are set to be owned by the root user. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_directory_ownership_root; kubernetes_pki_file_ownership_root; kubernetes_pki_directory_permissions_root; kubernetes_pki_file_permissions_root; kubernetes_pki_directory_and_file_ownership_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.20,Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive,Manual,Ensure that Kubernetes PKI certificate files have permissions of 600 or more restrictive.,Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.crt Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.crt Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.crt Default Value: By default, the certificates used by Kubernetes are set to have permissions of 644 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_certificate_file_permissions_restrictive; kubernetes_pki_certificate_file_permissions_600_or_stricter; kubernetes_certificate_file_permissions_restrictive; kubernetes_certificate_file_permissions_600_or_stricter; kubernetes_pki_certificate_file_permissions_secure,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.21,Ensure that the Kubernetes PKI key file permissions are set to 600,Manual,Ensure that Kubernetes PKI key files have permissions of 600.,Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.key Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.key Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.key Default Value: By default, the keys used by Kubernetes are set to have permissions of 600 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_key_file_permissions_600; kubernetes_pki_key_file_permissions_restricted; kubernetes_pki_key_file_permissions_secure; kubernetes_pki_key_file_permissions_strict; kubernetes_pki_key_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.1,Ensure that the --anonymous-auth argument is set to false,Manual,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Impact: Anonymous requests will be rejected.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--anonymous-auth' | grep -i false If the exit code is '1', then the control isn't present / failed","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --anonymous-auth=false Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_no_anonymous_auth; kubernetes_api_server_auth_enabled; kubernetes_api_server_secure_auth; kubernetes_api_server_anonymous_access_disabled,• Level 1 - Master Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests
1.2.2,Ensure that the --token-auth-file parameter is not set,Automated,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token- based authentication. Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist. Alternative Audit Method kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--token-auth-file' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the --token-auth-file=<filename> parameter. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_disabled; kubernetes_api_server_no_token_auth_file; kubernetes_auth_token_file_unset; kubernetes_api_server_token_auth_removed; kubernetes_auth_token_file_disabled,• Level 1 - Master Node,You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.,1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.3,Ensure that the DenyServiceExternalIPs is set,Manual,This admission controller rejects all net-new usage of the Service field externalIPs.,"Most users do not need the ability to set the externalIPs field for a Service at all, and cluster admins should consider disabling this functionality by enabling the DenyServiceExternalIPs admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage. Impact: When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the `DenyServiceExternalIPs' argument exist as a string value in --disable- admission-plugins.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the `-- DenyServiceExternalIPs'parameter or The Kubernetes API server flag disable-admission-plugins takes a comma-delimited list of admission control plugins to be disabled, even if they are in the list of plugins enabled by default. kube-apiserver --disable-admission-plugins=DenyServiceExternalIPs,AlwaysDeny ... Default Value: By default, --disable-admission-plugins=DenyServiceExternalIP argument is not set. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_service_external_ips_denied; kubernetes_service_external_ips_restricted; admission_controller_external_ips_denied; admission_controller_service_external_ips_blocked; kubernetes_admission_external_ips_disabled,• Level 1 - Master Node,"When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.4,Ensure that the --kubelet-client-certificate and --kubelet- client-key arguments are set as appropriate,Automated,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-client-certificate' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below. --kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file> Default Value: By default, certificate-based kubelet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured; kubernetes_kubelet_authentication_certificates_valid; kubernetes_kubelet_tls_authentication_required,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.5,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Automated,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-certificate-Authority' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string> Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_set; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_authority_secure; kubernetes_kubelet_certificate_authority_verified,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.6,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not always authorize all requests.,"The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Impact: Only authorized requests will be served.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below. --authorization-mode=RBAC Default Value: By default, AlwaysAllow is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",eks_cluster_authorization_mode_not_always_allow; eks_cluster_authorization_mode_restricted; eks_cluster_authorization_mode_secure; eks_cluster_authorization_mode_compliant; eks_cluster_authorization_mode_enforced,• Level 1 - Master Node,Only authorized requests will be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.2.7,Ensure that the --authorization-mode argument includes Node,Automated,Restrict kubelet nodes to reading only objects associated with them.,"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include Node.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC Default Value: By default, Node authorization is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_authorization_enabled; kubernetes_kubelet_node_authorization_required; kubernetes_kubelet_node_authorization_restricted; kubernetes_kubelet_node_authorization_mode_configured; kubernetes_kubelet_node_authorization_mode_included,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security
1.2.8,Ensure that the --authorization-mode argument includes RBAC,Automated,Turn on Role Based Access Control.,"Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Impact: When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include RBAC.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes RBAC, for example: --authorization-mode=Node,RBAC Default Value: By default, RBAC authorization is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/",kubernetes_cluster_rbac_enabled; kubernetes_cluster_authorization_mode_rbac; kubernetes_cluster_rbac_required; kubernetes_cluster_auth_mode_rbac_included; kubernetes_cluster_rbac_authorization_enabled,• Level 1 - Master Node,"When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
1.2.9,Ensure that the admission control plugin EventRateLimit is set,Manual,Limit the rate at which the API server accepts requests.,"Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Impact: You need to carefully tune in limits as per your environment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit.,"Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=<path/to/configuration/file> Default Value: By default, EventRateLimit is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md",kubernetes_api_server_event_rate_limit_enabled; kubernetes_admission_control_event_rate_limit_configured; kubernetes_api_request_rate_limiting_active; kubernetes_admission_plugin_event_rate_limit_set; kubernetes_api_server_rate_limiting_enabled,• Level 1 - Master Node,You need to carefully tune in limits as per your environment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md
1.2.10,Ensure that the admission control plugin AlwaysAdmit is not set,Automated,Do not allow all requests.,Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Impact: Only requests explicitly allowed by the admissions control plugins would be served.,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --enable-admission- plugins parameter, or set it to a value that does not include AlwaysAdmit. Default Value: AlwaysAdmit is not in the list of default admission plugins. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit",kubernetes_admission_controller_always_admit_disabled; kubernetes_admission_plugin_always_admit_not_set; kubernetes_admission_policy_always_admit_restricted; kubernetes_admission_control_always_admit_denied; kubernetes_admission_rule_always_admit_blocked,• Level 1 - Master Node,Only requests explicitly allowed by the admissions control plugins would be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit
1.2.11,Ensure that the admission control plugin AlwaysPullImages is set,Manual,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages",kubernetes_admission_controller_always_pull_images_enabled; kubernetes_pod_images_always_pulled; admission_controller_always_pull_images; kubernetes_pod_images_pull_policy_always; admission_controller_always_pull_images_enabled,• Level 1 - Master Node,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages
1.2.12,Ensure that the admission control plugin ServiceAccount is set,Automated,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Impact: None.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and ensure that the --disable-admission-plugins parameter is set to a value that does not include ServiceAccount. Default Value: By default, ServiceAccount is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_service_account_admission_plugin_enabled; kubernetes_service_account_automated_management_enabled; kubernetes_admission_controller_service_account_required; kubernetes_service_account_admission_control_enabled; kubernetes_admission_plugin_service_account_configured,• Level 2 - Master Node,None.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.2.13,Ensure that the admission control plugin NamespaceLifecycle is set,Automated,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --disable-admission-plugins parameter to ensure it does not include NamespaceLifecycle. Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle",kubernetes_namespace_admission_control_enabled; kubernetes_namespace_lifecycle_plugin_enabled; kubernetes_admission_namespace_termination_protected; kubernetes_namespace_termination_admission_enabled; kubernetes_admission_plugin_namespace_lifecycle_set,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle
1.2.14,Ensure that the admission control plugin NodeRestriction is set,Automated,Limit the Node and Pod objects that a kubelet could modify.,"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes NodeRestriction.,"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the -- enable-admission-plugins parameter to a value that includes NodeRestriction. --enable-admission-plugins=...,NodeRestriction,... Default Value: By default, NodeRestriction is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_restriction_enabled; kubernetes_admission_control_node_restriction_enabled; kubernetes_kubelet_pod_modification_restricted; kubernetes_admission_plugin_node_restriction_enabled; kubernetes_kubelet_node_restriction_active,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security
1.2.15,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_api_server_profiling_disabled; kubernetes_api_server_profiling_set_false; kubernetes_api_server_profiling_config_disabled; kubernetes_api_server_profiling_argument_disabled; kubernetes_api_server_profiling_flag_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.2.16,Ensure that the --audit-log-path argument is set,Automated,Enable auditing on the Kubernetes API Server and set the desired audit log path.,"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_path_set; kubernetes_api_server_audit_logging_enabled; kubernetes_api_server_audit_log_path_configured; kubernetes_api_server_audit_log_path_valid; kubernetes_api_server_audit_log_path_specified,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.17,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Automated,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxage_set_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_compliant; kubernetes_api_server_audit_log_retention_min_30d,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.18,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Automated,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_10_or_more; kubernetes_api_server_audit_log_retention_configured; kubernetes_api_server_audit_log_backup_limit_valid; kubernetes_api_server_audit_log_maxbackup_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.19,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Automated,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_rotation_enabled; kubernetes_api_server_audit_log_size_limited; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_api_server_audit_log_rotation_threshold_set,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.20,Ensure that the --request-timeout argument is set as appropriate,Manual,Set global request timeout for API server requests as appropriate.,"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --request-timeout argument is either not set or set to an appropriate value.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s Default Value: By default, --request-timeout is set to 60 seconds. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415",kubernetes_api_server_request_timeout_set; kubernetes_api_server_request_timeout_configured; kubernetes_api_server_request_timeout_appropriate; kubernetes_api_server_request_timeout_valid; kubernetes_api_server_request_timeout_within_range,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415
1.2.21,Ensure that the --service-account-lookup argument is set to true,Automated,Validate service account before validating token.,"If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --service-account-lookup argument exists it is set to true.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --service-account-lookup=true Alternatively, you can delete the --service-account-lookup parameter from this file so that the default takes effect. Default Value: By default, --service-account-lookup argument is set to true. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_service_account_pre_validation_enabled; kubernetes_api_server_service_account_token_validation_enabled; kubernetes_api_server_service_account_secure_lookup_enabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use
1.2.22,Ensure that the --service-account-key-file argument is set as appropriate,Automated,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file. Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --service-account-key-file parameter to the public key file for service accounts: --service-account-key-file=<filename> Default Value: By default, --service-account-key-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_explicit,• Level 1 - Master Node,The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.2.23,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters. --etcd-certfile=<path/to/client-certificate-file> --etcd-keyfile=<path/to/client-key-file> Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_certfile_configured; kubernetes_etcd_keyfile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_client_connections,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.24,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Automated,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the TLS certificate and private key file parameters. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Default Value: By default, --tls-cert-file and --tls-private-key-file are presented and created for use. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_cert_file_appropriate; kubernetes_api_server_tls_private_key_file_appropriate; kubernetes_api_server_tls_connection_configured,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.25,Ensure that the --client-ca-file argument is set as appropriate,Automated,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the client certificate authority file. --client-ca-file=<path/to/client-ca-file> Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_configured; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_certificate_validation_enabled; kubernetes_api_server_secure_connection_required; kubernetes_api_server_client_ca_file_set; kubernetes_api_server_tls_client_auth_enabled; kubernetes_api_server_client_ca_file_valid; kubernetes_api_server_tls_client_cert_auth_required,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.26,Ensure that the --etcd-cafile argument is set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter. --etcd-cafile=<path/to/ca-file> Default Value: By default, --etcd-cafile is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_cafile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_connection_required,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.27,Ensure that the --encryption-provider-config argument is set as appropriate,Manual,Encrypt etcd key-value store.,etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Impact: None,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --encryption-provider-config argument is set to a EncryptionConfig file. Additionally, ensure that the EncryptionConfig file has all the desired resources covered especially any secrets.","Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=</path/to/EncryptionConfig/File> Default Value: By default, --encryption-provider-config is not set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92",etcd_key_value_store_encryption_enabled; etcd_encryption_provider_config_set; etcd_data_encryption_enabled; etcd_encryption_config_valid; etcd_secure_encryption_provider_configured,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92
1.2.28,Ensure that encryption providers are appropriately configured,Manual,"Where etcd encryption is used, appropriate providers should be configured.","Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms and secretbox are likely to be appropriate options. Impact: None","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Get the EncryptionConfig file set for --encryption-provider-config argument. Verify that aescbc, kms or secretbox is set as the encryption provider for all the desired resources.","Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms or secretbox as the encryption provider. Default Value: By default, no encryption provider is set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers",,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers
1.2.29,Ensure that the API Server only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the API server is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter. --tls-cipher- suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SH A256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SH A256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SH A384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POL Y1305_SHA256,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_C BC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_S HA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 ,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TL S_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_2 56_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384. Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers References: 1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_api_server_strong_ciphers_enabled; kubernetes_api_server_weak_ciphers_disabled; kubernetes_api_server_tls_min_version_1_2; kubernetes_api_server_cipher_suites_restricted; kubernetes_api_server_insecure_ciphers_removed,• Level 1 - Master Node,API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.,"1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues"
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Manual,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --terminated-pod-gc- threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_cluster_pod_garbage_collection_enabled; kubernetes_cluster_terminated_pod_gc_threshold_set; kubernetes_cluster_pod_cleanup_threshold_configured; kubernetes_cluster_pod_termination_gc_active; kubernetes_cluster_pod_garbage_collection_threshold_appropriate,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; eks_cluster_profiling_set_false; eks_cluster_profiling_not_enabled; eks_cluster_profiling_config_disabled; eks_cluster_profiling_argument_false,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.3.3,Ensure that the --use-service-account-credentials argument is set to true,Automated,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service- account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node to set the below parameter. --use-service-account-credentials=true Default Value: By default, --use-service-account-credentials is set to false. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",kubernetes_controller_service_account_credentials_enabled; kubernetes_controller_service_account_credentials_required; kubernetes_controller_service_account_credentials_used; kubernetes_controller_service_account_credentials_exclusive; kubernetes_controller_service_account_credentials_enforced,• Level 1 - Master Node,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.4,Ensure that the --service-account-private-key-file argument is set as appropriate,Automated,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private- key-file as appropriate. Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --service-account- private-key-file parameter to the private key file for service accounts. --service-account-private-key-file=<filename> Default Value: By default, --service-account-private-key-file it not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_specified; kubernetes_controller_manager_service_account_private_key_file_provided; kubernetes_controller_manager_service_account_private_key_file_valid,• Level 1 - Master Node,You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.5,Ensure that the --root-ca-file argument is set as appropriate,Automated,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Impact: You need to setup and maintain root certificate authority file.,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`. --root-ca-file=<path/to/file> Default Value: By default, --root-ca-file is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_api_server_root_ca_file_configured; kubernetes_api_server_root_ca_file_valid; kubernetes_api_server_root_ca_file_secure; kubernetes_api_server_root_ca_file_present; kubernetes_api_server_root_ca_file_trusted,• Level 1 - Master Node,You need to setup and maintain root certificate authority file.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.3.6,Ensure that the RotateKubeletServerCertificate argument is set to true,Automated,Enable kubelet server certificate rotation on controller-manager.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true. --feature-gates=RotateKubeletServerCertificate=true Default Value: By default, RotateKubeletServerCertificate is set to 'true' this recommendation verifies that it has not been disabled. References: 1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_rotate_kubelet_server_certificate_enabled; kubernetes_controller_manager_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_auto_rotation_enabled; kubernetes_controller_manager_kubelet_certificate_rotation_enabled; kubernetes_kubelet_certificate_rotation_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.7,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the Controller Manager service to non-loopback insecure addresses.,"The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address",kubernetes_controller_manager_bind_address_localhost; kubernetes_controller_manager_loopback_only; kubernetes_controller_manager_secure_bind_address; kubernetes_controller_manager_localhost_bind_enabled; kubernetes_controller_manager_restrict_bind_address,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address
1.4.1,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml file on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_cluster_profiling_disabled; kubernetes_cluster_profiling_set_false; kubernetes_cluster_profiling_not_enabled; kubernetes_cluster_profiling_config_disabled; kubernetes_cluster_profiling_flag_false,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.4.2,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the scheduler service to non-loopback insecure addresses.,"The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml on the Control Plane node and ensure the correct value for the --bind- address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/",kubernetes_scheduler_bind_address_localhost; kubernetes_scheduler_loopback_only; kubernetes_scheduler_insecure_bind_disabled; kubernetes_scheduler_bind_address_secure; kubernetes_scheduler_localhost_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/
2.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_tls_cert_key_valid; etcd_service_secure_communication_enabled,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.2,Ensure that the --client-cert-auth argument is set to true,Automated,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: All clients attempting to access the etcd server will require a valid client certificate.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --client-cert-auth='true' Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_auth_enabled; etcd_client_cert_auth_required; etcd_client_authentication_enabled; etcd_service_client_cert_auth_enabled; etcd_client_auth_required,• Level 1 - Master Node,All clients attempting to access the etcd server will require a valid client certificate.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
2.3,Ensure that the --auto-tls argument is not set to true,Automated,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: Clients will not be able to use self-signed certificates for TLS.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --auto-tls parameter or set it to false. --auto-tls=false Default Value: By default, --auto-tls is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_domain_self_signed_certificates_disabled; cloud_cdn_domain_tls_certificate_valid; cloud_cdn_domain_tls_managed_certificate_required,• Level 1 - Master Node,Clients will not be able to use self-signed certificates for TLS.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
2.4,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Impact: etcd cluster peers would need to set up TLS for their communication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --peer-client-file=</path/to/peer-cert-file> --peer-key-file=</path/to/peer-key-file> Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_peer_certificate_file_set; etcd_peer_key_file_set; etcd_peer_tls_encryption_enabled; etcd_peer_certificate_file_configured; etcd_peer_key_file_configured,• Level 1 - Master Node,etcd cluster peers would need to set up TLS for their communication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.5,Ensure that the --peer-client-cert-auth argument is set to true,Automated,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --peer-client-cert-auth=true Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth",etcd_cluster_peer_client_cert_auth_enabled; etcd_cluster_peer_authentication_required; etcd_cluster_peer_tls_auth_enabled; etcd_cluster_peer_cert_validation_enabled; etcd_cluster_peer_client_cert_auth_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth
2.6,Ensure that the --peer-auto-tls argument is not set to true,Automated,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --peer-auto-tls parameter or set it to false. --peer-auto-tls=false Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",kubernetes_cluster_peer_auto_tls_disabled; kubernetes_cluster_peer_tls_manual_certificates_required; kubernetes_cluster_peer_tls_self_signed_disabled; kubernetes_cluster_peer_tls_auto_generation_disabled; kubernetes_cluster_peer_tls_custom_certificates_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
2.7,Ensure that a unique Certificate Authority is used for etcd,Manual,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster. Run the following command on the master node: ps -ef | grep etcd Note the file referenced by the --trusted-ca-file argument. Run the following command on the master node: ps -ef | grep apiserver Verify that the file referenced by the --client-ca-file for apiserver is different from the --trusted-ca-file used by etcd.,"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --trusted-ca-file=</path/to/ca-file> Default Value: By default, no etcd certificate is created and used. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_unique_certificate_authority; etcd_certificate_authority_separate_from_kubernetes; etcd_ca_not_shared_with_kubernetes; kubernetes_etcd_ca_distinct; etcd_unique_ca_required,• Level 2 - Master Node,Additional management of the certificates and keys for the dedicated certificate authority will be required.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
3.1.1,Client certificate authentication should not be used for users,Manual,"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication.,"Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. Default Value: Client certificate authentication is enabled by default. Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",kubernetes_user_client_certificate_auth_disabled; kubernetes_user_no_client_certificate_auth; kubernetes_auth_no_client_certificates; kubernetes_user_auth_no_client_certs; kubernetes_auth_client_certificate_disabled,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.2,Service account token authentication should not be used for users,Manual,"Kubernetes provides service account tokens which are intended for use by workloads running in the Kubernetes cluster, for authentication to the API server. These tokens are not designed for use by end-users and do not provide for features such as revocation or expiry, making them insecure. A newer version of the feature (Bound service account token volumes) does introduce expiry but still does not allow for specific revocation.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Service account token authentication does not allow for this due to the use of JWT tokens as an underlying technology. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of service account token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of service account tokens. Default Value: Service account token authentication is enabled by default.,kubernetes_service_account_token_user_auth_disabled; kubernetes_service_account_token_user_auth_not_used; kubernetes_service_account_token_user_auth_prohibited; kubernetes_service_account_token_user_auth_restricted; kubernetes_service_account_token_user_auth_revoked,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.3,Bootstrap token authentication should not be used for users,Manual,Kubernetes provides bootstrap tokens which are intended for use by new nodes joining the cluster These tokens are not designed for use by end-users they are specifically designed for the purpose of bootstrapping new nodes and not for general authentication,Bootstrap tokens are not intended for use as a general authentication mechanism and impose constraints on user and group naming that do not facilitate good RBAC design. They also cannot be used with MFA resulting in a weak authentication mechanism being available. Impact: External mechanisms for authentication generally require additional software to be deployed.,Review user access to the cluster and ensure that users are not making use of bootstrap token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of bootstrap tokens. Default Value: Bootstrap token authentication is not enabled by default and requires an API server parameter to be set.,kubernetes_user_no_bootstrap_token_auth; kubernetes_user_no_bootstrap_token_auth_enabled; kubernetes_auth_no_bootstrap_token_for_users; kubernetes_token_auth_no_bootstrap_for_users; kubernetes_user_auth_no_bootstrap_token,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.2.1,Ensure that a minimal audit policy is created,Manual,Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.,"Logging is an important detective control for all systems, to detect potential unauthorised access. Impact: Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",Run the following command on one of the cluster master nodes: ps -ef | grep kube-apiserver Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy.,"Create an audit policy file for your cluster. Default Value: Unless the --audit-policy-file flag is specified, no auditing will be carried out. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/",kubernetes_api_audit_policy_enabled; kubernetes_api_audit_policy_minimal; kubernetes_api_audit_logging_enabled; kubernetes_audit_policy_file_configured; kubernetes_audit_policy_file_exists; kubernetes_audit_policy_file_valid; kubernetes_audit_logging_minimal; kubernetes_audit_policy_minimal_requirements; kubernetes_audit_policy_file_set; kubernetes_audit_logging_active,• Level 1 - Master Node,"Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
3.2.2,Ensure that the audit policy covers key security concerns,Manual,Ensure that the audit policy created for the cluster covers key security concerns.,"Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Impact: Increasing audit logging will consume resources on the nodes or other log destination.","Review the audit policy provided for the cluster and ensure that it covers at least the following areas :- • Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data. • Modification of pod and deployment objects. • Use of pods/exec, pods/portforward, pods/proxy and services/proxy. For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","Consider modification of the audit policy in use on the cluster to include these items, at a minimum. Default Value: By default Kubernetes clusters do not log audit information. References: 1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735",kubernetes_audit_policy_security_concerns_covered; kubernetes_audit_policy_key_events_logged; kubernetes_audit_policy_cis_compliance; kubernetes_audit_policy_minimum_requirements_met; kubernetes_audit_policy_security_events_included,• Level 2 - Master Node,Increasing audit logging will consume resources on the nodes or other log destination.,1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735
4.1.1,Ensure that the kubelet service file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet service file has permissions of 600 or more restrictive.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, the kubelet service file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_permissions_restrictive; kubernetes_kubelet_service_file_permissions_600_or_stricter; kubernetes_kubelet_service_file_permissions_secure; kubernetes_kubelet_service_file_permissions_min_600; kubernetes_kubelet_service_file_permissions_enforced,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.2,Ensure that the kubelet service file ownership is set to root:root,Automated,Ensure that the kubelet service file ownership is set to root:root.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, kubelet service file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_ownership_root; kubernetes_kubelet_file_owner_root; kubernetes_kubelet_service_file_permissions_root; kubernetes_kubelet_config_ownership_root; kubernetes_kubelet_service_file_root_owned,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.3,If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive,Manual,"If kube-proxy is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of 600 or more restrictive.","The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None","Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a <path><filename> Verify that a file is specified and it exists with permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 <proxy kubeconfig file> Default Value: By default, proxy file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_file_permissions_restrictive; kubernetes_proxy_kubeconfig_file_permissions_600_or_stricter; kubernetes_proxy_kubeconfig_file_permissions_secure; kubernetes_proxy_kubeconfig_file_permissions_min_600; kubernetes_proxy_kubeconfig_file_permissions_protected,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.4,If proxy kubeconfig file exists ensure ownership is set to root:root,Manual,"If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G <path><filename> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_root_ownership; kubernetes_proxy_kubeconfig_file_secure_ownership; kubernetes_kubeconfig_root_user_group; kubernetes_proxy_config_file_root_only; kubernetes_kubeconfig_ownership_restricted,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.5,Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet.conf file has permissions of 600 or more restrictive.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file has permissions of 600. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_permissions_restrictive; kubernetes_kubeconfig_file_permissions_600; kubernetes_kubelet_conf_file_permissions_restrictive; kubernetes_kubelet_conf_file_permissions_600; kubernetes_kubeconfig_kubelet_conf_permissions_restrictive; kubernetes_kubeconfig_kubelet_conf_permissions_600,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.6,Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root,Automated,Ensure that the kubelet.conf file ownership is set to root:root.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_root_ownership; kubelet_config_file_root_owner; kubelet_conf_root_ownership; kubernetes_kubelet_conf_secure_ownership; kubeconfig_kubelet_root_owner,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.7,Ensure that the certificate authorities file permissions are set to 600 or more restrictive,Manual,Ensure that the certificate authorities file has permissions of 600 or more restrictive.,The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %a <filename> Verify that the permissions are 644 or more restrictive.,Run the following command to modify the file permissions of the --client-ca-file chmod 600 <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_certificate_authorities_file_permissions_600_or_more_restrictive; compute_ca_file_permissions_600_or_more_restrictive; compute_ssl_certificate_authorities_file_permissions_restrictive; compute_trust_store_file_permissions_600_or_more_restrictive; compute_ca_bundle_file_permissions_600_or_more_restrictive,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.8,Ensure that the client certificate authorities file ownership is set to root:root,Manual,Ensure that the certificate authorities file ownership is set to root:root.,The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %U:%G <filename> Verify that the ownership is set to root:root.,Run the following command to modify the ownership of the --client-ca-file. chown root:root <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_client_certificate_authorities_file_ownership_root; compute_ssl_certificate_authorities_file_ownership_root; compute_certificate_authorities_file_owner_root; compute_certificate_authorities_file_group_root; compute_client_certificate_authorities_file_owner_root; compute_client_certificate_authorities_file_group_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.9,If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 600 or more restrictive.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml Verify that the permissions are 600 or more restrictive.","Run the following command (using the config file location identied in the Audit step) chmod 600 /var/lib/kubelet/config.yaml Default Value: By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of 600. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_permissions_restrictive; kubernetes_kubelet_config_file_permissions_600_or_stricter; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_permissions_min_600; kubernetes_kubelet_config_file_permissions_protected,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.1.10,If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %aU %G /var/lib/kubelet/config.yaml ```Verify that the ownership is set to `root:root`.","Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by root:root. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_owner_root; kubernetes_kubelet_config_file_group_root; kubernetes_kubelet_config_file_ownership_root_root; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_access_restricted,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.2.1,Ensure that the --anonymous-auth argument is set to false,Automated,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.","If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false. This executable argument may be omitted, provided there is a corresponding entry set to false in the Kubelet config file.","If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --anonymous-auth=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_auth_enabled; kubernetes_kubelet_anonymous_requests_blocked; kubernetes_kubelet_auth_required; kubernetes_kubelet_secure_auth_enabled,• Level 1 - Worker Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.2,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.","Run the following command on each node: ps -ef | grep kubelet If the --authorization-mode argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by --config, and that file sets authorization: mode to something other than AlwaysAllow. It is also possible to review the running configuration of a Kubelet via the /configz endpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --authorization-mode=Webhook Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",eks_cluster_authorization_mode_not_always_allow; eks_cluster_explicit_authorization_enabled; eks_cluster_request_authorization_restricted; eks_cluster_always_allow_disabled,• Level 1 - Worker Node,Unauthorized requests will be denied.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.3,Ensure that the --client-ca-file argument is set as appropriate,Automated,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. If the --client-ca-file argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets authentication: x509: clientCAFile to the location of the client certificate authority file.","If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to the location of the client CA file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --client-ca-file=<path/to/client-ca-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/",kubernetes_kubelet_client_ca_file_set; kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_ca_file_configured; kubernetes_kubelet_auth_certificates_valid; kubernetes_kubelet_client_ca_file_present,• Level 1 - Worker Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/
4.2.4,Verify that the --read-only-port argument is set to 0,Manual,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,"Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.","If using a Kubelet config file, edit the file to set readOnlyPort to 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --read-only-port=0 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --read-only-port is set to 10255/TCP. However, if a config file is specified by --config the default value for readOnlyPort is 0. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_api_server_read_only_port_disabled; kubernetes_api_server_read_only_port_set_zero; kubernetes_api_server_read_only_port_unset; kubernetes_api_server_read_only_port_secure; kubernetes_api_server_read_only_port_restricted,• Level 1 - Worker Node,Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,1. https://kubernetes.io/docs/admin/kubelet/
4.2.5,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Manual,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.","Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0. If the argument is not present, and there is a Kubelet config file specified by --config, check that it does not set streamingConnectionIdleTimeout to 0.","If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a value other than 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --streaming-connection-idle-timeout=5m Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",kubernetes_api_server_streaming_connection_timeout_enabled; kubernetes_api_server_streaming_connection_timeout_not_disabled; kubernetes_api_server_streaming_connection_timeout_set; kubernetes_api_server_streaming_connection_timeout_configured; kubernetes_api_server_streaming_connection_timeout_non_zero,• Level 1 - Worker Node,Long-lived connections could be interrupted.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
4.2.6,Ensure that the --make-iptables-util-chains argument is set to true,Automated,Allow Kubelet to manage iptables.,"Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.","Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.","If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove the --make- iptables-util-chains argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --make-iptables-util-chains argument is set to true. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_util_chains_configured; kubernetes_kubelet_iptables_util_chains_set_true; kubernetes_kubelet_iptables_util_chains_required; kubernetes_kubelet_iptables_util_chains_enforced,• Level 1 - Worker Node,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",1. https://kubernetes.io/docs/admin/kubelet/
4.2.7,Ensure that the --hostname-override argument is not set,Manual,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Impact: Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist. Note This setting is not configurable via the Kubelet config file.,"Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_default; kubernetes_node_hostname_unchanged; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_preserved,• Level 1 - Worker Node,"Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063
4.2.8,Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture,Manual,"Security relevant information should be captured. The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,"Run the following command on each node: sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10- kubeadm.conf Review the value set for the argument and determine whether this has been set to an appropriate level for the cluster. If the argument does not exist, check that there is a Kubelet config file specified by -- config and review the value in this location.","If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, eventRecordQPS argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go",kubernetes_kubelet_event_record_qps_configured; kubernetes_kubelet_event_record_qps_within_limits; kubernetes_kubelet_event_record_qps_not_unlimited; kubernetes_kubelet_event_capture_rate_optimized; kubernetes_kubelet_event_record_qps_secure_value,• Level 2 - Worker Node,Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go
4.2.9,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Manual,Setup TLS connection on the Kubelets.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.","Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. If these arguments are not present, check that there is a Kubelet config specified by -- config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_configured; kubernetes_kubelet_tls_cert_key_files_valid; kubernetes_kubelet_tls_encryption_enabled,• Level 1 - Worker Node,,
4.2.10,Ensure that the --rotate-certificates argument is not set to false,Automated,Enable kubelet client certificate rotation.,The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None,"Run the following command on each node: ps -ef | grep kubelet Verify that the --rotate-certificates argument is not present, or is set to true. If the --rotate-certificates argument is not present, verify that if there is a Kubelet config file specified by --config, that file does not contain rotateCertificates: false.","If using a Kubelet config file, edit the file to add the line rotateCertificates: true or remove it altogether to use the default value. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove --rotate- certificates=false argument from the KUBELET_CERTIFICATE_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet client certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_rotate_certificates_not_disabled; kubernetes_kubelet_client_cert_rotation_required; kubernetes_kubelet_certificate_rotation_not_false; kubernetes_kubelet_secure_certificate_rotation_enabled,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/
4.2.11,Verify that the RotateKubeletServerCertificate argument is set to true,Manual,Enable kubelet server certificate rotation.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate- server-certificates parameter is set on kubelet Run the following command on each node: ps -ef | grep kubelet Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable. --feature-gates=RotateKubeletServerCertificate=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet server certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_auto_rotated; kubernetes_kubelet_tls_certificate_rotation_active; kubernetes_kubelet_server_cert_rotation_required; kubernetes_kubelet_auto_rotate_server_cert_enabled,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration
4.2.12,Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the Kubelet is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.","The set of cryptographic ciphers currently considered secure is the following: • TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 • TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_128_GCM_SHA256 Run the following command on each node: ps -ef | grep kubelet If the --tls-cipher-suites argument is present, ensure it only contains values included in this set. If it is not present check that there is a Kubelet config file specified by --config, and that file sets TLSCipherSuites: to only include values from this set.","If using a Kubelet config file, edit the file to set TLSCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 ,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 ,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 ,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the --tls-cipher-suites parameter as follows, or to a subset of these values. --tls-cipher- suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_kubelet_strong_ciphers_enabled; kubernetes_kubelet_weak_ciphers_disabled; kubernetes_kubelet_tls_ciphers_restricted; kubernetes_kubelet_cipher_suite_compliant; kubernetes_kubelet_secure_cipher_config; kubernetes_kubelet_crypto_policy_enforced; kubernetes_kubelet_cipher_strength_validated; kubernetes_kubelet_approved_ciphers_only,• Level 1 - Worker Node,Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.,
4.2.13,Ensure that a limit is set on pod PIDs,Manual,Ensure that the Kubelet sets limits on the number of PIDs that can be created by pods running on the node.,"By default pods running in a cluster can consume any number of PIDs, potentially exhausting the resources available on the node. Setting an appropriate limit reduces the risk of a denial of service attack on cluster nodes. Impact: Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.","Review the Kubelet's start-up parameters for the value of --pod-max-pids, and check the Kubelet configuration file for the PodPidsLimit . If neither of these values is set, then there is no limit in place.","Decide on an appropriate level for this parameter and set it, either via the --pod-max- pids command line parameter or the PodPidsLimit configuration file setting. Default Value: By default the number of PIDs is not limited. References: 1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 4.3 kube-proxy Recommendations relating to the kube-proxy component.",kubernetes_pod_pid_limit_enabled; kubernetes_pod_pid_limit_set; kubernetes_pod_pid_limit_configured; kubernetes_pod_pid_limit_enforced; kubernetes_pod_pid_limit_restricted,• Level 1 - Worker Node,Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.,1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 4.3 kube-proxy Recommendations relating to the kube-proxy component.
4.3.1,Ensure that the kube-proxy metrics service is bound to localhost,Automated,Do not bind the kube-proxy metrics port to non-loopback addresses.,kube-proxy has two APIs which provided access to information about the service and can be bound to network ports. The metrics API service includes endpoints (/metrics and /configz) which disclose information about the configuration and operation of kube- proxy. These endpoints should not be exposed to untrusted networks as they do not support encryption or authentication to restrict access to the data they provide. Impact: 3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,review the start-up flags provided to kube proxy ps -ef | grep -i kube-proxy Ensure that the --metrics-bind-address parameter is not set to a value other than 127.0.0.1. From the output of this command gather the location specified in the -- config parameter. Review any file stored at that location and ensure that it does not specify a value other than 127.0.0.1 for metricsBindAddress.,Modify or remove any values which bind the metrics service to a non-localhost address Default Value: The default value is 127.0.0.1:10249 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/,kubernetes_proxy_metrics_localhost_bound; kubernetes_proxy_metrics_non_loopback_blocked; kubernetes_proxy_metrics_loopback_only; kubernetes_proxy_metrics_secure_binding; kubernetes_proxy_metrics_restricted_interface,• Level 1 - Worker Node,3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/
5.1.1,Ensure that the cluster-admin role is only used where required,Automated,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",kubernetes_role_no_cluster_admin_unrestricted; kubernetes_role_cluster_admin_minimal_usage; kubernetes_role_cluster_admin_restricted_scope; kubernetes_role_cluster_admin_least_privilege; kubernetes_role_cluster_admin_required_only,• Level 1 - Master Node,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
5.1.2,Minimize access to secrets,Automated,"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation,"Review the users who have get, list or watch access to secrets objects in the Kubernetes API.","Where possible, remove get, list and watch access to secret objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have get privileges on secret objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:expand-controller expand-controller ServiceAccount kube-system system:controller:generic-garbage-collector generic-garbage- collector ServiceAccount kube-system system:controller:namespace-controller namespace-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:kube-controller-manager system:kube-controller- manager User",kubernetes_secret_access_restricted; kubernetes_secret_minimal_access; kubernetes_secret_no_public_access; kubernetes_secret_iam_restricted; kubernetes_secret_rbac_restricted; kubernetes_secret_service_account_restricted; kubernetes_secret_workload_access_restricted; kubernetes_secret_no_anonymous_access; kubernetes_secret_no_wildcard_access; kubernetes_secret_no_default_access,• Level 1 - Master Node,Care should be taken not to remove access to secrets to system components which require this for their operation,
5.1.3,Minimize wildcard use in Roles and ClusterRoles,Automated,Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard '*' which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.,The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.,Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml,Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.,kubernetes_role_wildcard_restricted; kubernetes_clusterrole_wildcard_restricted; kubernetes_role_minimal_permissions; kubernetes_clusterrole_minimal_permissions; kubernetes_role_no_wildcard_resources; kubernetes_clusterrole_no_wildcard_resources; kubernetes_role_no_wildcard_verbs; kubernetes_clusterrole_no_wildcard_verbs; kubernetes_role_explicit_permissions; kubernetes_clusterrole_explicit_permissions,• Level 1 - Worker Node,,
5.1.4,Minimize access to create pods,Automated,"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.","The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",Review the users who have create access to pod objects in the Kubernetes API.,"Where possible, remove create access to pod objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have create privileges on pod objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:daemon-set-controller daemon-set-controller ServiceAccount kube-system system:controller:job-controller job-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:controller:replicaset-controller replicaset-controller ServiceAccount kube-system system:controller:replication-controller replication-controller ServiceAccount kube-system system:controller:statefulset-controller statefulset-controller ServiceAccount kube-system",kubernetes_namespace_pod_creation_restricted; kubernetes_role_pod_creation_minimized; kubernetes_user_pod_creation_limited; kubernetes_service_account_pod_creation_restricted; kubernetes_rbac_pod_creation_permissions_minimized; kubernetes_cluster_pod_creation_access_controlled; kubernetes_policy_pod_creation_privileges_restricted,• Level 1 - Master Node,Care should be taken not to remove access to pods to system components which require this for their operation,
5.1.5,Ensure that default service accounts are not actively used.,Automated,The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.,"Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.","For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/,iam_service_account_default_not_used; iam_service_account_default_inactive; compute_service_account_default_disabled; compute_service_account_default_unused; service_account_default_no_active_usage,• Level 1 - Master Node,All workloads which require access to the Kubernetes API will require an explicit service account to be created.,1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.6,Ensure that Service Account Tokens are only mounted where necessary,Automated,Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server,"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.","Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false","Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_pod_service_account_token_unmounted; kubernetes_pod_service_account_token_restricted; kubernetes_pod_service_account_token_disabled; kubernetes_pod_service_account_token_minimized; kubernetes_pod_service_account_token_required_only,• Level 1 - Master Node,"Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.7,Avoid use of system:masters group,Manual,"The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed. When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster. Impact: Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.,Remove the system:masters group from all users in the cluster. Default Value: By default some clusters will create a 'break glass' client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations. References: 1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38,iam_group_system_masters_unused; iam_group_system_masters_restricted; iam_group_system_masters_no_grants; iam_group_system_masters_minimal_usage; iam_group_system_masters_bootstrapping_only,• Level 1 - Master Node,"Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38
5.1.8,"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",Manual,"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster- admin level. Impact: There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.","Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.","Where possible, remove the impersonate, bind and escalate rights from subjects. Default Value: In a default kubeadm cluster, the system:masters group and clusterrole-aggregation- controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate. References: 1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/",kubernetes_cluster_role_no_privilege_escalation; kubernetes_role_no_privilege_escalation; kubernetes_cluster_role_no_impersonation; kubernetes_role_no_impersonation; kubernetes_cluster_role_no_binding; kubernetes_role_no_binding; kubernetes_cluster_role_restricted_permissions; kubernetes_role_restricted_permissions,• Level 1 - Master Node,"There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/
5.1.9,Minimize access to create persistent volumes,Manual,"The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of hostPath volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.","The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have create access to PersistentVolume objects in the Kubernetes API.,"Where possible, remove create access to PersistentVolume objects in the cluster. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation",kubernetes_persistent_volume_creation_restricted; kubernetes_persistent_volume_admin_access_minimized; kubernetes_persistent_volume_hostpath_disabled; kubernetes_persistent_volume_privilege_escalation_prevented; kubernetes_persistent_volume_creation_scope_limited; kubernetes_persistent_volume_sensitive_host_access_blocked; kubernetes_persistent_volume_creation_roles_restricted; kubernetes_persistent_volume_creation_permissions_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation
5.1.10,Minimize access to the proxy sub-resource of nodes,Manual,"Users with access to the Proxy sub-resource of Node objects automatically have permissions to use the Kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs. The Kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the node object. The proxy sub-resource specifically allows wide ranging access to the Kubelet API. Direct access to the Kubelet API bypasses controls like audit logging (there is no audit log of Kubelet API access) and admission control.","The ability to use the proxy sub-resource of node objects opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have access to the proxy sub-resource of node objects in the Kubernetes API.,"Where possible, remove access to the proxy sub-resource of node objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization",kubernetes_node_proxy_access_restricted; kubernetes_node_proxy_no_admin_access; kubernetes_node_proxy_minimal_permissions; kubernetes_node_proxy_api_disabled; kubernetes_node_proxy_subresource_denied; kubernetes_node_proxy_privilege_escalation_prevented; kubernetes_node_proxy_kubelet_access_restricted; kubernetes_node_proxy_audit_log_bypass_blocked,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization
5.1.11,Minimize access to the approval sub-resource of certificatesigningrequests objects,Manual,"Users with access to the update the approval sub-resource of certificateaigningrequest objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts. This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster",The ability to update certificate signing requests should be limited.,Review the users who have access to update the approval sub-resource of certificatesigningrequest objects in the Kubernetes API.,"Where possible, remove access to the approval sub-resource of certificatesigningrequest objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing",kubernetes_certificatesigningrequest_approval_restricted; kubernetes_certificatesigningrequest_approval_min_access; kubernetes_certificatesigningrequest_approval_no_high_privileges; kubernetes_certificatesigningrequest_approval_no_admin_access; kubernetes_certificatesigningrequest_approval_privilege_escalation_prevented,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing
5.1.12,Minimize access to webhook configuration objects,Manual,"Users with rights to create/modify/delete validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.",The ability to manage webhook configuration should be limited,Review the users who have access to validatingwebhookconfigurations or mutatingwebhookconfigurations objects in the Kubernetes API.,"Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks",kubernetes_validating_webhook_configuration_access_minimized; kubernetes_mutating_webhook_configuration_access_minimized; kubernetes_webhook_configuration_admin_access_restricted; kubernetes_webhook_configuration_write_access_restricted; kubernetes_webhook_configuration_privileged_access_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks
5.1.13,Minimize access to the service account token creation,Manual,"Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.",The ability to create service account tokens should be limited.,Review the users who have access to create the token sub-resource of serviceaccount objects in the Kubernetes API.,"Where possible, remove access to the token sub-resource of serviceaccount objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request",kubernetes_service_account_token_creation_restricted; kubernetes_service_account_token_creation_minimized; kubernetes_cluster_service_account_token_creation_limited; kubernetes_service_account_token_creation_admin_restricted; kubernetes_cluster_service_account_token_creation_admin_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request
5.2.1,Ensure that the cluster has at least one active policy control mechanism in place,Manual,"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts. Impact: Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.,"Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads. Default Value: By default, Pod Security Admission is enabled but no policies are in place. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission",kubernetes_cluster_policy_control_enabled; kubernetes_cluster_pod_security_admission_enabled; kubernetes_cluster_third_party_policy_control_enabled; kubernetes_cluster_policy_control_active; kubernetes_cluster_security_policy_enforced,• Level 1 - Master Node,"Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",1. https://kubernetes.io/docs/concepts/security/pod-security-admission
5.2.2,Minimize the admission of privileged containers,Manual,Do not generally permit containers to be run with the securityContext.privileged flag set to true.,"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.","Run the following command: get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}' It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection. calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node- c4xv4: {} {'privileged':true} {'privileged':true} {'privileged':true} {'privileged':true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {'seccompProfile':{'type':'RuntimeDefault'}} {'allowPrivilegeEscalation':false,'readOnlyRootFilesystem':true,'runAsGroup':2001,'ru nAsUser':1001}","Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privileged_disabled; compute_container_privileged_escalation_disabled; compute_container_security_context_restricted; compute_container_privileged_flag_false; compute_container_privileged_mode_disabled,• Level 1 - Master Node,"Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.3,Minimize the admission of containers wishing to share the host process ID namespace,Manual,Do not generally permit containers to be run with the hostPID flag set to true.,"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostPID containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_pid_disabled; compute_container_host_pid_restricted; compute_container_host_pid_not_shared; compute_container_host_pid_isolated; compute_container_host_pid_protected,• Level 1 - Master Node,Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.4,Minimize the admission of containers wishing to share the host IPC namespace,Manual,Do not generally permit containers to be run with the hostIPC flag set to true.,"A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostIPC containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ipc_disabled; compute_container_host_ipc_not_shared; compute_container_host_ipc_restricted; compute_container_host_ipc_denied; compute_container_host_ipc_protected,• Level 1 - Master Node,Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.5,Minimize the admission of containers wishing to share the host network namespace,Manual,Do not generally permit containers to be run with the hostNetwork flag set to true.,"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostNetwork containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_network_disabled; compute_container_host_network_restricted; compute_container_host_network_prohibited; compute_container_host_network_denied; compute_container_host_network_not_shared,• Level 1 - Master Node,Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.6,Minimize the admission of containers with allowPrivilegeEscalation,Manual,"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with .spec.allowPrivilegeEscalationset to true. Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",kubernetes_container_privilege_escalation_disabled; kubernetes_pod_privilege_escalation_disabled; container_runtime_privilege_escalation_disabled; workload_privilege_escalation_disabled; deployment_privilege_escalation_disabled; statefulset_privilege_escalation_disabled; daemonset_privilege_escalation_disabled; cronjob_privilege_escalation_disabled; job_privilege_escalation_disabled; replica_set_privilege_escalation_disabled,• Level 1 - Master Node,Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.7,Minimize the admission of root containers,Manual,Do not generally permit containers to be run as the root user.,"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one admission control policy defined which does not permit root containers. If you need to run root containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run as the root user will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0.","Create a policy for each namespace in the cluster, ensuring that either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0, is set. Default Value: By default, there are no restrictions on the use of root containers and if a User is not specified in the image, the container will run as root. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_root_user_disabled; compute_container_root_privileges_restricted; compute_container_non_root_user_required; compute_container_root_admission_minimized; compute_container_root_execution_prohibited,• Level 2 - Master Node,Pods with containers which run as the root user will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.8,Minimize the admission of containers with the NET_RAW capability,Manual,Do not generally permit containers with the potentially dangerous NET_RAW capability.,"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability. If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run with the NET_RAW capability will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the NET_RAW capability.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the NET_RAW capability. Default Value: By default, there are no restrictions on the creation of containers with the NET_RAW capability. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_net_raw_capability_disabled; compute_container_net_raw_capability_restricted; compute_container_net_raw_capability_minimized; compute_container_net_raw_capability_denied; compute_container_net_raw_capability_blocked,• Level 1 - Master Node,Pods with containers which run with the NET_RAW capability will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.9,Minimize the admission of containers with added capabilities,Manual,Do not generally permit containers with capabilities assigned beyond the default set.,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which require capabilities outwith the default set will not be permitted.","List the policies in use for each namespace in the cluster, ensure that policies are present which prevent allowedCapabilities to be set to anything other than an empty array.","Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. Default Value: By default, there are no restrictions on adding capabilities to containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_restricted; compute_container_default_capabilities_only; compute_container_added_capabilities_minimized; compute_container_capabilities_limited_to_default; compute_container_no_added_capabilities,• Level 1 - Master Node,Pods with containers which require capabilities outwith the default set will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.10,Minimize the admission of containers with capabilities assigned,Manual,Do not generally permit containers with capabilities,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Impact: Pods with containers require capabilities to operate will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers.","Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities. Default Value: By default, there are no restrictions on the creation of containers with additional capabilities References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_restricted; compute_container_no_privileged_capabilities; compute_container_minimal_capabilities; compute_container_capabilities_disabled; compute_container_allowed_capabilities_limited,• Level 2 - Master Node,Pods with containers require capabilities to operate will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.11,Minimize the admission of Windows HostProcess Containers,Manual,Do not generally permit Windows containers to be run with the hostProcess flag set to true.,"A Windows container making use of the hostProcess flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides 'privileged access' to the Windows node. Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit hostProcess Windows containers. If you need to run Windows containers which require hostProcess, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostProcess containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostProcess containers. Default Value: By default, there are no restrictions on the creation of hostProcess containers. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_process_disabled; compute_windows_container_host_process_restricted; container_host_process_admission_minimized; windows_container_host_process_denied; compute_container_host_process_usage_limited,• Level 1 - Master Node,Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.12,Minimize the admission of HostPath volumes,Manual,Do not generally admit containers which make use of hostPath volumes.,"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem. There should be at least one admission control policy defined which does not permit containers to mount hostPath volumes. If you need to run containers which require hostPath volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with hostPath volumes.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPath volumes. Default Value: By default, there are no restrictions on the creation of hostPath volumes. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_hostpath_volumes_disabled; compute_container_hostpath_volumes_restricted; compute_container_hostpath_volumes_minimized; compute_container_hostpath_volumes_denied; compute_container_hostpath_volumes_prohibited,• Level 1 - Master Node,Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.13,Minimize the admission of containers which use HostPorts,Manual,Do not generally permit containers which require the use of HostPorts.,"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts. If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have hostPort sections.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPort sections. Default Value: By default, there are no restrictions on the use of HostPorts. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI",compute_container_host_ports_restricted; compute_container_host_ports_disabled; compute_container_host_ports_minimized; compute_container_host_ports_denied; compute_container_host_ports_blocked,• Level 1 - Master Node,"Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI
5.3.1,Ensure that the CNI in use supports Network Policies,Manual,There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.,Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None,"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",kubernetes_cni_network_policy_support; kubernetes_cni_network_policy_enabled; kubernetes_network_policy_cni_compliance; kubernetes_cni_network_policy_required; kubernetes_network_policy_cni_validation; kubernetes_cni_network_policy_enforcement; kubernetes_network_policy_cni_support_check; kubernetes_cni_network_policy_compliance; kubernetes_network_policy_cni_configured; kubernetes_cni_network_policy_availability,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.
5.3.2,Ensure that all Namespaces have Network Policies defined,Manual,Use network policies to isolate traffic in your cluster network.,"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces Ensure that each namespace defined in the cluster has at least one Network Policy.,"Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",kubernetes_namespace_network_policy_defined; kubernetes_namespace_network_policy_required; kubernetes_namespace_traffic_isolation_enabled; kubernetes_network_policy_namespace_coverage; kubernetes_namespace_network_policy_enforced,• Level 2 - Master Node,"Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/
5.4.1,Prefer using secrets as files over secrets as environment variables,Manual,Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.,"It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {'\n'}{end}' -A,"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",kubernetes_secret_files_preferred; kubernetes_secret_no_environment_variables; kubernetes_secret_volume_mounted; kubernetes_secret_environment_restricted; kubernetes_secret_files_over_env_vars,• Level 2 - Master Node,Application code which expects to read secrets in the form of environment variables would need modification,1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod
5.4.2,Consider external secret storage,Manual,"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",Review your secrets management implementation.,"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured.",kubernetes_secrets_external_storage_required; kubernetes_secrets_authentication_required; kubernetes_secrets_audit_logging_enabled; kubernetes_secrets_encryption_enabled; kubernetes_secrets_rotation_enabled,• Level 2 - Master Node,None,
5.5.1,Configure Image Provenance using ImagePolicyWebhook admission controller,Manual,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Impact: You need to regularly maintain your provenance configuration based on container image updates.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888",kubernetes_image_provenance_enabled; kubernetes_image_policy_webhook_enabled; kubernetes_admission_controller_image_provenance_configured; kubernetes_image_provenance_webhook_active; kubernetes_image_policy_webhook_configured,• Level 2 - Master Node,You need to regularly maintain your provenance configuration based on container image updates.,1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888
5.7.1,Create administrative boundaries between resources using namespaces,Manual,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with 4 initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-node-lease - Namespace used for node heartbeats 4. kube-public - Namespace used for public information in a cluster References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_enabled; kubernetes_namespace_resource_separation_enabled; kubernetes_namespace_security_boundaries_enabled; kubernetes_namespace_isolation_for_resources_enabled,• Level 1 - Master Node,You need to switch between namespaces for administration.,1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats
5.7.2,Ensure that the seccomp profile is set to docker/default in your pod definitions,Manual,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",Review the pod definitions in your cluster. It should create a line as below: securityContext: seccompProfile: type: RuntimeDefault,"Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_default; kubernetes_pod_seccomp_profile_docker_default; kubernetes_pod_security_profile_default; kubernetes_pod_security_seccomp_enabled; kubernetes_pod_security_seccomp_default; kubernetes_pod_security_profile_docker_default; kubernetes_pod_security_profile_configured; kubernetes_pod_security_seccomp_profile_set; kubernetes_pod_security_seccomp_profile_default; kubernetes_pod_security_profile_default_enabled,• Level 2 - Master Node,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/
5.7.3,Apply Security Context to Your Pods and Containers,Manual,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_configured; kubernetes_container_security_context_configured; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_privilege_escalation_disabled; kubernetes_container_privilege_escalation_disabled; kubernetes_pod_capabilities_dropped; kubernetes_container_capabilities_dropped,• Level 2 - Master Node,"If you incorrectly apply security contexts, you may have trouble running the pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
5.7.4,The default namespace should not be used,Manual,"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None","Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service","Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used",kubernetes_namespace_default_not_used; kubernetes_namespace_default_avoided; kubernetes_namespace_non_default_required; kubernetes_namespace_default_prohibited; kubernetes_namespace_custom_required,• Level 2 - Master Node,None,
1.1.1,Ensure that the API server pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the API server pod specification file has permissions of 600 or more restrictive.,The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_api_server_pod_spec_file_permissions_restrictive; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_permissions_strict; kubernetes_api_server_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.2,Ensure that the API server pod specification file ownership is set to root:root,Automated,Ensure that the API server pod specification file ownership is set to root:root.,The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_owner_root; kubernetes_api_server_pod_spec_file_group_root; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.3,Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the controller manager pod specification file has permissions of 600 or more restrictive.,The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, the kube-controller-manager.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_controller_manager_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_secure; kubernetes_controller_manager_pod_spec_file_permissions_strict; kubernetes_controller_manager_pod_spec_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.4,Ensure that the controller manager pod specification file ownership is set to root:root,Automated,Ensure that the controller manager pod specification file ownership is set to root:root.,The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, kube-controller-manager.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_pod_spec_root_owned; kubernetes_controller_manager_file_permissions_root; kubernetes_controller_manager_spec_ownership_root; kubernetes_controller_manager_pod_file_root_owner,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager
1.1.5,Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler pod specification file has permissions of 600 or more restrictive.,The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_scheduler_pod_spec_file_permissions_restrictive; kubernetes_scheduler_pod_spec_file_permissions_secure; kubernetes_scheduler_pod_spec_file_permissions_strict; kubernetes_scheduler_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.6,Ensure that the scheduler pod specification file ownership is set to root:root,Automated,Ensure that the scheduler pod specification file ownership is set to root:root.,The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_file_ownership_root; kubernetes_scheduler_pod_spec_root_owned; kubernetes_scheduler_file_ownership_root_root; kubernetes_pod_spec_scheduler_root_ownership; kubernetes_scheduler_spec_file_root_owned,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.7,Ensure that the etcd pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file has permissions of 600 or more restrictive.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/etcd.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_manifests_etcd_file_permissions_secure; kubernetes_etcd_yaml_file_permissions_restricted; kubernetes_etcd_pod_spec_file_permissions_compliant; kubernetes_etcd_manifest_file_permissions_600_or_stricter,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.8,Ensure that the etcd pod specification file ownership is set to root:root,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_manifest_file_owner_root; kubernetes_etcd_manifest_file_group_root; kubernetes_etcd_manifest_file_permissions_root_only; kubernetes_etcd_manifest_file_ownership_root_root,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.9,Ensure that the Container Network Interface file permissions are set to 600 or more restrictive,Manual,Ensure that the Container Network Interface files have permissions of 600 or more restrictive.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a <path/to/cni/files> Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_permissions_600_or_stricter; container_network_interface_file_permissions_restrictive; container_network_interface_file_permissions_secure; container_network_interface_file_permissions_compliant; container_network_interface_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.10,Ensure that the Container Network Interface file ownership is set to root:root,Manual,Ensure that the Container Network Interface files have ownership set to root:root.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G <path/to/cni/files> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_ownership_root; container_network_interface_file_ownership_root_root; cni_file_ownership_root_root; container_network_interface_file_permissions_root; cni_file_ownership_secure,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Automated,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd Default Value: By default, etcd data directory has permissions of 755. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_restrictive; etcd_data_directory_permissions_700_or_stricter; etcd_directory_permissions_restricted; etcd_data_directory_permissions_secure; etcd_directory_permissions_700_or_less,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Automated,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd Default Value: By default, etcd data directory ownership is set to etcd:etcd. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_ownership_etcd_etcd; etcd_directory_permissions_etcd_etcd; etcd_data_directory_user_group_etcd; etcd_data_directory_secure_ownership; etcd_directory_ownership_correct,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.13,Ensure that the default administrative credential file permissions are set to 600,Automated,"Ensure that the admin.conf file (and super-admin.conf file, where it exists) have permissions of 600.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should restrict their file permissions to maintain the integrity and confidentiality of the file(s). The file(s) should be readable and writable by only the administrators on the system. Impact: None.","Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %a /etc/kubernetes/super-admin.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chmod 600 /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf have permissions of 600. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",kubernetes_credential_file_permissions_600; kubernetes_admin_conf_file_permissions_600; kubernetes_super_admin_conf_file_permissions_600; kubernetes_default_credential_file_permissions_600; kubernetes_config_file_permissions_600,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.14,Ensure that the default administrative credential file ownership is set to root:root,Automated,"Ensure that the admin.conf (and super-admin.conf file, where it exists) file ownership is set to root:root.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should set their file ownership to maintain the integrity and confidentiality of the file. The file(s) should be owned by root:root. Impact: None.","Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %U:%G /etc/kubernetes/super-admin.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chown root:root /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",compute_admin_credential_file_ownership_root; compute_admin_credential_file_ownership_root_root; compute_admin_conf_ownership_root; compute_super_admin_conf_ownership_root; compute_credential_file_ownership_root_root,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.15,Ensure that the scheduler.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler.conf file has permissions of 600 or more restrictive.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/scheduler.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",compute_scheduler_conf_file_permissions_600_or_stricter; compute_scheduler_conf_file_permissions_restrictive; compute_scheduler_conf_file_permissions_secure; compute_scheduler_conf_file_permissions_compliant; compute_scheduler_conf_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.16,Ensure that the scheduler.conf file ownership is set to root:root,Automated,Ensure that the scheduler.conf file ownership is set to root:root.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/scheduler.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",scheduler_file_root_ownership; scheduler_conf_root_ownership; scheduler_config_root_ownership; scheduler_file_secure_ownership; scheduler_conf_secure_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.17,Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the controller-manager.conf file has permissions of 600 or more restrictive.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/controller-manager.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_permissions_600_or_more_restrictive; kubernetes_controller_manager_conf_file_permissions_restrictive; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_permissions_compliant; kubernetes_controller_manager_conf_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.18,Ensure that the controller-manager.conf file ownership is set to root:root,Automated,Ensure that the controller-manager.conf file ownership is set to root:root.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/controller-manager.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_ownership_root; kubernetes_controller_manager_conf_file_group_ownership_root; kubernetes_controller_manager_conf_file_permissions_root_only,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.19,Ensure that the Kubernetes PKI directory and file ownership is set to root:root,Automated,Ensure that the Kubernetes PKI directory and file ownership is set to root:root.,Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/ Verify that the ownership of all files and directories in this hierarchy is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown -R root:root /etc/kubernetes/pki/ Default Value: By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained within it, are set to be owned by the root user. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_directory_ownership_root; kubernetes_pki_file_ownership_root; kubernetes_pki_directory_permissions_root; kubernetes_pki_file_permissions_root; kubernetes_pki_directory_secure_ownership; kubernetes_pki_file_secure_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.20,Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive,Manual,Ensure that Kubernetes PKI certificate files have permissions of 644 or more restrictive.,Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 644 or more restrictive to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.crt Verify that the permissions are 644 or more restrictive. or ls -l /etc/kubernetes/pki/*.crt Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 644 /etc/kubernetes/pki/*.crt Default Value: By default, the certificates used by Kubernetes are set to have permissions of 644 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_certificate_file_permissions_restrictive; kubernetes_pki_certificate_file_permissions_644_or_stricter; kubernetes_pki_certificate_file_permissions_secure; kubernetes_pki_certificate_file_permissions_compliant; kubernetes_pki_certificate_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.21,Ensure that the Kubernetes PKI key file permissions are set to 600,Manual,Ensure that Kubernetes PKI key files have permissions of 600.,Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.key Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.key Verify that the permissions are -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.key Default Value: By default, the keys used by Kubernetes are set to have permissions of 600 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_key_file_permissions_600; kubernetes_pki_key_file_permissions_restricted; kubernetes_pki_key_file_permissions_secure; kubernetes_pki_key_file_permissions_strict; kubernetes_pki_key_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.1,Ensure that the --anonymous-auth argument is set to false,Manual,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Impact: Anonymous requests will be rejected.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[*]}{.spec.containers[*].command} {'\n'}{end}' | grep '\--anonymous- auth' | grep -i false If the exit code is '1', then the control isn't present / failed","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --anonymous-auth=false Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_no_anonymous_auth; kubernetes_api_server_auth_enabled; kubernetes_api_server_anonymous_access_blocked; kubernetes_api_server_auth_required,• Level 1 - Master Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests
1.2.2,Ensure that the --token-auth-file parameter is not set,Automated,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token- based authentication. Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist. Alternative Audit Method kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[*]}{.spec.containers[*].command} {'\n'}{end}' | grep '\--token-auth- file' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the --token-auth- file=<filename> parameter. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_disabled; kubernetes_api_server_token_auth_file_unset; kubernetes_api_server_token_auth_not_configured; kubernetes_api_server_token_auth_file_absent; kubernetes_api_server_token_auth_prohibited,• Level 1 - Master Node,You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.,1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.3,Ensure that the DenyServiceExternalIPs is set,Manual,This admission controller rejects all net-new usage of the Service field externalIPs.,"Most users do not need the ability to set the externalIPs field for a Service at all, and cluster admins should consider disabling this functionality by enabling the DenyServiceExternalIPs admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage. Impact: When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the `DenyServiceExternalIPs' argument exist as a string value in --enable- admission-plugins.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and append the Kubernetes API server flag -- enable-admission-plugins with the DenyServiceExternalIPs plugin. Note, the Kubernetes API server flag --enable-admission-plugins takes a comma-delimited list of admission control plugins to be enabled, even if they are in the list of plugins enabled by default. kube-apiserver --enable-admission-plugins=DenyServiceExternalIPs Default Value: By default, --enable-admission-plugins=DenyServiceExternalIP argument is not set, and the use of externalIPs is authorized. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_admission_controller_deny_service_external_ips_enabled; kubernetes_service_external_ips_restricted; kubernetes_admission_controller_external_ips_denied; kubernetes_service_external_ips_blocked; kubernetes_admission_controller_service_external_ips_disabled,• Level 1 - Master Node,"When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.4,Ensure that the --kubelet-client-certificate and --kubelet- client-key arguments are set as appropriate,Automated,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-client- certificate' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below. --kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file> Default Value: By default, certificate-based kubelet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured; kubernetes_kubelet_authentication_certificate_based; kubernetes_kubelet_tls_authentication_enabled,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.5,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Automated,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet- certificate-Authority' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string> Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_secure; kubernetes_kubelet_certificate_authority_trusted; kubernetes_kubelet_certificate_authority_enforced,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.6,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not always authorize all requests.,"The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Impact: Only authorized requests will be served.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below. --authorization-mode=RBAC Default Value: By default, AlwaysAllow is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",eks_cluster_authorization_mode_not_always_allow; eks_cluster_authorization_mode_restricted; eks_cluster_secure_authorization_mode; eks_cluster_no_always_allow_auth; eks_cluster_auth_mode_not_always_allow,• Level 1 - Master Node,Only authorized requests will be served.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.2.7,Ensure that the --authorization-mode argument includes Node,Automated,Restrict kubelet nodes to reading only objects associated with them.,"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include Node.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC Default Value: By default, Node authorization is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_authorization_enabled; kubernetes_kubelet_node_authorization_required; kubernetes_kubelet_authorization_mode_node_included; kubernetes_kubelet_authorization_node_restricted; kubernetes_kubelet_node_authorization_enforced,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security
1.2.8,Ensure that the --authorization-mode argument includes RBAC,Automated,Turn on Role Based Access Control.,"Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Impact: When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include RBAC.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes RBAC, for example: --authorization-mode=Node,RBAC Default Value: By default, RBAC authorization is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/",kubernetes_cluster_rbac_enabled; kubernetes_cluster_authorization_mode_rbac; kubernetes_cluster_rbac_required; kubernetes_cluster_auth_mode_rbac_included; kubernetes_cluster_rbac_authorization_enabled,• Level 1 - Master Node,"When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
1.2.9,Ensure that the admission control plugin EventRateLimit is set,Manual,Limit the rate at which the API server accepts requests.,"Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Impact: You need to carefully tune in limits as per your environment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit.,"Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=<path/to/configuration/file> Default Value: By default, EventRateLimit is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md",kubernetes_api_server_event_rate_limit_enabled; kubernetes_admission_control_event_rate_limit_configured; kubernetes_api_server_request_rate_limited; kubernetes_admission_plugin_event_rate_limit_enabled; kubernetes_api_request_rate_limit_enabled,• Level 1 - Master Node,You need to carefully tune in limits as per your environment.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md
1.2.10,Ensure that the admission control plugin AlwaysAdmit is not set,Automated,Do not allow all requests.,Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Impact: Only requests explicitly allowed by the admissions control plugins would be served.,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --enable- admission-plugins parameter, or set it to a value that does not include AlwaysAdmit. Default Value: AlwaysAdmit is not in the list of default admission plugins. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwaysadmit",kubernetes_admission_controller_always_admit_disabled; kubernetes_admission_plugin_always_admit_not_set; admission_controller_always_admit_restricted; kubernetes_admission_policy_always_admit_denied; admission_plugin_always_admit_disabled,• Level 1 - Master Node,Only requests explicitly allowed by the admissions control plugins would be served.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwaysadmit
1.2.11,Ensure that the admission control plugin AlwaysPullImages is set,Manual,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images preloaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission- plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwayspullimages",kubernetes_admission_controller_always_pull_images_enabled; kubernetes_pod_images_always_pulled; admission_controller_always_pull_images_policy_enabled; kubernetes_pod_spec_always_pull_images; admission_controller_always_pull_images_required,• Level 1 - Master Node,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images preloaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwayspullimages
1.2.12,Ensure that the admission control plugin ServiceAccount is set,Automated,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Impact: None.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and ensure that the --disable-admission-plugins parameter is set to a value that does not include ServiceAccount. Default Value: By default, ServiceAccount is set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_service_account_admission_plugin_enabled; kubernetes_admission_controller_service_account_required; kubernetes_pod_service_account_auto_creation_disabled; kubernetes_service_account_default_deny_enabled; kubernetes_admission_service_account_validation_enabled,• Level 2 - Master Node,None.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.2.13,Ensure that the admission control plugin NamespaceLifecycle is set,Automated,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --disable-admission- plugins parameter to ensure it does not include NamespaceLifecycle. Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#namespacelifecycle",kubernetes_namespace_lifecycle_admission_plugin_enabled; kubernetes_namespace_termination_protection_enabled; kubernetes_admission_controller_namespace_lifecycle_enabled; kubernetes_namespace_deletion_safety_enabled; kubernetes_admission_plugin_namespace_lifecycle_configured,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#namespacelifecycle
1.2.14,Ensure that the admission control plugin NodeRestriction is set,Automated,Limit the Node and Pod objects that a kubelet could modify.,"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes NodeRestriction.,"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to a value that includes NodeRestriction. --enable-admission-plugins=...,NodeRestriction,... Default Value: By default, NodeRestriction is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#noderestriction 3. https://kubernetes.io/docs/reference/access-authn-authz/node/",kubernetes_kubelet_noderestriction_enabled; kubernetes_admission_plugin_noderestriction_enabled; kubernetes_node_pod_restriction_enabled; kubernetes_kubelet_admission_restricted; kubernetes_noderestriction_plugin_active,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#noderestriction 3. https://kubernetes.io/docs/reference/access-authn-authz/node/
1.2.15,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",eks_cluster_profiling_disabled; eks_node_group_profiling_disabled; kubernetes_pod_profiling_disabled; container_runtime_profiling_disabled; compute_instance_profiling_disabled; serverless_function_profiling_disabled; cloud_service_profiling_disabled; runtime_environment_profiling_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.2.16,Ensure that the --audit-log-path argument is set,Automated,Enable auditing on the Kubernetes API Server and set the desired audit log path.,"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_path_set; kubernetes_api_server_audit_logging_enabled; kubernetes_api_server_audit_log_path_configured; kubernetes_api_server_audit_log_path_valid; kubernetes_api_server_audit_log_path_specified,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.17,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Automated,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_maxage_set_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_compliant; kubernetes_api_server_audit_log_retention_min_30d,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.18,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Automated,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_10_or_appropriate; kubernetes_audit_log_retention_configured; kubernetes_audit_log_maxbackup_limit_set; kubernetes_api_server_log_retention_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.19,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Automated,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_api_server_audit_log_rotation_enabled; kubernetes_api_server_audit_log_size_limited; kubernetes_api_server_audit_log_maxsize_configured,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.20,Ensure that the --request-timeout argument is set as appropriate,Manual,Set global request timeout for API server requests as appropriate.,"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --request-timeout argument is either not set or set to an appropriate value.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s Default Value: By default, --request-timeout is set to 60 seconds. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415",kubernetes_api_server_request_timeout_set; kubernetes_api_server_request_timeout_appropriate; kubernetes_api_server_request_timeout_configured; kubernetes_api_server_request_timeout_valid; kubernetes_api_server_request_timeout_within_limits,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415
1.2.21,Ensure that the --service-account-lookup argument is set to true,Automated,Validate service account before validating token.,"If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --service-account-lookup argument exists it is set to true.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --service-account-lookup=true Alternatively, you can delete the --service-account-lookup parameter from this file so that the default takes effect. Default Value: By default, --service-account-lookup argument is set to true. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_token_validation_precheck_enabled; kubernetes_api_server_service_account_lookup_required; kubernetes_api_server_service_account_validation_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use
1.2.22,Ensure that the --service-account-key-file argument is set as appropriate,Automated,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file. Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --service-account-key- file parameter to the public key file for service accounts: --service-account-key-file=<filename> Default Value: By default, --service-account-key-file argument is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_protected,• Level 1 - Master Node,The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.2.23,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters. --etcd-certfile=<path/to/client-certificate-file> --etcd-keyfile=<path/to/client-key-file> Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",etcd_server_tls_encryption_enabled; etcd_server_certfile_configured; etcd_server_keyfile_configured; etcd_client_connection_tls_enabled; etcd_certfile_keyfile_valid_pair; etcd_tls_certificate_authentication_enabled,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.2.24,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Automated,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the TLS certificate and private key file parameters. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Default Value: By default, --tls-cert-file and --tls-private-key-file are presented and created for use. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_cert_and_key_configured; kubernetes_api_server_tls_connection_secure; kubernetes_api_server_tls_files_valid,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.25,Ensure that the --client-ca-file argument is set as appropriate,Automated,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the client certificate authority file. --client-ca-file=<path/to/client-ca-file> Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_configured; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_certificate_auth_required; kubernetes_api_server_secure_connection_configured; kubernetes_api_server_client_ca_validation_enabled,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.26,Ensure that the --etcd-cafile argument is set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter. --etcd-cafile=<path/to/ca-file> Default Value: By default, --etcd-cafile is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",etcd_client_tls_encryption_enabled; etcd_cafile_argument_configured; etcd_client_connection_secure; etcd_tls_certificate_authority_configured; etcd_secure_client_communication_enabled,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.2.27,Ensure that the --encryption-provider-config argument is set as appropriate,Manual,Encrypt etcd key-value store.,etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Impact: None,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --encryption-provider-config argument is set to a EncryptionConfig file. Additionally, ensure that the EncryptionConfig file has all the desired resources covered especially any secrets.","Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=</path/to/EncryptionConfig/File> Default Value: By default, --encryption-provider-config is not set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92",etcd_store_encryption_enabled; etcd_store_encryption_provider_configured; etcd_store_encryption_config_valid; etcd_store_encryption_key_secure; etcd_store_encryption_provider_appropriate,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92
1.2.28,Ensure that encryption providers are appropriately configured,Manual,"Where etcd encryption is used, appropriate providers should be configured.","Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms, and secretbox are likely to be appropriate options. Impact: None","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Get the EncryptionConfig file set for --encryption-provider-config argument. Verify that aescbc, kms, or secretbox is set as the encryption provider for all the desired resources.","Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms, or secretbox as the encryption provider. Default Value: By default, no encryption provider is set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers",etcd_encryption_provider_configured; etcd_encryption_provider_secure; etcd_encryption_provider_valid; etcd_encryption_provider_approved; etcd_encryption_provider_compliant; etcd_encryption_provider_active; etcd_encryption_provider_enabled; etcd_encryption_provider_correct,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers
1.2.29,Ensure that the API Server only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the API server is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS cipher suites including some that have security concerns, weakening the protection provided. Impact: API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter. --tls-cipher-suites=TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256. Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: Insecure values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_RC4_128_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_RC4_128_SHA.",kubernetes_api_server_strong_ciphers_enabled; kubernetes_api_server_weak_ciphers_disabled; kubernetes_api_server_tls_min_version_1_2; kubernetes_api_server_cipher_suites_restricted; kubernetes_api_server_insecure_ciphers_removed,• Level 1 - Master Node,API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.,"1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: Insecure values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_RC4_128_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_RC4_128_SHA."
1.2.30,Ensure that the --service-account-extend-token-expiration parameter is set to false,Automated,By default Kubernetes extends service account token lifetimes to one year to aid in transition from the legacy token settings.,"This default setting is not ideal for security as it ignores other settings related to maximum token lifetime and means that a lost or stolen credential could be valid for an extended period of time. Impact: Disabling this setting means that the service account token expiry set in the cluster will be enforced, and service account tokens will expire at the end of that time frame.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-extend-token-expiration argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --service-account-extend-token-expiration parameter to false. --service-account-extend-token-expiration=false Default Value: By default, this parameter is set to true References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",kubernetes_service_account_token_extend_expiration_disabled; kubernetes_service_account_token_expiration_default; kubernetes_service_account_token_lifetime_restricted; kubernetes_service_account_token_extend_expiration_false; kubernetes_service_account_token_expiration_not_extended,• Level 1 - Master Node,"Disabling this setting means that the service account token expiry set in the cluster will be enforced, and service account tokens will expire at the end of that time frame.",1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Manual,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --terminated- pod-gc-threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_pod_terminated_gc_threshold_set; kubernetes_pod_terminated_gc_threshold_configured; kubernetes_pod_gc_threshold_set; kubernetes_pod_termination_gc_enabled; kubernetes_pod_termination_gc_threshold_configured,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; eks_node_profiling_disabled; kubernetes_pod_profiling_disabled; container_runtime_profiling_disabled; compute_instance_profiling_disabled; serverless_function_profiling_disabled; cloud_service_profiling_disabled; application_logging_profiling_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.3.3,Ensure that the --use-service-account-credentials argument is set to true,Automated,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service- account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node to set the below parameter. --use-service-account-credentials=true Default Value: By default, --use-service-account-credentials is set to false. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",kubernetes_controller_use_service_account_credentials_enabled; kubernetes_controller_service_account_credentials_required; kubernetes_controller_individual_service_account_credentials; kubernetes_controller_credentials_argument_set_true; kubernetes_controller_service_account_credentials_enforced,• Level 1 - Master Node,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.4,Ensure that the --service-account-private-key-file argument is set as appropriate,Automated,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private- key-file as appropriate. Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --service- account-private-key-file parameter to the private key file for service accounts. --service-account-private-key-file=<filename> Default Value: By default, --service-account-private-key-file it not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_explicitly_set; kubernetes_controller_manager_service_account_private_key_file_defined; kubernetes_controller_manager_service_account_private_key_file_specified,• Level 1 - Master Node,You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.5,Ensure that the --root-ca-file argument is set as appropriate,Automated,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Impact: You need to setup and maintain root certificate authority file.,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`. --root-ca-file=<path/to/file> Default Value: By default, --root-ca-file is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_api_server_root_ca_file_configured; kubernetes_api_server_root_ca_file_valid; kubernetes_api_server_root_ca_file_secure; kubernetes_api_server_root_ca_file_present; kubernetes_api_server_root_ca_file_trusted,• Level 1 - Master Node,You need to setup and maintain root certificate authority file.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.3.6,Ensure that the RotateKubeletServerCertificate argument is set to true,Automated,Enable kubelet server certificate rotation on controller-manager.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true. --feature-gates=RotateKubeletServerCertificate=true Default Value: By default, RotateKubeletServerCertificate is set to 'true' this recommendation verifies that it has not been disabled. References: 1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_rotate_kubelet_server_certificate_enabled; kubernetes_controller_manager_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubernetes_controller_manager_tls_certificate_rotation_enabled; kubernetes_kubelet_tls_certificate_rotation_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.7,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the Controller Manager service to non-loopback insecure addresses.,"The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address",kubernetes_controller_manager_bind_address_localhost; kubernetes_controller_manager_network_bind_restricted; kubernetes_controller_manager_loopback_only_enabled; kubernetes_controller_manager_insecure_bind_disabled; kubernetes_controller_manager_localhost_bind_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address
1.4.1,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml file on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_cluster_profiling_disabled; kubernetes_cluster_profiling_set_false; kubernetes_api_profiling_disabled; kubernetes_api_profiling_set_false,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.4.2,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the scheduler service to non-loopback insecure addresses.,"The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml on the Control Plane node and ensure the correct value for the -- bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/",kubernetes_scheduler_bind_address_localhost; kubernetes_scheduler_loopback_address_only; kubernetes_scheduler_insecure_bind_disabled; kubernetes_scheduler_localhost_bind_enabled; kubernetes_scheduler_network_bind_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/
2.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_tls_cert_key_valid; etcd_service_secure_communication_enabled,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.2,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_tls_certificates_valid; etcd_service_tls_authentication_enabled,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.3,Ensure that the --client-cert-auth argument is set to true,Automated,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: All clients attempting to access the etcd server will require a valid client certificate.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --client-cert-auth='true' Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_cert_auth_enabled; etcd_client_authentication_required; etcd_service_tls_client_auth_enabled; etcd_client_cert_authentication_enabled; etcd_secure_client_auth_enabled,• Level 1 - Master Node,All clients attempting to access the etcd server will require a valid client certificate.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
2.4,Ensure that the --auto-tls argument is not set to true,Automated,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: Clients will not be able to use self-signed certificates for TLS.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --auto-tls parameter or set it to false. --auto-tls=false Default Value: By default, --auto-tls is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_certificate_self_signed_disabled; cloud_cdn_tls_auto_cert_disabled; cloud_cdn_domain_tls_self_signed_disabled; cloud_cdn_tls_auto_issuance_disabled,• Level 1 - Master Node,Clients will not be able to use self-signed certificates for TLS.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
2.5,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Impact: etcd cluster peers would need to set up TLS for their communication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --peer-client-file=</path/to/peer-cert-file> --peer-key-file=</path/to/peer-key-file> Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_peer_certificate_file_set; etcd_peer_key_file_set; etcd_peer_tls_encryption_enabled; etcd_peer_certificate_file_valid; etcd_peer_key_file_valid; etcd_peer_tls_configuration_secure,• Level 1 - Master Node,etcd cluster peers would need to set up TLS for their communication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.6,Ensure that the --peer-client-cert-auth argument is set to true,Automated,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --peer-client-cert-auth=true Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth",etcd_cluster_peer_client_cert_auth_enabled; etcd_peer_authentication_required; etcd_peer_tls_auth_enabled; etcd_cluster_peer_cert_auth_enabled; etcd_peer_client_cert_auth_enabled,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth
2.7,Ensure that the --peer-auto-tls argument is not set to true,Automated,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --peer-auto-tls parameter or set it to false. --peer-auto-tls=false Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",kubernetes_cluster_peer_auto_tls_disabled; kubernetes_cluster_peer_tls_manual_certificates; kubernetes_cluster_peer_tls_auto_generated_disabled; kubernetes_cluster_peer_tls_self_signed_disabled; kubernetes_cluster_peer_tls_custom_certificates_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
2.8,Ensure that a unique Certificate Authority is used for etcd,Manual,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster. Run the following command on the master node: ps -ef | grep etcd Note the file referenced by the --trusted-ca-file argument. Run the following command on the master node: ps -ef | grep apiserver Verify that the file referenced by the --client-ca-file for apiserver is different from the --trusted-ca-file used by etcd.,"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --trusted-ca-file=</path/to/ca-file> Default Value: By default, no etcd certificate is created and used. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html",compute_etcd_unique_certificate_authority; compute_etcd_ca_different_from_kubernetes; compute_etcd_separate_certificate_authority; compute_etcd_distinct_ca_required; compute_etcd_ca_isolation_enforced,• Level 2 - Master Node,Additional management of the certificates and keys for the dedicated certificate authority will be required.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
3.1.1,Client certificate authentication should not be used for users,Manual,"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication.,"Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. Default Value: Client certificate authentication is enabled by default. Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",kubernetes_user_no_client_certificate_authentication; kubernetes_user_client_certificate_authentication_disabled; kubernetes_authentication_no_user_client_certificates; kubernetes_user_authentication_no_client_certificates; kubernetes_authentication_client_certificate_revocation_unsupported,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.2,Service account token authentication should not be used for users,Manual,"Kubernetes provides service account tokens which are intended for use by workloads running in the Kubernetes cluster, for authentication to the API server. These tokens are not designed for use by end-users and do not provide for features such as revocation or expiry, making them insecure. A newer version of the feature (Bound service account token volumes) does introduce expiry but still does not allow for specific revocation.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Service account token authentication does not allow for this due to the use of JWT tokens as an underlying technology. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of service account token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of service account tokens. Default Value: Service account token authentication is enabled by default.,kubernetes_service_account_token_auth_disabled; kubernetes_service_account_user_auth_disabled; kubernetes_service_account_no_user_tokens; kubernetes_service_account_token_revocation_enabled; kubernetes_service_account_token_expiry_enabled; kubernetes_service_account_bound_token_volumes_enabled,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.3,Bootstrap token authentication should not be used for users,Manual,Kubernetes provides bootstrap tokens which are intended for use by new nodes joining the cluster These tokens are not designed for use by end-users they are specifically designed for the purpose of bootstrapping new nodes and not for general authentication,Bootstrap tokens are not intended for use as a general authentication mechanism and impose constraints on user and group naming that do not facilitate good RBAC design. They also cannot be used with MFA resulting in a weak authentication mechanism being available. Impact: External mechanisms for authentication generally require additional software to be deployed.,Review user access to the cluster and ensure that users are not making use of bootstrap token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of bootstrap tokens. Default Value: Bootstrap token authentication is not enabled by default and requires an API server parameter to be set.,kubernetes_user_no_bootstrap_token_auth; kubernetes_user_bootstrap_token_disabled; kubernetes_auth_no_bootstrap_token_usage; kubernetes_user_no_node_bootstrap_token; kubernetes_auth_bootstrap_token_restricted,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.2.1,Ensure that a minimal audit policy is created,Manual,Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.,"Logging is an important detective control for all systems, to detect potential unauthorised access. Impact: Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",Run the following command on one of the cluster master nodes: ps -ef | grep kube-apiserver Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy.,"Create an audit policy file for your cluster. Default Value: Unless the --audit-policy-file flag is specified, no auditing will be carried out. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/",kubernetes_api_audit_policy_enabled; kubernetes_api_audit_policy_minimal; kubernetes_api_audit_policy_configured; kubernetes_api_audit_logging_enabled; kubernetes_audit_policy_file_set; kubernetes_audit_policy_minimal_rules; kubernetes_api_audit_policy_active; kubernetes_audit_logging_policy_configured,• Level 1 - Master Node,"Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
3.2.2,Ensure that the audit policy covers key security concerns,Manual,Ensure that the audit policy created for the cluster covers key security concerns.,"Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Impact: Increasing audit logging will consume resources on the nodes or other log destination.","Review the audit policy provided for the cluster and ensure that it covers at least the following areas :- • Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data. • Modification of pod and deployment objects. • Use of pods/exec, pods/portforward, pods/proxy and services/proxy. For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","Consider modification of the audit policy in use on the cluster to include these items, at a minimum. Default Value: By default Kubernetes clusters do not log audit information. References: 1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735",kubernetes_audit_policy_security_concerns_covered; kubernetes_audit_policy_key_events_logged; kubernetes_audit_policy_minimum_requirements_met; kubernetes_audit_policy_compliance_standards_met; kubernetes_audit_policy_sensitive_actions_included; kubernetes_audit_policy_cis_benchmark_compliance; kubernetes_audit_policy_security_events_captured; kubernetes_audit_policy_critical_operations_audited,• Level 2 - Master Node,Increasing audit logging will consume resources on the nodes or other log destination.,1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735
4.1.1,Ensure that the kubelet service file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet service file has permissions of 600 or more restrictive.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, the kubelet service file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_permissions_600_or_stricter; kubernetes_kubelet_service_file_permissions_restrictive; kubernetes_kubelet_service_file_permissions_secure; kubernetes_kubelet_service_file_permissions_compliant; kubernetes_kubelet_service_file_permissions_cis_4_1_1,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.2,Ensure that the kubelet service file ownership is set to root:root,Automated,Ensure that the kubelet service file ownership is set to root:root.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, kubelet service file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_ownership_root; kubernetes_kubelet_service_file_owner_root; kubernetes_kubelet_file_ownership_root_root; kubernetes_service_file_ownership_root; kubelet_service_file_ownership_root_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.3,If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive,Manual,"If kube-proxy is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of 600 or more restrictive.","The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None","Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a <path><filename> Verify that a file is specified and it exists with permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 <proxy kubeconfig file> Default Value: By default, proxy file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_permissions_600; kubernetes_proxy_kubeconfig_file_restrictive_permissions; kubernetes_proxy_kubeconfig_file_permissions_secure; kubernetes_kubeconfig_file_permissions_600_or_stricter; kubernetes_proxy_kubeconfig_file_mode_600,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.4,If proxy kubeconfig file exists ensure ownership is set to root:root,Manual,"If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G <path><filename> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",compute_proxy_kubeconfig_root_ownership; compute_kubeconfig_root_owner; compute_proxy_kubeconfig_file_secure_ownership; compute_kubeconfig_file_root_ownership; compute_kube_proxy_kubeconfig_root_owner,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.5,Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet.conf file has permissions of 600 or more restrictive.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file has permissions of 600. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_permissions_restrictive; kubernetes_kubelet_conf_file_permissions_600; kubernetes_kubeconfig_file_permissions_secure; kubernetes_kubelet_conf_file_permissions_restricted; kubernetes_kubeconfig_file_permissions_min_600,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.6,Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root,Automated,Ensure that the kubelet.conf file ownership is set to root:root.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_ownership_root; kubernetes_kubelet_conf_root_owned; kubelet_config_file_ownership_root; kubeconfig_kubelet_root_ownership; kubernetes_kubelet_conf_file_root_owner,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.7,Ensure that the certificate authorities file permissions are set to 644 or more restrictive,Manual,Ensure that the certificate authorities file has permissions of 644 or more restrictive.,The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %a <filename> Verify that the permissions are 644 or more restrictive.,Run the following command to modify the file permissions of the --client-ca-file chmod 644 <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_certificate_authorities_file_permissions_644_or_restrictive; compute_ca_file_permissions_secure; compute_ssl_certificate_authorities_permissions_restricted; compute_trusted_ca_file_permissions_644_or_stricter; compute_ca_bundle_file_permissions_compliant,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.8,Ensure that the client certificate authorities file ownership is set to root:root,Manual,Ensure that the certificate authorities file ownership is set to root:root.,The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %U:%G <filename> Verify that the ownership is set to root:root.,Run the following command to modify the ownership of the --client-ca-file. chown root:root <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_client_certificate_authorities_file_ownership_root; compute_client_certificate_authorities_file_ownership_root_root; compute_certificate_authorities_file_ownership_root; compute_certificate_authorities_file_ownership_root_root; compute_ssl_certificate_authorities_file_ownership_root; compute_ssl_certificate_authorities_file_ownership_root_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.9,If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 600 or more restrictive.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml Verify that the permissions are 600 or more restrictive.","Run the following command (using the config file location identified in the Audit step) chmod 600 /var/lib/kubelet/config.yaml Default Value: By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of 600. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_permissions_restrictive; kubernetes_kubelet_config_file_permissions_600_or_stricter; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_permissions_min_600; kubernetes_kubelet_config_file_permissions_protected,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.1.10,If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /var/lib/kubelet/config.yaml ```Verify that the ownership is set to `root:root`.","Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by root:root. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_owner_root; kubernetes_kubelet_config_file_group_root; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_ownership_correct; kubernetes_kubelet_config_file_root_owned,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.2.1,Ensure that the --anonymous-auth argument is set to false,Automated,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.","If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false. This executable argument may be omitted, provided there is a corresponding entry set to false in the Kubelet config file.","If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --anonymous-auth=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_anonymous_auth_set_false; kubernetes_kubelet_auth_anonymous_disabled; kubernetes_kubelet_auth_no_anonymous; kubernetes_kubelet_secure_auth_enabled,• Level 1 - Worker Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.2,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.","Run the following command on each node: ps -ef | grep kubelet If the --authorization-mode argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by -- config, and that file sets authorization: mode to something other than AlwaysAllow. It is also possible to review the running configuration of a Kubelet via the /configz endpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --authorization-mode=Webhook Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",eks_cluster_authorization_mode_not_always_allow; eks_cluster_explicit_authorization_required; eks_cluster_request_authorization_enabled; eks_cluster_always_allow_disabled,• Level 1 - Worker Node,Unauthorized requests will be denied.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.3,Ensure that the --client-ca-file argument is set as appropriate,Automated,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. If the --client-ca-file argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets authentication: x509: clientCAFile to the location of the client certificate authority file.","If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to the location of the client CA file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --client-ca-file=<path/to/client-ca-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/",kubernetes_kubelet_client_ca_file_set; kubernetes_kubelet_authentication_certificate_enabled; kubernetes_kubelet_client_ca_file_configured; kubernetes_kubelet_certificate_authentication_required; kubernetes_kubelet_client_ca_file_valid,• Level 1 - Worker Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/
4.2.4,"Verify that if defined, readOnlyPort is set to 0",Manual,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,"Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.","If using a Kubelet config file, edit the file to set readOnlyPort to 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --read-only-port=0 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/6cedc0853faa118df0ba3d41b48b 993422ad3df6/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L142 Additional Information: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/",kubernetes_api_server_read_only_port_disabled; kubernetes_api_server_read_only_port_set_to_zero; kubernetes_api_server_read_only_port_secure; kubernetes_api_server_read_only_port_unset; kubernetes_api_server_read_only_port_default_secure,• Level 1 - Worker Node,Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/6cedc0853faa118df0ba3d41b48b 993422ad3df6/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L142 Additional Information: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
4.2.5,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Manual,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.","Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0. If the argument is not present, and there is a Kubelet config file specified by --config, check that it does not set streamingConnectionIdleTimeout to 0.","If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a value other than 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --streaming-connection-idle-timeout=5m Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",eks_cluster_streaming_connection_timeout_set; eks_cluster_streaming_idle_timeout_not_disabled; eks_cluster_streaming_timeout_configured; eks_streaming_connection_timeout_valid; eks_streaming_idle_timeout_non_zero,• Level 1 - Worker Node,Long-lived connections could be interrupted.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
4.2.6,Ensure that the --make-iptables-util-chains argument is set to true,Automated,Allow Kubelet to manage iptables.,"Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.","Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.","If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove the --make- iptables-util-chains argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --make-iptables-util-chains argument is set to true. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_management_enabled; kubernetes_kubelet_iptables_chains_configured; kubernetes_kubelet_iptables_util_chains_set; kubernetes_kubelet_iptables_util_chains_true,• Level 1 - Worker Node,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",1. https://kubernetes.io/docs/admin/kubelet/
4.2.7,Ensure that the --hostname-override argument is not set,Manual,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Impact: Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist. Note This setting is not configurable via the Kubelet config file.,"Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10- kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_default; kubernetes_node_hostname_unchanged; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_standardized,• Level 1 - Worker Node,"Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063
4.2.8,Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture,Manual,"Security relevant information should be captured. The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,"Run the following command on each node: sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10- kubeadm.conf or If using command line arguments, kubelet service file is located /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10-kubelet- args.conf Review the value set for the argument and determine whether this has been set to an appropriate level for the cluster. If the argument does not exist, check that there is a Kubelet config file specified by -- config and review the value in this location. If using command line arguments","If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, eventRecordQPS argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go",kubernetes_kubelet_event_record_qps_configured; kubernetes_kubelet_event_capture_rate_limited; kubernetes_kubelet_event_record_qps_within_bounds; kubernetes_kubelet_event_logging_rate_optimized; kubernetes_kubelet_event_record_qps_non_zero; kubernetes_kubelet_event_capture_rate_secure; kubernetes_kubelet_event_record_qps_protected; kubernetes_kubelet_event_logging_rate_limited,• Level 2 - Worker Node,Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go
4.2.9,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Manual,Setup TLS connection on the Kubelets.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.","Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. If these arguments are not present, check that there is a Kubelet config specified by -- config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_configured; kubernetes_kubelet_tls_cert_and_key_valid; kubernetes_kubelet_tls_encryption_enabled,• Level 1 - Worker Node,,
4.2.10,Ensure that the --rotate-certificates argument is not set to false,Automated,Enable kubelet client certificate rotation.,The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None,"Run the following command on each node: ps -ef | grep kubelet Verify that the RotateKubeletServerCertificate argument is not present, or is set to true. If the RotateKubeletServerCertificate argument is not present, verify that if there is a Kubelet config file specified by --config, that file does not contain RotateKubeletServerCertificate: false.","If using a Kubelet config file, edit the file to add the line rotateCertificates: true or remove it altogether to use the default value. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove --rotate- certificates=false argument from the KUBELET_CERTIFICATE_ARGS variable or set - -rotate-certificates=true . Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet client certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_rotate_certificates_not_disabled; kubernetes_kubelet_client_cert_rotation_required; kubernetes_kubelet_tls_cert_rotation_enabled; kubernetes_kubelet_auto_cert_rotation_enabled,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/
4.2.11,Verify that the RotateKubeletServerCertificate argument is set to true,Manual,Enable kubelet server certificate rotation.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate- server-certificates parameter is set on kubelet Run the following command on each node: ps -ef | grep kubelet Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable. --feature-gates=RotateKubeletServerCertificate=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet server certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",compute_kubelet_certificate_rotation_enabled; compute_kubelet_server_certificate_rotation_enabled; kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubelet_server_certificate_rotation_enabled,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration
4.2.12,Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the Kubelet is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.","The set of cryptographic ciphers currently considered secure is the following: • TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 • TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_128_GCM_SHA256 Run the following command on each node: ps -ef | grep kubelet If the --tls-cipher-suites argument is present, ensure it only contains values included in this set. If it is not present check that there is a Kubelet config file specified by --config, and that file sets tlsCipherSuites: to only include values from this set.","If using a Kubelet config file, edit the file to set tlsCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_ 256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WI TH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES _128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the --tls-cipher- suites parameter as follows, or to a subset of these values. --tls-cipher- suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_kubelet_strong_ciphers_enabled; kubernetes_kubelet_weak_ciphers_disabled; kubernetes_kubelet_tls_ciphers_restricted; kubernetes_kubelet_cipher_suite_compliant; kubernetes_kubelet_secure_cipher_config; kubernetes_kubelet_cryptographic_ciphers_enforced; kubernetes_kubelet_approved_ciphers_only; kubernetes_kubelet_cipher_strength_validated,• Level 1 - Worker Node,Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.,
4.2.13,Ensure that a limit is set on pod PIDs,Manual,Ensure that the Kubelet sets limits on the number of PIDs that can be created by pods running on the node.,"By default pods running in a cluster can consume any number of PIDs, potentially exhausting the resources available on the node. Setting an appropriate limit reduces the risk of a denial of service attack on cluster nodes. Impact: Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.","Review the Kubelet's start-up parameters for the value of --pod-max-pids, and check the Kubelet configuration file for the PodPidsLimit . If neither of these values is set, then there is no limit in place.","Decide on an appropriate level for this parameter and set it, either via the --pod-max- pids command line parameter or the PodPidsLimit configuration file setting. Default Value: By default the number of PIDs is not limited. References: 1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits",kubernetes_kubelet_pid_limit_enabled; kubernetes_pod_pid_limit_configured; kubernetes_node_pid_limit_enforced; kubernetes_kubelet_pid_max_set; kubernetes_pod_pid_restriction_active,• Level 1 - Worker Node,Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.,1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits
4.2.14,Ensure that the --seccomp-default parameter is set to true,Manual,Ensure that the Kubelet enforces the use of the RuntimeDefault seccomp profile,"By default, Kubernetes disables the seccomp profile which ships with most container runtimes. Setting this parameter will ensure workloads running on the node are protected by the runtime's seccomp profile. Impact: Setting this will remove some rights from pods running on the node.","Review the Kubelet's start-up parameters for the value of --seccomp-default, and check the Kubelet configuration file for the seccompDefault . If neither of these values is set, then the seccomp profile is not in use.","Set the parameter, either via the --seccomp-default command line parameter or the seccompDefault configuration file setting. Default Value: By default the seccomp profile is not enabled. References: 1. https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of- runtimedefault-as-the-default-seccomp-profile-for-all-workloads",kubernetes_kubelet_seccomp_default_enabled; kubernetes_kubelet_runtime_default_profile_enabled; kubernetes_kubelet_seccomp_profile_default_set; kubernetes_kubelet_seccomp_default_true; kubernetes_kubelet_runtime_default_seccomp_enabled,• Level 1 - Worker Node,Setting this will remove some rights from pods running on the node.,1. https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of- runtimedefault-as-the-default-seccomp-profile-for-all-workloads
4.2.15,Ensure that the --IPAddressDeny is set to any,Manual,Ensuring that --IPAddressDeny is set to 'Any' will facilitate allowlisting of only IP addresses that are explicitly set with the --IPAddressAllow parameter which will block unspecified IP addresses from communicating with the kubelet component.,"By default, Kubernetes allows any IP address to communicate with the kubelet component IP restrictions and IP whitelisting are security best practices and reduce the attack surface of the kubelet . Impact: Configuring the setting IPAddressDeny=any will deny service to any IP address not specified in the complimentary setting IPAddressDeny=any configuration parameter. Applying IPAddressDeny=any alone will completely disable communication with the component.","Review the Kubelet's start-up parameters for the value of --IPAddressDeny, and check the Kubelet configuration file for IPAddressDeny=any. If this entry is present it should be accompanied by IPAddressAllow={{ kubelet_secure_addresses }} to allow the control plane to communicate with the component.","IPAddressDeny=any IPAddressAllow={{ kubelet_secure_addresses }} *Note kubelet_secure_addresses: 'localhost link-local {{ kube_pods_subnets | regex_replace(',', ' ') }} {{ kube_node_addresses }} {{ loadbalancer_apiserver.address | default('')' Default Value: By default IPAddressDeny is not enabled. References: 1. https://github.com/kubernetes-sigs/kubespray/pull/9194/files 2. https://kubernetes.io/docs/concepts/services-networking/network-policies/",kubernetes_kubelet_ip_address_deny_set_to_any; kubernetes_kubelet_ip_deny_all_enabled; kubernetes_kubelet_ip_allowlist_restricted; kubernetes_kubelet_network_access_restricted; kubernetes_kubelet_ip_deny_all_configured,• Level 2 - Worker Node,Configuring the setting IPAddressDeny=any will deny service to any IP address not specified in the complimentary setting IPAddressDeny=any configuration parameter. Applying IPAddressDeny=any alone will completely disable communication with the component.,1. https://github.com/kubernetes-sigs/kubespray/pull/9194/files 2. https://kubernetes.io/docs/concepts/services-networking/network-policies/
4.3.1,Ensure that the kube-proxy metrics service is bound to localhost,Manual,Do not bind the kube-proxy metrics port to non-loopback addresses.,kube-proxy has two APIs which provided access to information about the service and can be bound to network ports. The metrics API service includes endpoints (/metrics and /configz) which disclose information about the configuration and operation of kube-proxy. These endpoints should not be exposed to untrusted networks as they do not support encryption or authentication to restrict access to the data they provide. Impact: 3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,review the start-up flags provided to kube proxy Run the following command on each node: ps -ef | grep -i kube-proxy Ensure that the --metrics-bind-address parameter is not set to a value other than 127.0.0.1. From the output of this command gather the location specified in the -- config parameter. Review any file stored at that location and ensure that it does not specify a value other than 127.0.0.1 for metricsBindAddress.,Modify or remove any values which bind the metrics service to a non-localhost address Default Value: The default value is 127.0.0.1:10249 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/,kubernetes_proxy_metrics_localhost_bound; kubernetes_proxy_metrics_non_loopback_blocked; kubernetes_proxy_metrics_loopback_only; kubernetes_proxy_metrics_restricted_to_localhost; kubernetes_proxy_metrics_external_access_disabled,• Level 1 - Worker Node,3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/
5.1.1,Ensure that the cluster-admin role is only used where required,Manual,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",kubernetes_role_no_cluster_admin; kubernetes_role_cluster_admin_restricted; kubernetes_role_cluster_admin_minimal_usage; kubernetes_role_cluster_admin_least_privilege; kubernetes_role_cluster_admin_required_only,• Level 1 - Master Node,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
5.1.2,Minimize access to secrets,Manual,"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation,"Review the users who have get, list, or watch access to secrets objects in the Kubernetes API.","Where possible, restrict access to secret objects in the cluster by removing get, list, and watch permissions. Default Value: By default in a kubeadm cluster the following list of principals have get privileges on secret objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:expand-controller expand-controller ServiceAccount kube-system system:controller:generic-garbage-collector generic-garbage- collector ServiceAccount kube-system system:controller:namespace-controller namespace-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:kube-controller-manager system:kube-controller- manager User",kubernetes_secret_access_restricted; kubernetes_secret_minimal_access; kubernetes_secret_no_public_access; kubernetes_secret_no_anonymous_access; kubernetes_secret_no_wildcard_access; kubernetes_secret_no_default_service_account; kubernetes_secret_automount_disabled; kubernetes_secret_read_only_access; kubernetes_secret_no_broad_permissions; kubernetes_secret_no_excessive_roles,• Level 1 - Master Node,Care should be taken not to remove access to secrets to system components which require this for their operation,
5.1.3,Minimize wildcard use in Roles and ClusterRoles,Manual,Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard '*' which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.,The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.,Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml,Where possible replace any use of wildcards in ClusterRoles and Roles with specific objects or actions.,kubernetes_role_no_wildcard_resources; kubernetes_clusterrole_no_wildcard_resources; kubernetes_role_no_wildcard_actions; kubernetes_clusterrole_no_wildcard_actions; kubernetes_role_no_wildcard_all; kubernetes_clusterrole_no_wildcard_all; kubernetes_role_minimize_wildcard_usage; kubernetes_clusterrole_minimize_wildcard_usage,• Level 1 - Worker Node,,
5.1.4,Minimize access to create pods,Manual,"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.","The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",Review the users who have create access to pod objects in the Kubernetes API.,"Where possible, remove create access to pod objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have create privileges on pod objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:daemon-set-controller daemon-set-controller ServiceAccount kube-system system:controller:job-controller job-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:controller:replicaset-controller replicaset-controller ServiceAccount kube-system system:controller:replication-controller replication-controller ServiceAccount kube-system system:controller:statefulset-controller statefulset-controller ServiceAccount kube-system",kubernetes_namespace_pod_creation_restricted; kubernetes_role_pod_creation_minimized; kubernetes_service_account_pod_creation_limited; kubernetes_cluster_pod_creation_access_controlled; kubernetes_rbac_pod_creation_permissions_restricted; kubernetes_user_pod_creation_privileges_minimized; kubernetes_group_pod_creation_access_limited; kubernetes_policy_pod_creation_privilege_escalation_prevented,• Level 1 - Master Node,Care should be taken not to remove access to pods to system components which require this for their operation,
5.1.5,Ensure that default service accounts are not actively used.,Manual,The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.,"Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured to ensure that it does not automatically provide a service account token, and it must not have any non-default role bindings or custom role assignments Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.","For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/,iam_service_account_default_not_used; iam_service_account_default_inactive; compute_service_account_default_not_used; compute_service_account_default_inactive; service_account_default_no_active_usage; service_account_default_no_active_roles; service_account_default_no_active_permissions,• Level 1 - Master Node,All workloads which require access to the Kubernetes API will require an explicit service account to be created.,1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.6,Ensure that Service Account Tokens are only mounted where necessary,Manual,Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server,"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.","Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false","Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_pod_service_account_token_unmounted; kubernetes_pod_service_account_token_restricted; kubernetes_pod_service_account_token_minimized; kubernetes_pod_service_account_token_disabled_by_default; kubernetes_pod_service_account_token_required_only,• Level 1 - Master Node,"Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.7,Avoid use of system:masters group,Manual,"The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed. When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster. Impact: Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.,Remove the system:masters group from all users in the cluster. Default Value: By default some clusters will create a 'break glass' client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations. References: 1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38,iam_group_system_masters_unused; iam_group_system_masters_restricted; iam_group_system_masters_no_assignments; iam_group_system_masters_minimal_usage; iam_group_system_masters_bootstrap_only,• Level 1 - Master Node,"Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38
5.1.8,"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",Manual,"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster- admin level. Impact: There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.","Review the users who have access to cluster roles or roles which provide the impersonate, bind, or escalate privileges.","Where possible, remove the impersonate, bind, and escalate rights from subjects. Default Value: In a default kubeadm cluster, the system:masters group and clusterrole-aggregation- controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate. References: 1. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 2. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/",kubernetes_cluster_role_no_privilege_escalation; kubernetes_role_no_privilege_escalation; kubernetes_cluster_role_no_impersonation; kubernetes_role_no_impersonation; kubernetes_cluster_role_no_binding; kubernetes_role_no_binding; kubernetes_cluster_role_restrict_escalate_permissions; kubernetes_role_restrict_escalate_permissions; kubernetes_cluster_role_restrict_impersonate_permissions; kubernetes_role_restrict_impersonate_permissions; kubernetes_cluster_role_restrict_bind_permissions; kubernetes_role_restrict_bind_permissions,• Level 1 - Master Node,"There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",1. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 2. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/
5.1.9,Minimize access to create persistent volumes,Manual,"The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of hostPath volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.","The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have create access to PersistentVolume objects in the Kubernetes API.,"Where possible, remove create access to PersistentVolume objects in the cluster. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation",kubernetes_persistent_volume_creation_restricted; kubernetes_persistent_volume_creation_minimized; kubernetes_persistent_volume_hostpath_disabled; kubernetes_persistent_volume_privilege_escalation_prevented; kubernetes_persistent_volume_creation_admin_only; kubernetes_persistent_volume_creation_role_based; kubernetes_persistent_volume_creation_least_privilege; kubernetes_persistent_volume_creation_sensitive_host_access_blocked,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation
5.1.10,Minimize access to the proxy sub-resource of nodes,Manual,"Users with access to the Proxy sub-resource of Node objects automatically have permissions to use the kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs. The kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the node object. The proxy sub-resource specifically allows wide ranging access to the kubelet API. Direct access to the kubelet API bypasses controls like audit logging (there is no audit log of kubelet API access) and admission control.","The ability to use the proxy sub-resource of node objects opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have access to the proxy sub-resource of node objects in the Kubernetes API.,"Where possible, remove access to the proxy sub-resource of node objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization",kubernetes_node_proxy_access_restricted; kubernetes_node_proxy_no_admin_access; kubernetes_node_proxy_minimal_permissions; kubernetes_node_proxy_api_disabled; kubernetes_node_proxy_privilege_escalation_prevented; kubernetes_node_proxy_audit_logging_bypassed; kubernetes_node_proxy_kubelet_api_restricted; kubernetes_node_proxy_subresource_access_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization
5.1.11,Minimize access to the approval sub-resource of certificatesigningrequests objects,Manual,"Users with access to the update the approval sub-resource of CertificateSigningRequests objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts. This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster",The ability to update certificate signing requests should be limited.,Review the users who have access to update the approval sub-resource of CertificateSigningRequests objects in the Kubernetes API.,"Where possible, remove access to the approval sub-resource of CertificateSigningRequests objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing",kubernetes_certificate_signing_request_approval_restricted; kubernetes_csr_approval_minimal_access; kubernetes_csr_approval_no_public_access; kubernetes_csr_approval_admin_restricted; kubernetes_csr_approval_privilege_escalation_prevented,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing
5.1.12,Minimize access to webhook configuration objects,Manual,"Users with rights to create/modify/delete validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.",The ability to manage webhook configuration should be limited,Review the users who have access to validatingwebhookconfigurations or mutatingwebhookconfigurations objects in the Kubernetes API.,"Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks",kubernetes_validating_webhook_configuration_access_minimized; kubernetes_mutating_webhook_configuration_access_minimized; kubernetes_webhook_configuration_admin_access_restricted; kubernetes_webhook_configuration_write_access_restricted; kubernetes_webhook_configuration_modify_access_restricted; kubernetes_webhook_configuration_delete_access_restricted; kubernetes_webhook_configuration_privileged_access_minimized; kubernetes_webhook_configuration_escalation_prevented,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks
5.1.13,Minimize access to the service account token creation,Manual,"Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.",The ability to create service account tokens should be limited.,Review the users who have access to create the token sub-resource of serviceaccount objects in the Kubernetes API.,"Where possible, remove access to the token sub-resource of serviceaccount objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request",kubernetes_service_account_token_creation_restricted; kubernetes_service_account_token_creation_minimized; kubernetes_cluster_service_account_token_creation_limited; kubernetes_service_account_token_creation_admin_restricted; kubernetes_cluster_service_account_token_creation_admin_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request
5.2.1,Ensure that the cluster has at least one active policy control mechanism in place,Manual,"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts. Impact: Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.,"Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads. Default Value: By default, Pod Security Admission is enabled but no policies are in place. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission",kubernetes_cluster_policy_control_enabled; kubernetes_cluster_pod_security_admission_enabled; kubernetes_cluster_third_party_policy_control_enabled; kubernetes_cluster_active_policy_control_mechanism; kubernetes_cluster_policy_enforcement_active,• Level 1 - Master Node,"Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",1. https://kubernetes.io/docs/concepts/security/pod-security-admission
5.2.2,Minimize the admission of privileged containers,Manual,Do not generally permit containers to be run with the securityContext.privileged flag set to true.,"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.","Run the following command: get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}' It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection. calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node- c4xv4: {} {'privileged':true} {'privileged':true} {'privileged':true} {'privileged':true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {'seccompProfile':{'type':'RuntimeDefault'}} {'allowPrivilegeEscalation':false,'readOnlyRootFilesystem':true,'runAsGroup':2001,'ru nAsUser':1001}","Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privileged_disabled; compute_container_privileged_denied; compute_container_privileged_restricted; compute_container_security_context_privileged_disabled; compute_container_security_context_privileged_denied,• Level 1 - Master Node,"Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.3,Minimize the admission of containers wishing to share the host process ID namespace,Manual,Do not generally permit containers to be run with the hostPID flag set to true.,"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",Fetch hostPID from each pod with get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostPID}\n{end}',"Configure the Admission Controller to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_pid_disabled; compute_container_host_pid_restricted; compute_container_host_pid_not_shared; compute_container_host_pid_protected; compute_container_host_pid_isolated,• Level 1 - Master Node,Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.4,Minimize the admission of containers wishing to share the host IPC namespace,Manual,Do not generally permit containers to be run with the hostIPC flag set to true.,"A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",To fetch hostIPC from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostIPC}\n{end}',"Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ipc_disabled; compute_container_host_ipc_restricted; compute_container_ipc_namespace_isolated; compute_container_host_ipc_denied; compute_container_ipc_sharing_blocked,• Level 1 - Master Node,Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.5,Minimize the admission of containers wishing to share the host network namespace,Manual,Do not generally permit containers to be run with the hostNetwork flag set to true.,"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namespaces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",To fetch hostNetwork from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostNetwork}\n{end}',"Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_network_disabled; compute_container_host_network_restricted; compute_container_host_network_denied; compute_container_network_isolation_enabled; compute_container_host_network_prohibited; compute_container_network_namespace_separated,• Level 1 - Master Node,Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.6,Minimize the admission of containers with allowPrivilegeEscalation,Manual,"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext: allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation. To fetch a list of pods which allowPrivilegeEscalation run this command :- get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}'","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with securityContext: allowPrivilegeEscalation: true Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",container_pod_allow_privilege_escalation_disabled; container_pod_privilege_escalation_restricted; container_workload_privilege_escalation_blocked; kubernetes_pod_privilege_escalation_minimized; container_runtime_privilege_escalation_denied,• Level 1 - Master Node,Pods defined with securityContext: allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.7,Minimize the admission of root containers,Manual,Do not generally permit containers to be run as the root user.,"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one admission control policy defined which does not permit root containers. If you need to run root containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run as the root user will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0.","Create a policy for each namespace in the cluster, ensuring that either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0, is set. Default Value: By default, there are no restrictions on the use of root containers and if a User is not specified in the image, the container will run as root. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_root_disabled; compute_container_root_user_restricted; compute_container_non_root_user_required; compute_container_root_privileges_denied; compute_container_root_access_minimized,• Level 2 - Master Node,Pods with containers which run as the root user will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.8,Minimize the admission of containers with the NET_RAW capability,Manual,Do not generally permit containers with the potentially dangerous NET_RAW capability.,"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability. If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run with the NET_RAW capability will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the NET_RAW capability.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the NET_RAW capability. Default Value: By default, there are no restrictions on the creation of containers with the NET_RAW capability. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_net_raw_capability_disabled; compute_container_net_raw_capability_restricted; compute_container_net_raw_capability_minimized; compute_container_net_raw_capability_denied; compute_container_net_raw_capability_prohibited,• Level 1 - Master Node,Pods with containers which run with the NET_RAW capability will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.9,Minimize the admission of containers with added capabilities,Manual,Do not generally permit containers with capabilities assigned beyond the default set.,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which require capabilities outwith the default set will not be permitted.",Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}',"Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. Default Value: By default, there are no restrictions on adding capabilities to containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",container_capabilities_minimized; container_capabilities_default_only; container_added_capabilities_restricted; container_capabilities_no_privilege_escalation; container_capabilities_baseline_enforced; container_capabilities_whitelist_only; container_capabilities_no_additional_assignments; container_capabilities_default_set_only,• Level 1 - Master Node,Pods with containers which require capabilities outwith the default set will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.10,Minimize the admission of containers with capabilities assigned,Manual,Do not generally permit containers with capabilities,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Impact: Pods with containers require capabilities to operate will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers.","Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities. Default Value: By default, there are no restrictions on the creation of containers with additional capabilities References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_restricted; compute_container_capabilities_minimized; compute_container_no_privileged_capabilities; compute_container_capabilities_disabled; compute_container_capabilities_denied,• Level 2 - Master Node,Pods with containers require capabilities to operate will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.11,Minimize the admission of Windows HostProcess Containers,Manual,Do not generally permit Windows containers to be run with the hostProcess flag set to true.,"A Windows container making use of the hostProcess flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides 'privileged access' to the Windows node. Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit hostProcess Windows containers. If you need to run Windows containers which require hostProcess, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostProcess containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostProcess containers. Default Value: By default, there are no restrictions on the creation of hostProcess containers. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",,• Level 1 - Master Node,Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.12,Minimize the admission of HostPath volumes,Manual,Do not generally admit containers which make use of hostPath volumes.,"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem. There should be at least one admission control policy defined which does not permit containers to mount hostPath volumes. If you need to run containers which require hostPath volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined which make use of hostPath volumes will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with hostPath volumes.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPath volumes. Default Value: By default, there are no restrictions on the creation of hostPath volumes. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_hostpath_volumes_restricted; compute_container_hostpath_volumes_minimized; compute_container_hostpath_volumes_disabled; compute_container_hostpath_volumes_denied; compute_container_hostpath_volumes_blocked,• Level 1 - Master Node,Pods defined which make use of hostPath volumes will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.13,Minimize the admission of containers which use HostPorts,Manual,Do not generally permit containers which require the use of HostPorts.,"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts. If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have hostPort sections.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPort sections. Default Value: By default, there are no restrictions on the use of HostPorts. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ports_restricted; compute_container_host_ports_disabled; compute_container_host_ports_minimized; compute_container_host_ports_denied; compute_container_host_ports_blocked,• Level 1 - Master Node,"Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.3.1,Ensure that the CNI in use supports Network Policies,Manual,There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.,Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None,"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",kubernetes_cni_network_policies_supported; kubernetes_network_plugin_network_policies_enabled; kubernetes_cni_network_policy_compliance; kubernetes_network_plugin_policy_support_required; kubernetes_cni_network_policy_capability_verified,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.
5.3.2,Ensure that all Namespaces have Network Policies defined,Manual,Use network policies to isolate traffic in your cluster network.,"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces Ensure that each namespace defined in the cluster has at least one Network Policy.,"Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",kubernetes_namespace_network_policy_defined; kubernetes_namespace_traffic_isolation_enabled; kubernetes_namespace_network_policy_required; kubernetes_namespace_network_restrictions_enforced,• Level 2 - Master Node,"Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/
5.4.1,Prefer using secrets as files over secrets as environment variables,Manual,Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.,"It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {'\n'}{end}' -A,"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",kubernetes_pod_secrets_as_files; kubernetes_deployment_secrets_as_files; kubernetes_statefulset_secrets_as_files; kubernetes_daemonset_secrets_as_files; kubernetes_cronjob_secrets_as_files; kubernetes_job_secrets_as_files; kubernetes_container_secrets_as_files; kubernetes_workload_secrets_as_files; kubernetes_init_container_secrets_as_files; kubernetes_sidecar_container_secrets_as_files,• Level 2 - Master Node,Application code which expects to read secrets in the form of environment variables would need modification,1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod
5.4.2,Consider external secret storage,Manual,"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",Review your secrets management implementation.,"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured.",kubernetes_secrets_external_storage_required; kubernetes_secrets_authentication_enabled; kubernetes_secrets_audit_logging_enabled; kubernetes_secrets_encryption_enabled; kubernetes_secrets_rotation_enabled,• Level 2 - Master Node,None,
5.5.1,Configure Image Provenance using ImagePolicyWebhook admission controller,Manual,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Impact: You need to regularly maintain your provenance configuration based on container image updates.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888",compute_image_provenance_enabled; compute_image_policy_webhook_enabled; compute_image_admission_controller_enabled; compute_image_provenance_webhook_enabled; compute_image_policy_webhook_configured; compute_image_provenance_configured; compute_image_admission_controller_configured; compute_image_provenance_webhook_configured,• Level 2 - Master Node,You need to regularly maintain your provenance configuration based on container image updates.,1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888
5.6.1,Create administrative boundaries between resources using namespaces,Manual,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with 4 initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-node-lease - Namespace used for node heartbeats 4. kube-public - Namespace used for public information in a cluster References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_enforced; kubernetes_namespace_resource_separation_required; kubernetes_namespace_default_deny_policy; kubernetes_namespace_network_policies_enforced; kubernetes_namespace_rbac_restrictions_applied; kubernetes_namespace_resource_quotas_set; kubernetes_namespace_pod_security_policies_enabled,• Level 1 - Master Node,You need to switch between namespaces for administration.,1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats
5.6.2,Ensure that the seccomp profile is set to docker/default in your pod definitions,Manual,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",Review the pod definitions in your cluster. It should create a line as below: securityContext: seccompProfile: type: RuntimeDefault,"Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_docker_default; kubernetes_pod_seccomp_profile_enabled; kubernetes_pod_seccomp_profile_configured; kubernetes_pod_security_profile_docker_default; kubernetes_pod_security_seccomp_enabled,• Level 2 - Master Node,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/
5.6.3,Apply Security Context to Your Pods and Containers,Manual,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_configured; kubernetes_container_security_context_configured; kubernetes_pod_privileged_mode_disabled; kubernetes_container_privileged_mode_disabled; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_capabilities_dropped; kubernetes_container_capabilities_dropped; kubernetes_pod_seccomp_profile_configured; kubernetes_container_seccomp_profile_configured; kubernetes_pod_seccomp_profile_minimal; kubernetes_container_seccomp_profile_minimal; kubernetes_pod_seccomp_profile_runtime_default; kubernetes_container_seccomp_profile_runtime_default,• Level 2 - Master Node,"If you incorrectly apply security contexts, you may have trouble running the pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
5.6.4,The default namespace should not be used,Manual,"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None","Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service","Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used",kubernetes_namespace_default_not_used; kubernetes_namespace_default_avoided; kubernetes_namespace_non_default_required; kubernetes_namespace_explicitly_defined; kubernetes_namespace_default_restricted,• Level 2 - Master Node,None,
1.1.1,Ensure that the API server pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the API server pod specification file has permissions of 600 or more restrictive.,The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_api_server_pod_spec_file_permissions_restrictive; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_permissions_strict; kubernetes_api_server_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.2,Ensure that the API server pod specification file ownership is set to root:root,Automated,Ensure that the API server pod specification file ownership is set to root:root.,The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_file_ownership_root_root; kubernetes_api_server_pod_spec_file_ownership_root; kubernetes_pod_spec_file_ownership_root_root; kubernetes_api_server_file_ownership_root; kubernetes_api_pod_spec_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.3,Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the controller manager pod specification file has permissions of 600 or more restrictive.,The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, the kube-controller-manager.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_controller_manager_pod_spec_file_permissions_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_600_or_stricter; kubernetes_controller_manager_pod_spec_file_permissions_secure; kubernetes_controller_manager_pod_spec_file_permissions_min_600; kubernetes_controller_manager_pod_spec_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.4,Ensure that the controller manager pod specification file ownership is set to root:root,Automated,Ensure that the controller manager pod specification file ownership is set to root:root.,The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, kube-controller-manager.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_pod_spec_root_owner; kubernetes_controller_manager_file_permissions_root; kubernetes_controller_manager_spec_ownership_root_root; kubernetes_controller_manager_pod_file_root_owned,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager
1.1.5,Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler pod specification file has permissions of 600 or more restrictive.,The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_scheduler_pod_spec_file_permissions_restrictive; kubernetes_scheduler_pod_spec_file_permissions_secure; kubernetes_scheduler_pod_spec_file_permissions_strict; kubernetes_scheduler_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.6,Ensure that the scheduler pod specification file ownership is set to root:root,Automated,Ensure that the scheduler pod specification file ownership is set to root:root.,The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_file_ownership_root; kubernetes_scheduler_pod_spec_root_owned; kubernetes_scheduler_file_ownership_root_root; kubernetes_pod_spec_ownership_root_root; kubernetes_scheduler_spec_file_root_owned,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.7,Ensure that the etcd pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file has permissions of 600 or more restrictive.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/etcd.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_pod_file_permissions_600_or_stricter; kubernetes_etcd_manifest_file_permissions_restricted; kubernetes_etcd_yaml_file_permissions_secure; kubernetes_manifest_etcd_file_permissions_600; kubernetes_etcd_config_file_permissions_compliant,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.8,Ensure that the etcd pod specification file ownership is set to root:root,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_manifest_file_ownership_root; kubernetes_etcd_pod_spec_ownership_root; kubernetes_manifest_file_ownership_root; kubernetes_etcd_config_file_ownership_root; kubernetes_etcd_yaml_file_ownership_root,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.9,Ensure that the Container Network Interface file permissions are set to 600 or more restrictive,Manual,Ensure that the Container Network Interface files have permissions of 600 or more restrictive.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a <path/to/cni/files> Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",compute_cni_file_permissions_restrictive; compute_cni_file_permissions_600_or_stricter; compute_cni_config_file_permissions_restrictive; compute_cni_config_file_permissions_600_or_stricter; container_cni_file_permissions_restrictive; container_cni_file_permissions_600_or_stricter,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.10,Ensure that the Container Network Interface file ownership is set to root:root,Manual,Ensure that the Container Network Interface files have ownership set to root:root.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G <path/to/cni/files> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_ownership_root_root; cni_file_ownership_root_root; container_network_interface_ownership_root_root; cni_config_ownership_root_root; container_network_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Automated,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd Default Value: By default, etcd data directory has permissions of 755. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_restrictive; etcd_data_directory_permissions_700_or_stricter; etcd_directory_permissions_restrictive; etcd_directory_permissions_700_or_stricter; etcd_data_directory_permissions_secure,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Automated,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd Default Value: By default, etcd data directory ownership is set to etcd:etcd. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_ownership_etcd_etcd; etcd_directory_permissions_etcd_etcd; etcd_data_directory_secure_ownership; etcd_directory_ownership_correct; etcd_data_directory_user_group_etcd,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.13,Ensure that the default administrative credential file permissions are set to 600,Automated,"Ensure that the admin.conf file (and super-admin.conf file, where it exists) have permissions of 600.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should restrict their file permissions to maintain the integrity and confidentiality of the file(s). The file(s) should be readable and writable by only the administrators on the system. Impact: None.","Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %a /etc/kubernetes/super-admin.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chmod 600 /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf have permissions of 600. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",kubernetes_admin_credential_file_permissions_600; kubernetes_admin_conf_file_permissions_600; kubernetes_super_admin_conf_file_permissions_600; kubernetes_credential_file_restrictive_permissions; kubernetes_admin_file_permissions_secure,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.14,Ensure that the default administrative credential file ownership is set to root:root,Automated,"Ensure that the admin.conf (and super-admin.conf file, where it exists) file ownership is set to root:root.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should set their file ownership to maintain the integrity and confidentiality of the file. The file(s) should be owned by root:root. Impact: None.","Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %U:%G /etc/kubernetes/super-admin.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chown root:root /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",compute_credential_file_ownership_root; compute_admin_conf_ownership_root; compute_super_admin_conf_ownership_root; compute_default_credential_file_ownership_root; compute_credential_file_ownership_root_root,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.15,Ensure that the scheduler.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler.conf file has permissions of 600 or more restrictive.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/scheduler.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",compute_scheduler_conf_file_permissions_600_or_stricter; compute_scheduler_conf_file_permissions_restrictive; compute_scheduler_conf_file_permissions_secure; compute_scheduler_conf_file_permissions_compliant; compute_scheduler_conf_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.16,Ensure that the scheduler.conf file ownership is set to root:root,Automated,Ensure that the scheduler.conf file ownership is set to root:root.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/scheduler.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",compute_scheduler_conf_file_ownership_root_root; compute_scheduler_conf_file_ownership_secure; scheduler_conf_file_ownership_root_root; scheduler_conf_file_ownership_secure,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.17,Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the controller-manager.conf file has permissions of 600 or more restrictive.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/controller-manager.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_permissions_600_or_stricter; kubernetes_controller_manager_conf_file_permissions_restrictive; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_permissions_compliant; kubernetes_controller_manager_conf_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.18,Ensure that the controller-manager.conf file ownership is set to root:root,Automated,Ensure that the controller-manager.conf file ownership is set to root:root.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/controller-manager.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_ownership_root_root; kubernetes_controller_manager_conf_file_permissions_root_only; kubernetes_controller_manager_conf_file_secure_ownership; kubernetes_controller_manager_conf_file_root_ownership_enforced; kubernetes_controller_manager_conf_file_ownership_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.19,Ensure that the Kubernetes PKI directory and file ownership is set to root:root,Automated,Ensure that the Kubernetes PKI directory and file ownership is set to root:root.,Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/ Verify that the ownership of all files and directories in this hierarchy is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown -R root:root /etc/kubernetes/pki/ Default Value: By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained within it, are set to be owned by the root user. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_directory_ownership_root; kubernetes_pki_file_ownership_root; kubernetes_pki_directory_permissions_root; kubernetes_pki_file_permissions_root; kubernetes_pki_directory_root_owned; kubernetes_pki_file_root_owned,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.20,Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive,Manual,Ensure that Kubernetes PKI certificate files have permissions of 600 or more restrictive.,Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.crt Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.crt Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.crt Default Value: By default, the certificates used by Kubernetes are set to have permissions of 644 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_certificate_file_permissions_600_or_stricter; kubernetes_pki_certificate_file_permissions_restrictive; kubernetes_certificate_file_permissions_secure; kubernetes_pki_file_permissions_600_or_less; kubernetes_certificate_file_permissions_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.21,Ensure that the Kubernetes PKI key file permissions are set to 600,Manual,Ensure that Kubernetes PKI key files have permissions of 600.,Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.key Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.key Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.key Default Value: By default, the keys used by Kubernetes are set to have permissions of 600 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_key_file_permissions_600; kubernetes_pki_key_file_permissions_restricted; kubernetes_pki_key_file_permissions_secure; kubernetes_pki_key_file_permissions_strict; kubernetes_pki_key_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.1,Ensure that the --anonymous-auth argument is set to false,Manual,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Impact: Anonymous requests will be rejected.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--anonymous-auth' | grep -i false If the exit code is '1', then the control isn't present / failed","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --anonymous-auth=false Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_auth_enabled; kubernetes_api_server_no_anonymous_access; kubernetes_api_server_secure_auth_required; kubernetes_api_server_authentication_enabled,• Level 1 - Master Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests
1.2.2,Ensure that the --token-auth-file parameter is not set,Automated,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token- based authentication. Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist. Alternative Audit Method kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--token-auth-file' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the --token-auth- file=<filename> parameter. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_file_disabled; kubernetes_api_server_token_auth_file_not_set; kubernetes_api_server_token_authentication_disabled; kubernetes_api_server_token_auth_file_unused; kubernetes_api_server_token_auth_file_absent,• Level 1 - Master Node,You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.,1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.3,Ensure that the DenyServiceExternalIPs is set,Manual,This admission controller rejects all net-new usage of the Service field externalIPs.,"Most users do not need the ability to set the externalIPs field for a Service at all, and cluster admins should consider disabling this functionality by enabling the DenyServiceExternalIPs admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage. Impact: When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the `DenyServiceExternalIPs' argument exist as a string value in --enable- admission-plugins.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and append the Kubernetes API server flag --enable-admission- plugins with the DenyServiceExternalIPs plugin. Note, the Kubernetes API server flag -- enable-admission-plugins takes a comma-delimited list of admission control plugins to be enabled, even if they are in the list of plugins enabled by default. kube-apiserver --enable-admission-plugins=DenyServiceExternalIPs Default Value: By default, --enable-admission-plugins=DenyServiceExternalIP argument is not set, and the use of externalIPs is authorized. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_admission_controller_deny_service_external_ips_enabled; kubernetes_service_external_ips_denied; kubernetes_admission_controller_external_ips_restricted; kubernetes_service_external_ips_blocked; kubernetes_admission_controller_deny_external_ips_enabled,• Level 1 - Master Node,"When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.4,Ensure that the --kubelet-client-certificate and --kubelet- client-key arguments are set as appropriate,Automated,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-client-certificate' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below. --kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file> Default Value: By default, certificate-based kubelet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured; kubernetes_kubelet_tls_authentication_enabled; kubernetes_kubelet_secure_authentication_required,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.5,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Automated,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-certificate-Authority' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string> Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_secure; kubernetes_kubelet_certificate_authority_trusted; kubernetes_kubelet_certificate_authority_enforced,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.6,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not always authorize all requests.,"The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Impact: Only authorized requests will be served.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below. --authorization-mode=RBAC Default Value: By default, AlwaysAllow is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_authorization_mode_restricted; kubernetes_api_server_secure_authorization_mode; kubernetes_api_server_authorization_mode_compliant; kubernetes_api_server_authorization_mode_enforced,• Level 1 - Master Node,Only authorized requests will be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.2.7,Ensure that the --authorization-mode argument includes Node,Automated,Restrict kubelet nodes to reading only objects associated with them.,"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include Node.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC Default Value: By default, Node authorization is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_authorization_enabled; kubernetes_kubelet_authorization_mode_node; kubernetes_kubelet_node_restricted_access; kubernetes_kubelet_node_authorization_required; kubernetes_kubelet_node_scope_authorization,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security
1.2.8,Ensure that the --authorization-mode argument includes RBAC,Automated,Turn on Role Based Access Control.,"Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Impact: When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include RBAC.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes RBAC, for example: --authorization-mode=Node,RBAC Default Value: By default, RBAC authorization is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/",kubernetes_cluster_rbac_enabled; kubernetes_cluster_authorization_mode_rbac; kubernetes_cluster_rbac_required; kubernetes_cluster_auth_mode_rbac_included; kubernetes_cluster_rbac_authorization_enabled,• Level 1 - Master Node,"When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
1.2.9,Ensure that the admission control plugin EventRateLimit is set,Manual,Limit the rate at which the API server accepts requests.,"Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Impact: You need to carefully tune in limits as per your environment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit.,"Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=<path/to/configuration/file> Default Value: By default, EventRateLimit is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md",kubernetes_api_server_event_rate_limit_enabled; kubernetes_admission_control_event_rate_limit_configured; kubernetes_api_request_rate_limiting_active; kubernetes_event_rate_limit_plugin_enabled; kubernetes_admission_plugin_event_rate_limit_set,• Level 1 - Master Node,You need to carefully tune in limits as per your environment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md
1.2.10,Ensure that the admission control plugin AlwaysAdmit is not set,Automated,Do not allow all requests.,Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Impact: Only requests explicitly allowed by the admissions control plugins would be served.,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --enable- admission-plugins parameter, or set it to a value that does not include AlwaysAdmit. Default Value: AlwaysAdmit is not in the list of default admission plugins. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit",kubernetes_admission_controller_always_admit_disabled; kubernetes_admission_plugin_always_admit_not_set; kubernetes_admission_policy_always_admit_restricted; kubernetes_admission_control_always_admit_denied; kubernetes_admission_rule_always_admit_blocked,• Level 1 - Master Node,Only requests explicitly allowed by the admissions control plugins would be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit
1.2.11,Ensure that the admission control plugin AlwaysPullImages is set,Manual,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission- plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages",kubernetes_admission_controller_always_pull_images_enabled; kubernetes_pod_image_always_pull_enabled; kubernetes_admission_plugin_always_pull_images_configured; kubernetes_workload_image_always_pull_required; kubernetes_admission_policy_always_pull_images_enforced,• Level 1 - Master Node,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages
1.2.12,Ensure that the admission control plugin ServiceAccount is set,Automated,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Impact: None.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and ensure that the --disable-admission-plugins parameter is set to a value that does not include ServiceAccount. Default Value: By default, ServiceAccount is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_service_account_admission_plugin_enabled; kubernetes_admission_controller_service_account_required; kubernetes_service_account_auto_creation_disabled; kubernetes_admission_plugin_service_account_enforced; kubernetes_service_account_automation_enabled,• Level 2 - Master Node,None.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.2.13,Ensure that the admission control plugin NamespaceLifecycle is set,Automated,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --disable-admission- plugins parameter to ensure it does not include NamespaceLifecycle. Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle",kubernetes_namespace_admission_control_enabled; kubernetes_namespace_lifecycle_protected; kubernetes_admission_plugin_namespace_lifecycle_set; kubernetes_namespace_termination_protected; kubernetes_admission_control_namespace_lifecycle_enabled,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle
1.2.14,Ensure that the admission control plugin NodeRestriction is set,Automated,Limit the Node and Pod objects that a kubelet could modify.,"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes NodeRestriction.,"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to a value that includes NodeRestriction. --enable-admission-plugins=...,NodeRestriction,... Default Value: By default, NodeRestriction is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_restriction_enabled; kubernetes_admission_node_restriction_active; kubernetes_node_restriction_plugin_enabled; kubernetes_kubelet_pod_modification_restricted; kubernetes_admission_node_restriction_enforced,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security
1.2.15,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_cluster_profiling_disabled; kubernetes_cluster_no_profiling_enabled; kubernetes_cluster_profiling_argument_false; kubernetes_cluster_profiling_setting_disabled; kubernetes_cluster_profiling_feature_off,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.2.16,Ensure that the --audit-log-path argument is set,Automated,Enable auditing on the Kubernetes API Server and set the desired audit log path.,"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_path_set; kubernetes_api_server_audit_logging_enabled; kubernetes_api_server_audit_log_path_configured; kubernetes_api_server_audit_log_path_valid; kubernetes_api_server_audit_log_path_specified,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.17,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Automated,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxage_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_set; kubernetes_audit_log_retention_30d; kubernetes_api_server_audit_log_duration_configured,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.18,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Automated,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_10_or_more; kubernetes_audit_log_retention_configured; kubernetes_audit_log_maxbackup_compliant; kubernetes_api_server_audit_log_retention_valid,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.19,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Automated,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_audit_log_rotation_size_configured; kubernetes_audit_log_maxsize_within_limit; kubernetes_api_server_audit_log_size_appropriate,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.20,Ensure that the --request-timeout argument is set as appropriate,Manual,Set global request timeout for API server requests as appropriate.,"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --request-timeout argument is either not set or set to an appropriate value.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s Default Value: By default, --request-timeout is set to 60 seconds. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415",kubernetes_api_server_request_timeout_set; kubernetes_api_server_request_timeout_configured; kubernetes_api_server_request_timeout_within_limits; kubernetes_api_server_request_timeout_appropriate; kubernetes_api_server_request_timeout_optimized,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415
1.2.21,Ensure that the --service-account-lookup argument is set to true,Automated,Validate service account before validating token.,"If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --service-account-lookup argument exists it is set to true.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --service-account-lookup=true Alternatively, you can delete the --service-account-lookup parameter from this file so that the default takes effect. Default Value: By default, --service-account-lookup argument is set to true. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_service_account_pre_validation_enabled; kubernetes_api_server_service_account_token_validation_enabled; kubernetes_api_server_service_account_lookup_before_validation_enabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use
1.2.22,Ensure that the --service-account-key-file argument is set as appropriate,Automated,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file. Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --service-account-key- file parameter to the public key file for service accounts: --service-account-key-file=<filename> Default Value: By default, --service-account-key-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_explicitly_set,• Level 1 - Master Node,The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.2.23,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters. --etcd-certfile=<path/to/client-certificate-file> --etcd-keyfile=<path/to/client-key-file> Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_certfile_configured; kubernetes_etcd_keyfile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_connection_required,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.24,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Automated,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the TLS certificate and private key file parameters. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Default Value: By default, --tls-cert-file and --tls-private-key-file are presented and created for use. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_configured; kubernetes_api_server_tls_cert_key_pair_valid; kubernetes_api_server_tls_encryption_enabled,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.25,Ensure that the --client-ca-file argument is set as appropriate,Automated,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the client certificate authority file. --client-ca-file=<path/to/client-ca-file> Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_configured; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_certificate_validation_enabled; kubernetes_api_server_secure_connection_required; kubernetes_api_server_ca_file_specified,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.26,Ensure that the --etcd-cafile argument is set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter. --etcd-cafile=<path/to/ca-file> Default Value: By default, --etcd-cafile is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_cafile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_connection_required; kubernetes_etcd_ca_certificate_valid,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.27,Ensure that the --encryption-provider-config argument is set as appropriate,Manual,Encrypt etcd key-value store.,etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Impact: None,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --encryption-provider-config argument is set to a EncryptionConfig file. Additionally, ensure that the EncryptionConfig file has all the desired resources covered especially any secrets.","Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=</path/to/EncryptionConfig/File> Default Value: By default, --encryption-provider-config is not set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92",etcd_store_encryption_enabled; etcd_store_encryption_provider_configured; etcd_store_encryption_config_valid; etcd_store_encryption_key_secure; etcd_store_encryption_provider_appropriate,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92
1.2.28,Ensure that encryption providers are appropriately configured,Manual,"Where etcd encryption is used, appropriate providers should be configured.","Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms and secretbox are likely to be appropriate options. Impact: None","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Get the EncryptionConfig file set for --encryption-provider-config argument. Verify that aescbc, kms or secretbox is set as the encryption provider for all the desired resources.","Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms or secretbox as the encryption provider. Default Value: By default, no encryption provider is set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers",compute_etcd_encryption_providers_configured; compute_etcd_encryption_providers_valid; compute_etcd_encryption_providers_secure; compute_etcd_encryption_providers_approved; compute_etcd_encryption_providers_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers
1.2.29,Ensure that the API Server only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the API server is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter. --tls-cipher-suites=TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256. Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: Insecure values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_RC4_128_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_RC4_128_SHA.",kubernetes_api_server_strong_ciphers_enabled; kubernetes_api_server_weak_ciphers_disabled; kubernetes_api_server_tls_min_version_enforced; kubernetes_api_server_cipher_suite_restricted; kubernetes_api_server_insecure_ciphers_removed,• Level 1 - Master Node,API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.,"1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: Insecure values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_RC4_128_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_RC4_128_SHA."
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Manual,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --terminated- pod-gc-threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_pod_garbage_collector_enabled; kubernetes_pod_termination_threshold_set; kubernetes_pod_gc_threshold_configured; kubernetes_pod_termination_cleanup_enabled; kubernetes_pod_garbage_collection_threshold_set,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; kubernetes_cluster_profiling_disabled; container_service_profiling_disabled; k8s_cluster_profiling_disabled; managed_kubernetes_profiling_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.3.3,Ensure that the --use-service-account-credentials argument is set to true,Automated,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service- account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node to set the below parameter. --use-service-account-credentials=true Default Value: By default, --use-service-account-credentials is set to false. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",kubernetes_controller_use_service_account_credentials; kubernetes_controller_service_account_credentials_enabled; kubernetes_controller_credentials_service_account_required; kubernetes_controller_service_account_auth_enabled; kubernetes_controller_credentials_individual_service_account,• Level 1 - Master Node,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.4,Ensure that the --service-account-private-key-file argument is set as appropriate,Automated,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private- key-file as appropriate. Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --service- account-private-key-file parameter to the private key file for service accounts. --service-account-private-key-file=<filename> Default Value: By default, --service-account-private-key-file it not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_specified; kubernetes_controller_manager_service_account_private_key_file_valid; kubernetes_controller_manager_service_account_private_key_file_provided,• Level 1 - Master Node,You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.5,Ensure that the --root-ca-file argument is set as appropriate,Automated,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Impact: You need to setup and maintain root certificate authority file.,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`. --root-ca-file=<path/to/file> Default Value: By default, --root-ca-file is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_api_server_root_ca_file_configured; kubernetes_api_server_root_ca_file_valid; kubernetes_api_server_root_ca_file_secure; kubernetes_api_server_root_ca_file_present; kubernetes_api_server_root_ca_file_trusted,• Level 1 - Master Node,You need to setup and maintain root certificate authority file.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.3.6,Ensure that the RotateKubeletServerCertificate argument is set to true,Automated,Enable kubelet server certificate rotation on controller-manager.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true. --feature-gates=RotateKubeletServerCertificate=true Default Value: By default, RotateKubeletServerCertificate is set to 'true' this recommendation verifies that it has not been disabled. References: 1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_rotate_kubelet_server_certificate_enabled; kubernetes_controller_manager_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubernetes_controller_manager_tls_certificate_rotation_enabled; kubernetes_kubelet_tls_certificate_rotation_enabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.7,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the Controller Manager service to non-loopback insecure addresses.,"The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address",controller_manager_bind_address_localhost; controller_manager_network_bind_restricted; controller_manager_loopback_only_enabled; controller_manager_insecure_bind_disabled; controller_manager_localhost_bind_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address
1.4.1,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml file on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_cluster_profiling_disabled; kubernetes_cluster_profiling_set_false; kubernetes_api_profiling_disabled; kubernetes_api_profiling_set_false; kubernetes_control_plane_profiling_disabled; kubernetes_control_plane_profiling_set_false,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.4.2,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the scheduler service to non-loopback insecure addresses.,"The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml on the Control Plane node and ensure the correct value for the -- bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/",kubernetes_scheduler_bind_address_localhost; kubernetes_scheduler_loopback_only; kubernetes_scheduler_network_bind_restricted; kubernetes_scheduler_insecure_bind_disabled; kubernetes_scheduler_localhost_bind_enabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/
2.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_tls_cert_key_valid; etcd_service_tls_cert_key_secure,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.2,Ensure that the --client-cert-auth argument is set to true,Automated,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: All clients attempting to access the etcd server will require a valid client certificate.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --client-cert-auth='true' Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_auth_enabled; etcd_client_cert_auth_required; etcd_tls_client_authentication_enabled; etcd_client_certificate_authentication_enabled; etcd_client_authentication_required,• Level 1 - Master Node,All clients attempting to access the etcd server will require a valid client certificate.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
2.3,Ensure that the --auto-tls argument is not set to true,Automated,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: Clients will not be able to use self-signed certificates for TLS.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --auto-tls parameter or set it to false. --auto-tls=false Default Value: By default, --auto-tls is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_certificate_self_signed_disallowed; cloud_cdn_tls_auto_renewal_disabled; cloud_cdn_tls_custom_certificate_required; cloud_cdn_tls_managed_certificate_disabled,• Level 1 - Master Node,Clients will not be able to use self-signed certificates for TLS.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
2.4,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Impact: etcd cluster peers would need to set up TLS for their communication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --peer-client-file=</path/to/peer-cert-file> --peer-key-file=</path/to/peer-key-file> Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_peer_cert_file_set; etcd_peer_key_file_set; etcd_peer_tls_encryption_enabled; etcd_peer_cert_file_configured; etcd_peer_key_file_configured,• Level 1 - Master Node,etcd cluster peers would need to set up TLS for their communication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.5,Ensure that the --peer-client-cert-auth argument is set to true,Automated,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --peer-client-cert-auth=true Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth",etcd_cluster_peer_client_cert_auth_enabled; etcd_peer_authentication_enabled; etcd_peer_cert_auth_required; etcd_peer_tls_auth_enabled; etcd_peer_client_cert_auth_true,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth
2.6,Ensure that the --peer-auto-tls argument is not set to true,Automated,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --peer-auto-tls parameter or set it to false. --peer-auto-tls=false Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",kubernetes_cluster_peer_auto_tls_disabled; kubernetes_cluster_tls_auto_cert_disabled; kubernetes_peer_connection_auto_tls_disabled; kubernetes_peer_tls_self_signed_disabled; kubernetes_cluster_peer_tls_manual_cert_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
2.7,Ensure that a unique Certificate Authority is used for etcd,Manual,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster. Run the following command on the master node: ps -ef | grep etcd Note the file referenced by the --trusted-ca-file argument. Run the following command on the master node: ps -ef | grep apiserver Verify that the file referenced by the --client-ca-file for apiserver is different from the --trusted-ca-file used by etcd.,"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --trusted-ca-file=</path/to/ca-file> Default Value: By default, no etcd certificate is created and used. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html",etcd_certificate_authority_unique; kubernetes_etcd_ca_separation_required; etcd_ca_distinct_from_kubernetes; etcd_tls_certificate_authority_unique; kubernetes_etcd_ca_isolation_enforced,• Level 2 - Master Node,Additional management of the certificates and keys for the dedicated certificate authority will be required.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
3.1.1,Client certificate authentication should not be used for users,Manual,"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication.,"Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. Default Value: Client certificate authentication is enabled by default. Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",kubernetes_user_no_client_cert_auth; kubernetes_user_client_cert_auth_disabled; kubernetes_auth_no_user_client_cert; kubernetes_auth_user_client_cert_revocation; kubernetes_user_auth_no_client_cert,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.2,Service account token authentication should not be used for users,Manual,"Kubernetes provides service account tokens which are intended for use by workloads running in the Kubernetes cluster, for authentication to the API server. These tokens are not designed for use by end-users and do not provide for features such as revocation or expiry, making them insecure. A newer version of the feature (Bound service account token volumes) does introduce expiry but still does not allow for specific revocation.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Service account token authentication does not allow for this due to the use of JWT tokens as an underlying technology. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of service account token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of service account tokens. Default Value: Service account token authentication is enabled by default.,kubernetes_service_account_token_authentication_disabled; kubernetes_service_account_user_authentication_disabled; kubernetes_service_account_token_revocation_enabled; kubernetes_service_account_token_expiry_enabled; kubernetes_service_account_bound_token_volumes_enabled,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.3,Bootstrap token authentication should not be used for users,Manual,Kubernetes provides bootstrap tokens which are intended for use by new nodes joining the cluster These tokens are not designed for use by end-users they are specifically designed for the purpose of bootstrapping new nodes and not for general authentication,Bootstrap tokens are not intended for use as a general authentication mechanism and impose constraints on user and group naming that do not facilitate good RBAC design. They also cannot be used with MFA resulting in a weak authentication mechanism being available. Impact: External mechanisms for authentication generally require additional software to be deployed.,Review user access to the cluster and ensure that users are not making use of bootstrap token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of bootstrap tokens. Default Value: Bootstrap token authentication is not enabled by default and requires an API server parameter to be set.,kubernetes_user_no_bootstrap_token_auth; kubernetes_user_no_bootstrap_token_auth_enabled; kubernetes_auth_no_bootstrap_token_for_users; kubernetes_auth_no_user_bootstrap_token; kubernetes_token_no_bootstrap_for_users,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.2.1,Ensure that a minimal audit policy is created,Manual,Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.,"Logging is an important detective control for all systems, to detect potential unauthorised access. Impact: Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",Run the following command on one of the cluster master nodes: ps -ef | grep kube-apiserver Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy.,"Create an audit policy file for your cluster. Default Value: Unless the --audit-policy-file flag is specified, no auditing will be carried out. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/",kubernetes_api_audit_policy_created; kubernetes_api_audit_policy_minimal; kubernetes_api_audit_logging_enabled; kubernetes_api_audit_policy_file_set; kubernetes_api_audit_policy_configured,• Level 1 - Master Node,"Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
3.2.2,Ensure that the audit policy covers key security concerns,Manual,Ensure that the audit policy created for the cluster covers key security concerns.,"Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Impact: Increasing audit logging will consume resources on the nodes or other log destination.","Review the audit policy provided for the cluster and ensure that it covers at least the following areas :- • Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data. • Modification of pod and deployment objects. • Use of pods/exec, pods/portforward, pods/proxy and services/proxy. For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","Consider modification of the audit policy in use on the cluster to include these items, at a minimum. Default Value: By default Kubernetes clusters do not log audit information. References: 1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735",kubernetes_cluster_audit_policy_key_security_covered; kubernetes_cluster_audit_policy_security_concerns_included; kubernetes_audit_policy_security_requirements_met; kubernetes_audit_policy_key_security_configured; kubernetes_cluster_audit_policy_comprehensive_security,• Level 2 - Master Node,Increasing audit logging will consume resources on the nodes or other log destination.,1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735
4.1.1,Ensure that the kubelet service file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet service file has permissions of 600 or more restrictive.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, the kubelet service file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_permissions_600_or_stricter; kubernetes_kubelet_service_file_permissions_restrictive; kubernetes_kubelet_service_file_permissions_secure; kubernetes_kubelet_service_file_permissions_min_600; kubernetes_kubelet_service_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.2,Ensure that the kubelet service file ownership is set to root:root,Automated,Ensure that the kubelet service file ownership is set to root:root.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, kubelet service file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_ownership_root; kubernetes_kubelet_service_file_permissions_root; kubernetes_kubelet_service_file_ownership_root_root; kubernetes_kubelet_service_file_secure_ownership; kubernetes_kubelet_service_file_root_ownership_enforced,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.3,If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive,Manual,"If kube-proxy is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of 600 or more restrictive.","The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None","Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a <path><filename> Verify that a file is specified and it exists with permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 <proxy kubeconfig file> Default Value: By default, proxy file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_permissions_600_or_stricter; kubernetes_proxy_kubeconfig_file_restrictive_permissions; kubernetes_proxy_kubeconfig_file_mode_600; kubernetes_proxy_kubeconfig_file_permissions_restricted; kubernetes_proxy_kubeconfig_file_access_600_or_less,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.4,If proxy kubeconfig file exists ensure ownership is set to root:root,Manual,"If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G <path><filename> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_root_ownership; kubernetes_proxy_kubeconfig_file_secure_ownership; kubernetes_kubeconfig_root_user_group; kubernetes_proxy_config_file_root_owned; kubernetes_kubeconfig_ownership_restricted,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.5,Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet.conf file has permissions of 600 or more restrictive.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file has permissions of 600. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_permissions_restrictive; kubernetes_kubeconfig_file_permissions_600_or_stricter; kubernetes_kubelet_conf_file_permissions_restrictive; kubernetes_kubelet_conf_file_permissions_600_or_stricter; kubernetes_kubeconfig_kubelet_conf_file_permissions_restrictive,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.6,Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root,Automated,Ensure that the kubelet.conf file ownership is set to root:root.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_root_ownership; kubernetes_kubelet_conf_root_ownership; kubelet_config_file_root_ownership; kubeconfig_file_root_ownership; kubernetes_kubelet_conf_secure_ownership,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.7,Ensure that the certificate authorities file permissions are set to 600 or more restrictive,Manual,Ensure that the certificate authorities file has permissions of 600 or more restrictive.,The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %a <filename> Verify that the permissions are 644 or more restrictive.,Run the following command to modify the file permissions of the --client-ca-file chmod 600 <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_certificate_authorities_file_permissions_600_or_more_restrictive; compute_ca_file_permissions_600_or_more_restrictive; compute_ssl_certificate_authorities_file_permissions_secure; compute_ca_file_permissions_restrictive; compute_certificate_authorities_file_permissions_compliant,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.8,Ensure that the client certificate authorities file ownership is set to root:root,Manual,Ensure that the certificate authorities file ownership is set to root:root.,The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %U:%G <filename> Verify that the ownership is set to root:root.,Run the following command to modify the ownership of the --client-ca-file. chown root:root <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_ssl_certificate_authority_file_ownership_root; compute_client_certificate_authority_file_ownership_root; compute_certificate_authority_file_ownership_root_root; compute_ca_file_ownership_root_root; compute_client_ca_file_ownership_root_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.9,If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 600 or more restrictive.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml Verify that the permissions are 600 or more restrictive.","Run the following command (using the config file location identied in the Audit step) chmod 600 /var/lib/kubelet/config.yaml Default Value: By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of 600. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_permissions_600_or_stricter; kubernetes_kubelet_config_file_restrictive_permissions; kubernetes_config_file_permissions_secure; kubelet_config_file_permissions_restricted; kubernetes_kubelet_config_file_permissions_compliant,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.1.10,If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %aU %G /var/lib/kubelet/config.yaml ```Verify that the ownership is set to `root:root`.","Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by root:root. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_owner_root; kubernetes_kubelet_config_file_group_root; kubernetes_kubelet_config_file_ownership_root_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.2.1,Ensure that the --anonymous-auth argument is set to false,Automated,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.","If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false. This executable argument may be omitted, provided there is a corresponding entry set to false in the Kubelet config file.","If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --anonymous-auth=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_anonymous_auth_set_false; kubernetes_kubelet_auth_no_anonymous; kubernetes_kubelet_secure_auth_enabled; kubernetes_kubelet_anonymous_requests_blocked,• Level 1 - Worker Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.2,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.","Run the following command on each node: ps -ef | grep kubelet If the --authorization-mode argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by -- config, and that file sets authorization: mode to something other than AlwaysAllow. It is also possible to review the running configuration of a Kubelet via the /configz endpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --authorization-mode=Webhook Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",eks_cluster_authorization_mode_not_always_allow; eks_cluster_explicit_authorization_enabled; eks_cluster_authorization_restricted; eks_cluster_always_allow_disabled,• Level 1 - Worker Node,Unauthorized requests will be denied.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.3,Ensure that the --client-ca-file argument is set as appropriate,Manual,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. If the --client-ca-file argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets authentication: x509: clientCAFile to the location of the client certificate authority file.","If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to the location of the client CA file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --client-ca-file=<path/to/client-ca-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/",kubernetes_kubelet_client_ca_file_set; kubernetes_kubelet_authentication_certificates_enabled; kubernetes_kubelet_client_ca_file_configured; kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_ca_file_valid,• Level 1 - Worker Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/
4.2.4,Verify that the --read-only-port argument is set to 0,Manual,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,"Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.","If using a Kubelet config file, edit the file to set readOnlyPort to 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --read-only-port=0 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --read-only-port is set to 10255/TCP. However, if a config file is specified by --config the default value for readOnlyPort is 0. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_api_server_read_only_port_disabled; kubernetes_api_server_read_only_port_zero; kubernetes_api_server_read_only_port_secure; kubernetes_api_server_read_only_port_unset; kubernetes_api_server_read_only_port_restricted,• Level 1 - Worker Node,Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,1. https://kubernetes.io/docs/admin/kubelet/
4.2.5,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Manual,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.","Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0. If the argument is not present, and there is a Kubelet config file specified by --config, check that it does not set streamingConnectionIdleTimeout to 0.","If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a value other than 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --streaming-connection-idle-timeout=5m Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",kubernetes_api_server_streaming_connection_timeout_enabled; kubernetes_api_server_streaming_connection_idle_timeout_configured; kubernetes_api_server_streaming_connection_timeout_not_disabled; kubernetes_api_server_streaming_connection_idle_timeout_non_zero; kubernetes_api_server_streaming_connection_timeout_valid,• Level 1 - Worker Node,Long-lived connections could be interrupted.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
4.2.6,Ensure that the --make-iptables-util-chains argument is set to true,Automated,Allow Kubelet to manage iptables.,"Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.","Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.","If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove the --make- iptables-util-chains argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --make-iptables-util-chains argument is set to true. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_management_enabled; kubernetes_kubelet_iptables_chains_configured; kubernetes_kubelet_iptables_util_chains_set; kubernetes_kubelet_iptables_util_chains_true,• Level 1 - Worker Node,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",1. https://kubernetes.io/docs/admin/kubelet/
4.2.7,Ensure that the --hostname-override argument is not set,Manual,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Impact: Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist. Note This setting is not configurable via the Kubelet config file.,"Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10- kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_default; kubernetes_node_hostname_unchanged; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_preserved,• Level 1 - Worker Node,"Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063
4.2.8,Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture,Manual,"Security relevant information should be captured. The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,"Run the following command on each node: sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10- kubeadm.conf or If using command line arguments, kubelet service file is located /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10-kubelet- args.conf Review the value set for the argument and determine whether this has been set to an appropriate level for the cluster. If the argument does not exist, check that there is a Kubelet config file specified by -- config and review the value in this location. If using command line arguments","If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, eventRecordQPS argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go",kubernetes_kubelet_event_record_qps_set; kubernetes_kubelet_event_record_qps_appropriate; kubernetes_kubelet_event_capture_sufficient; kubernetes_kubelet_event_record_qps_configured; kubernetes_kubelet_event_record_qps_limited; kubernetes_kubelet_event_record_qps_not_unlimited; kubernetes_kubelet_event_record_qps_secure_level,• Level 2 - Worker Node,Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go
4.2.9,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Manual,Setup TLS connection on the Kubelets.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.","Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. If these arguments are not present, check that there is a Kubelet config specified by -- config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_connection_configured; kubernetes_kubelet_tls_cert_and_key_valid; kubernetes_kubelet_tls_encryption_enabled,• Level 1 - Worker Node,,
4.2.10,Ensure that the --rotate-certificates argument is not set to false,Automated,Enable kubelet client certificate rotation.,The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None,"Run the following command on each node: ps -ef | grep kubelet Verify that the RotateKubeletServerCertificate argument is not present, or is set to true. If the RotateKubeletServerCertificate argument is not present, verify that if there is a Kubelet config file specified by --config, that file does not contain RotateKubeletServerCertificate: false.","If using a Kubelet config file, edit the file to add the line rotateCertificates: true or remove it altogether to use the default value. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove --rotate- certificates=false argument from the KUBELET_CERTIFICATE_ARGS variable or set - -rotate-certificates=true . Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet client certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_rotate_certificates_not_false; kubernetes_kubelet_client_cert_rotation_required; kubernetes_kubelet_cert_rotation_compliant; kubernetes_kubelet_secure_certificate_rotation,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/
4.2.11,Verify that the RotateKubeletServerCertificate argument is set to true,Manual,Enable kubelet server certificate rotation.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate- server-certificates parameter is set on kubelet Run the following command on each node: ps -ef | grep kubelet Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable. --feature-gates=RotateKubeletServerCertificate=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet server certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubernetes_kubelet_rotate_server_certificate_enabled; kubernetes_kubelet_tls_certificate_rotation_enabled; kubernetes_kubelet_auto_rotate_certificate_enabled,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration
4.2.12,Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the Kubelet is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.","The set of cryptographic ciphers currently considered secure is the following: • TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 • TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_128_GCM_SHA256 Run the following command on each node: ps -ef | grep kubelet If the --tls-cipher-suites argument is present, ensure it only contains values included in this set. If it is not present check that there is a Kubelet config file specified by --config, and that file sets TLSCipherSuites: to only include values from this set.","If using a Kubelet config file, edit the file to set TLSCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_ 256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WI TH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES _128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the --tls-cipher- suites parameter as follows, or to a subset of these values. --tls-cipher- suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_kubelet_strong_ciphers_enabled; kubernetes_kubelet_weak_ciphers_disabled; kubernetes_kubelet_tls_ciphers_restricted; kubernetes_kubelet_cipher_suite_compliant; kubernetes_kubelet_secure_ciphers_enforced,• Level 1 - Worker Node,Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.,
4.2.13,Ensure that a limit is set on pod PIDs,Manual,Ensure that the Kubelet sets limits on the number of PIDs that can be created by pods running on the node.,"By default pods running in a cluster can consume any number of PIDs, potentially exhausting the resources available on the node. Setting an appropriate limit reduces the risk of a denial of service attack on cluster nodes. Impact: Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.","Review the Kubelet's start-up parameters for the value of --pod-max-pids, and check the Kubelet configuration file for the PodPidsLimit . If neither of these values is set, then there is no limit in place.","Decide on an appropriate level for this parameter and set it, either via the --pod-max- pids command line parameter or the PodPidsLimit configuration file setting. Default Value: By default the number of PIDs is not limited. References: 1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 4.3 kube-proxy Recommendations relating to the kube-proxy component.",kubernetes_pod_pid_limit_enabled; kubernetes_kubelet_pid_limit_configured; kubernetes_pod_pid_limit_set; kubernetes_node_pid_limit_enforced; kubernetes_kubelet_pid_limit_valid,• Level 1 - Worker Node,Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.,1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 4.3 kube-proxy Recommendations relating to the kube-proxy component.
4.3.1,Ensure that the kube-proxy metrics service is bound to localhost,Automated,Do not bind the kube-proxy metrics port to non-loopback addresses.,kube-proxy has two APIs which provided access to information about the service and can be bound to network ports. The metrics API service includes endpoints (/metrics and /configz) which disclose information about the configuration and operation of kube-proxy. These endpoints should not be exposed to untrusted networks as they do not support encryption or authentication to restrict access to the data they provide. Impact: 3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,review the start-up flags provided to kube proxy ps -ef | grep -i kube-proxy Ensure that the --metrics-bind-address parameter is not set to a value other than 127.0.0.1. From the output of this command gather the location specified in the -- config parameter. Review any file stored at that location and ensure that it does not specify a value other than 127.0.0.1 for metricsBindAddress.,Modify or remove any values which bind the metrics service to a non-localhost address Default Value: The default value is 127.0.0.1:10249 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/,kubernetes_proxy_metrics_localhost_bound; kubernetes_proxy_metrics_non_loopback_blocked; kubernetes_proxy_metrics_loopback_only; kubernetes_proxy_metrics_external_access_disabled; kubernetes_proxy_metrics_interface_restricted,• Level 1 - Worker Node,3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/
5.1.1,Ensure that the cluster-admin role is only used where required,Automated,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",kubernetes_role_no_cluster_admin; kubernetes_role_cluster_admin_restricted; kubernetes_role_cluster_admin_minimal_usage; kubernetes_role_cluster_admin_least_privilege; kubernetes_role_cluster_admin_required_only,• Level 1 - Master Node,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
5.1.2,Minimize access to secrets,Automated,"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation,"Review the users who have get, list or watch access to secrets objects in the Kubernetes API.","Where possible, remove get, list and watch access to secret objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have get privileges on secret objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:expand-controller expand-controller ServiceAccount kube-system system:controller:generic-garbage-collector generic-garbage- collector ServiceAccount kube-system system:controller:namespace-controller namespace-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:kube-controller-manager system:kube-controller- manager User",kubernetes_secret_access_restricted; kubernetes_secret_minimal_access; kubernetes_secret_no_public_access; kubernetes_secret_iam_restricted; kubernetes_secret_service_account_restricted; kubernetes_secret_workload_access_restricted; kubernetes_secret_no_anonymous_access; kubernetes_secret_rbac_restricted; kubernetes_secret_no_wildcard_access; kubernetes_secret_no_default_access,• Level 1 - Master Node,Care should be taken not to remove access to secrets to system components which require this for their operation,
5.1.3,Minimize wildcard use in Roles and ClusterRoles,Automated,Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard '*' which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.,The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.,Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml,Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.,kubernetes_role_no_wildcard_resources; kubernetes_clusterrole_no_wildcard_resources; kubernetes_role_no_wildcard_actions; kubernetes_clusterrole_no_wildcard_actions; kubernetes_role_no_wildcard_resources_or_actions; kubernetes_clusterrole_no_wildcard_resources_or_actions,• Level 1 - Worker Node,,
5.1.4,Minimize access to create pods,Automated,"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.","The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",Review the users who have create access to pod objects in the Kubernetes API.,"Where possible, remove create access to pod objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have create privileges on pod objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:daemon-set-controller daemon-set-controller ServiceAccount kube-system system:controller:job-controller job-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:controller:replicaset-controller replicaset-controller ServiceAccount kube-system system:controller:replication-controller replication-controller ServiceAccount kube-system system:controller:statefulset-controller statefulset-controller ServiceAccount kube-system",kubernetes_namespace_pod_creation_restricted; kubernetes_role_pod_creation_minimized; kubernetes_cluster_pod_creation_limited; kubernetes_service_account_pod_creation_restricted; kubernetes_rbac_pod_creation_minimized; kubernetes_policy_pod_creation_limited; kubernetes_user_pod_creation_restricted; kubernetes_group_pod_creation_minimized,• Level 1 - Master Node,Care should be taken not to remove access to pods to system components which require this for their operation,
5.1.5,Ensure that default service accounts are not actively used.,Automated,The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.,"Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.","For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/,iam_service_account_no_default_usage; iam_service_account_default_inactive; compute_service_account_no_default_usage; compute_service_account_default_unused; service_account_no_default_active_usage; service_account_default_no_active_roles; service_account_default_no_active_permissions,• Level 1 - Master Node,All workloads which require access to the Kubernetes API will require an explicit service account to be created.,1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.6,Ensure that Service Account Tokens are only mounted where necessary,Automated,Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server,"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.","Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false","Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_pod_service_account_token_not_mounted; kubernetes_pod_service_account_token_unnecessary_mount_disabled; kubernetes_pod_service_account_token_mount_restricted; kubernetes_pod_service_account_token_mount_required_only; kubernetes_pod_service_account_token_mount_minimized,• Level 1 - Master Node,"Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.7,Avoid use of system:masters group,Manual,"The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed. When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster. Impact: Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.,Remove the system:masters group from all users in the cluster. Default Value: By default some clusters will create a 'break glass' client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations. References: 1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38,iam_group_system_masters_unused; iam_group_system_masters_restricted; iam_group_system_masters_no_grants; iam_group_system_masters_minimal_access; iam_group_system_masters_bootstrap_only,• Level 1 - Master Node,"Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38
5.1.8,"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",Manual,"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster- admin level. Impact: There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.","Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.","Where possible, remove the impersonate, bind and escalate rights from subjects. Default Value: In a default kubeadm cluster, the system:masters group and clusterrole-aggregation- controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate. References: 1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/",kubernetes_role_no_privilege_escalation; kubernetes_cluster_role_no_impersonation; kubernetes_role_no_bind_permission; kubernetes_cluster_role_no_escalate_permission; kubernetes_role_no_privilege_escalation_permissions; kubernetes_cluster_role_no_impersonate_bind_escalate,• Level 1 - Master Node,"There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/
5.1.9,Minimize access to create persistent volumes,Manual,"The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of hostPath volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.","The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have create access to PersistentVolume objects in the Kubernetes API.,"Where possible, remove create access to PersistentVolume objects in the cluster. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation",kubernetes_persistent_volume_creation_restricted; kubernetes_persistent_volume_admin_access_minimized; kubernetes_persistent_volume_hostpath_creation_blocked; kubernetes_persistent_volume_privilege_escalation_prevented; kubernetes_persistent_volume_creation_roles_limited,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation
5.1.10,Minimize access to the proxy sub-resource of nodes,Manual,"Users with access to the Proxy sub-resource of Node objects automatically have permissions to use the Kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs. The Kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the node object. The proxy sub-resource specifically allows wide ranging access to the Kubelet API. Direct access to the Kubelet API bypasses controls like audit logging (there is no audit log of Kubelet API access) and admission control.","The ability to use the proxy sub-resource of node objects opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have access to the proxy sub-resource of node objects in the Kubernetes API.,"Where possible, remove access to the proxy sub-resource of node objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization",kubernetes_node_proxy_access_restricted; kubernetes_node_proxy_no_wildcard_permissions; kubernetes_node_proxy_minimal_roles; kubernetes_node_proxy_no_admin_access; kubernetes_node_proxy_api_disabled; kubernetes_node_proxy_audit_logging_enabled; kubernetes_node_proxy_privilege_escalation_prevented; kubernetes_node_proxy_kubelet_access_limited,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization
5.1.11,Minimize access to the approval sub-resource of certificatesigningrequests objects,Manual,"Users with access to the update the approval sub-resource of certificateaigningrequests objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts. This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster",The ability to update certificate signing requests should be limited.,Review the users who have access to update the approval sub-resource of certificatesigningrequests objects in the Kubernetes API.,"Where possible, remove access to the approval sub-resource of certificatesigningrequests objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing",kubernetes_certificatesigningrequest_approval_restricted; kubernetes_certificatesigningrequest_approval_no_admin_access; kubernetes_certificatesigningrequest_approval_minimal_access; kubernetes_certificatesigningrequest_approval_privilege_escalation_prevented; kubernetes_certificatesigningrequest_approval_high_privilege_restricted,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing
5.1.12,Minimize access to webhook configuration objects,Manual,"Users with rights to create/modify/delete validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.",The ability to manage webhook configuration should be limited,Review the users who have access to validatingwebhookconfigurations or mutatingwebhookconfigurations objects in the Kubernetes API.,"Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks",kubernetes_validatingwebhookconfiguration_minimize_access; kubernetes_mutatingwebhookconfiguration_minimize_access; kubernetes_webhookconfiguration_restrict_modify_access; kubernetes_webhookconfiguration_restrict_delete_access; kubernetes_webhookconfiguration_restrict_create_access; kubernetes_webhookconfiguration_admin_access_restricted; kubernetes_webhookconfiguration_privilege_escalation_prevented,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks
5.1.13,Minimize access to the service account token creation,Manual,"Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.",The ability to create service account tokens should be limited.,Review the users who have access to create the token sub-resource of serviceaccount objects in the Kubernetes API.,"Where possible, remove access to the token sub-resource of serviceaccount objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request",kubernetes_service_account_token_creation_restricted; kubernetes_service_account_token_creation_minimized; kubernetes_service_account_token_creation_limited; kubernetes_cluster_service_account_token_creation_restricted; kubernetes_cluster_service_account_token_creation_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request
5.2.1,Ensure that the cluster has at least one active policy control mechanism in place,Manual,"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts. Impact: Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.,"Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads. Default Value: By default, Pod Security Admission is enabled but no policies are in place. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission",kubernetes_cluster_policy_control_enabled; kubernetes_cluster_pod_security_admission_enabled; kubernetes_cluster_third_party_policy_control_enabled; kubernetes_cluster_active_policy_control_mechanism_exists; kubernetes_cluster_policy_enforcement_mechanism_active,• Level 1 - Master Node,"Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",1. https://kubernetes.io/docs/concepts/security/pod-security-admission
5.2.2,Minimize the admission of privileged containers,Manual,Do not generally permit containers to be run with the securityContext.privileged flag set to true.,"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.","Run the following command: get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}' It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection. calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node- c4xv4: {} {'privileged':true} {'privileged':true} {'privileged':true} {'privileged':true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {'seccompProfile':{'type':'RuntimeDefault'}} {'allowPrivilegeEscalation':false,'readOnlyRootFilesystem':true,'runAsGroup':2001,'ru nAsUser':1001}","Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privileged_disabled; compute_container_privileged_escalation_disabled; compute_container_security_context_restricted; compute_container_privileged_flag_false; compute_container_privileged_mode_disabled,• Level 1 - Master Node,"Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.3,Minimize the admission of containers wishing to share the host process ID namespace,Manual,Do not generally permit containers to be run with the hostPID flag set to true.,"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",Fetch hostPID from each pod with get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostPID}\n{end}',"Configure the Admission Controller to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_pid_disabled; compute_container_host_pid_restricted; compute_container_host_pid_not_shared; compute_container_host_pid_protected; compute_container_host_pid_isolated,• Level 1 - Master Node,Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.4,Minimize the admission of containers wishing to share the host IPC namespace,Manual,Do not generally permit containers to be run with the hostIPC flag set to true.,"A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",To fetch hostIPC from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostIPC}\n{end}',"Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ipc_disabled; compute_container_host_ipc_restricted; compute_container_host_ipc_not_shared; compute_container_host_ipc_denied; compute_container_host_ipc_protected,• Level 1 - Master Node,Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.5,Minimize the admission of containers wishing to share the host network namespace,Manual,Do not generally permit containers to be run with the hostNetwork flag set to true.,"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",To fetch hostNetwork from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostNetwork}\n{end}',"Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_network_disabled; compute_container_host_network_restricted; compute_container_host_network_prohibited; compute_container_host_network_denied; compute_container_host_network_not_shared,• Level 1 - Master Node,Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.6,Minimize the admission of containers with allowPrivilegeEscalation,Manual,"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext: allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation. To fetch hostNetwork from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}'","Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with securityContext: allowPrivilegeEscalation: true Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",container_pod_allow_privilege_escalation_disabled; container_pod_privilege_escalation_blocked; container_workload_privilege_escalation_restricted; container_runtime_privilege_escalation_denied; container_security_privilege_escalation_prohibited,• Level 1 - Master Node,Pods defined with securityContext: allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.7,Minimize the admission of root containers,Manual,Do not generally permit containers to be run as the root user.,"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one admission control policy defined which does not permit root containers. If you need to run root containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run as the root user will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0.","Create a policy for each namespace in the cluster, ensuring that either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0, is set. Default Value: By default, there are no restrictions on the use of root containers and if a User is not specified in the image, the container will run as root. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_root_user_disabled; compute_container_root_privileges_restricted; compute_container_non_root_user_required; compute_container_root_user_minimized; compute_container_root_access_denied,• Level 2 - Master Node,Pods with containers which run as the root user will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.8,Minimize the admission of containers with the NET_RAW capability,Manual,Do not generally permit containers with the potentially dangerous NET_RAW capability.,"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability. If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run with the NET_RAW capability will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the NET_RAW capability.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the NET_RAW capability. Default Value: By default, there are no restrictions on the creation of containers with the NET_RAW capability. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_net_raw_capability_disabled; compute_container_net_raw_capability_restricted; compute_container_net_raw_capability_denied; compute_container_net_raw_capability_minimized; compute_container_net_raw_capability_prohibited,• Level 1 - Master Node,Pods with containers which run with the NET_RAW capability will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.9,Minimize the admission of containers with added capabilities,Manual,Do not generally permit containers with capabilities assigned beyond the default set.,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which require capabilities outwith the default set will not be permitted.",Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}',"Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. Default Value: By default, there are no restrictions on adding capabilities to containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_minimized; compute_container_added_capabilities_restricted; compute_container_default_capabilities_only; compute_container_privileged_capabilities_disabled; compute_container_capabilities_whitelisted; compute_container_capabilities_baseline_enforced; compute_container_capabilities_non_default_denied,• Level 1 - Master Node,Pods with containers which require capabilities outwith the default set will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.10,Minimize the admission of containers with capabilities assigned,Manual,Do not generally permit containers with capabilities,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Impact: Pods with containers require capabilities to operate will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers.","Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities. Default Value: By default, there are no restrictions on the creation of containers with additional capabilities References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",container_capabilities_minimized; container_capabilities_restricted; container_capabilities_disabled; container_capabilities_no_privileged; container_capabilities_limited; container_capabilities_denied; container_capabilities_blocked; container_capabilities_unassigned; container_capabilities_secure_default; container_capabilities_compliance_standard,• Level 2 - Master Node,Pods with containers require capabilities to operate will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.11,Minimize the admission of Windows HostProcess Containers,Manual,Do not generally permit Windows containers to be run with the hostProcess flag set to true.,"A Windows container making use of the hostProcess flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides 'privileged access' to the Windows node. Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit hostProcess Windows containers. If you need to run Windows containers which require hostProcess, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostProcess containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostProcess containers. Default Value: By default, there are no restrictions on the creation of hostProcess containers. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_process_disabled; compute_container_host_process_restricted; compute_container_host_process_minimized; compute_container_windows_host_process_disabled; compute_container_windows_host_process_restricted,• Level 1 - Master Node,Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.12,Minimize the admission of HostPath volumes,Manual,Do not generally admit containers which make use of hostPath volumes.,"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem. There should be at least one admission control policy defined which does not permit containers to mount hostPath volumes. If you need to run containers which require hostPath volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with hostPath volumes.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPath volumes. Default Value: By default, there are no restrictions on the creation of hostPath volumes. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_hostpath_volumes_disabled; compute_container_hostpath_volumes_restricted; compute_container_hostpath_volumes_minimized; compute_container_hostpath_volumes_denied; compute_container_hostpath_volumes_blocked,• Level 1 - Master Node,Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.13,Minimize the admission of containers which use HostPorts,Manual,Do not generally permit containers which require the use of HostPorts.,"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts. If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have hostPort sections.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPort sections. Default Value: By default, there are no restrictions on the use of HostPorts. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI",compute_container_host_ports_restricted; compute_container_host_ports_disabled; compute_container_host_ports_minimized; compute_container_host_ports_denied; compute_container_host_ports_blocked,• Level 1 - Master Node,"Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI
5.3.1,Ensure that the CNI in use supports Network Policies,Manual,There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.,Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None,"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",kubernetes_cni_network_policies_supported; kubernetes_network_plugin_network_policies_enabled; kubernetes_cni_network_policy_compliance; kubernetes_network_plugin_policy_support_required; kubernetes_cni_network_policy_capability_verified,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.
5.3.2,Ensure that all Namespaces have Network Policies defined,Manual,Use network policies to isolate traffic in your cluster network.,"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces Ensure that each namespace defined in the cluster has at least one Network Policy.,"Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",kubernetes_namespace_network_policy_defined; kubernetes_namespace_network_policy_required; kubernetes_namespace_traffic_isolation_enabled; kubernetes_network_policy_namespace_coverage; kubernetes_namespace_network_policy_exists,• Level 2 - Master Node,"Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/
5.4.1,Prefer using secrets as files over secrets as environment variables,Manual,Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.,"It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {'\n'}{end}' -A,"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",kubernetes_secret_files_preferred; kubernetes_secret_no_environment_variables; kubernetes_secret_volume_mounted; kubernetes_secret_environment_avoided; kubernetes_secret_files_over_env_vars,• Level 2 - Master Node,Application code which expects to read secrets in the form of environment variables would need modification,1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod
5.4.2,Consider external secret storage,Manual,"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",Review your secrets management implementation.,"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured.",secrets_manager_external_storage_used; secrets_manager_authentication_required; secrets_manager_audit_logging_enabled; secrets_manager_encryption_enabled; secrets_manager_secret_rotation_enabled; secrets_manager_complex_needs_handled; secrets_manager_kubernetes_secrets_avoided,• Level 2 - Master Node,None,
5.5.1,Configure Image Provenance using ImagePolicyWebhook admission controller,Manual,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Impact: You need to regularly maintain your provenance configuration based on container image updates.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888",compute_image_provenance_enabled; compute_image_policy_webhook_enabled; compute_image_admission_controller_enabled; compute_image_provenance_webhook_enabled; compute_image_policy_webhook_configured; compute_image_provenance_configured; compute_image_admission_controller_configured; compute_image_policy_webhook_active; compute_image_provenance_active; compute_image_admission_controller_active,• Level 2 - Master Node,You need to regularly maintain your provenance configuration based on container image updates.,1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888
5.7.1,Create administrative boundaries between resources using namespaces,Manual,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with 4 initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-node-lease - Namespace used for node heartbeats 4. kube-public - Namespace used for public information in a cluster References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_enabled; kubernetes_namespace_resource_separation_enabled; kubernetes_namespace_security_boundaries_enabled; kubernetes_namespace_isolation_for_resources_enabled,• Level 1 - Master Node,You need to switch between namespaces for administration.,1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats
5.7.2,Ensure that the seccomp profile is set to docker/default in your pod definitions,Manual,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",Review the pod definitions in your cluster. It should create a line as below: securityContext: seccompProfile: type: RuntimeDefault,"Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_enabled; kubernetes_pod_seccomp_profile_docker_default; kubernetes_pod_security_seccomp_profile_set; kubernetes_pod_security_seccomp_profile_docker_default; kubernetes_pod_seccomp_profile_docker_default_required,• Level 2 - Master Node,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/
5.7.3,Apply Security Context to Your Pods and Containers,Manual,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_configured; kubernetes_container_security_context_configured; kubernetes_pod_privileged_mode_disabled; kubernetes_container_privileged_mode_disabled; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_capabilities_restricted; kubernetes_container_capabilities_restricted,• Level 2 - Master Node,"If you incorrectly apply security contexts, you may have trouble running the pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
5.7.4,The default namespace should not be used,Manual,"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None","Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service","Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used",,• Level 2 - Master Node,None,
1.1.1,Ensure that the API server pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the API server pod specification file has permissions of 600 or more restrictive.,The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_api_server_pod_spec_file_permissions_restrictive; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_permissions_strict; kubernetes_api_server_pod_spec_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.2,Ensure that the API server pod specification file ownership is set to root:root,Automated,Ensure that the API server pod specification file ownership is set to root:root.,The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_owner_root; kubernetes_api_server_pod_spec_file_group_root; kubernetes_api_server_pod_spec_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.3,Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the controller manager pod specification file has permissions of 600 or more restrictive.,The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, the kube-controller-manager.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_controller_manager_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_secure; kubernetes_controller_manager_pod_spec_file_permissions_strict; kubernetes_controller_manager_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.4,Ensure that the controller manager pod specification file ownership is set to root:root,Automated,Ensure that the controller manager pod specification file ownership is set to root:root.,The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, kube-controller-manager.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_pod_spec_root_owned; kubernetes_controller_manager_file_permissions_root; kubernetes_controller_manager_spec_ownership_root_root; kubernetes_controller_manager_pod_file_root_owner,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager
1.1.5,Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler pod specification file has permissions of 600 or more restrictive.,The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_scheduler_pod_spec_file_permissions_restrictive; kubernetes_scheduler_pod_spec_file_permissions_secure; kubernetes_scheduler_pod_spec_file_permissions_strict; kubernetes_scheduler_pod_spec_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.6,Ensure that the scheduler pod specification file ownership is set to root:root,Automated,Ensure that the scheduler pod specification file ownership is set to root:root.,The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_file_ownership_root; kubernetes_scheduler_pod_spec_file_root_owner; kubernetes_scheduler_pod_spec_file_root_root; kubernetes_scheduler_pod_file_permissions_root; kubernetes_scheduler_pod_spec_file_ownership_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.7,Ensure that the etcd pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file has permissions of 600 or more restrictive.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/etcd.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_pod_spec_file_permissions_600_or_stricter; kubernetes_etcd_manifest_file_permissions_restrictive; kubernetes_etcd_yaml_file_permissions_secure; kubernetes_manifest_etcd_file_permissions_600; kubernetes_etcd_spec_file_permissions_compliant,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.8,Ensure that the etcd pod specification file ownership is set to root:root,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_manifest_file_ownership_root; kubernetes_etcd_manifest_file_permissions_root; kubernetes_etcd_pod_spec_ownership_root; kubernetes_etcd_config_file_ownership_root; kubernetes_manifest_file_ownership_root; kubernetes_etcd_yaml_file_ownership_root; kubernetes_etcd_spec_file_ownership_root; kubernetes_etcd_manifest_file_root_owned,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.9,Ensure that the Container Network Interface file permissions are set to 600 or more restrictive,Manual,Ensure that the Container Network Interface files have permissions of 600 or more restrictive.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a <path/to/cni/files> Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_permissions_600_or_stricter; container_network_interface_file_permissions_restrictive; container_network_interface_file_permissions_secure; container_network_interface_file_permissions_compliant; container_network_interface_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.10,Ensure that the Container Network Interface file ownership is set to root:root,Manual,Ensure that the Container Network Interface files have ownership set to root:root.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G <path/to/cni/files> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_ownership_root; container_network_interface_file_group_ownership_root; container_network_interface_file_permissions_root_only; container_network_interface_file_ownership_correct; container_network_interface_file_secure_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Automated,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd Default Value: By default, etcd data directory has permissions of 755. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_700_or_more_restrictive; etcd_data_directory_permissions_restrictive; etcd_directory_permissions_secure; etcd_data_directory_permissions_compliant; etcd_directory_permissions_700_or_stricter,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Automated,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd Default Value: By default, etcd data directory ownership is set to etcd:etcd. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_owner_etcd; etcd_data_directory_group_etcd; etcd_data_directory_ownership_etcd_etcd,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.13,Ensure that the default administrative credential file permissions are set to 600,Automated,"Ensure that the admin.conf file (and super-admin.conf file, where it exists) have permissions of 600.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should restrict their file permissions to maintain the integrity and confidentiality of the file(s). The file(s) should be readable and writable by only the administrators on the system. Impact: None.","Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %a /etc/kubernetes/super-admin.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chmod 600 /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf have permissions of 600. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",kubernetes_admin_credential_file_permissions_600; kubernetes_super_admin_credential_file_permissions_600; kubernetes_default_credential_file_permissions_secure; kubernetes_admin_conf_file_permissions_restricted; kubernetes_super_admin_conf_file_permissions_restricted,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.14,Ensure that the default administrative credential file ownership is set to root:root,Automated,"Ensure that the admin.conf (and super-admin.conf file, where it exists) file ownership is set to root:root.","As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should set their file ownership to maintain the integrity and confidentiality of the file. The file(s) should be owned by root:root. Impact: None.","Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/admin.conf On Kubernetes version 1.29 and higher run the following command as well :- stat -c %U:%G /etc/kubernetes/super-admin.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/admin.conf On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example, chown root:root /etc/kubernetes/super-admin.conf Default Value: By default, admin.conf and super-admin.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/",compute_credential_file_root_ownership; compute_admin_conf_root_ownership; compute_super_admin_conf_root_ownership; compute_default_credential_file_secure_ownership; compute_admin_credential_file_secure_ownership,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/admin/kubeadm/ 2. https://raesene.github.io/blog/2024/01/06/when-is-admin-not-admin/
1.1.15,Ensure that the scheduler.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler.conf file has permissions of 600 or more restrictive.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/scheduler.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",compute_scheduler_conf_file_permissions_600_or_stricter; compute_scheduler_conf_file_permissions_restrictive; compute_scheduler_conf_file_permissions_secure; compute_scheduler_conf_file_permissions_compliant; compute_scheduler_conf_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.16,Ensure that the scheduler.conf file ownership is set to root:root,Automated,Ensure that the scheduler.conf file ownership is set to root:root.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/scheduler.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",scheduler_file_root_ownership; scheduler_conf_root_ownership; scheduler_file_owner_root; scheduler_conf_owner_root; scheduler_file_group_root; scheduler_conf_group_root; scheduler_file_root_root_ownership; scheduler_conf_root_root_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.17,Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the controller-manager.conf file has permissions of 600 or more restrictive.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/controller-manager.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_permissions_600_or_more_restrictive; kubernetes_controller_manager_conf_file_permissions_restrictive; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_permissions_strict; kubernetes_controller_manager_conf_file_permissions_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.18,Ensure that the controller-manager.conf file ownership is set to root:root,Automated,Ensure that the controller-manager.conf file ownership is set to root:root.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/controller-manager.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_conf_root_ownership; kubernetes_controller_manager_file_root_owner; kubernetes_conf_file_ownership_root; kubernetes_controller_manager_conf_secure_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.19,Ensure that the Kubernetes PKI directory and file ownership is set to root:root,Automated,Ensure that the Kubernetes PKI directory and file ownership is set to root:root.,Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/ Verify that the ownership of all files and directories in this hierarchy is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown -R root:root /etc/kubernetes/pki/ Default Value: By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained within it, are set to be owned by the root user. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_directory_ownership_root; kubernetes_pki_file_ownership_root; kubernetes_pki_directory_permissions_root; kubernetes_pki_file_permissions_root; kubernetes_pki_directory_secure_ownership; kubernetes_pki_file_secure_ownership,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.20,Ensure that the Kubernetes PKI certificate file permissions are set to 644 or more restrictive,Manual,Ensure that Kubernetes PKI certificate files have permissions of 644 or more restrictive.,Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 644 or more restrictive to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.crt Verify that the permissions are 644 or more restrictive. or ls -l /etc/kubernetes/pki/*.crt Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 644 /etc/kubernetes/pki/*.crt Default Value: By default, the certificates used by Kubernetes are set to have permissions of 644 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_certificate_file_permissions_644_or_restrictive; kubernetes_pki_certificate_file_permissions_restrictive; kubernetes_certificate_file_permissions_secure; kubernetes_pki_file_permissions_restrictive; kubernetes_certificate_file_permissions_644_or_stricter,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.21,Ensure that the Kubernetes PKI key file permissions are set to 600,Manual,Ensure that Kubernetes PKI key files have permissions of 600.,Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.key Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.key Verify that the permissions are -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.key Default Value: By default, the keys used by Kubernetes are set to have permissions of 600 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_key_file_permissions_600; kubernetes_pki_key_file_permissions_restricted; kubernetes_pki_key_file_permissions_secure; kubernetes_pki_key_file_permissions_strict; kubernetes_pki_key_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.1,Ensure that the --anonymous-auth argument is set to false,Manual,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Impact: Anonymous requests will be rejected.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[*]}{.spec.containers[*].command} {'\n'}{end}' | grep '\--anonymous- auth' | grep -i false If the exit code is '1', then the control isn't present / failed","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --anonymous-auth=false Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_no_anonymous_auth; kubernetes_api_server_auth_anonymous_disabled; kubernetes_api_server_anonymous_access_disabled; kubernetes_api_server_auth_anonymous_denied,• Level 1 - Master Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests
1.2.2,Ensure that the --token-auth-file parameter is not set,Automated,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token- based authentication. Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist. Alternative Audit Method kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[*]}{.spec.containers[*].command} {'\n'}{end}' | grep '\--token-auth- file' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the --token-auth- file=<filename> parameter. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_disabled; kubernetes_auth_token_file_unset; kubernetes_api_token_authentication_disabled; kubernetes_auth_token_based_auth_disabled; kubernetes_api_server_no_token_auth,• Level 1 - Master Node,You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.,1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.3,Ensure that the DenyServiceExternalIPs is set,Manual,This admission controller rejects all net-new usage of the Service field externalIPs.,"Most users do not need the ability to set the externalIPs field for a Service at all, and cluster admins should consider disabling this functionality by enabling the DenyServiceExternalIPs admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage. Impact: When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the `DenyServiceExternalIPs' argument exist as a string value in --enable- admission-plugins.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and append the Kubernetes API server flag -- enable-admission-plugins with the DenyServiceExternalIPs plugin. Note, the Kubernetes API server flag --enable-admission-plugins takes a comma-delimited list of admission control plugins to be enabled, even if they are in the list of plugins enabled by default. kube-apiserver --enable-admission-plugins=DenyServiceExternalIPs Default Value: By default, --enable-admission-plugins=DenyServiceExternalIP argument is not set, and the use of externalIPs is authorized. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_admission_controller_deny_service_external_ips_enabled; kubernetes_service_external_ips_denied; admission_controller_service_external_ips_restricted; kubernetes_service_external_ips_blocked; admission_controller_deny_external_ips_enabled,• Level 1 - Master Node,"When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.4,Ensure that the --kubelet-client-certificate and --kubelet- client-key arguments are set as appropriate,Automated,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-client- certificate' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below. --kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file> Default Value: By default, certificate-based kubelet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured; kubernetes_kubelet_authentication_certificates_valid; kubernetes_kubelet_tls_authentication_required,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.5,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Automated,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet- certificate-Authority' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string> Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_secure; kubernetes_kubelet_certificate_authority_trusted; kubernetes_kubelet_certificate_authority_enabled,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.6,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not always authorize all requests.,"The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Impact: Only authorized requests will be served.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below. --authorization-mode=RBAC Default Value: By default, AlwaysAllow is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_authorization_mode_restricted; kubernetes_api_server_secure_authorization_mode; kubernetes_api_server_authorization_mode_compliant; kubernetes_api_server_authorization_mode_enforced,• Level 1 - Master Node,Only authorized requests will be served.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.2.7,Ensure that the --authorization-mode argument includes Node,Automated,Restrict kubelet nodes to reading only objects associated with them.,"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include Node.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC Default Value: By default, Node authorization is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_authorization_enabled; kubernetes_kubelet_node_auth_mode_restricted; kubernetes_kubelet_node_read_only_authorization; kubernetes_kubelet_node_authorization_mode_configured; kubernetes_kubelet_node_auth_restriction_enabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security
1.2.8,Ensure that the --authorization-mode argument includes RBAC,Automated,Turn on Role Based Access Control.,"Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Impact: When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include RBAC.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes RBAC, for example: --authorization-mode=Node,RBAC Default Value: By default, RBAC authorization is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/",kubernetes_cluster_rbac_enabled; kubernetes_api_authorization_rbac_required; kubernetes_cluster_auth_mode_rbac_included; kubernetes_api_rbac_authorization_enabled; kubernetes_cluster_rbac_authorization_mode_set,• Level 1 - Master Node,"When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
1.2.9,Ensure that the admission control plugin EventRateLimit is set,Manual,Limit the rate at which the API server accepts requests.,"Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Impact: You need to carefully tune in limits as per your environment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit.,"Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=<path/to/configuration/file> Default Value: By default, EventRateLimit is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md",kubernetes_api_server_event_rate_limit_enabled; kubernetes_api_server_event_rate_limit_configured; kubernetes_admission_control_event_rate_limit_set; kubernetes_api_server_request_rate_limited; kubernetes_admission_plugin_event_rate_limit_enabled,• Level 1 - Master Node,You need to carefully tune in limits as per your environment.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md
1.2.10,Ensure that the admission control plugin AlwaysAdmit is not set,Automated,Do not allow all requests.,Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Impact: Only requests explicitly allowed by the admissions control plugins would be served.,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --enable- admission-plugins parameter, or set it to a value that does not include AlwaysAdmit. Default Value: AlwaysAdmit is not in the list of default admission plugins. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwaysadmit",kubernetes_admission_control_always_admit_disabled; kubernetes_admission_plugin_always_admit_not_set; admission_control_always_admit_restricted; kubernetes_admission_policy_always_admit_denied; admission_plugin_always_admit_unconfigured,• Level 1 - Master Node,Only requests explicitly allowed by the admissions control plugins would be served.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwaysadmit
1.2.11,Ensure that the admission control plugin AlwaysPullImages is set,Manual,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images preloaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission- plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwayspullimages",kubernetes_admission_controller_always_pull_images_enabled; kubernetes_pod_images_always_pulled; admission_controller_always_pull_images_enabled; pod_spec_images_always_pulled; kubernetes_workload_images_always_pulled,• Level 1 - Master Node,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images preloaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#alwayspullimages
1.2.12,Ensure that the admission control plugin ServiceAccount is set,Automated,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Impact: None.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and ensure that the --disable-admission-plugins parameter is set to a value that does not include ServiceAccount. Default Value: By default, ServiceAccount is set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_service_account_admission_plugin_enabled; kubernetes_service_account_automated_management_enabled; kubernetes_admission_controller_service_account_required; kubernetes_service_account_admission_control_enabled; kubernetes_admission_plugin_service_account_enforced,• Level 2 - Master Node,None.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.2.13,Ensure that the admission control plugin NamespaceLifecycle is set,Automated,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --disable-admission- plugins parameter to ensure it does not include NamespaceLifecycle. Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#namespacelifecycle",kubernetes_namespace_lifecycle_admission_plugin_enabled; kubernetes_namespace_termination_protection_enabled; kubernetes_admission_controller_namespace_lifecycle_enabled; kubernetes_namespace_deletion_safety_enabled; kubernetes_admission_plugin_namespace_lifecycle_configured,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#namespacelifecycle
1.2.14,Ensure that the admission control plugin NodeRestriction is set,Automated,Limit the Node and Pod objects that a kubelet could modify.,"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes NodeRestriction.,"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to a value that includes NodeRestriction. --enable-admission-plugins=...,NodeRestriction,... Default Value: By default, NodeRestriction is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#noderestriction 3. https://kubernetes.io/docs/reference/access-authn-authz/node/",kubernetes_kubelet_node_restriction_enabled; kubernetes_admission_control_node_restriction_enabled; kubernetes_kubelet_admission_restriction_enabled; kubernetes_node_restriction_plugin_enabled; kubernetes_admission_node_restriction_enabled,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/reference/access-authn-authz/admission- controllers/#noderestriction 3. https://kubernetes.io/docs/reference/access-authn-authz/node/
1.2.15,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",eks_cluster_profiling_disabled; eks_node_group_profiling_disabled; kubernetes_pod_profiling_disabled; kubernetes_deployment_profiling_disabled; container_runtime_profiling_disabled; compute_instance_profiling_disabled; serverless_function_profiling_disabled; cloud_service_profiling_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.2.16,Ensure that the --audit-log-path argument is set,Automated,Enable auditing on the Kubernetes API Server and set the desired audit log path.,"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_path_set; kubernetes_api_audit_logging_enabled; kubernetes_audit_log_path_configured; kubernetes_api_server_audit_logging_enabled; kubernetes_audit_log_path_set,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.17,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Automated,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_maxage_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_set; kubernetes_audit_log_retention_30d; kubernetes_api_server_audit_log_retention_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.18,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Automated,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_sufficient; kubernetes_api_server_audit_log_retention_configured; kubernetes_api_server_audit_log_backup_limit_enforced; kubernetes_api_server_audit_log_maxbackup_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.19,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Automated,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_api_server_audit_log_rotation_enabled; kubernetes_api_server_audit_log_size_limited; kubernetes_audit_log_maxsize_within_limit,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/ 3. https://github.com/kubernetes/enhancements/issues/22
1.2.20,Ensure that the --request-timeout argument is set as appropriate,Manual,Set global request timeout for API server requests as appropriate.,"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --request-timeout argument is either not set or set to an appropriate value.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s Default Value: By default, --request-timeout is set to 60 seconds. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415",kubernetes_api_server_request_timeout_set; kubernetes_api_server_request_timeout_appropriate; kubernetes_api_server_request_timeout_configured; kubernetes_api_server_request_timeout_valid; kubernetes_api_server_request_timeout_within_limits,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415
1.2.21,Ensure that the --service-account-lookup argument is set to true,Automated,Validate service account before validating token.,"If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --service-account-lookup argument exists it is set to true.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --service-account-lookup=true Alternatively, you can delete the --service-account-lookup parameter from this file so that the default takes effect. Default Value: By default, --service-account-lookup argument is set to true. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use",compute_cluster_service_account_lookup_enabled; compute_node_service_account_lookup_enabled; compute_instance_service_account_lookup_enabled; compute_service_account_lookup_enabled; kubernetes_service_account_lookup_enabled; container_service_account_lookup_enabled; gke_service_account_lookup_enabled; eks_service_account_lookup_enabled; aks_service_account_lookup_enabled; compute_service_account_lookup_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time-of-check_to_time-of-use
1.2.22,Ensure that the --service-account-key-file argument is set as appropriate,Automated,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file. Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --service-account-key- file parameter to the public key file for service accounts: --service-account-key-file=<filename> Default Value: By default, --service-account-key-file argument is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_protected,• Level 1 - Master Node,The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.2.23,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters. --etcd-certfile=<path/to/client-certificate-file> --etcd-keyfile=<path/to/client-key-file> Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",etcd_server_tls_encryption_enabled; etcd_server_certfile_configured; etcd_server_keyfile_configured; etcd_client_connection_encrypted; etcd_tls_authentication_enabled,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.2.24,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Automated,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the TLS certificate and private key file parameters. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Default Value: By default, --tls-cert-file and --tls-private-key-file are presented and created for use. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_cert_and_key_configured; kubernetes_api_server_tls_encryption_enabled; kubernetes_api_server_tls_connection_secure,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.25,Ensure that the --client-ca-file argument is set as appropriate,Automated,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the client certificate authority file. --client-ca-file=<path/to/client-ca-file> Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_configured; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_certificate_validation_enabled; kubernetes_api_server_secure_client_authentication_required; kubernetes_api_server_client_ca_file_provided,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.26,Ensure that the --etcd-cafile argument is set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter. --etcd-cafile=<path/to/ca-file> Default Value: By default, --etcd-cafile is not set. References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/",etcd_server_tls_encryption_enabled; etcd_server_cafile_configured; etcd_client_connection_secure; etcd_tls_certificate_valid; etcd_cafile_argument_set,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/
1.2.27,Ensure that the --encryption-provider-config argument is set as appropriate,Manual,Encrypt etcd key-value store.,etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Impact: None,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --encryption-provider-config argument is set to a EncryptionConfig file. Additionally, ensure that the EncryptionConfig file has all the desired resources covered especially any secrets.","Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=</path/to/EncryptionConfig/File> Default Value: By default, --encryption-provider-config is not set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92",etcd_key_value_store_encryption_enabled; etcd_key_value_store_encryption_provider_config_set; etcd_key_value_store_encryption_configured; etcd_data_encryption_enabled; etcd_encryption_provider_config_valid,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92
1.2.28,Ensure that encryption providers are appropriately configured,Manual,"Where etcd encryption is used, appropriate providers should be configured.","Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms, and secretbox are likely to be appropriate options. Impact: None","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Get the EncryptionConfig file set for --encryption-provider-config argument. Verify that aescbc, kms, or secretbox is set as the encryption provider for all the desired resources.","Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms, or secretbox as the encryption provider. Default Value: By default, no encryption provider is set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers",etcd_encryption_provider_configured; etcd_encryption_provider_secure; etcd_encryption_provider_valid; etcd_encryption_provider_approved; etcd_encryption_provider_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 4. https://github.com/kubernetes/enhancements/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers
1.2.29,Ensure that the API Server only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the API server is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS cipher suites including some that have security concerns, weakening the protection provided. Impact: API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter. --tls-cipher-suites=TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256. Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: Insecure values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_RC4_128_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_RC4_128_SHA.",kubernetes_api_server_strong_ciphers_enabled; kubernetes_api_server_weak_ciphers_disabled; kubernetes_api_server_tls_min_version_1_2; kubernetes_api_server_cipher_suite_restricted; kubernetes_api_server_insecure_ciphers_removed,• Level 1 - Master Node,API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.,"1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 2. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: Insecure values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_RC4_128_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_RC4_128_SHA."
1.2.30,Ensure that the --service-account-extend-token-expiration parameter is set to false,Automated,By default Kubernetes extends service account token lifetimes to one year to aid in transition from the legacy token settings.,"This default setting is not ideal for security as it ignores other settings related to maximum token lifetime and means that a lost or stolen credential could be valid for an extended period of time. Impact: Disabling this setting means that the service account token expiry set in the cluster will be enforced, and service account tokens will expire at the end of that time frame.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-extend-token-expiration argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --service-account-extend-token-expiration parameter to false. --service-account-extend-token-expiration=false Default Value: By default, this parameter is set to true References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 1.3 Controller Manager This section contains recommendations relating to Controller Manager configuration flags",kubernetes_service_account_token_expiration_extended_disabled; kubernetes_service_account_token_expiration_default; kubernetes_service_account_token_lifetime_not_extended; kubernetes_service_account_token_expiration_false; kubernetes_service_account_token_no_extension,• Level 1 - Master Node,"Disabling this setting means that the service account token expiry set in the cluster will be enforced, and service account tokens will expire at the end of that time frame.",1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- apiserver/ 1.3 Controller Manager This section contains recommendations relating to Controller Manager configuration flags
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Manual,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --terminated- pod-gc-threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_pod_garbage_collector_enabled; kubernetes_pod_termination_threshold_set; kubernetes_pod_gc_threshold_configured; kubernetes_pod_termination_cleanup_enabled; kubernetes_pod_gc_threshold_appropriate,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; kubernetes_cluster_profiling_disabled; container_cluster_profiling_disabled; k8s_cluster_profiling_disabled; eks_node_group_profiling_disabled; kubernetes_node_profiling_disabled; container_service_profiling_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.3.3,Ensure that the --use-service-account-credentials argument is set to true,Automated,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service- account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node to set the below parameter. --use-service-account-credentials=true Default Value: By default, --use-service-account-credentials is set to false. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",gke_cluster_use_service_account_credentials_enabled; gke_controller_service_account_credentials_required; gke_cluster_service_account_credentials_enforced; gke_controller_individual_service_account_credentials; gke_cluster_service_account_credentials_set_true,• Level 1 - Master Node,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.4,Ensure that the --service-account-private-key-file argument is set as appropriate,Automated,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private- key-file as appropriate. Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --service- account-private-key-file parameter to the private key file for service accounts. --service-account-private-key-file=<filename> Default Value: By default, --service-account-private-key-file it not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_specified; kubernetes_controller_manager_service_account_private_key_file_valid; kubernetes_controller_manager_service_account_private_key_file_secure,• Level 1 - Master Node,You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.5,Ensure that the --root-ca-file argument is set as appropriate,Automated,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Impact: You need to setup and maintain root certificate authority file.,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`. --root-ca-file=<path/to/file> Default Value: By default, --root-ca-file is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_cluster_root_ca_file_configured; kubernetes_api_server_certificate_verification_enabled; kubernetes_pod_api_server_certificate_validated; kubernetes_cluster_root_ca_file_set; kubernetes_api_server_trusted_certificate_configured,• Level 1 - Master Node,You need to setup and maintain root certificate authority file.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.3.6,Ensure that the RotateKubeletServerCertificate argument is set to true,Automated,Enable kubelet server certificate rotation on controller-manager.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true. --feature-gates=RotateKubeletServerCertificate=true Default Value: By default, RotateKubeletServerCertificate is set to 'true' this recommendation verifies that it has not been disabled. References: 1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_rotate_kubelet_server_certificate_enabled; kubernetes_controller_manager_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_auto_rotation_enabled; kubernetes_controller_manager_tls_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_required,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.7,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the Controller Manager service to non-loopback insecure addresses.,"The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address",controller_manager_bind_address_localhost; controller_manager_network_bind_restricted; controller_manager_loopback_only_enabled; controller_manager_insecure_bind_disabled; controller_manager_localhost_bind_enforced,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address
1.4.1,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml file on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; kubernetes_cluster_profiling_disabled; container_cluster_profiling_disabled; k8s_cluster_profiling_disabled; eks_node_group_profiling_disabled; kubernetes_node_profiling_disabled; container_node_profiling_disabled; k8s_node_profiling_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.4.2,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the scheduler service to non-loopback insecure addresses.,"The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml on the Control Plane node and ensure the correct value for the -- bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/",kubernetes_scheduler_bind_address_localhost; kubernetes_scheduler_loopback_only; kubernetes_scheduler_network_bind_restricted; kubernetes_scheduler_localhost_bind_enabled; kubernetes_scheduler_insecure_bind_disabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/
2.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_cert_key_files_valid; etcd_service_tls_certificates_present,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.2,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_cert_file_set; etcd_service_key_file_set; etcd_service_tls_encryption_enabled; etcd_service_cert_file_valid; etcd_service_key_file_valid; etcd_service_tls_configuration_secure; etcd_service_cert_key_pair_matching; etcd_service_tls_min_version_enforced,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.3,Ensure that the --client-cert-auth argument is set to true,Automated,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: All clients attempting to access the etcd server will require a valid client certificate.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --client-cert-auth='true' Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_auth_enabled; etcd_client_cert_auth_required; etcd_client_authentication_enabled; etcd_service_client_cert_auth_enabled; etcd_client_cert_auth_set_true,• Level 1 - Master Node,All clients attempting to access the etcd server will require a valid client certificate.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
2.4,Ensure that the --auto-tls argument is not set to true,Automated,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: Clients will not be able to use self-signed certificates for TLS.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --auto-tls parameter or set it to false. --auto-tls=false Default Value: By default, --auto-tls is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_certificate_self_signed_disabled; cloud_cdn_tls_auto_renewal_disabled; cloud_cdn_tls_custom_certificate_required; cloud_cdn_tls_managed_certificate_disabled,• Level 1 - Master Node,Clients will not be able to use self-signed certificates for TLS.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
2.5,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Impact: etcd cluster peers would need to set up TLS for their communication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --peer-client-file=</path/to/peer-cert-file> --peer-key-file=</path/to/peer-key-file> Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_peer_certificate_file_set; etcd_peer_key_file_set; etcd_peer_tls_encryption_enabled; etcd_peer_connection_secure; etcd_peer_certificate_validation_enabled,• Level 1 - Master Node,etcd cluster peers would need to set up TLS for their communication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.6,Ensure that the --peer-client-cert-auth argument is set to true,Automated,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --peer-client-cert-auth=true Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth",etcd_cluster_peer_client_cert_auth_enabled; etcd_peer_authentication_required; etcd_peer_tls_client_cert_auth_enabled; etcd_cluster_peer_authentication_enabled; etcd_peer_client_cert_auth_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth
2.7,Ensure that the --peer-auto-tls argument is not set to true,Automated,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --peer-auto-tls parameter or set it to false. --peer-auto-tls=false Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",kubernetes_cluster_peer_auto_tls_disabled; kubernetes_cluster_tls_auto_cert_disabled; kubernetes_cluster_peer_tls_manual_cert_required; kubernetes_cluster_auto_tls_disabled; kubernetes_cluster_peer_tls_self_signed_disabled,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
2.8,Ensure that a unique Certificate Authority is used for etcd,Manual,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster. Run the following command on the master node: ps -ef | grep etcd Note the file referenced by the --trusted-ca-file argument. Run the following command on the master node: ps -ef | grep apiserver Verify that the file referenced by the --client-ca-file for apiserver is different from the --trusted-ca-file used by etcd.,"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --trusted-ca-file=</path/to/ca-file> Default Value: By default, no etcd certificate is created and used. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_unique_certificate_authority; etcd_certificate_authority_separate_from_kubernetes; etcd_certificate_authority_unique; kubernetes_etcd_ca_not_shared; etcd_ca_distinct_from_kubernetes,• Level 2 - Master Node,Additional management of the certificates and keys for the dedicated certificate authority will be required.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
3.1.1,Client certificate authentication should not be used for users,Manual,"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication.,"Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. Default Value: Client certificate authentication is enabled by default. Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",kubernetes_user_no_client_certificate_auth; kubernetes_user_client_certificate_disabled; kubernetes_auth_no_user_client_certificates; kubernetes_user_auth_no_client_certificates; kubernetes_auth_client_certificate_revocation_check,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.2,Service account token authentication should not be used for users,Manual,"Kubernetes provides service account tokens which are intended for use by workloads running in the Kubernetes cluster, for authentication to the API server. These tokens are not designed for use by end-users and do not provide for features such as revocation or expiry, making them insecure. A newer version of the feature (Bound service account token volumes) does introduce expiry but still does not allow for specific revocation.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Service account token authentication does not allow for this due to the use of JWT tokens as an underlying technology. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of service account token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of service account tokens. Default Value: Service account token authentication is enabled by default.,kubernetes_service_account_token_authentication_disabled; kubernetes_service_account_user_authentication_disabled; kubernetes_service_account_no_user_tokens; kubernetes_service_account_bound_token_required; kubernetes_service_account_token_revocation_enabled; kubernetes_service_account_token_expiry_enabled; kubernetes_service_account_no_static_tokens; kubernetes_service_account_token_volume_disabled,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.3,Bootstrap token authentication should not be used for users,Manual,Kubernetes provides bootstrap tokens which are intended for use by new nodes joining the cluster These tokens are not designed for use by end-users they are specifically designed for the purpose of bootstrapping new nodes and not for general authentication,Bootstrap tokens are not intended for use as a general authentication mechanism and impose constraints on user and group naming that do not facilitate good RBAC design. They also cannot be used with MFA resulting in a weak authentication mechanism being available. Impact: External mechanisms for authentication generally require additional software to be deployed.,Review user access to the cluster and ensure that users are not making use of bootstrap token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of bootstrap tokens. Default Value: Bootstrap token authentication is not enabled by default and requires an API server parameter to be set.,kubernetes_user_no_bootstrap_token_auth; kubernetes_user_no_bootstrap_token_auth_enabled; kubernetes_auth_no_bootstrap_token_for_users; kubernetes_auth_bootstrap_token_disabled_for_users; kubernetes_user_auth_no_bootstrap_token_usage,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.2.1,Ensure that a minimal audit policy is created,Manual,Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.,"Logging is an important detective control for all systems, to detect potential unauthorised access. Impact: Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",Run the following command on one of the cluster master nodes: ps -ef | grep kube-apiserver Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy.,"Create an audit policy file for your cluster. Default Value: Unless the --audit-policy-file flag is specified, no auditing will be carried out. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/",kubernetes_api_audit_policy_enabled; kubernetes_api_audit_policy_minimal; kubernetes_api_audit_logging_enabled; kubernetes_api_audit_policy_configured; kubernetes_api_audit_policy_file_set,• Level 1 - Master Node,"Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
3.2.2,Ensure that the audit policy covers key security concerns,Manual,Ensure that the audit policy created for the cluster covers key security concerns.,"Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Impact: Increasing audit logging will consume resources on the nodes or other log destination.","Review the audit policy provided for the cluster and ensure that it covers at least the following areas :- • Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data. • Modification of pod and deployment objects. • Use of pods/exec, pods/portforward, pods/proxy and services/proxy. For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","Consider modification of the audit policy in use on the cluster to include these items, at a minimum. Default Value: By default Kubernetes clusters do not log audit information. References: 1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735",kubernetes_audit_policy_key_security_covered; kubernetes_audit_policy_security_concerns_included; kubernetes_audit_policy_comprehensive_security; kubernetes_audit_policy_minimum_security_requirements; kubernetes_audit_policy_cis_benchmark_compliance,• Level 2 - Master Node,Increasing audit logging will consume resources on the nodes or other log destination.,1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735
4.1.1,Ensure that the kubelet service file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet service file has permissions of 600 or more restrictive.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, the kubelet service file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_file_permissions_600_or_stricter; kubernetes_kubelet_service_file_permissions_restrictive; kubernetes_kubelet_file_permissions_restricted; kubernetes_kubelet_service_file_permissions_secure; kubernetes_kubelet_file_permissions_secure,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.2,Ensure that the kubelet service file ownership is set to root:root,Automated,Ensure that the kubelet service file ownership is set to root:root.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, kubelet service file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_ownership_root; kubernetes_kubelet_file_owner_root; kubernetes_kubelet_service_file_root_owned; kubernetes_kubelet_ownership_root_root; kubernetes_service_file_kubelet_root_owned,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.3,If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive,Manual,"If kube-proxy is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of 600 or more restrictive.","The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None","Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a <path><filename> Verify that a file is specified and it exists with permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 <proxy kubeconfig file> Default Value: By default, proxy file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_permissions_restrictive; kubernetes_proxy_kubeconfig_file_mode_600; kubernetes_kubeconfig_file_permissions_restrictive; kubernetes_proxy_kubeconfig_file_permissions_secure; kubernetes_kubeconfig_file_mode_600_or_stricter,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.4,If proxy kubeconfig file exists ensure ownership is set to root:root,Manual,"If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G <path><filename> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_root_ownership; kubernetes_proxy_kubeconfig_file_secure_ownership; kubernetes_kubeconfig_proxy_root_owner; kubernetes_proxy_config_file_root_owned; kubernetes_kubeconfig_proxy_secure_permissions,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.5,Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet.conf file has permissions of 600 or more restrictive.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file has permissions of 600. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_permissions_restrictive; kubernetes_kubeconfig_file_permissions_600; kubernetes_kubelet_conf_file_permissions_restrictive; kubernetes_kubelet_conf_file_permissions_600; kubernetes_kubeconfig_kubelet_conf_file_permissions_restrictive; kubernetes_kubeconfig_kubelet_conf_file_permissions_600,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.6,Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root,Automated,Ensure that the kubelet.conf file ownership is set to root:root.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_root_ownership; kubelet_config_file_root_ownership; kubeconfig_kubelet_root_user_group; kubelet_conf_file_secure_ownership; kubernetes_kubelet_config_root_only,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.7,Ensure that the certificate authorities file permissions are set to 600 or more restrictive,Manual,Ensure that the certificate authorities file has permissions of 600 or more restrictive.,The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %a <filename> Verify that the permissions are 600 or more restrictive.,Run the following command to modify the file permissions of the --client-ca-file chmod 600 <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_certificate_authorities_file_permissions_600_or_more_restrictive; compute_certificate_authorities_file_permissions_restrictive; compute_ca_file_permissions_600_or_stricter; compute_ca_file_permissions_restricted; compute_ssl_certificate_authorities_file_permissions_secure,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.8,Ensure that the client certificate authorities file ownership is set to root:root,Manual,Ensure that the certificate authorities file ownership is set to root:root.,The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %U:%G <filename> Verify that the ownership is set to root:root.,Run the following command to modify the ownership of the --client-ca-file. chown root:root <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_client_certificate_authorities_file_ownership_root; compute_ca_file_ownership_root_root; compute_certificate_authorities_file_ownership_secure; compute_ca_file_ownership_restricted; compute_client_ca_file_ownership_root_only,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.9,If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 600 or more restrictive.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml Verify that the permissions are 600 or more restrictive.","Run the following command (using the config file location identified in the Audit step) chmod 600 /var/lib/kubelet/config.yaml Default Value: By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of 600. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_permissions_600_or_stricter; kubernetes_kubelet_config_file_permissions_restrictive; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_permissions_minimum_600; kubernetes_kubelet_config_file_permissions_protected,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.1.10,If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /var/lib/kubelet/config.yaml ```Verify that the ownership is set to `root:root`.","Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by root:root. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubelet_config_file_root_ownership; kubelet_config_file_root_owner; kubelet_config_file_root_group; kubelet_config_file_secure_ownership; kubelet_config_file_proper_ownership,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.2.1,Ensure that the --anonymous-auth argument is set to false,Automated,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.","If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false. This executable argument may be omitted, provided there is a corresponding entry set to false in the Kubelet config file.","If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --anonymous-auth=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_anonymous_auth_set_false; kubernetes_kubelet_anonymous_requests_blocked; kubernetes_kubelet_auth_anonymous_disabled; kubernetes_kubelet_secure_auth_enabled,• Level 1 - Worker Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.2,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.","Run the following command on each node: ps -ef | grep kubelet If the --authorization-mode argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by -- config, and that file sets authorization: mode to something other than AlwaysAllow. It is also possible to review the running configuration of a Kubelet via the /configz endpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --authorization-mode=Webhook Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_explicit_authorization_enabled; kubernetes_api_server_authorization_restricted; kubernetes_api_server_always_allow_disabled,• Level 1 - Worker Node,Unauthorized requests will be denied.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.3,Ensure that the --client-ca-file argument is set as appropriate,Automated,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. If the --client-ca-file argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets authentication: x509: clientCAFile to the location of the client certificate authority file.","If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to the location of the client CA file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --client-ca-file=<path/to/client-ca-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/",kubernetes_kubelet_client_ca_file_set; kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_tls_client_auth_required; kubernetes_kubelet_ca_certificate_configured; kubernetes_kubelet_auth_certificates_valid,• Level 1 - Worker Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/
4.2.4,"Verify that if defined, readOnlyPort is set to 0",Manual,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,"Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.","If using a Kubelet config file, edit the file to set readOnlyPort to 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --read-only-port=0 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/6cedc0853faa118df0ba3d41b48b 993422ad3df6/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L142 Additional Information: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/",kubernetes_api_server_readonly_port_disabled; kubernetes_api_server_readonly_port_set_to_zero; kubernetes_api_server_readonly_port_secure_config; kubernetes_api_server_readonly_port_zero; kubernetes_api_server_readonly_port_unset,• Level 1 - Worker Node,Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/6cedc0853faa118df0ba3d41b48b 993422ad3df6/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L142 Additional Information: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
4.2.5,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Manual,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.","Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0. If the argument is not present, and there is a Kubelet config file specified by --config, check that it does not set streamingConnectionIdleTimeout to 0.","If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a value other than 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --streaming-connection-idle-timeout=5m Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",eks_cluster_streaming_connection_timeout_enabled; eks_cluster_streaming_connection_idle_timeout_configured; eks_cluster_streaming_connection_timeout_not_disabled; eks_cluster_streaming_connection_idle_timeout_nonzero; eks_cluster_streaming_connection_timeout_valid,• Level 1 - Worker Node,Long-lived connections could be interrupted.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
4.2.6,Ensure that the --make-iptables-util-chains argument is set to true,Automated,Allow Kubelet to manage iptables.,"Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.","Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.","If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove the --make- iptables-util-chains argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --make-iptables-util-chains argument is set to true. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_util_chains_set_true; kubernetes_kubelet_iptables_chains_managed; kubernetes_kubelet_iptables_util_chains_configured; kubernetes_kubelet_iptables_util_chains_active,• Level 1 - Worker Node,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",1. https://kubernetes.io/docs/admin/kubelet/
4.2.7,Ensure that the --hostname-override argument is not set,Manual,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Impact: Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist. Note This setting is not configurable via the Kubelet config file.,"Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10- kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_default; kubernetes_node_hostname_unchanged; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_preserved,• Level 1 - Worker Node,"Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063
4.2.8,Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture,Manual,"Security relevant information should be captured. The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,"Run the following command on each node: sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10- kubeadm.conf or If using command line arguments, kubelet service file is located /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10-kubelet- args.conf Review the value set for the argument and determine whether this has been set to an appropriate level for the cluster. If the argument does not exist, check that there is a Kubelet config file specified by -- config and review the value in this location. If using command line arguments","If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, eventRecordQPS argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go",kubernetes_kubelet_event_record_qps_configured; kubernetes_kubelet_event_capture_rate_limited; kubernetes_kubelet_event_record_qps_non_zero; kubernetes_kubelet_event_logging_rate_optimized; kubernetes_kubelet_event_record_qps_within_bounds,• Level 2 - Worker Node,Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go
4.2.9,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Manual,Setup TLS connection on the Kubelets.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.","Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. If these arguments are not present, check that there is a Kubelet config specified by -- config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_configured; kubernetes_kubelet_tls_cert_and_key_valid; kubernetes_kubelet_tls_encryption_enabled,• Level 1 - Worker Node,,
4.2.10,Ensure that the --rotate-certificates argument is not set to false,Automated,Enable kubelet client certificate rotation.,The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None,"Run the following command on each node: ps -ef | grep kubelet Verify that the RotateKubeletServerCertificate argument is not present, or is set to true. If the RotateKubeletServerCertificate argument is not present, verify that if there is a Kubelet config file specified by --config, that file does not contain RotateKubeletServerCertificate: false.","If using a Kubelet config file, edit the file to add the line rotateCertificates: true or remove it altogether to use the default value. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove --rotate- certificates=false argument from the KUBELET_CERTIFICATE_ARGS variable or set - -rotate-certificates=true . Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet client certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_rotate_certificates_not_false; kubernetes_kubelet_client_cert_rotation_required; kubernetes_kubelet_tls_cert_rotation_enabled; kubernetes_kubelet_secure_certificate_rotation,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/
4.2.11,Verify that the RotateKubeletServerCertificate argument is set to true,Manual,Enable kubelet server certificate rotation.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate- server-certificates parameter is set on kubelet Run the following command on each node: ps -ef | grep kubelet Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable. --feature-gates=RotateKubeletServerCertificate=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet server certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubernetes_kubelet_rotate_server_certificate_enabled; kubernetes_kubelet_rotate_certificate_enabled; kubernetes_kubelet_auto_rotate_certificate_enabled,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration
4.2.12,Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the Kubelet is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.","The set of cryptographic ciphers currently considered secure is the following: • TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 • TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_128_GCM_SHA256 Run the following command on each node: ps -ef | grep kubelet If the --tls-cipher-suites argument is present, ensure it only contains values included in this set. If it is not present check that there is a Kubelet config file specified by --config, and that file sets tlsCipherSuites: to only include values from this set.","If using a Kubelet config file, edit the file to set tlsCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_ 256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WI TH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES _128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the --tls-cipher- suites parameter as follows, or to a subset of these values. --tls-cipher- suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_kubelet_strong_ciphers_enabled; kubernetes_kubelet_weak_ciphers_disabled; kubernetes_kubelet_tls_ciphers_restricted; kubernetes_kubelet_cipher_suite_compliant; kubernetes_kubelet_secure_ciphers_only,• Level 1 - Worker Node,Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.,
4.2.13,Ensure that a limit is set on pod PIDs,Manual,Ensure that the Kubelet sets limits on the number of PIDs that can be created by pods running on the node.,"By default pods running in a cluster can consume any number of PIDs, potentially exhausting the resources available on the node. Setting an appropriate limit reduces the risk of a denial of service attack on cluster nodes. Impact: Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.","Review the Kubelet's start-up parameters for the value of --pod-max-pids, and check the Kubelet configuration file for the PodPidsLimit . If neither of these values is set, then there is no limit in place.","Decide on an appropriate level for this parameter and set it, either via the --pod-max- pids command line parameter or the PodPidsLimit configuration file setting. Default Value: By default the number of PIDs is not limited. References: 1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits",kubernetes_kubelet_pid_limit_enabled; kubernetes_pod_pid_limit_configured; kubernetes_node_pid_limit_enforced; kubernetes_kubelet_pid_limit_per_pod; kubernetes_pod_pid_limit_set,• Level 1 - Worker Node,Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.,1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits
4.2.14,Ensure that the --seccomp-default parameter is set to true,Manual,Ensure that the Kubelet enforces the use of the RuntimeDefault seccomp profile,"By default, Kubernetes disables the seccomp profile which ships with most container runtimes. Setting this parameter will ensure workloads running on the node are protected by the runtime's seccomp profile. Impact: Setting this will remove some rights from pods running on the node.","Review the Kubelet's start-up parameters for the value of --seccomp-default, and check the Kubelet configuration file for the seccompDefault . If neither of these values is set, then the seccomp profile is not in use.","Set the parameter, either via the --seccomp-default command line parameter or the seccompDefault configuration file setting. Default Value: By default the seccomp profile is not enabled. References: 1. https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of- runtimedefault-as-the-default-seccomp-profile-for-all-workloads",kubernetes_kubelet_seccomp_default_enabled; kubernetes_kubelet_runtime_default_profile_enabled; kubernetes_kubelet_seccomp_profile_default_enabled; kubernetes_kubelet_seccomp_default_true; kubernetes_kubelet_runtime_default_seccomp_enabled,• Level 1 - Worker Node,Setting this will remove some rights from pods running on the node.,1. https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of- runtimedefault-as-the-default-seccomp-profile-for-all-workloads
4.2.15,Ensure that the --IPAddressDeny is set to any,Manual,Ensuring that --IPAddressDeny is set to 'Any' will facilitate allowlisting of only IP addresses that are explicitly set with the --IPAddressAllow parameter which will block unspecified IP addresses from communicating with the kubelet component.,"By default, Kubernetes allows any IP address to communicate with the kubelet component IP restrictions and IP whitelisting are security best practices and reduce the attack surface of the kubelet . Impact: Configuring the setting IPAddressDeny=any will deny service to any IP address not specified in the complimentary setting IPAddressDeny=any configuration parameter. Applying IPAddressDeny=any alone will completely disable communication with the component.","Review the Kubelet's start-up parameters for the value of --IPAddressDeny, and check the Kubelet configuration file for IPAddressDeny=any. If this entry is present it should be accompanied by IPAddressAllow={{ kubelet_secure_addresses }} to allow the control plane to communicate with the component.","IPAddressDeny=any IPAddressAllow={{ kubelet_secure_addresses }} *Note kubelet_secure_addresses: 'localhost link-local {{ kube_pods_subnets | regex_replace(',', ' ') }} {{ kube_node_addresses }} {{ loadbalancer_apiserver.address | default('')' Default Value: By default IPAddressDeny is not enabled. References: 1. https://github.com/kubernetes-sigs/kubespray/pull/9194/files 2. https://kubernetes.io/docs/concepts/services-networking/network-policies/",kubernetes_kubelet_ip_address_deny_set_to_any; kubernetes_kubelet_ip_deny_all_enabled; kubernetes_kubelet_ip_restriction_configured; kubernetes_kubelet_ip_allowlist_enforced; kubernetes_kubelet_ip_deny_all_active,• Level 2 - Worker Node,Configuring the setting IPAddressDeny=any will deny service to any IP address not specified in the complimentary setting IPAddressDeny=any configuration parameter. Applying IPAddressDeny=any alone will completely disable communication with the component.,1. https://github.com/kubernetes-sigs/kubespray/pull/9194/files 2. https://kubernetes.io/docs/concepts/services-networking/network-policies/
4.3.1,Ensure that the kube-proxy metrics service is bound to localhost,Manual,Do not bind the kube-proxy metrics port to non-loopback addresses.,kube-proxy has two APIs which provided access to information about the service and can be bound to network ports. The metrics API service includes endpoints (/metrics and /configz) which disclose information about the configuration and operation of kube-proxy. These endpoints should not be exposed to untrusted networks as they do not support encryption or authentication to restrict access to the data they provide. Impact: 3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,review the start-up flags provided to kube proxy Run the following command on each node: ps -ef | grep -i kube-proxy Ensure that the --metrics-bind-address parameter is not set to a value other than 127.0.0.1. From the output of this command gather the location specified in the -- config parameter. Review any file stored at that location and ensure that it does not specify a value other than 127.0.0.1 for metricsBindAddress.,Modify or remove any values which bind the metrics service to a non-localhost address Default Value: The default value is 127.0.0.1:10249 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/,kubernetes_proxy_metrics_localhost_bound; kubernetes_proxy_metrics_non_loopback_blocked; kubernetes_proxy_metrics_loopback_only; kubernetes_proxy_metrics_external_access_disabled; kubernetes_proxy_metrics_interface_restricted,• Level 1 - Worker Node,3rd party services which try to access metrics or configuration information related to kube-proxy will require access to the localhost interface of the node.,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/
5.1.1,Ensure that the cluster-admin role is only used where required,Manual,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",,• Level 1 - Master Node,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
5.1.2,Minimize access to secrets,Manual,"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation,"Review the users who have get, list, or watch access to secrets objects in the Kubernetes API.","Where possible, restrict access to secret objects in the cluster by removing get, list, and watch permissions. Default Value: By default in a kubeadm cluster the following list of principals have get privileges on secret objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:expand-controller expand-controller ServiceAccount kube-system system:controller:generic-garbage-collector generic-garbage- collector ServiceAccount kube-system system:controller:namespace-controller namespace-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:kube-controller-manager system:kube-controller- manager User",kubernetes_secret_access_restricted; kubernetes_secret_minimal_user_access; kubernetes_secret_no_public_access; kubernetes_secret_service_account_restricted; kubernetes_secret_workload_credentials_restricted; kubernetes_secret_admin_access_denied; kubernetes_secret_least_privilege_enforced; kubernetes_secret_access_scope_limited,• Level 1 - Master Node,Care should be taken not to remove access to secrets to system components which require this for their operation,
5.1.3,Minimize wildcard use in Roles and ClusterRoles,Manual,Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard '*' which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.,The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.,Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml,Where possible replace any use of wildcards in ClusterRoles and Roles with specific objects or actions.,kubernetes_role_wildcard_restricted; kubernetes_clusterrole_wildcard_restricted; kubernetes_role_minimal_permissions; kubernetes_clusterrole_minimal_permissions; kubernetes_role_no_wildcard_actions; kubernetes_clusterrole_no_wildcard_actions; kubernetes_role_no_wildcard_resources; kubernetes_clusterrole_no_wildcard_resources,• Level 1 - Worker Node,,
5.1.4,Minimize access to create pods,Manual,"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.","The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",Review the users who have create access to pod objects in the Kubernetes API.,"Where possible, remove create access to pod objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have create privileges on pod objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:daemon-set-controller daemon-set-controller ServiceAccount kube-system system:controller:job-controller job-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:controller:replicaset-controller replicaset-controller ServiceAccount kube-system system:controller:replication-controller replication-controller ServiceAccount kube-system system:controller:statefulset-controller statefulset-controller ServiceAccount kube-system",kubernetes_namespace_pod_creation_restricted; kubernetes_role_pod_creation_minimized; kubernetes_cluster_pod_creation_limited; kubernetes_service_account_pod_creation_restricted; kubernetes_rbac_pod_creation_minimized; kubernetes_policy_pod_creation_limited; kubernetes_user_pod_creation_restricted; kubernetes_group_pod_creation_minimized,• Level 1 - Master Node,Care should be taken not to remove access to pods to system components which require this for their operation,
5.1.5,Ensure that default service accounts are not actively used.,Manual,The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.,"Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured to ensure that it does not automatically provide a service account token, and it must not have any non-default role bindings or custom role assignments Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.","For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/,iam_service_account_default_not_used; iam_service_account_default_inactive; compute_service_account_default_disabled; compute_service_account_default_unused; service_account_default_no_active_usage,• Level 1 - Master Node,All workloads which require access to the Kubernetes API will require an explicit service account to be created.,1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.6,Ensure that Service Account Tokens are only mounted where necessary,Manual,Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server,"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.","Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false","Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_pod_service_account_token_not_mounted; kubernetes_pod_service_account_token_unnecessary_mount_disabled; kubernetes_pod_service_account_token_mount_restricted; kubernetes_pod_service_account_token_mount_required_only; kubernetes_pod_service_account_token_mount_minimized,• Level 1 - Master Node,"Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.7,Avoid use of system:masters group,Manual,"The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed. When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster. Impact: Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.,Remove the system:masters group from all users in the cluster. Default Value: By default some clusters will create a 'break glass' client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations. References: 1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38,iam_group_system_masters_unused; iam_group_system_masters_restricted; iam_group_system_masters_no_grants; iam_group_system_masters_minimal_usage; iam_group_system_masters_bootstrap_only,• Level 1 - Master Node,"Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38
5.1.8,"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",Manual,"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster- admin level. Impact: There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.","Review the users who have access to cluster roles or roles which provide the impersonate, bind, or escalate privileges.","Where possible, remove the impersonate, bind, and escalate rights from subjects. Default Value: In a default kubeadm cluster, the system:masters group and clusterrole-aggregation- controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate. References: 1. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 2. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/",kubernetes_role_no_impersonate_permission; kubernetes_role_no_bind_permission; kubernetes_role_no_escalate_permission; kubernetes_cluster_role_no_impersonate_permission; kubernetes_cluster_role_no_bind_permission; kubernetes_cluster_role_no_escalate_permission; kubernetes_role_no_privilege_escalation; kubernetes_cluster_role_no_privilege_escalation,• Level 1 - Master Node,"There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",1. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 2. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/
5.1.9,Minimize access to create persistent volumes,Manual,"The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of hostPath volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.","The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have create access to PersistentVolume objects in the Kubernetes API.,"Where possible, remove create access to PersistentVolume objects in the cluster. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation",kubernetes_persistent_volume_creation_restricted; kubernetes_persistent_volume_hostpath_disabled; kubernetes_persistent_volume_privilege_escalation_prevented; kubernetes_persistent_volume_access_minimized; kubernetes_persistent_volume_creation_admin_only,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation
5.1.10,Minimize access to the proxy sub-resource of nodes,Manual,"Users with access to the Proxy sub-resource of Node objects automatically have permissions to use the kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs. The kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the node object. The proxy sub-resource specifically allows wide ranging access to the kubelet API. Direct access to the kubelet API bypasses controls like audit logging (there is no audit log of kubelet API access) and admission control.","The ability to use the proxy sub-resource of node objects opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have access to the proxy sub-resource of node objects in the Kubernetes API.,"Where possible, remove access to the proxy sub-resource of node objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization",kubernetes_node_proxy_access_restricted; kubernetes_node_proxy_no_wildcard_access; kubernetes_node_proxy_minimal_permissions; kubernetes_node_proxy_no_privilege_escalation; kubernetes_node_proxy_audit_bypass_prevented; kubernetes_node_proxy_kubelet_api_restricted; kubernetes_node_proxy_no_broad_permissions; kubernetes_node_proxy_access_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization
5.1.11,Minimize access to the approval sub-resource of certificatesigningrequests objects,Manual,"Users with access to the update the approval sub-resource of CertificateSigningRequests objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts. This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster",The ability to update certificate signing requests should be limited.,Review the users who have access to update the approval sub-resource of CertificateSigningRequests objects in the Kubernetes API.,"Where possible, remove access to the approval sub-resource of CertificateSigningRequests objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing",kubernetes_certificate_signing_request_approval_restricted; kubernetes_csr_approval_minimal_access; kubernetes_csr_approval_no_high_privileges; kubernetes_csr_approval_no_admin_access; kubernetes_csr_approval_no_privilege_escalation,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing
5.1.12,Minimize access to webhook configuration objects,Manual,"Users with rights to create/modify/delete validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.",The ability to manage webhook configuration should be limited,Review the users who have access to validatingwebhookconfigurations or mutatingwebhookconfigurations objects in the Kubernetes API.,"Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks",kubernetes_validatingwebhookconfiguration_minimize_access; kubernetes_mutatingwebhookconfiguration_minimize_access; kubernetes_webhookconfiguration_restrict_modify_access; kubernetes_webhookconfiguration_restrict_delete_access; kubernetes_webhookconfiguration_restrict_create_access; kubernetes_webhookconfiguration_admin_access_restricted; kubernetes_webhookconfiguration_privilege_escalation_prevented,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks
5.1.13,Minimize access to the service account token creation,Manual,"Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.",The ability to create service account tokens should be limited.,Review the users who have access to create the token sub-resource of serviceaccount objects in the Kubernetes API.,"Where possible, remove access to the token sub-resource of serviceaccount objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request",kubernetes_service_account_token_creation_restricted; kubernetes_service_account_token_creation_minimized; kubernetes_service_account_token_creation_limited; kubernetes_service_account_token_creation_admin_restricted; kubernetes_service_account_token_creation_privileged_restricted,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request
5.2.1,Ensure that the cluster has at least one active policy control mechanism in place,Manual,"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts. Impact: Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.,"Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads. Default Value: By default, Pod Security Admission is enabled but no policies are in place. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission",kubernetes_cluster_policy_control_enabled; kubernetes_cluster_pod_security_admission_enabled; kubernetes_cluster_third_party_policy_control_enabled; kubernetes_cluster_active_policy_control_exists; kubernetes_cluster_policy_enforcement_mechanism_active,• Level 1 - Master Node,"Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",1. https://kubernetes.io/docs/concepts/security/pod-security-admission
5.2.2,Minimize the admission of privileged containers,Manual,Do not generally permit containers to be run with the securityContext.privileged flag set to true.,"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.","Run the following command: get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}' It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection. calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node- c4xv4: {} {'privileged':true} {'privileged':true} {'privileged':true} {'privileged':true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {'seccompProfile':{'type':'RuntimeDefault'}} {'allowPrivilegeEscalation':false,'readOnlyRootFilesystem':true,'runAsGroup':2001,'ru nAsUser':1001}","Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privileged_disabled; compute_container_privileged_denied; compute_container_privileged_restricted; compute_container_security_context_privileged_false; compute_container_privileged_escalation_blocked,• Level 1 - Master Node,"Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.3,Minimize the admission of containers wishing to share the host process ID namespace,Manual,Do not generally permit containers to be run with the hostPID flag set to true.,"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",Fetch hostPID from each pod with get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostPID}\n{end}',"Configure the Admission Controller to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_pid_disabled; compute_container_host_pid_restricted; compute_container_host_pid_not_shared; compute_container_host_pid_isolated; compute_container_host_pid_protected,• Level 1 - Master Node,Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.4,Minimize the admission of containers wishing to share the host IPC namespace,Manual,Do not generally permit containers to be run with the hostIPC flag set to true.,"A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",To fetch hostIPC from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostIPC}\n{end}',"Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ipc_disabled; compute_container_host_ipc_restricted; compute_container_host_ipc_not_shared; compute_container_host_ipc_denied; compute_container_host_ipc_protected,• Level 1 - Master Node,Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.5,Minimize the admission of containers wishing to share the host network namespace,Manual,Do not generally permit containers to be run with the hostNetwork flag set to true.,"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namespaces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",To fetch hostNetwork from each pod. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@.spec.hostNetwork}\n{end}',"Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_network_disabled; compute_container_host_network_restricted; compute_container_host_network_denied; compute_container_host_network_prohibited; compute_container_host_network_not_shared; compute_container_host_network_isolated; compute_container_host_network_blocked; compute_container_host_network_secure; compute_container_host_network_minimized; compute_container_host_network_protected,• Level 1 - Master Node,Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.6,Minimize the admission of containers with allowPrivilegeEscalation,Manual,"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext: allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation. To fetch a list of pods which allowPrivilegeEscalation run this command :- get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}'","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with securityContext: allowPrivilegeEscalation: true Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privilege_escalation_disabled; compute_container_allow_privilege_escalation_false; compute_container_privilege_escalation_restricted; compute_container_privilege_escalation_minimized; compute_container_privilege_escalation_protected,• Level 1 - Master Node,Pods defined with securityContext: allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.7,Minimize the admission of root containers,Manual,Do not generally permit containers to be run as the root user.,"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one admission control policy defined which does not permit root containers. If you need to run root containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run as the root user will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0.","Create a policy for each namespace in the cluster, ensuring that either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0, is set. Default Value: By default, there are no restrictions on the use of root containers and if a User is not specified in the image, the container will run as root. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_root_user_disabled; compute_container_root_privileges_restricted; compute_container_non_root_user_required; compute_container_root_admission_minimized; compute_container_root_execution_prohibited,• Level 2 - Master Node,Pods with containers which run as the root user will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.8,Minimize the admission of containers with the NET_RAW capability,Manual,Do not generally permit containers with the potentially dangerous NET_RAW capability.,"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability. If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run with the NET_RAW capability will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the NET_RAW capability.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the NET_RAW capability. Default Value: By default, there are no restrictions on the creation of containers with the NET_RAW capability. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_net_raw_capability_disabled; compute_container_net_raw_capability_restricted; compute_container_net_raw_capability_minimized; compute_container_net_raw_capability_denied; compute_container_net_raw_capability_prohibited,• Level 1 - Master Node,Pods with containers which run with the NET_RAW capability will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.9,Minimize the admission of containers with added capabilities,Manual,Do not generally permit containers with capabilities assigned beyond the default set.,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which require capabilities outwith the default set will not be permitted.",Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}',"Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. Default Value: By default, there are no restrictions on adding capabilities to containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_minimized; compute_container_default_capabilities_only; compute_container_added_capabilities_restricted; compute_container_capabilities_baseline_enforced; compute_container_privileged_capabilities_disabled,• Level 1 - Master Node,Pods with containers which require capabilities outwith the default set will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.10,Minimize the admission of containers with capabilities assigned,Manual,Do not generally permit containers with capabilities,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Impact: Pods with containers require capabilities to operate will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers.","Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities. Default Value: By default, there are no restrictions on the creation of containers with additional capabilities References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_restricted; compute_container_no_privileged_capabilities; compute_container_minimal_capabilities; compute_container_capabilities_disabled; compute_container_privileged_capabilities_denied,• Level 2 - Master Node,Pods with containers require capabilities to operate will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.11,Minimize the admission of Windows HostProcess Containers,Manual,Do not generally permit Windows containers to be run with the hostProcess flag set to true.,"A Windows container making use of the hostProcess flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides 'privileged access' to the Windows node. Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit hostProcess Windows containers. If you need to run Windows containers which require hostProcess, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostProcess containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostProcess containers. Default Value: By default, there are no restrictions on the creation of hostProcess containers. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_process_disabled; compute_windows_container_host_process_restricted; container_host_process_admission_minimized; container_windows_host_process_disabled; compute_windows_container_host_process_denied,• Level 1 - Master Node,Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.12,Minimize the admission of HostPath volumes,Manual,Do not generally admit containers which make use of hostPath volumes.,"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem. There should be at least one admission control policy defined which does not permit containers to mount hostPath volumes. If you need to run containers which require hostPath volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined which make use of hostPath volumes will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with hostPath volumes.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPath volumes. Default Value: By default, there are no restrictions on the creation of hostPath volumes. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_hostpath_volumes_disabled; compute_container_hostpath_volumes_restricted; compute_container_hostpath_volumes_minimized; compute_container_hostpath_volumes_denied; compute_container_hostpath_volumes_blocked,• Level 1 - Master Node,Pods defined which make use of hostPath volumes will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.13,Minimize the admission of containers which use HostPorts,Manual,Do not generally permit containers which require the use of HostPorts.,"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts. If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have hostPort sections.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPort sections. Default Value: By default, there are no restrictions on the use of HostPorts. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI",compute_container_host_ports_restricted; compute_container_host_ports_disabled; compute_container_host_ports_minimized; compute_container_host_ports_denied; compute_container_host_ports_blocked,• Level 1 - Master Node,"Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI
5.3.1,Ensure that the CNI in use supports Network Policies,Manual,There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.,Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None,"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",kubernetes_cni_network_policy_support; kubernetes_cni_network_policy_compliance; kubernetes_network_policy_cni_enabled; kubernetes_cni_network_policy_capable; kubernetes_network_policy_cni_supported,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.
5.3.2,Ensure that all Namespaces have Network Policies defined,Manual,Use network policies to isolate traffic in your cluster network.,"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces Ensure that each namespace defined in the cluster has at least one Network Policy.,"Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",kubernetes_namespace_network_policy_defined; kubernetes_namespace_traffic_isolation_enabled; kubernetes_namespace_network_policy_required; kubernetes_network_policy_namespace_coverage; kubernetes_namespace_network_restrictions_enforced,• Level 2 - Master Node,"Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/
5.4.1,Prefer using secrets as files over secrets as environment variables,Manual,Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.,"It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {'\n'}{end}' -A,"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",kubernetes_pod_secrets_as_files; kubernetes_pod_no_environment_variable_secrets; kubernetes_deployment_secrets_as_files; kubernetes_deployment_no_environment_variable_secrets; kubernetes_container_secrets_as_files; kubernetes_container_no_environment_variable_secrets,• Level 2 - Master Node,Application code which expects to read secrets in the form of environment variables would need modification,1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod
5.4.2,Consider external secret storage,Manual,"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",Review your secrets management implementation.,"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured.",secrets_manager_external_storage_authentication_required; secrets_manager_external_storage_auditing_enabled; secrets_manager_external_storage_encryption_enabled; secrets_manager_external_storage_rotation_enabled; secrets_manager_external_storage_authentication_auditing_encryption_enabled; kubernetes_secrets_external_storage_recommended; secrets_manager_external_storage_compliance_standards_met,• Level 2 - Master Node,None,
5.5.1,Configure Image Provenance using ImagePolicyWebhook admission controller,Manual,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Impact: You need to regularly maintain your provenance configuration based on container image updates.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888",compute_image_provenance_enabled; compute_image_policy_webhook_enabled; compute_image_admission_controller_enabled; compute_image_provenance_webhook_enabled; compute_image_policy_webhook_configured; compute_image_provenance_configured; compute_image_admission_controller_configured; compute_image_provenance_webhook_configured,• Level 2 - Master Node,You need to regularly maintain your provenance configuration based on container image updates.,1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888
5.7.1,Create administrative boundaries between resources using namespaces,Manual,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with 4 initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-node-lease - Namespace used for node heartbeats 4. kube-public - Namespace used for public information in a cluster References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_configured; kubernetes_namespace_resource_separation_enforced; kubernetes_namespace_security_boundaries_defined; kubernetes_namespace_isolation_policy_applied,• Level 1 - Master Node,You need to switch between namespaces for administration.,1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats
5.7.2,Ensure that the seccomp profile is set to docker/default in your pod definitions,Manual,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",Review the pod definitions in your cluster. It should create a line as below: securityContext: seccompProfile: type: RuntimeDefault,"Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_enabled; kubernetes_pod_seccomp_profile_docker_default; kubernetes_pod_security_seccomp_profile_set; kubernetes_pod_security_seccomp_profile_docker_default; kubernetes_pod_security_seccomp_profile_enabled,• Level 2 - Master Node,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/
5.7.3,Apply Security Context to Your Pods and Containers,Manual,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_configured; kubernetes_container_security_context_configured; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_privilege_escalation_disabled; kubernetes_container_privilege_escalation_disabled; kubernetes_pod_capabilities_dropped; kubernetes_container_capabilities_dropped,• Level 2 - Master Node,"If you incorrectly apply security contexts, you may have trouble running the pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
5.7.4,The default namespace should not be used,Manual,"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None","Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service","Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used",kubernetes_namespace_default_not_used; kubernetes_namespace_default_avoided; kubernetes_namespace_non_default_required; kubernetes_namespace_default_prohibited; kubernetes_namespace_custom_required,• Level 2 - Master Node,None,
1.1.1,Ensure that the --allow-privileged argument is set to false,Scored,Do not allow privileged containers.,"The privileged container has all the system capabilities, and it also lifts all the limitations enforced by the device cgroup controller. In other words, the container can then do almost everything that the host can do. This flag exists to allow special use-cases, like running Docker within Docker and hence should be avoided for production workloads.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --allow-privileged argument is set to false.,"Edit the /etc/kubernetes/config file on the master node and set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false': KUBE_ALLOW_PRIV='--allow-privileged=false' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service 13 | P a g e Impact: You will not be able to run any privileged containers. Note: A number of components used by Kubernetes clusters currently make use of privileged containers (e.g. Container Network Interface plugins). Care should be taken in ensuring that the use of such plugins is minimized and in particular any use of privileged containers outside of the kube-system namespace should be scrutinized. Where possible, review the rights required by such plugins to determine if a more fine grained permission set can be applied. Default Value: By default, privileged containers are not allowed. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/user-guide/security-context/",container_runtime_allow_privileged_disabled; container_runtime_privileged_containers_blocked; container_runtime_privileged_mode_disabled; container_runtime_no_privileged_containers; container_runtime_privileged_flag_false,• Level 1,"You will not be able to run any privileged containers. Note: A number of components used by Kubernetes clusters currently make use of privileged containers (e.g. Container Network Interface plugins). Care should be taken in ensuring that the use of such plugins is minimized and in particular any use of privileged containers outside of the kube-system namespace should be scrutinized. Where possible, review the rights required by such plugins to determine if a more fine grained permission set can be applied. Default Value: By default, privileged containers are not allowed.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/user-guide/security-context/
1.1.2,Ensure that the --anonymous-auth argument is set to false,Scored,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--anonymous-auth=false': KUBE_API_ARGS='--anonymous-auth=false' Based on your system, restart the kube-apiserver service. For example, systemctl restart kube-apiserver.service Impact: Anonymous requests will be rejected. Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests 15 | P a g e",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_auth_enabled; kubernetes_api_server_no_anonymous_access,• Level 1,"Anonymous requests will be rejected. Default Value: By default, anonymous access is enabled.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests 15 | P a g e
1.1.3,Ensure that the --basic-auth-file argument is not set,Scored,Do not use basic authentication.,"Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --basic-auth-file argument does not exist.,"Follow the documentation and configure alternate mechanisms for authentication. Then, edit the /etc/kubernetes/apiserver file on the master node and remove the '--basic- auth-file=<filename>' argument from the KUBE_API_ARGS parameter. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: You will have to configure and use alternate authentication mechanisms such as tokens and certificates. Username and password for basic authentication could no more be used. Default Value: By default, basic authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 17 | P a g e 2. https://kubernetes.io/docs/admin/authentication/#static-password-file",kubernetes_api_server_no_basic_auth_file; kubernetes_api_server_basic_auth_disabled; kubernetes_api_server_auth_file_unset; kubernetes_api_server_no_basic_auth; kubernetes_api_server_auth_file_empty,• Level 1,"You will have to configure and use alternate authentication mechanisms such as tokens and certificates. Username and password for basic authentication could no more be used. Default Value: By default, basic authentication is not set.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 17 | P a g e 2. https://kubernetes.io/docs/admin/authentication/#static-password-file
1.1.4,Ensure that the --insecure-allow-any-token argument is not set,Scored,Do not allow any insecure tokens,Accepting insecure tokens would allow any token without actually authenticating anything. User information is parsed from the token and connections are allowed.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --insecure-allow-any-token argument does not exist.,"Edit the /etc/kubernetes/apiserver file on the master node and remove the --insecure- allow-any-token argument from the KUBE_API_ARGS parameter. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, insecure tokens are not allowed. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 19 | P a g e",kubernetes_api_server_insecure_allow_any_token_disabled; kubernetes_api_server_token_authentication_restricted; kubernetes_api_server_secure_token_validation_enabled; kubernetes_api_server_insecure_token_argument_absent; kubernetes_api_server_token_security_enforced,• Level 1,"None Default Value: By default, insecure tokens are not allowed.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 19 | P a g e
1.1.5,Ensure that the --kubelet-https argument is set to true,Scored,Use https for kubelet connections.,Connections from apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --kubelet-https argument either does not exist or is set to true.,"Edit the /etc/kubernetes/apiserver file on the master node and remove the --kubelet- https argument from the KUBE_API_ARGS parameter. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: You require TLS to be configured on apiserver as well as kubelets. Default Value: By default, kubelet connections are over https. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 21 | P a g e",kubernetes_kubelet_https_enabled; kubernetes_kubelet_secure_connection_required; kubernetes_kubelet_tls_enabled; kubernetes_kubelet_encrypted_communication_required; kubernetes_kubelet_https_only_enabled,• Level 1,"You require TLS to be configured on apiserver as well as kubelets. Default Value: By default, kubelet connections are over https.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 21 | P a g e
1.1.6,Ensure that the --insecure-bind-address argument is not set,Scored,Do not bind to non-loopback insecure addresses.,"If you bind the apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to your master node. The apiserver doesn't do any authentication checking for insecure binds and neither the insecure traffic is encrypted. Hence, you should not bind the apiserver to an insecure address.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --insecure-bind-address argument does not exist or is set to 127.0.0.1.,"Edit the /etc/kubernetes/apiserver file on the master node and remove the --insecure- bind-address argument from the KUBE_API_ADDRESS parameter. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, insecure bind address is set to 127.0.0.1. 23 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_insecure_bind_address_disabled; kubernetes_api_server_loopback_bind_address_required; kubernetes_api_server_secure_bind_address_enabled; kubernetes_api_server_insecure_address_binding_blocked,• Level 1,"None Default Value: By default, insecure bind address is set to 127.0.0.1. 23 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.7,Ensure that the --insecure-port argument is set to 0,Scored,Do not bind to insecure port.,"Setting up the apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to your master node. It is assumed that firewall rules are set up such that this port is not reachable from outside of the cluster. But, as a defense in depth measure, you should not use an insecure port.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --insecure-port argument is set to 0.,"Edit the /etc/kubernetes/apiserver file on the master node and set --insecure-port=0 in the KUBE_API_PORT parameter. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: All components that use the API must connect via the secured port, authenticate themselves, and be authorized to use the API. This includes: • kube-controller-manager • kube-proxy • kube-scheduler • kubelets 25 | P a g e Default Value: By default, the insecure port is set to 8080. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_insecure_port_disabled; kubernetes_api_server_insecure_port_zero; kubernetes_api_server_secure_port_only; kubernetes_api_server_insecure_bind_disabled; kubernetes_api_server_port_zero_enforced,• Level 1,"All components that use the API must connect via the secured port, authenticate themselves, and be authorized to use the API. This includes: • kube-controller-manager • kube-proxy • kube-scheduler • kubelets 25 | P a g e Default Value: By default, the insecure port is set to 8080.",1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.8,Ensure that the --secure-port argument is not set to 0,Scored,Do not disable the secure port.,"The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535.,"Edit the /etc/kubernetes/apiserver file on the master node and either remove the -- secure-port argument from the KUBE_API_ARGS parameter or set it to a different desired port. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: You need to set the apiserver up with the right TLS certificates. Default Value: By default, port 6443 is used as the secure port. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 27 | P a g e",kubernetes_api_server_secure_port_not_disabled; kubernetes_api_server_secure_port_enabled; kubernetes_api_server_secure_port_valid; kubernetes_api_server_secure_port_configured; kubernetes_api_server_secure_port_non_zero,• Level 1,"You need to set the apiserver up with the right TLS certificates. Default Value: By default, port 6443 is used as the secure port.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 27 | P a g e
1.1.9,Ensure that the --profiling argument is set to false,Scored,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--profiling=false': KUBE_API_ARGS='--profiling=false' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: Profiling information would not be available. Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 29 | P a g e 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md",kubernetes_api_server_profiling_disabled; kubernetes_api_server_profiling_not_enabled; kubernetes_api_server_profiling_set_false; kubernetes_api_server_profiling_config_disabled; kubernetes_api_server_profiling_flag_disabled,• Level 1,"Profiling information would not be available. Default Value: By default, profiling is enabled.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 29 | P a g e 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md
1.1.10,Ensure that the --repair-malformed-updates argument is set to false,Scored,Disable fixing of malformed updates.,The apiserver will potentially attempt to fix the update requests to pass the validation even if the requests are malformed. Malformed requests are one of the potential ways to interact with a service without legitimate information. Such requests could potentially be used to sabotage apiserver responses.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --repair-malformed-updates argument is set to false.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--repair-malformed-updates=false': KUBE_API_ARGS='--repair-malformed-updates=false' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: Malformed requests from clients would be rejected. Default Value: By default, malformed updates are allowed. 31 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/15580",eks_cluster_repair_malformed_updates_disabled; eks_node_repair_malformed_updates_disabled; kubernetes_cluster_repair_malformed_updates_disabled; kubernetes_node_repair_malformed_updates_disabled; container_service_repair_malformed_updates_disabled,• Level 1,"Malformed requests from clients would be rejected. Default Value: By default, malformed updates are allowed. 31 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/15580
1.1.11,Ensure that the admission control policy is not set to AlwaysAdmit,Scored,Do not allow all requests.,Setting admission control policy to AlwaysAdmit allows all requests and do not filter any requests.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that does not include AlwaysAdmit.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to a value that does not include AlwaysAdmit. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: Only requests explicitly allowed by the admissions control policy would be served. Default Value: By default, AlwaysAdmit is used if no --admission-control flag is provided. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit 33 | P a g e",kubernetes_admission_policy_not_always_admit; kubernetes_admission_policy_restrictive; admission_control_policy_deny_all; kubernetes_admission_policy_secure; admission_policy_not_always_admit,• Level 1,"Only requests explicitly allowed by the admissions control policy would be served. Default Value: By default, AlwaysAdmit is used if no --admission-control flag is provided.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit 33 | P a g e
1.1.12,Ensure that the admission control policy is set to AlwaysPullImages,Scored,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multitenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admisssion control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that includes AlwaysPullImages.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to '--admission- control=...,AlwaysPullImages,...': KUBE_ADMISSION_CONTROL='--admission-control=...,AlwaysPullImages,...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. 35 | P a g e Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages",kubernetes_pod_admission_control_always_pull_images; kubernetes_pod_image_pull_policy_always; kubernetes_admission_controller_always_pull_images; kubernetes_pod_spec_image_pull_policy_always; kubernetes_workload_image_pull_policy_always_pull,• Level 1,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. 35 | P a g e Default Value: By default, AlwaysPullImages is not set.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages
1.1.13,Ensure that the admission control policy is set to DenyEscalatingExec,Scored,Deny execution of exec and attach commands in privileged pods.,"Setting admission control policy to DenyEscalatingExec denies exec and attach commands to pods that run with escalated privileges that allow host access. This includes pods that run as privileged, have access to the host IPC namespace, and have access to the host PID namespace.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that includes DenyEscalatingExec.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to '--admission- control=...,DenyEscalatingExec,...': KUBE_ADMISSION_CONTROL='--admission-control=...,DenyEscalatingExec,...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: exec and attach commands will not work in privileged pods. Default Value: By default, DenyEscalatingExec is not set. 37 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#denyescalatingexec",kubernetes_admission_policy_deny_escalating_exec; kubernetes_pod_exec_privileged_denied; kubernetes_admission_control_deny_privileged_exec; kubernetes_pod_security_deny_escalating_exec; kubernetes_admission_policy_exec_privilege_restricted,• Level 1,"exec and attach commands will not work in privileged pods. Default Value: By default, DenyEscalatingExec is not set. 37 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#denyescalatingexec
1.1.14,Ensure that the admission control policy is set to SecurityContextDeny,Scored,"Restrict pod level SecurityContext customization. Instead of using a customized SecurityContext for your pods, use a Pod Security Policy (PSP), which is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access.",Setting admission control policy to SecurityContextDeny denies the pod level SecurityContext customization. Any attempts to customize the SecurityContexts that are not explicitly defined in the Pod Security Policy (PSP) are blocked. This ensures that all the pods adhere to the PSP defined by your organization and you have a uniform pod level security posture.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that includes SecurityContextDeny.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to '--admission- control=...,SecurityContextDeny,...': KUBE_ADMISSION_CONTROL='--admission-control=...,SecurityContextDeny,...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None 39 | P a g e Default Value: By default, SecurityContextDeny is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny 3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac",kubernetes_pod_security_context_denied; kubernetes_admission_policy_security_context_deny; kubernetes_pod_security_policy_enforced; kubernetes_admission_control_security_context_restricted; kubernetes_pod_security_context_customization_denied,• Level 1,"None 39 | P a g e Default Value: By default, SecurityContextDeny is set.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny 3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac
1.1.15,Ensure that the admission control policy is set to NamespaceLifecycle,Scored,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that includes NamespaceLifecycle.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to '--admission- control=NamespaceLifecycle,...': KUBE_ADMISSION_CONTROL='--admission-control=NamespaceLifecycle,...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, NamespaceLifecycle is set. 41 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle",kubernetes_admission_policy_namespace_lifecycle; kubernetes_namespace_admission_policy_termination_protected; kubernetes_admission_controller_namespace_lifecycle_enabled; kubernetes_namespace_lifecycle_admission_policy_enforced; kubernetes_admission_policy_terminating_namespace_blocked,• Level 1,"None Default Value: By default, NamespaceLifecycle is set. 41 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle
1.1.16,Ensure that the --audit-log-path argument is set as appropriate,Scored,Enable auditing on kubernetes apiserver and set the desired audit log path as appropriate.,"Auditing Kubernetes apiserver provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--audit-log-path=<filename>': KUBE_API_ARGS='--audit-log-path=/var/log/apiserver/audit.log' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, auditing is not enabled. 43 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_apiserver_audit_logging_enabled; kubernetes_apiserver_audit_log_path_set; kubernetes_apiserver_audit_log_path_valid; kubernetes_apiserver_audit_logging_configured; kubernetes_apiserver_audit_log_path_customized,• Level 1,"None Default Value: By default, auditing is not enabled. 43 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.1.17,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Scored,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--audit-log-maxage=30': KUBE_API_ARGS='--audit-log-maxage=30' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 45 | P a g e 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxage_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_set; kubernetes_audit_log_retention_30d; kubernetes_api_server_audit_log_retention_compliant,• Level 1,"None Default Value: By default, auditing is not enabled.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 45 | P a g e 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.1.18,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Scored,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--audit-log-maxbackup=10': KUBE_API_ARGS='--audit-log-maxbackup=10' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, auditing is not enabled. 47 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_sufficient; kubernetes_api_server_audit_log_retention_configured; kubernetes_api_server_audit_log_backup_limit_enforced; kubernetes_api_server_audit_log_maxbackup_compliant,• Level 1,"None Default Value: By default, auditing is not enabled. 47 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.1.19,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Scored,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--audit-log-maxsize=100': KUBE_API_ARGS='--audit-log-maxsize=100' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, auditing is not enabled. 49 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_audit_log_rotation_size_configured; kubernetes_audit_log_maxsize_within_limit; kubernetes_api_server_audit_log_size_restricted,• Level 1,"None Default Value: By default, auditing is not enabled. 49 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.1.20,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Scored,Do not always authorize all requests.,"The apiserver, by default, allows all requests. You should restrict this behavior to only allow the authorization modes that you explicitly use in your environment. For example, if you don't use REST APIs in your environment, it is a good security best practice to switch off that capability.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to values other than --authorization-mode=AlwaysAllow. One such example could be as below: KUBE_API_ARGS='--authorization-mode=RBAC' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: Only authorized requests will be served. Default Value: By default, AlwaysAllow is enabled. 51 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",eks_cluster_authorization_mode_not_always_allow; eks_cluster_authorization_mode_restricted; eks_cluster_authorization_mode_secure; eks_cluster_authorization_mode_compliant; eks_cluster_authorization_mode_enforced,• Level 1,"Only authorized requests will be served. Default Value: By default, AlwaysAllow is enabled. 51 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.1.21,Ensure that the --token-auth-file parameter is not set,Scored,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist.,"Follow the documentation and configure alternate mechanisms for authentication. Then, edit the /etc/kubernetes/apiserver file on the master node and remove the '--token- auth-file=<filename>' argument from the KUBE_API_ARGS parameter. Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 53 | P a g e 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_disabled; kubernetes_api_server_no_token_auth_file; kubernetes_auth_token_file_unset; kubernetes_api_server_token_auth_removed; kubernetes_auth_token_file_disabled,• Level 1,"You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used. Default Value: By default, --token-auth-file argument is not set.",1. https://kubernetes.io/docs/admin/authentication/#static-token-file 53 | P a g e 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.22,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Scored,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate.,"Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--kubelet-certificate-authority=<ca-string>': KUBE_API_ARGS='--kubelet-certificate-authority=<ca-string>' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: You require TLS to be configured on apiserver as well as kubelets. 55 | P a g e Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_secure; kubernetes_kubelet_certificate_authority_trusted; kubernetes_kubelet_certificate_authority_verified,• Level 1,"You require TLS to be configured on apiserver as well as kubelets. 55 | P a g e Default Value: By default, --kubelet-certificate-authority argument is not set.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.1.23,Ensure that the --kubelet-client-certificate and --kubelet-client- key arguments are set as appropriate,Scored,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--kubelet-client- certificate=<path/to/client-certificate-file>' and '--kubelet-client- key=<path/to/client-key-file>': KUBE_API_ARGS='--kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file>' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: You require TLS to be configured on apiserver as well as kubelets. 57 | P a g e Default Value: By default, certificate-based kublet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured; kubernetes_kubelet_tls_authentication_enabled; kubernetes_kubelet_certificate_based_auth_required,• Level 1,"You require TLS to be configured on apiserver as well as kubelets. 57 | P a g e Default Value: By default, certificate-based kublet authentication is not set.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.1.24,Ensure that the --service-account-lookup argument is set to true,Scored,Validate service account before validating token.,"By default, the apiserver only verifies that the authentication token is valid. However, it does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --service-account-lookup argument exists and is set to true.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--service-account-lookup=true': KUBE_API_ARGS='--service-account-lookup=true' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: None Default Value: By default, --service-account-lookup argument is set to false. 59 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_service_account_pre_validation_enabled; kubernetes_api_server_service_account_lookup_before_token_validation_enabled; kubernetes_api_server_service_account_secure_validation_enabled,• Level 1,"None Default Value: By default, --service-account-lookup argument is set to false. 59 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use
1.1.25,Ensure that the admission control policy is set to PodSecurityPolicy,Scored,Reject creating pods that do not match Pod Security Policies.,A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that includes PodSecurityPolicy.,"Follow the documentation and create Pod Security Policy objects as per your environment. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to '--admission- control=...,PodSecurityPolicy,...': KUBE_ADMISSION_CONTROL='--admission-control=...,PodSecurityPolicy,...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: The policy objects must be created and granted before pod creation would be allowed. 61 | P a g e Default Value: By default, PodSecurityPolicy is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#podsecuritypolicy 3. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod- security-policies",kubernetes_admission_control_policy_pod_security_policy_enabled; kubernetes_pod_security_policy_admission_control_enabled; kubernetes_admission_controller_pod_security_policy_enforced; kubernetes_pod_security_policy_admission_controller_required; kubernetes_admission_policy_pod_security_policy_restricted,• Level 1,"The policy objects must be created and granted before pod creation would be allowed. 61 | P a g e Default Value: By default, PodSecurityPolicy is not set.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#podsecuritypolicy 3. https://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod- security-policies
1.1.26,Ensure that the --service-account-key-file argument is set as appropriate,Scored,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--service-account-key-file=<filename>': KUBE_API_ARGS='--service-account-key-file=<filename>' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy. 63 | P a g e Default Value: By default, --service-account-key-file argument is not set, and the private key from the TLS serving certificate is used. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_explicitly_set,• Level 1,"The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy. 63 | P a g e Default Value: By default, --service-account-key-file argument is not set, and the private key from the TLS serving certificate is used.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.1.27,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Scored,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to include '--etcd-certfile=<path/to/client- certificate-file>' and '--etcd-keyfile=<path/to/client-key-file>': KUBE_API_ARGS='... --etcd-certfile=<path/to/client-certificate-file> --etcd- keyfile=<path/to/client-key-file> ...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: TLS and client certificate authentication must be configured for etcd. 65 | P a g e Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",etcd_server_tls_encryption_enabled; etcd_server_certfile_configured; etcd_server_keyfile_configured; etcd_client_connection_tls_enabled; etcd_certfile_keyfile_valid_pair; etcd_tls_certificate_expiry_valid; etcd_tls_private_key_permissions_secure; etcd_tls_certificate_permissions_secure,• Level 1,"TLS and client certificate authentication must be configured for etcd. 65 | P a g e Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.1.28,Ensure that the admission control policy is set to ServiceAccount,Scored,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --admission-control argument is set to a value that includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_ADMISSION_CONTROL parameter to '--admission- control=...,ServiceAccount,...': KUBE_ADMISSION_CONTROL='--admission-control=...,ServiceAccount,...' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: The ServiceAccount objects must be created and granted before pod creation would be allowed. Default Value: By default, ServiceAccount is not set. 67 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_admission_control_policy_service_account; kubernetes_service_account_admission_control_enabled; admission_control_policy_service_account_required; service_account_admission_control_enforced; kubernetes_pod_admission_service_account_restricted,• Level 1,"The ServiceAccount objects must be created and granted before pod creation would be allowed. Default Value: By default, ServiceAccount is not set. 67 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.1.29,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Scored,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to include '--tls-cert-file=<path/to/tls-certificate- file>' and '--tls-private-key-file=<path/to/tls-key-file>': KUBE_API_ARGS='--tls-cert-file=<path/to/tls-certificate-file> --tls-private- key-file=<path/to/tls-key-file>' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. 69 | P a g e Default Value: By default, --tls-cert-file and --tls-private-key-file arguments are not set. If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_cert_and_key_configured,• Level 1,"TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. 69 | P a g e Default Value: By default, --tls-cert-file and --tls-private-key-file arguments are not set. If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.1.30,Ensure that the --client-ca-file argument is set as appropriate,Scored,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.",Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to include '--client-ca-file=<path/to/client-ca-file>': KUBE_API_ARGS='--client-ca-file=<path/to/client-ca-file>' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. Default Value: By default, --client-ca-file argument is not set. 71 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_configured; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_certificate_validation_enabled; kubernetes_api_server_secure_client_authentication_required; kubernetes_api_server_client_ca_file_specified,• Level 1,"TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. Default Value: By default, --client-ca-file argument is not set. 71 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.1.31,Ensure that the --etcd-cafile argument is set as appropriate,Scored,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file.,Run the following command on the master node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to include '--etcd-cafile=<path/to/ca-file>': KUBE_API_ARGS='--etcd-cafile=<path/to/ca-file>' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Impact: TLS and client certificate authentication must be configured for etcd. Default Value: By default, --etcd-cafile is not set. 73 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_cafile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_connection_required,• Level 1,"TLS and client certificate authentication must be configured for etcd. Default Value: By default, --etcd-cafile is not set. 73 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.1,Ensure that the --profiling argument is set to false,Scored,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.",Run the following command on the master node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the /etc/kubernetes/scheduler file on the master node and set the KUBE_SCHEDULER_ARGS parameter to '--profiling=false': KUBE_SCHEDULER_ARGS='--profiling=false' Based on your system, restart the kube-scheduler service. For example: systemctl restart kube-scheduler.service Impact: Profiling information would not be available. 75 | P a g e Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md",kubernetes_api_server_profiling_disabled; kubernetes_api_server_profiling_set_false; kubernetes_api_server_no_profiling_enabled; kubernetes_api_server_profiling_config_disabled; kubernetes_api_server_profiling_argument_false,• Level 1,"Profiling information would not be available. 75 | P a g e Default Value: By default, profiling is enabled.",1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Scored,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.",Run the following command on the master node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the /etc/kubernetes/controller-manager file on the master node and set the KUBE_CONTROLLER_MANAGER_ARGS parameter to '--terminated-pod-gc- threshold=<appropriate-number>': KUBE_CONTROLLER_MANAGER_ARGS='--terminated-pod-gc-threshold=10' Based on your system, restart the kube-controller-manager service. For example: systemctl restart kube-controller-manager.service Impact: None 77 | P a g e Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_pod_garbage_collection_enabled; kubernetes_pod_termination_threshold_set; kubernetes_pod_gc_threshold_configured; kubernetes_pod_termination_cleanup_active; kubernetes_pod_garbage_collection_threshold_appropriate,• Level 1,"None 77 | P a g e Default Value: By default, --terminated-pod-gc-threshold is set to 12500.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Scored,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.",Run the following command on the master node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the /etc/kubernetes/controller-manager file on the master node and set the KUBE_CONTROLLER_MANAGER_ARGS parameter to '--profiling=false': KUBE_CONTROLLER_MANAGER_ARGS='--profiling=false' Based on your system, restart the kube-controller-manager service. For example: systemctl restart kube-controller-manager.service Impact: Profiling information would not be available. Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 79 | P a g e 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md",kubernetes_api_server_profiling_disabled; kubernetes_api_server_profiling_set_false; kubernetes_api_server_profiling_not_enabled; kubernetes_api_server_profiling_config_disabled; kubernetes_api_server_profiling_flag_disabled,• Level 1,"Profiling information would not be available. Default Value: By default, profiling is enabled.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 79 | P a g e 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md
1.3.3,Ensure that the --insecure-experimental-approve-all-kubelet-csrs- for-group argument is not set,Scored,Do not accept all certificates.,Setting the --insecure-experimental-approve-all-kubelet-csrs-for-group flag circumvents the desired “approval” process. All the certificates are auto-approved without checking their integrity. This flag is meant to be used for development and testing purposes only and hence should not be used in the production.,Run the following command on the master node: ps -ef | grep kube-controller-manager Verify that the --insecure-experimental-approve-all-kubelet-csrs-for-group argument is not set.,"Edit the /etc/kubernetes/controller-manager file on the master node and remove the -- insecure-experimental-approve-all-kubelet-csrs-for-group argument from the KUBE_CONTROLLER_MANAGER_ARGS parameter. Based on your system, restart the kube-controller-manager service. For example: systemctl restart kube-controller-manager.service Impact: Invalid certificates will be rejected. Default Value: By default, --insecure-experimental-approve-all-kubelet-csrs-for-group is not set. 81 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#auto-approval",kubernetes_cluster_no_insecure_approve_all_csrs; kubernetes_cluster_secure_certificate_signing; kubernetes_cluster_csr_approval_restricted; kubernetes_cluster_no_experimental_csr_approval; kubernetes_cluster_csr_validation_enabled,• Level 1,"Invalid certificates will be rejected. Default Value: By default, --insecure-experimental-approve-all-kubelet-csrs-for-group is not set. 81 | P a g e",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#auto-approval
1.3.4,Ensure that the --use-service-account-credentials argument is set to true,Scored,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service-account- credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks.",Run the following command on the master node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the /etc/kubernetes/controller-manager file on the master node and set the KUBE_CONTROLLER_MANAGER_ARGS parameter to --use-service-account- credentials=true: KUBE_CONTROLLER_MANAGER_ARGS='--use-service-account-credentials=true' Based on your system, restart the kube-controller-manager service. For example: systemctl restart kube-controller-manager.service Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube-system 83 | P a g e namespace automatically with default roles and rolebindings that are auto-reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller-role- bindings.yaml files for the RBAC roles. Default Value: By default, --use-service-account-credentials is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller- roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",gke_cluster_use_service_account_credentials_enabled; gke_controller_service_account_credentials_required; gke_cluster_service_account_credentials_enabled; gke_controller_individual_service_account_credentials; gke_cluster_service_account_credentials_true,• Level 1,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube-system 83 | P a g e namespace automatically with default roles and rolebindings that are auto-reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller-role- bindings.yaml files for the RBAC roles. Default Value: By default, --use-service-account-credentials is not set.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller- roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.5,Ensure that the --service-account-private-key-file argument is set as appropriate,Scored,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private-key-file as appropriate.",Run the following command on the master node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the /etc/kubernetes/controller-manager file on the master node and set the KUBE_CONTROLLER_MANAGER_ARGS parameter to --service-account-private-key- file=<filename>: KUBE_CONTROLLER_MANAGER_ARGS='--service-account-private-key-file=<filename>' Based on your system, restart the kube-controller-manager service. For example: systemctl restart kube-controller-manager.service Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy. Default Value: By default, --service-account-private-key-file is not set. 85 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_specified; kubernetes_controller_manager_service_account_private_key_file_valid; kubernetes_controller_manager_service_account_private_key_file_secure,• Level 1,"You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy. Default Value: By default, --service-account-private-key-file is not set. 85 | P a g e",1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.6,Ensure that the --root-ca-file argument is set as appropriate,Scored,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server.,Run the following command on the master node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the /etc/kubernetes/controller-manager file on the master node and set the KUBE_CONTROLLER_MANAGER_ARGS parameter to include --root-ca-file=<file>: KUBE_CONTROLLER_MANAGER_ARGS='--root-ca-file=<file>' Based on your system, restart the kube-controller-manager service. For example: systemctl restart kube-controller-manager.service Impact: You need to setup and maintain root certificate authority file. Default Value: By default, --root-ca-file is not set 87 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_pod_root_ca_file_configured; kubernetes_pod_api_server_certificate_verified; kubernetes_pod_secure_api_connection_enabled; kubernetes_pod_root_ca_file_valid; kubernetes_pod_api_server_trusted_certificate,• Level 1,"You need to setup and maintain root certificate authority file. Default Value: By default, --root-ca-file is not set 87 | P a g e",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.4.1,Ensure that the apiserver file permissions are set to 644 or more restrictive,Scored,Ensure that the apiserver file has permissions of 644 or more restrictive.,The apiserver file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %a /etc/kubernetes/apiserver Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the master node. For example, chmod 644 /etc/kubernetes/apiserver Impact: None Default Value: By default, apiserver file has permissions of 644. 89 | P a g e References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_apiserver_file_permissions_restrictive; kubernetes_apiserver_file_permissions_644_or_stricter; kubernetes_apiserver_file_permissions_secure; kubernetes_apiserver_file_permissions_compliant; kubernetes_apiserver_file_permissions_cis_benchmark,• Level 1,"None Default Value: By default, apiserver file has permissions of 644. 89 | P a g e",1. https://kubernetes.io/docs/admin/kube-apiserver/
1.4.2,Ensure that the apiserver file ownership is set to root:root,Scored,Ensure that the apiserver file ownership is set to root:root.,The apiserver file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %U:%G /etc/kubernetes/apiserver Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the master node. For example, chown root:root /etc/kubernetes/apiserver Impact: None Default Value: By default, apiserver file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 91 | P a g e",kubernetes_apiserver_file_ownership_root; kubernetes_apiserver_file_ownership_root_root; kubernetes_apiserver_file_owner_root; kubernetes_apiserver_file_group_root; kubernetes_apiserver_file_permissions_root,• Level 1,"None Default Value: By default, apiserver file ownership is set to root:root.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 91 | P a g e
1.4.3,Ensure that the config file permissions are set to 644 or more restrictive,Scored,Ensure that the config file has permissions of 644 or more restrictive.,The config file controls various parameters that set the behavior of various components of the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %a /etc/kubernetes/config Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the master node. For example, chmod 644 /etc/kubernetes/config Impact: None Default Value: By default, config file has permissions of 644. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 93 | P a g e",config_file_permissions_644_or_restrictive; config_file_permissions_not_world_writable; config_file_permissions_not_group_writable; config_file_permissions_not_world_readable; config_file_permissions_not_group_readable; config_file_permissions_not_world_executable; config_file_permissions_not_group_executable,• Level 1,"None Default Value: By default, config file has permissions of 644.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 93 | P a g e
1.4.4,Ensure that the config file ownership is set to root:root,Scored,Ensure that the config file ownership is set to root:root.,The config file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %U:%G /etc/kubernetes/config Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the master node. For example, chown root:root /etc/kubernetes/config Impact: None Default Value: By default, config file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 95 | P a g e",config_file_root_ownership; config_file_root_group_ownership; config_file_ownership_root_root; config_file_secure_ownership; config_file_root_user_ownership; config_file_root_group_ownership_set; config_file_ownership_restricted_to_root,• Level 1,"None Default Value: By default, config file ownership is set to root:root.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 95 | P a g e
1.4.5,Ensure that the scheduler file permissions are set to 644 or more restrictive,Scored,Ensure that the scheduler file has permissions of 644 or more restrictive.,The scheduler file controls various parameters that set the behavior of the kube- scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %a /etc/kubernetes/scheduler Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the master node. For example, chmod 644 /etc/kubernetes/scheduler Impact: None Default Value: By default, scheduler file has permissions of 644. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 97 | P a g e",compute_scheduler_file_permissions_644_or_restrictive; compute_scheduler_file_permissions_restrictive; scheduler_file_permissions_644_or_stricter; scheduler_file_permissions_restrictive; compute_scheduler_file_permissions_strict,• Level 1,"None Default Value: By default, scheduler file has permissions of 644.",1. https://kubernetes.io/docs/admin/kube-scheduler/ 97 | P a g e
1.4.6,Ensure that the scheduler file ownership is set to root:root,Scored,Ensure that the scheduler file ownership is set to root:root.,The scheduler file controls various parameters that set the behavior of the kube- scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %U:%G /etc/kubernetes/scheduler Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the master node. For example, chown root:root /etc/kubernetes/scheduler Impact: None Default Value: By default, scheduler file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",scheduler_file_ownership_root_root; scheduler_file_root_ownership; scheduler_root_ownership; scheduler_file_secure_ownership; scheduler_file_root_user_group,• Level 1,"None Default Value: By default, scheduler file ownership is set to root:root.",1. https://kubernetes.io/docs/admin/kube-scheduler/
1.4.7,Ensure that the etcd.conf file permissions are set to 644 or more restrictive,Scored,Ensure that the etcd.conf file has permissions of 644 or more restrictive.,The etcd.conf file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %a /etc/etcd/etcd.conf Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the master node. For example, chmod 644 /etc/etcd/etcd.conf Impact: None Default Value: By default, etcd.conf file has permissions of 644. References: 1. https://coreos.com/etcd 101 | P a g e 2. https://kubernetes.io/docs/admin/etcd/",etcd_conf_file_permissions_644_or_restrictive; etcd_conf_file_permissions_restrictive; etcd_conf_file_permissions_secure; etcd_conf_file_permissions_compliant; etcd_conf_file_permissions_cis_benchmark,• Level 1,"None Default Value: By default, etcd.conf file has permissions of 644.",1. https://coreos.com/etcd 101 | P a g e 2. https://kubernetes.io/docs/admin/etcd/
1.4.8,Ensure that the etcd.conf file ownership is set to root:root,Scored,Ensure that the etcd.conf file ownership is set to root:root.,The etcd.conf file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %U:%G /etc/etcd/etcd.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the master node. For example, chown root:root /etc/etcd/etcd.conf Impact: None Default Value: By default, etcd.conf file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/ 103 | P a g e",etcd_config_file_root_ownership; etcd_conf_root_user_group; etcd_config_root_ownership; etcd_file_root_ownership; etcd_conf_file_root_owner,• Level 1,"None Default Value: By default, etcd.conf file ownership is set to root:root.",1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/ 103 | P a g e
1.4.9,Ensure that the flanneld file permissions are set to 644 or more restrictive,Scored,Ensure that the flanneld file has permissions of 644 or more restrictive.,The flanneld file controls various parameters that set the behavior of the flanneld service in the master node. Flannel is one of the various options for a simple overlay network. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %a /etc/sysconfig/flanneld Verify that the permissions are 644 or more restrictive. Note: Flannel is an optional component of Kubernetes. If you are not using Flannel then this requirement is not applicable. If you are using any other option for configuring your networking, please extend this recommendation to cover important configuration files as appropriate.","Run the below command (based on the file location on your system) on the master node. For example, chmod 644 /etc/sysconfig/flanneld Impact: None 105 | P a g e Default Value: Note: Flannel is an optional component of Kubernetes and there are other alternatives that might be used in its place. Please checkout the Kubernetes documentation for other options. If you are using Flannel for setting up your networking then, by default, flanneld file has permissions of 644. References: 1. https://coreos.com/flannel/docs/latest/ 2. https://kubernetes.io/docs/concepts/cluster-administration/networking/#flannel",flannel_file_permissions_restrictive; flannel_file_permissions_644_or_stricter; flannel_config_file_permissions_restrictive; flannel_config_file_permissions_644_or_stricter; flannel_file_permissions_secure; flannel_config_file_permissions_secure,• Level 1,"None 105 | P a g e Default Value: Note: Flannel is an optional component of Kubernetes and there are other alternatives that might be used in its place. Please checkout the Kubernetes documentation for other options. If you are using Flannel for setting up your networking then, by default, flanneld file has permissions of 644.",1. https://coreos.com/flannel/docs/latest/ 2. https://kubernetes.io/docs/concepts/cluster-administration/networking/#flannel
1.4.10,Ensure that the flanneld file ownership is set to root:root,Scored,Ensure that the flanneld file ownership is set to root:root.,The flanneld file controls various parameters that set the behavior of the flanneld service in the master node. Flannel is one of the various options for a simple overlay network. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the master node. For example, stat -c %U:%G /etc/sysconfig/flanneld Verify that the ownership is set to root:root. Note: Flannel is an optional component of Kubernetes. If you are not using Flannel then this requirement is not applicable. If you are using any other option for configuring your networking, please extend this recommendation to cover important configuration files as appropriate.","Run the below command (based on the file location on your system) on the master node. For example, chown root:root /etc/sysconfig/flanneld Impact: None 107 | P a g e Default Value: Note: Flannel is an optional component of Kubernetes and there are other alternatives that might be used in its place. Please checkout the Kubernetes documentation for other options. If you are using Flannel for setting up your networking then, by default, flanneld file ownership is set to root:root. References: 1. https://coreos.com/flannel/docs/latest/ 2. https://kubernetes.io/docs/concepts/cluster-administration/networking/#flannel",flannel_file_root_ownership; flannel_config_root_ownership; flanneld_file_secure_ownership; flanneld_config_secure_ownership; flannel_daemon_file_root_ownership,• Level 1,"None 107 | P a g e Default Value: Note: Flannel is an optional component of Kubernetes and there are other alternatives that might be used in its place. Please checkout the Kubernetes documentation for other options. If you are using Flannel for setting up your networking then, by default, flanneld file ownership is set to root:root.",1. https://coreos.com/flannel/docs/latest/ 2. https://kubernetes.io/docs/concepts/cluster-administration/networking/#flannel
1.4.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Scored,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world.,"On the etcd server node, get the etcd data directory, passed as an argument --data-dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd/default.etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data-dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd/default.etcd Impact: None 109 | P a g e Default Value: By default, etcd data directory has permissions of 700. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_restrictive; etcd_data_directory_permissions_700_or_stricter; etcd_directory_permissions_restrictive; etcd_directory_permissions_700_or_less; etcd_data_directory_permissions_secure,• Level 1,"None 109 | P a g e Default Value: By default, etcd data directory has permissions of 700.",1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.4.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Scored,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd.,"On the etcd server node, get the etcd data directory, passed as an argument --data-dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd/default.etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data-dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd/default.etcd Impact: None Default Value: By default, etcd data directory ownership is set to etcd:etcd. 111 | P a g e References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_ownership_etcd_etcd; etcd_directory_permissions_etcd_etcd; etcd_data_directory_correct_ownership; etcd_directory_ownership_secure; etcd_data_directory_user_group_etcd,• Level 1,"None Default Value: By default, etcd data directory ownership is set to etcd:etcd. 111 | P a g e",1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.5.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Scored,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Impact: Client connections only over TLS would be served. Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 113 | P a g e",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_tls_certificates_valid; etcd_service_tls_secure_configuration,• Level 1,"Client connections only over TLS would be served. Default Value: By default, TLS encryption is not set.",1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 113 | P a g e
1.5.2,Ensure that the --client-cert-auth argument is set to true,Scored,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd envrironment file (for example, /etc/etcd/etcd.conf) on the etcd server node and set the ETCD_CLIENT_CERT_AUTH parameter to 'true': ETCD_CLIENT_CERT_AUTH='true' Edit the etcd startup file (for example, /etc/systemd/system/multi- user.target.wants/etcd.service) and configure the startup parameter for --client- cert-auth and set it to \'${ETCD_CLIENT_CERT_AUTH}\': ExecStart=/bin/bash -c 'GOMAXPROCS=$(nproc) /usr/bin/etcd -- name=\'${ETCD_NAME}\' --data-dir=\'${ETCD_DATA_DIR}\' --listen-client- urls=\'${ETCD_LISTEN_CLIENT_URLS}\' --client-cert- auth=\'${ETCD_CLIENT_CERT_AUTH}\'' Based on your system, reload the daemon and restart the etcd service. For example, systemctl daemon-reload systemctl restart etcd.service Impact: All clients attempting to access the etcd server will require a valid client certificate. 115 | P a g e Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_auth_enabled; etcd_client_cert_auth_required; etcd_tls_client_authentication_enabled; etcd_client_certificate_auth_enabled; etcd_auth_client_cert_required,• Level 1,"All clients attempting to access the etcd server will require a valid client certificate. 115 | P a g e Default Value: By default, the etcd service can be queried by unauthenticated clients.",1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
1.5.3,Ensure that the --auto-tls argument is not set to true,Scored,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd environment file (for example, /etc/etcd/etcd.conf) on the etcd server node and comment out the ETCD_AUTO_TLS parameter. #ETCD_AUTO_TLS='true' Edit the etcd startup file (for example, /etc/systemd/system/multi- user.target.wants/etcd.service) and remove the startup parameter for --auto-tls. Based on your system, reload the daemon and restart the etcd service. For example, systemctl daemon-reload systemctl restart etcd.service Impact: Clients will not be able to use self-signed certificates for TLS. Default Value: By default, --auto-tls is set to false. 117 | P a g e References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_domain_self_signed_certificates_disabled; cloud_cdn_domain_tls_certificate_valid; cloud_cdn_domain_tls_managed_certificates_enabled,• Level 1,"Clients will not be able to use self-signed certificates for TLS. Default Value: By default, --auto-tls is set to false. 117 | P a g e",1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
1.5.4,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Scored,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Impact: etcd cluster peers would need to set up TLS for their communication. Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. 119 | P a g e References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_peer_cert_file_set; etcd_peer_key_file_set; etcd_peer_tls_encryption_enabled; etcd_peer_cert_file_configured; etcd_peer_key_file_configured,• Level 1,"etcd cluster peers would need to set up TLS for their communication. Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. 119 | P a g e",1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
1.5.5,Ensure that the --peer-client-cert-auth argument is set to true,Scored,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd environment file (for example, /etc/etcd/etcd.conf) on the etcd server node and set the ETCD_PEER_CLIENT_CERT_AUTH parameter to 'true': ETCD_PEER_CLIENT_CERT_AUTH='true' Edit the etcd startup file (for example, /etc/systemd/system/multi- user.target.wants/etcd.service) and configure the startup parameter for --peer- client-cert-auth and set it to \'${ETCD_PEER_CLIENT_CERT_AUTH}\': ExecStart=/bin/bash -c 'GOMAXPROCS=$(nproc) /usr/bin/etcd -- name=\'${ETCD_NAME}\' --data-dir=\'${ETCD_DATA_DIR}\' --listen-client- urls=\'${ETCD_LISTEN_CLIENT_URLS}\' --peer-client-cert- auth=\'${ETCD_PEER_CLIENT_CERT_AUTH}\'' Based on your system, reload the daemon and restart the etcd service. For example, systemctl daemon-reload systemctl restart etcd.service 121 | P a g e Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication. Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client- cert-auth",etcd_peer_client_cert_auth_enabled; etcd_peer_authentication_required; etcd_client_cert_auth_enabled; etcd_peer_tls_auth_enabled; etcd_secure_peer_communication_enabled,• Level 1,"All peers attempting to communicate with the etcd server will require a valid client certificate for authentication. Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false.",1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client- cert-auth
1.5.6,Ensure that the --peer-auto-tls argument is not set to true,Scored,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self- signed certificates for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd environment file (for example, /etc/etcd/etcd.conf) on the etcd server node and comment out the ETCD_PEER_AUTO_TLS parameter: #ETCD_PEER_AUTO_TLS='true' Edit the etcd startup file (for example, /etc/systemd/system/multi- user.target.wants/etcd.service) and remove the startup parameter for --peer-auto- tls. Based on your system, reload the daemon and restart the etcd service. For example, systemctl daemon-reload systemctl restart etcd.service Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication. 123 | P a g e Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",compute_cluster_peer_auto_tls_disabled; compute_cluster_peer_tls_manual_certificates; compute_cluster_peer_tls_self_signed_disabled; compute_cluster_peer_tls_auto_generated_disabled; compute_cluster_peer_tls_custom_certificates_required,• Level 1,"All peers attempting to communicate with the etcd server will require a valid client certificate for authentication. 123 | P a g e Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false.",1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
1.5.7,Ensure that the --wal-dir argument is set as appropriate,Scored,Store etcd logs separately from etcd data.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be mixed with log data. Keeping the log data separate from the etcd data also ensures that those two types of data could individually be safeguarded. Also, you could use a centralized and remote log directory for persistent logging. Additionally, this separation also helps to avoid IO competition between logging and other IO operations.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that --wal-dir argument exists, and it is set as appropriate. At the minimum, it should not be set to the same directory as set for --data-dir argument.","Edit the etcd environment file (for example, /etc/etcd/etcd.conf) on the etcd server node and set the ETCD_WAL_DIR parameter as appropriate: ETCD_WAL_DIR='<dir-name>' Edit the etcd startup file (for example, /etc/systemd/system/multi- user.target.wants/etcd.service) and configure the startup parameter for --wal-dir and set it to \'${ETCD_WAL_DIR}\': ExecStart=/bin/bash -c 'GOMAXPROCS=$(nproc) /usr/bin/etcd -- name=\'${ETCD_NAME}\' --data-dir=\'${ETCD_DATA_DIR}\' --listen-client- urls=\'${ETCD_LISTEN_CLIENT_URLS}\' --wal-dir=\'${ETCD_WAL_DIR}\'' Based on your system, reload the daemon and restart the etcd service. For example, systemctl daemon-reload systemctl restart etcd.service 125 | P a g e Impact: None Default Value: By default, --wal-dir argument is not set. References: 1. https://kubernetes.io/docs/admin/etcd/ 2. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#wal-dir 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir",etcd_cluster_wal_dir_configured; etcd_cluster_logs_separate_from_data; etcd_cluster_wal_dir_appropriate; etcd_cluster_log_storage_isolated; etcd_cluster_wal_dir_valid_path,• Level 1,"None Default Value: By default, --wal-dir argument is not set.",1. https://kubernetes.io/docs/admin/etcd/ 2. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#wal-dir 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir
1.5.8,Ensure that the --max-wals argument is set to 0,Scored,Do not auto rotate logs.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. You should avoid automatic log rotation and instead safeguard the logs in a centralized repository or through a separate log management system.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that --max-wals argument exists and it is set to 0.,"Edit the etcd environment file (for example, /etc/etcd/etcd.conf) on the etcd server node and set the ETCD_MAX_WALS parameter to 0: ETCD_MAX_WALS='0' Edit the etcd startup file (for example, /etc/systemd/system/multi- user.target.wants/etcd.service) and configure the startup parameter for --max-wals and set it to \'${ETCD_MAX_WALS}\': ExecStart=/bin/bash -c 'GOMAXPROCS=$(nproc) /usr/bin/etcd -- name=\'${ETCD_NAME}\' --data-dir=\'${ETCD_DATA_DIR}\' --listen-client- urls=\'${ETCD_LISTEN_CLIENT_URLS}\' --max-walsr=\'${ETCD_MAX_WALS}\'' Based on your system, reload the daemon and restart the etcd service. For example, systemctl daemon-reload systemctl restart etcd.service Impact: You will have to manage log rotation and archiving. 127 | P a g e Default Value: By default, --max-wals argument is set to 5. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#max-wals 2. https://kubernetes.io/docs/admin/etcd/",etcd_cluster_max_wals_disabled; etcd_log_rotation_disabled; etcd_max_wals_zero; etcd_auto_rotation_disabled; etcd_wal_retention_disabled,• Level 1,"You will have to manage log rotation and archiving. 127 | P a g e Default Value: By default, --max-wals argument is set to 5.",1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#max-wals 2. https://kubernetes.io/docs/admin/etcd/
1.5.9,Ensure that a unique Certificate Authority is used for etcd,Not Scored,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate used by Kubernetes. Run the following command on the etcd server node: ps -ef | grep etcd Review the file referenced by the --trusted-ca-file argument and ensure that the referenced CA is not the same one as is used for management of the overall Kubernetes cluster.,Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required. 129 | P a g e Default Value: NA References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html,kubernetes_etcd_unique_certificate_authority; etcd_certificate_authority_separate_from_kubernetes; etcd_ca_not_shared_with_kubernetes; kubernetes_etcd_ca_distinct; etcd_ca_unique_per_cluster,• Level 2,Additional management of the certificates and keys for the dedicated certificate authority will be required. 129 | P a g e Default Value: NA,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.6.1,Ensure that the cluster-admin role is only used where required,Not Scored,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster-admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Remove any unneeded clusterrolebindings: 131 | P a g e kubectl delete clusterrolebinding [name] Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components. Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",kubernetes_role_no_cluster_admin; kubernetes_role_cluster_admin_restricted; kubernetes_role_cluster_admin_minimal_usage; kubernetes_role_cluster_admin_least_privilege; kubernetes_role_cluster_admin_required_only,• Level 1,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components. Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
1.6.2,Create Pod Security Policies for your cluster,Not Scored,Create and enforce Pod Security Policies for your cluster.,A Pod Security Policy is a cluster-level resource that controls the actions that a pod can perform and what it has the ability to access. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to be accepted into the system. Pod Security Policies are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions.,Run the below command and review the Pod Security Policies enforced on the cluster. kubectl get psp Ensure that these policies are configured as per your security requirements.,"Follow the documentation and create and enforce Pod Security Policies for your cluster. Additionally, you could refer the 'CIS Security Benchmark for Docker' and follow the suggested Pod Security Policies for your environment. Impact: Pods must align with the Pod Security Policies enforced on the cluster. Default Value: By default, Pod Security Policies are not created. References: 1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/ 2. https://benchmarks.cisecurity.org/downloads/browse/index.cfm?category=bench marks.servers.virtualization.docker 133 | P a g e",kubernetes_cluster_pod_security_policy_enabled; kubernetes_cluster_pod_security_policy_enforced; kubernetes_cluster_pod_security_policy_defined; kubernetes_cluster_pod_security_policy_applied; kubernetes_cluster_pod_security_policy_restrictive,• Level 1,"Pods must align with the Pod Security Policies enforced on the cluster. Default Value: By default, Pod Security Policies are not created.",1. https://kubernetes.io/docs/concepts/policy/pod-security-policy/ 2. https://benchmarks.cisecurity.org/downloads/browse/index.cfm?category=bench marks.servers.virtualization.docker 133 | P a g e
1.6.3,Create administrative boundaries between resources using namespaces,Not Scored,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Impact: You need to switch between namespaces for administration. Default Value: By default, Kubernetes starts with two initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 135 | P a g e References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/ 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_enforced; kubernetes_namespace_resource_separation_required; kubernetes_namespace_default_deny_policy; kubernetes_namespace_network_policies_enforced; kubernetes_namespace_rbac_restricted; kubernetes_namespace_resource_quotas_set; kubernetes_namespace_pod_security_policies_enabled; kubernetes_namespace_label_selector_restrictions; kubernetes_namespace_immutable_labels_required,• Level 1,"You need to switch between namespaces for administration. Default Value: By default, Kubernetes starts with two initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 135 | P a g e",1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/ 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html
1.6.4,Create network segmentation using Network Policies,Not Scored,Use network policies to isolate your cluster network.,Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define whitelist rules which allow traffic to the selected pods in addition to what is allowed by the isolation policy for a given namespace.,Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get pods --namespace=kube-system Ensure that these NetworkPolicy objects are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create NetworkPolicy objects as you need them. Impact: You need a networking solution which supports NetworkPolicy - simply creating the resource without a controller to implement it will have no effect. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 137 | P a g e 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network- policy/",kubernetes_network_policy_segmentation_enabled; kubernetes_network_policy_isolation_required; kubernetes_network_policy_traffic_restricted; kubernetes_network_policy_ingress_egress_controlled; kubernetes_network_policy_default_deny_enabled; kubernetes_network_policy_pod_isolation_required; kubernetes_network_policy_namespace_isolation_required; kubernetes_network_policy_minimal_access_required,• Level 2,"You need a networking solution which supports NetworkPolicy - simply creating the resource without a controller to implement it will have no effect. Default Value: By default, network policies are not created.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 137 | P a g e 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network- policy/
1.6.5,Avoid using Kubernetes Secrets,Not Scored,Avoid using Kubernetes secret.,"Kubernetes objects of type secret are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys. Its current implementation is very basic. It has plenty of risks as highlighted in the reference links including storing secrets as plaintext. Avoid using Kubernetes secrets until you have devised a mechanism to protect them using your own means.",Run the below command and review if there are any secret objects created in the cluster. kubectl get secrets Ensure that these secret objects are the ones you need and are adequately administered as per your requirements.,"Use other mechanisms such as vaults to manage your cluster secrets. Impact: You need to use other mechanisms for managing secrets in your cluster. Default Value: By default, Kubernetes automatically creates secrets which contain credentials for accessing the API and it automatically modifies your pods to use this type of secret. Please note that those default token secrets are automatically created and deleting them won't be of any use, because Kubernetes will just recreate them. References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#risks 2. https://github.com/kubernetes/kubernetes/issues/10439 139 | P a g e 3. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/secrets.md",kubernetes_secret_avoid_usage; kubernetes_secret_not_used; kubernetes_secret_disabled; kubernetes_secret_prohibited; kubernetes_secret_restricted,• Level 2,"You need to use other mechanisms for managing secrets in your cluster. Default Value: By default, Kubernetes automatically creates secrets which contain credentials for accessing the API and it automatically modifies your pods to use this type of secret. Please note that those default token secrets are automatically created and deleting them won't be of any use, because Kubernetes will just recreate them.",1. https://kubernetes.io/docs/concepts/configuration/secret/#risks 2. https://github.com/kubernetes/kubernetes/issues/10439 139 | P a g e 3. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/secrets.md
1.6.6,Ensure that the seccomp profile is set to docker/default in your pod definitions,Not Scored,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.",Review the pod definitions in your cluster. It should create a line as below: annotations: seccomp.security.alpha.kubernetes.io/pod: docker/default,"Seccomp is an alpha feature currently. By default, all alpha features are disabled. So, you would need to enable alpha features in the apiserver by passing '--feature- gates=AllAlpha=true' argument. Edit the /etc/kubernetes/apiserver file on the master node and set the KUBE_API_ARGS parameter to '--feature-gates=AllAlpha=true' KUBE_API_ARGS='--feature-gates=AllAlpha=true' Based on your system, restart the kube-apiserver service. For example: systemctl restart kube-apiserver.service Use annotations to enable the docker/default seccomp profile in your pod definitions. An example is as below: apiVersion: v1 kind: Pod metadata: 141 | P a g e name: trustworthy-pod annotations: seccomp.security.alpha.kubernetes.io/pod: docker/default spec: containers: - name: trustworthy-container image: sotrustworthy:latest Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles. Also, you need to enable all alpha features for this to work. There is no individual switch to turn on this feature. Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://github.com/kubernetes/kubernetes/issues/39845 2. https://github.com/kubernetes/kubernetes/pull/21790 3. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/seccomp.md#examples 4. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_docker_default; kubernetes_pod_seccomp_profile_enabled; kubernetes_pod_security_profile_docker_default; kubernetes_pod_security_seccomp_enabled; kubernetes_pod_security_seccomp_docker_default,• Level 2,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles. Also, you need to enable all alpha features for this to work. There is no individual switch to turn on this feature. Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled.",1. https://github.com/kubernetes/kubernetes/issues/39845 2. https://github.com/kubernetes/kubernetes/pull/21790 3. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/seccomp.md#examples 4. https://docs.docker.com/engine/security/seccomp/
1.6.7,Apply Security Context to Your Pods and Containers,Not Scored,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Impact: If you incorrectly apply security contexts, you may have trouble running the pods. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_applied; kubernetes_container_security_context_applied; kubernetes_pod_privilege_escalation_disabled; kubernetes_container_privilege_escalation_disabled; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_capabilities_dropped; kubernetes_container_capabilities_dropped,• Level 2,"If you incorrectly apply security contexts, you may have trouble running the pods. Default Value: By default, no security contexts are automatically applied to pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
1.6.8,Configure Image Provenance using ImagePolicyWebhook admission controller,Not Scored,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Impact: You need to regularly maintain your provenance configuration based on container image updates. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888 145 | P a g e",kubernetes_image_provenance_enabled; kubernetes_image_policy_webhook_enabled; kubernetes_admission_controller_image_provenance_enabled; kubernetes_image_provenance_webhook_configured; kubernetes_image_policy_webhook_configured; kubernetes_admission_controller_image_policy_webhook_enabled; kubernetes_image_provenance_validation_enabled; kubernetes_image_policy_webhook_validation_enabled,• Level 2,"You need to regularly maintain your provenance configuration based on container image updates. Default Value: By default, image provenance is not set.",1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888 145 | P a g e
2.1.1,Ensure that the --allow-privileged argument is set to false,Scored,Do not allow privileged containers.,"The privileged container has all the system capabilities, and it also lifts all the limitations enforced by the device cgroup controller. In other words, the container can then do almost everything that the host can do. This flag exists to allow special use-cases, like running Docker within Docker and hence should be avoided for production workloads.",Run the following command on each node: ps -ef | grep kubelet Verify that the --allow-privileged argument is set to false.,"Edit the /etc/kubernetes/config file on each node and set the KUBE_ALLOW_PRIV parameter to '--allow-privileged=false': KUBE_ALLOW_PRIV='--allow-privileged=false' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: You will not be able to run any privileged containers. 147 | P a g e Note: A number of components used by Kubernetes clusters currently make use of privileged containers (e.g. Container Network Interface plugins). Care should be taken in ensuring that the use of such plugins is minimized and in particular any use of privileged containers outside of the kube-system namespace should be scrutinized. Where possible, review the rights required by such plugins to determine if a more fine grained permission set can be applied. Default Value: By default, privileged containers are not allowed. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/user-guide/security-context/",container_runtime_privileged_disabled; container_runtime_allow_privileged_false; container_runtime_privileged_containers_blocked; container_runtime_privileged_mode_disabled; container_runtime_privileged_flag_false,• Level 1,"You will not be able to run any privileged containers. 147 | P a g e Note: A number of components used by Kubernetes clusters currently make use of privileged containers (e.g. Container Network Interface plugins). Care should be taken in ensuring that the use of such plugins is minimized and in particular any use of privileged containers outside of the kube-system namespace should be scrutinized. Where possible, review the rights required by such plugins to determine if a more fine grained permission set can be applied. Default Value: By default, privileged containers are not allowed.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/user-guide/security-context/
2.1.2,Ensure that the --anonymous-auth argument is set to false,Scored,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests.",Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--anonymous-auth=false': KUBELET_ARGS='--anonymous-auth=false' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Anonymous requests will be rejected. Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 149 | P a g e",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_anonymous_auth_set_false; kubernetes_kubelet_anonymous_auth_blocked; kubernetes_kubelet_anonymous_requests_disabled; kubernetes_kubelet_auth_anonymous_disabled,• Level 1,"Anonymous requests will be rejected. Default Value: By default, anonymous access is enabled.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 149 | P a g e
2.1.3,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Scored,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.",Run the following command on each node: ps -ef | grep kubelet Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--authorization-mode=Webhook': KUBELET_ARGS='--authorization-mode=Webhook' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Unauthorized requests will be denied. Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 151 | P a g e 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_explicit_authorization_enabled; kubernetes_api_server_authorization_restricted; kubernetes_api_server_always_allow_disabled,• Level 1,"Unauthorized requests will be denied. Default Value: By default, --authorization-mode argument is set to AlwaysAllow.",1. https://kubernetes.io/docs/admin/kubelet/ 151 | P a g e 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
2.1.4,Ensure that the --client-ca-file argument is set as appropriate,Scored,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests.",Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set as appropriate.,"Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--client-ca-file=<path/to/client-ca-file>': KUBELET_ARGS='--client-ca-file=<path/to/client-ca-file>' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: You require TLS to be configured on apiserver as well as kubelets. 153 | P a g e Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",,• Level 1,"You require TLS to be configured on apiserver as well as kubelets. 153 | P a g e Default Value: By default, --client-ca-file argument is not set.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
2.1.5,Ensure that the --read-only-port argument is set to 0,Scored,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster.,Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--read-only-port=0' KUBELET_ARGS='--read-only-port=0' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API. Default Value: By default, --read-only-port is set to 10255/TCP. References: 1. https://kubernetes.io/docs/admin/kubelet/ 155 | P a g e",kubernetes_api_server_read_only_port_disabled; kubernetes_api_server_read_only_port_set_zero; kubernetes_api_server_read_only_port_secure; kubernetes_api_server_read_only_port_unset; kubernetes_api_server_read_only_port_0,• Level 1,"Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API. Default Value: By default, --read-only-port is set to 10255/TCP.",1. https://kubernetes.io/docs/admin/kubelet/ 155 | P a g e
2.1.6,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Scored,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases.",Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--streaming-connection-idle-timeout=<appropriate-timeout-value>' KUBELET_ARGS='--streaming-connection-idle-timeout=5m' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Long-lived connections could be interrupted. Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. 157 | P a g e References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",kubernetes_api_server_streaming_connection_idle_timeout_not_zero; kubernetes_api_server_streaming_connection_timeout_enabled; kubernetes_api_server_streaming_connection_timeout_configured; kubernetes_api_server_streaming_connection_timeout_non_zero; kubernetes_api_server_streaming_connection_timeout_valid,• Level 1,"Long-lived connections could be interrupted. Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. 157 | P a g e",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
2.1.7,Ensure that the --protect-kernel-defaults argument is set to true,Scored,Protect tuned kernel parameters from overriding kubelet default kernel parameter values.,Kernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior.,Run the following command on each node: ps -ef | grep kubelet Verify that the --protect-kernel-defaults argument is set to true.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--protect-kernel-defaults=true' KUBELET_ARGS='--protect-kernel-defaults=true' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: You would have to re-tune kernel parameters to match kubelet parameters. Default Value: By default, --protect-kernel-defaults is not set. 159 | P a g e References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_protect_kernel_defaults_enabled; kubernetes_kubelet_kernel_defaults_protected; kubernetes_kubelet_kernel_parameters_protected; kubernetes_kubelet_kernel_defaults_preserved; kubernetes_kubelet_kernel_tuned_protection_enabled,• Level 1,"You would have to re-tune kernel parameters to match kubelet parameters. Default Value: By default, --protect-kernel-defaults is not set. 159 | P a g e",1. https://kubernetes.io/docs/admin/kubelet/
2.1.8,Ensure that the --make-iptables-util-chains argument is set to true,Scored,Allow Kubelet to manage iptables.,Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open.,Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true.,"Edit the /etc/kubernetes/kubelet file on each node and remove the --make-iptables- util-chains argument from the KUBELET_ARGS parameter. Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts. Default Value: By default, --make-iptables-util-chains argument is set to true. 161 | P a g e References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_management_enabled; kubernetes_kubelet_iptables_chains_configured; kubernetes_kubelet_iptables_util_chains_set; kubernetes_kubelet_iptables_util_chains_true,• Level 1,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts. Default Value: By default, --make-iptables-util-chains argument is set to true. 161 | P a g e",1. https://kubernetes.io/docs/admin/kubelet/
2.1.9,Ensure that the --keep-terminated-pod-volumes argument is set to false,Scored,Unmount volumes from the nodes on pod termination.,"On pod termination, you should unmount the volumes. Those volumes might have sensitive data that might be exposed if kept mounted on the node without any use. Additionally, such mounted volumes could be modified and later could be mounted on pods. Also, if you retain all mounted volumes for a long time, it might exhaust system resources and you might not be able to mount any more volumes on new pods.",Run the following command on each node: ps -ef | grep kubelet Verify that --keep-terminated-pod-volumes argument exists and is set to false.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--keep-terminated-pod-volumes=false': KUBELET_ARGS='--keep-terminated-pod-volumes=false' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Volumes will not be available for debugging. Default Value: By default, --keep-terminated-pod-volumes argument is set to true. 163 | P a g e References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_pod_volumes_unmounted_on_termination; kubernetes_pod_keep_terminated_volumes_disabled; kubernetes_pod_termination_volumes_cleanup_enabled; kubernetes_pod_volumes_auto_unmount_enabled; kubernetes_pod_terminated_volumes_retained_disabled,• Level 1,"Volumes will not be available for debugging. Default Value: By default, --keep-terminated-pod-volumes argument is set to true. 163 | P a g e",1. https://kubernetes.io/docs/admin/kubelet/
2.1.10,Ensure that the --hostname-override argument is not set,Scored,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_HOSTNAME parameter to '': KUBELET_HOSTNAME='' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: Node hostnames should have resolvable FQDNs. Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 165 | P a g e 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_default; kubernetes_node_hostname_unchanged; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_preserved,• Level 1,"Node hostnames should have resolvable FQDNs. Default Value: By default, --hostname-override argument is not set.",1. https://kubernetes.io/docs/admin/kubelet/ 165 | P a g e 2. https://github.com/kubernetes/kubernetes/issues/22063
2.1.11,Ensure that the --event-qps argument is set to 0,Scored,Do not limit event creation.,It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data.,Run the following command on each node: ps -ef | grep kubelet Verify that --event-qps argument exists and is set to 0.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--event-qps=0': KUBELET_ARGS='--event-qps=0' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: You might need to scale up your event storage and processing capabilitles. Default Value: By default, --event-qps argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 167 | P a g e",cloudtrail_trail_event_qps_unlimited; cloudtrail_event_qps_zero; cloudtrail_trail_event_qps_disabled; cloudtrail_event_qps_no_limit; cloudtrail_trail_event_qps_unrestricted,• Level 1,"You might need to scale up your event storage and processing capabilitles. Default Value: By default, --event-qps argument is set to 5.",1. https://kubernetes.io/docs/admin/kubelet/ 167 | P a g e
2.1.12,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Scored,Setup TLS connection on the Kubelets.,Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic.,Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the Kubelet. Then, edit the /etc/kubernetes/kubelet file on the master node and set the KUBELET_ARGS parameter to include '--tls-cert-file=<path/to/tls-certificate-file>' and '--tls- private-key-file=<path/to/tls-key-file>': KUBELET_ARGS='--tls-cert-file=<path/to/tls-certificate-file> --tls-private- key-file=<path/to/tls-key-file>' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. 169 | P a g e Default Value: By default, --tls-cert-file and --tls-private-key-file arguments are not set. If -- tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_configured; kubernetes_kubelet_tls_cert_and_key_valid; kubernetes_kubelet_tls_connection_secure,• Level 1,"TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. 169 | P a g e Default Value: By default, --tls-cert-file and --tls-private-key-file arguments are not set. If -- tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir.",1. https://kubernetes.io/docs/admin/kubelet/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
2.1.13,Ensure that the --cadvisor-port argument is set to 0,Scored,Disable cAdvisor.,"cAdvisor provides potentially sensitive data and there's currently no way to block access to it using anything other than iptables. It does not require authentication/authorization to connect to the cAdvisor port. Hence, you should disable the port.",Run the following command on each node: ps -ef | grep kubelet Verify that --cadvisor-port argument exists and is set to 0.,"Edit the /etc/kubernetes/kubelet file on each node and set the KUBELET_ARGS parameter to '--cadvisor-port=0': KUBELET_ARGS='--cadvisor-port=0' Based on your system, restart the kubelet service. For example: systemctl restart kubelet.service Impact: cAdvisor will not be available directly. You need to work with /metrics endpoint on the API server. Default Value: By default, --cadvisor-port argument is set to 4194. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/11710 171 | P a g e 3. https://github.com/kubernetes/kubernetes/issues/32638 4. https://raesene.github.io/blog/2016/10/14/Kubernetes-Attack-Surface-cAdvisor/",compute_container_cadvisor_disabled; container_runtime_cadvisor_port_unset; kubernetes_node_cadvisor_deactivated; cadvisor_port_zero_enforced; container_engine_cadvisor_disabled,• Level 1,"cAdvisor will not be available directly. You need to work with /metrics endpoint on the API server. Default Value: By default, --cadvisor-port argument is set to 4194.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/11710 171 | P a g e 3. https://github.com/kubernetes/kubernetes/issues/32638 4. https://raesene.github.io/blog/2016/10/14/Kubernetes-Attack-Surface-cAdvisor/
2.2.1,Ensure that the config file permissions are set to 644 or more restrictive,Scored,Ensure that the config file has permissions of 644 or more restrictive.,The config file controls various parameters that set the behavior of various components of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/config Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 /etc/kubernetes/config Impact: None Default Value: By default, config file has permissions of 644. 173 | P a g e References: 1. https://kubernetes.io/docs/admin/kubelet/",config_file_permissions_restrictive; config_file_permissions_644_or_stricter; config_file_permissions_not_world_writable; config_file_permissions_secure; config_file_permissions_min_644,• Level 1,"None Default Value: By default, config file has permissions of 644. 173 | P a g e",1. https://kubernetes.io/docs/admin/kubelet/
2.2.2,Ensure that the config file ownership is set to root:root,Scored,Ensure that the config file ownership is set to root:root.,The config file controls various parameters that set the behavior of various components of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/config Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/config Impact: None Default Value: By default, config file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 175 | P a g e",config_file_ownership_root_root; config_file_owner_root; config_file_group_root; config_file_root_ownership; config_file_secure_ownership,• Level 1,"None Default Value: By default, config file ownership is set to root:root.",1. https://kubernetes.io/docs/admin/kubelet/ 175 | P a g e
2.2.3,Ensure that the kubelet file permissions are set to 644 or more restrictive,Scored,Ensure that the kubelet file has permissions of 644 or more restrictive.,The kubelet file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 /etc/kubernetes/kubelet Impact: None Default Value: By default, kubelet file has permissions of 644. References: 1. https://kubernetes.io/docs/admin/kubelet/ 177 | P a g e",kubernetes_kubelet_file_permissions_644_or_stricter; kubernetes_kubelet_file_permissions_restrictive; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_file_permissions_compliant; kubernetes_kubelet_config_file_permissions_644_or_lower,• Level 1,"None Default Value: By default, kubelet file has permissions of 644.",1. https://kubernetes.io/docs/admin/kubelet/ 177 | P a g e
2.2.4,Ensure that the kubelet file ownership is set to root:root,Scored,Ensure that the kubelet file ownership is set to root:root.,The kubelet file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/kubelet Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet Impact: None Default Value: By default, kubelet file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 179 | P a g e",kubernetes_kubelet_file_ownership_root_root; kubernetes_kubelet_file_permissions_root_only; kubernetes_kubelet_config_secure_ownership; kubernetes_kubelet_privileged_file_ownership; kubernetes_kubelet_sensitive_file_root_owned,• Level 1,"None Default Value: By default, kubelet file ownership is set to root:root.",1. https://kubernetes.io/docs/admin/kubelet/ 179 | P a g e
2.2.5,Ensure that the proxy file permissions are set to 644 or more restrictive,Scored,Ensure that the proxy file has permissions of 644 or more restrictive.,The proxy file controls various parameters that set the behavior of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.,"Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/proxy Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 /etc/kubernetes/proxy Impact: None Default Value: By default, proxy file has permissions of 644. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 181 | P a g e",compute_proxy_file_permissions_restrictive; compute_proxy_file_permissions_644_or_stricter; compute_proxy_file_permissions_secure; compute_proxy_file_permissions_compliant; compute_proxy_file_permissions_cis_benchmark,• Level 1,"None Default Value: By default, proxy file has permissions of 644.",1. https://kubernetes.io/docs/admin/kube-proxy/ 181 | P a g e
2.2.6,Ensure that the proxy file ownership is set to root:root,Scored,Ensure that the proxy file ownership is set to root:root.,The proxy file controls various parameters that set the behavior of the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.,"Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/proxy Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/proxy Impact: None Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 183 | P a g e",proxy_file_root_ownership; proxy_file_root_group_ownership; proxy_file_ownership_root_root; proxy_file_secure_ownership; proxy_file_root_user_ownership; proxy_file_root_group_ownership_set; proxy_file_ownership_restricted_to_root,• Level 1,"None Default Value: By default, proxy file ownership is set to root:root.",1. https://kubernetes.io/docs/admin/kube-proxy/ 183 | P a g e
3.1.1,Ensure that the --anonymous-auth argument is set to false,Scored,Disable anonymous requests to the federation API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the federation API server. You should rely on authentication to authorize access and disallow anonymous requests.",Run the following command: ps -ef | grep federation-apiserver Verify that the --anonymous-auth argument is set to false.,"Edit the deployment specs and set --anonymous-auth=false. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: Anonymous requests will be rejected. 185 | P a g e Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_anonymous_auth_disabled; federation_api_anonymous_auth_disabled; api_server_anonymous_auth_disabled,• Level 1,"Anonymous requests will be rejected. 185 | P a g e Default Value: By default, anonymous access is enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.2,Ensure that the --basic-auth-file argument is not set,Scored,Do not use basic authentication.,"Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the federation API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used.",Run the following command: ps -ef | grep federation-apiserver Verify that the --basic-auth-file argument does not exist.,"Follow the documentation and configure alternate mechanisms for authentication. Then, edit the deployment specs and remove '--basic-auth-file=<filename>'. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: You will have to configure and use alternate authentication mechanisms such as tokens and certificates. Username and password for basic authentication could no more be used. Default Value: By default, basic authentication is not set. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 187 | P a g e 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_basic_auth_disabled; kubernetes_api_server_no_basic_auth_file; kubernetes_api_server_auth_file_unset; kubernetes_api_server_basic_auth_file_removed; kubernetes_api_server_auth_file_disabled,• Level 1,"You will have to configure and use alternate authentication mechanisms such as tokens and certificates. Username and password for basic authentication could no more be used. Default Value: By default, basic authentication is not set.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 187 | P a g e 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.3,Ensure that the --insecure-allow-any-token argument is not set,Scored,Do not allow any insecure tokens.,Accepting insecure tokens would allow any token without actually authenticating anything. User information is parsed from the token and connections are allowed.,Run the following command: ps -ef | grep federation-apiserver Verify that the --insecure-allow-any-token argument does not exist.,"Edit the deployment specs and remove --insecure-allow-any-token. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, insecure tokens are not allowed. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 189 | P a g e",kubernetes_api_server_insecure_allow_any_token_disabled; kubernetes_api_server_token_authentication_restricted; kubernetes_api_server_secure_token_validation_enabled; kubernetes_api_server_insecure_token_argument_absent; kubernetes_api_server_token_authentication_enforced,• Level 1,"None Default Value: By default, insecure tokens are not allowed.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 189 | P a g e
3.1.4,Ensure that the --insecure-bind-address argument is not set,Scored,Do not bind to insecure addresses.,"If you bind the federation apiserver to an insecure address, basically anyone who could connect to it over the insecure port, would have unauthenticated and unencrypted access to it. The federation apiserver doesn't do any authentication checking for insecure binds and neither the insecure traffic is encrypted. Hence, you should not bind the federation apiserver to an insecure address.",Run the following command: ps -ef | grep federation-apiserver Verify that the --insecure-bind-address argument does not exist or is set to 127.0.0.1.,"Edit the deployment specs and remove --insecure-bind-address. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, insecure bind address is set to 127.0.0.1. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 191 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_insecure_bind_address_disabled; kubernetes_api_server_bind_address_secure; kubernetes_api_server_insecure_config_removed; kubernetes_api_server_secure_bind_address_enabled; kubernetes_api_server_insecure_argument_unset,• Level 1,"None Default Value: By default, insecure bind address is set to 127.0.0.1.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 191 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.5,Ensure that the --insecure-port argument is set to 0,Scored,Do not bind to insecure port.,"Setting up the federation apiserver to serve on an insecure port would allow unauthenticated and unencrypted access to it. It is assumed that firewall rules are set up such that this port is not reachable from outside of the cluster. But, as a defense in depth measure, you should not use an insecure port.",Run the following command: ps -ef | grep federation-apiserver Verify that the --insecure-port argument is set to 0.,"Edit the deployment specs and set --insecure-port=0. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, the insecure port is set to 8080. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 193 | P a g e",kubernetes_api_server_insecure_port_disabled; kubernetes_api_server_insecure_port_zero; kubernetes_api_server_secure_port_only; kubernetes_api_server_insecure_port_unset; kubernetes_api_server_port_security_enabled,• Level 1,"None Default Value: By default, the insecure port is set to 8080.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 193 | P a g e
3.1.6,Ensure that the --secure-port argument is not set to 0,Scored,Do not disable the secure port.,"The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted.",Run the following command: ps -ef | grep federation-apiserver Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535.,"Edit the deployment specs and set the --secure-port argument to the desired port. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: You need to set the federation apiserver up with the right TLS certificates. Default Value: By default, port 6443 is used as the secure port. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 195 | P a g e",kubernetes_api_server_secure_port_not_disabled; kubernetes_api_server_secure_port_enabled; kubernetes_api_server_secure_port_valid; kubernetes_api_server_secure_port_non_zero; kubernetes_api_server_secure_port_configured,• Level 1,"You need to set the federation apiserver up with the right TLS certificates. Default Value: By default, port 6443 is used as the secure port.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 195 | P a g e
3.1.7,Ensure that the --profiling argument is set to false,Scored,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.",Run the following command: ps -ef | grep federation-apiserver Verify that the --profiling argument is set to false.,"Edit the deployment specs and set '--profiling=false': kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: Profiling information would not be available. Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 197 | P a g e",eks_cluster_profiling_disabled; kubernetes_cluster_profiling_disabled; container_service_profiling_disabled; k8s_cluster_profiling_disabled; managed_kubernetes_profiling_disabled,• Level 1,"Profiling information would not be available. Default Value: By default, profiling is enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 197 | P a g e
3.1.8,Ensure that the admission control policy is not set to AlwaysAdmit,Scored,Do not allow all requests.,Setting admission control policy to AlwaysAdmit allows all requests and do not filter any requests.,Run the following command: ps -ef | grep federation-apiserver Verify that the --admission-control argument is set to a value that does not include AlwaysAdmit.,"Edit the deployment specs and set --admission-control argument to a value that does not include AlwaysAdmit. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: Only requests explicitly allowed by the admissions control policy would be served. Default Value: By default, AlwaysAdmit is used if no --admission-control flag is provided. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 199 | P a g e 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_admission_policy_not_always_admit; kubernetes_admission_controller_restrictive_policy; admission_control_policy_deny_all_requests; kubernetes_admission_policy_secure_defaults; admission_controller_policy_restrictive_settings,• Level 1,"Only requests explicitly allowed by the admissions control policy would be served. Default Value: By default, AlwaysAdmit is used if no --admission-control flag is provided.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 199 | P a g e 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.9,Ensure that the admission control policy is set to NamespaceLifecycle,Scored,Reject creating objects in a namespace that is undergoing termination.,Setting admission control policy to NamespaceLifecycle ensures that the namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects.,Run the following command: ps -ef | grep federation-apiserver Verify that the --admission-control argument is set to a value that includes NamespaceLifecycle.,"Edit the deployment specs and set --admission-control argument to a value that includes NamespaceLifecycle. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 201 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_namespace_admission_control_policy_namespace_lifecycle; kubernetes_namespace_termination_protection_enabled; kubernetes_admission_policy_namespace_lifecycle_enforced; kubernetes_namespace_lifecycle_admission_control_enabled; kubernetes_admission_control_namespace_termination_protected,• Level 1,"None Default Value: By default, NamespaceLifecycle is set.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 201 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.10,Ensure that the --audit-log-path argument is set as appropriate,Scored,Enable auditing on kubernetes federation apiserver and set the desired audit log path as appropriate.,"Auditing Kubernetes federation apiserver provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path.",Run the following command: ps -ef | grep federation-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the deployment specs and set --audit-log-path argument as appropriate. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 203 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_apiserver_audit_log_path_set; kubernetes_apiserver_audit_logging_enabled; kubernetes_apiserver_audit_log_path_configured; kubernetes_federation_audit_log_path_set; kubernetes_federation_audit_logging_enabled; kubernetes_federation_audit_log_path_configured; kubernetes_audit_log_path_valid; kubernetes_audit_logging_active,• Level 1,"None Default Value: By default, auditing is not enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 203 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.11,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Scored,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements.,Run the following command: ps -ef | grep federation-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the deployment specs and set --audit-log-maxage to 30 or as appropriate. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 205 | P a g e",kubernetes_api_server_audit_log_maxage_set; kubernetes_api_server_audit_log_retention_30d; kubernetes_audit_log_maxage_configured; kubernetes_audit_log_retention_min_30d; kubernetes_api_server_audit_log_retention_valid; kubernetes_audit_log_maxage_compliant; kubernetes_audit_log_retention_appropriate; kubernetes_api_server_audit_log_maxage_30d,• Level 1,"None Default Value: By default, auditing is not enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 205 | P a g e
3.1.12,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Scored,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.",Run the following command: ps -ef | grep federation-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the deployment specs and set --audit-log-maxbackup to 10 or as appropriate. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver 207 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_10_or_appropriate; kubernetes_audit_log_retention_configured; kubernetes_audit_log_maxbackup_valid; kubernetes_api_server_log_retention_compliant,• Level 1,"None Default Value: By default, auditing is not enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver 207 | P a g e 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.13,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Scored,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.",Run the following command: ps -ef | grep federation-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the deployment specs and set --audit-log-maxsize=100 to 100 or as appropriate. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 209 | P a g e",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_api_server_audit_log_rotation_enabled; kubernetes_api_server_audit_log_size_limited; kubernetes_api_server_audit_log_maxsize_configured,• Level 1,"None Default Value: By default, auditing is not enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 209 | P a g e
3.1.14,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Scored,Do not always authorize all requests.,"The federation apiserver, by default, allows all requests. You should restrict this behavior to only allow the authorization modes that you explicitly use in your environment. For example, if you don't use REST APIs in your environment, it is a good security best practice to switch-off that capability.",Run the following command: ps -ef | grep federation-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the deployment specs and set --authorization-mode argument to a value other than AlwaysAllow kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: Only authorized requests will be served. Default Value: By default, AlwaysAllow is enabled. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 211 | P a g e 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_authorization_mode_restricted; kubernetes_api_server_secure_authorization_mode; kubernetes_api_server_no_always_allow_auth; kubernetes_api_server_auth_mode_compliant,• Level 1,"Only authorized requests will be served. Default Value: By default, AlwaysAllow is enabled.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 211 | P a g e 3. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.15,Ensure that the --token-auth-file parameter is not set,Scored,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the federation apiserver. The tokens are stored in clear-text in a file on the federation apiserver, and cannot be revoked or rotated without restarting the federation apiserver. Hence, do not use static token-based authentication.",Run the following command: ps -ef | grep federation-apiserver Verify that the --token-auth-file argument does not exist.,"Follow the documentation and configure alternate mechanisms for authentication. Then, edit the deployment specs and remove the --token-auth-file=<filename> argument. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/federation-apiserver/ 213 | P a g e 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 4. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_token_auth_disabled; kubernetes_api_server_no_token_auth_file; kubernetes_api_server_token_auth_file_unset; kubernetes_api_server_token_auth_not_configured; kubernetes_api_server_token_auth_file_absent,• Level 1,"You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used. Default Value: By default, --token-auth-file argument is not set.",1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/federation-apiserver/ 213 | P a g e 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 4. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.16,Ensure that the --service-account-lookup argument is set to true,Scored,Validate service account before validating token.,"By default, the apiserver only verifies that the authentication token is valid. However, it does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue.",Run the following command: ps -ef | grep federation-apiserver Verify that the --service-account-lookup argument exists and is set to true.,"Edit the deployment specs and set '--service-account-lookup=true'. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: None Default Value: By default, --service-account-lookup argument is set to false. References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use 215 | P a g e 4. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 5. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_service_account_pre_authentication_enabled; kubernetes_api_server_service_account_lookup_required; kubernetes_api_server_service_account_validation_required,• Level 1,"None Default Value: By default, --service-account-lookup argument is set to false.",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use 215 | P a g e 4. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 5. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.17,Ensure that the --service-account-key-file argument is set as appropriate,Scored,Explicitly set a service account public key file for service accounts on the federation apiserver.,"By default, if no --service-account-key-file is specified to the federation apiserver, it uses the private key from the TLS serving certificate to verify the account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file.",Run the following command: ps -ef | grep federation-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the deployment specs and set --service-account-key-file argument as appropriate. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy. Default Value: By default, --service-account-key-file argument is not set, and the private key from the TLS serving certificate is used. 217 | P a g e References: 1. https://kubernetes.io/docs/admin/federation-apiserver 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 4. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",federation_apiserver_service_account_key_file_set; federation_apiserver_service_account_key_file_configured; federation_apiserver_service_account_key_file_valid; federation_apiserver_service_account_key_file_secure; federation_apiserver_service_account_key_file_protected,• Level 1,"The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy. Default Value: By default, --service-account-key-file argument is not set, and the private key from the TLS serving certificate is used. 217 | P a g e",1. https://kubernetes.io/docs/admin/federation-apiserver 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 4. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.18,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Scored,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the federation API server to identify itself to the etcd server using a client certificate and key.,Run the following command: ps -ef | grep federation-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the federation apiserver and etcd. Then, edit the deployment specs and set '--etcd- certfile=<path/to/client-certificate-file>' and '--etcd- keyfile=<path/to/client-key-file>' arguments. kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: TLS and client certificate authentication must be configured for etcd. Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set 219 | P a g e References: 1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 4. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_certfile_configured; kubernetes_etcd_keyfile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_client_connections,• Level 1,"TLS and client certificate authentication must be configured for etcd. Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set 219 | P a g e",1. https://kubernetes.io/docs/admin/federation-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 4. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.1.19,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Scored,Setup TLS connection on the federation API server.,Federation API server communication contains sensitive parameters that should remain encrypted in transit. Configure the federation API server to serve only HTTPS traffic.,Run the following command: ps -ef | grep federation-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the federation apiserver. Then, edit the deployment specs and set '--tls-cert-file=<path/to/tls- certificate-file>' and '--tls-private-key-file=<path/to/tls-key-file>': kubectl edit deployments federation-apiserver-deployment -- namespace=federation-system Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. Default Value: By default, --tls-cert-file and --tls-private-key-file arguments are not set. If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes. 221 | P a g e References: 1. https://kubernetes.io/docs/admin/federation-apiserver 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide 4. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 5. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_cert_file_valid; kubernetes_api_server_tls_private_key_file_valid; kubernetes_api_server_tls_cert_file_secure; kubernetes_api_server_tls_private_key_file_secure; kubernetes_api_server_tls_cert_file_configured; kubernetes_api_server_tls_private_key_file_configured,• Level 1,"TLS and client certificate authentication must be configured for your Kubernetes cluster deployment. Default Value: By default, --tls-cert-file and --tls-private-key-file arguments are not set. If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes. 221 | P a g e",1. https://kubernetes.io/docs/admin/federation-apiserver 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide 4. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-apiserver-deployment.yaml 5. https://kubernetes.io/docs/concepts/workloads/controllers/deployment/
3.2.1,Ensure that the --profiling argument is set to false,Scored,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.",Run the following command: ps -ef | grep federation-controller-manager Verify that the --profiling argument is set to false.,"Edit the deployment specs and set '--profiling=false': kubectl edit deployments federation-controller-manager-deployment -- namespace=federation-system Impact: Profiling information would not be available. Default Value: By default, profiling is enabled. 223 | P a g e References: 1. https://kubernetes.io/docs/admin/federation-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-controller-manager-deployment.yaml",eks_cluster_profiling_disabled; kubernetes_cluster_profiling_disabled; container_service_profiling_disabled; k8s_cluster_profiling_disabled; managed_kubernetes_profiling_disabled,• Level 1,"Profiling information would not be available. Default Value: By default, profiling is enabled. 223 | P a g e",1. https://kubernetes.io/docs/admin/federation-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profi ling.md 3. https://github.com/kubernetes/kubernetes/blob/master/federation/manifests/fe deration-controller-manager-deployment.yaml
1.1.1,Ensure that the API server pod specification file permissions are set to 644 or more restrictive,Automated,Ensure that the API server pod specification file has permissions of 644 or more restrictive.,The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_permissions_644_or_restrictive; kubernetes_api_server_pod_spec_file_permissions_restrictive; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_permissions_compliant; kubernetes_api_server_pod_spec_file_permissions_cis_benchmark,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.2,Ensure that the API server pod specification file ownership is set to root:root,Automated,Ensure that the API server pod specification file ownership is set to root:root.,The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_ownership_root_root; kubernetes_api_server_pod_spec_file_permissions_root; kubernetes_api_server_pod_spec_file_owner_root; kubernetes_api_server_pod_spec_file_group_root; kubernetes_api_server_pod_spec_file_secure_ownership,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.3,Ensure that the controller manager pod specification file permissions are set to 644 or more restrictive,Automated,Ensure that the controller manager pod specification file has permissions of 644 or more restrictive.,The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, the kube-controller-manager.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_controller_manager_pod_spec_file_permissions_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_644_or_stricter; kubernetes_controller_manager_pod_spec_file_permissions_secure; kubernetes_controller_manager_pod_spec_file_permissions_compliant; kubernetes_controller_manager_pod_spec_file_permissions_protected,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.4,Ensure that the controller manager pod specification file ownership is set to root:root,Automated,Ensure that the controller manager pod specification file ownership is set to root:root.,The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, kube-controller-manager.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_pod_spec_ownership_root; kubernetes_controller_manager_file_permissions_root; kubernetes_controller_manager_spec_file_ownership_root; kubernetes_controller_manager_pod_file_ownership_root,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager
1.1.5,Ensure that the scheduler pod specification file permissions are set to 644 or more restrictive,Automated,Ensure that the scheduler pod specification file has permissions of 644 or more restrictive.,The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_spec_file_permissions_644_or_restrictive; kubernetes_scheduler_pod_spec_file_permissions_restrictive; kubernetes_scheduler_pod_spec_file_permissions_secure; kubernetes_scheduler_pod_spec_file_permissions_compliant; kubernetes_scheduler_pod_spec_file_permissions_cis_benchmark,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.6,Ensure that the scheduler pod specification file ownership is set to root:root,Automated,Ensure that the scheduler pod specification file ownership is set to root:root.,The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_ownership_root; kubernetes_scheduler_pod_spec_root_owner; kubernetes_scheduler_file_ownership_root; kubernetes_pod_spec_ownership_root_root; kubernetes_scheduler_spec_file_root_owner,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.7,Ensure that the etcd pod specification file permissions are set to 644 or more restrictive,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file has permissions of 644 or more restrictive.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/etcd.yaml Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_pod_file_permissions_restrictive; kubernetes_etcd_pod_file_permissions_644_or_stricter; kubernetes_etcd_manifest_file_permissions_secure; kubernetes_etcd_manifest_file_permissions_restricted; kubernetes_etcd_pod_spec_file_permissions_compliant,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.8,Ensure that the etcd pod specification file ownership is set to root:root,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_manifest_file_owner_root; kubernetes_etcd_manifest_file_group_root; kubernetes_etcd_manifest_file_ownership_root_root; kubernetes_etcd_pod_spec_file_owner_root; kubernetes_etcd_pod_spec_file_group_root; kubernetes_etcd_pod_spec_file_ownership_root_root; kubernetes_manifest_etcd_file_owner_root; kubernetes_manifest_etcd_file_group_root; kubernetes_manifest_etcd_file_ownership_root_root; kubernetes_etcd_yaml_file_owner_root; kubernetes_etcd_yaml_file_group_root; kubernetes_etcd_yaml_file_ownership_root_root,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.9,Ensure that the Container Network Interface file permissions are set to 644 or more restrictive,Manual,Ensure that the Container Network Interface files have permissions of 644 or more restrictive.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a <path/to/cni/files> Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_permissions_restrictive; container_network_interface_file_permissions_644_or_stricter; container_network_interface_file_permissions_secure; container_network_interface_file_permissions_compliant; container_network_interface_file_permissions_cis_benchmark,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.10,Ensure that the Container Network Interface file ownership is set to root:root,Manual,Ensure that the Container Network Interface files have ownership set to root:root.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G <path/to/cni/files> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_ownership_root; container_network_interface_file_ownership_root_root; cni_file_ownership_root; cni_file_ownership_root_root; container_network_interface_file_ownership_secure; cni_file_ownership_secure,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Automated,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd Default Value: By default, etcd data directory has permissions of 755. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_700_or_restrictive; etcd_data_directory_permissions_restrictive; etcd_directory_permissions_secure; etcd_data_directory_permissions_compliant; etcd_directory_permissions_700_or_stricter,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Automated,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd Default Value: By default, etcd data directory ownership is set to etcd:etcd. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_ownership_etcd_etcd; etcd_directory_permissions_etcd_etcd; etcd_data_directory_secure_ownership; etcd_directory_ownership_correct; etcd_data_directory_user_group_etcd,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.13,Ensure that the admin.conf file permissions are set to 644,Automated,Ensure that the admin.conf file has permissions of 644.,The admin.conf is the administrator kubeconfig file defining various settings for the administration of the cluster. This file contains private key and respective certificate allowed to fully manage the cluster. You should restrict its file permissions to maintain the integrity and confidentiality of the file. The file should be readable and writable by only the administrators on the system. Impact: None.,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/admin.conf Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 /etc/kubernetes/admin.conf Default Value: By default, admin.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",kubernetes_admin_conf_file_permissions_644; kubernetes_admin_conf_file_permissions_restricted; kubernetes_admin_conf_file_permissions_secure; kubernetes_admin_conf_file_permissions_compliant; kubernetes_admin_conf_file_permissions_cis_benchmark,• Level 1 - Master Node • Level 2 - Master Node,None.,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.14,Ensure that the admin.conf file ownership is set to root:root,Automated,Ensure that the admin.conf file ownership is set to root:root.,The admin.conf file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity and confidentiality of the file. The file should be owned by root:root. Impact: None.,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/admin.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/admin.conf Default Value: By default, admin.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",kubernetes_admin_conf_file_ownership_root; kubernetes_admin_conf_file_group_ownership_root; kubernetes_admin_conf_file_permissions_secure; kubernetes_admin_conf_file_ownership_correct; kubernetes_admin_conf_file_group_correct,• Level 1 - Master Node • Level 2 - Master Node,None.,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.15,Ensure that the scheduler.conf file permissions are set to 644 or more restrictive,Automated,Ensure that the scheduler.conf file has permissions of 644 or more restrictive.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/scheduler.conf Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",compute_scheduler_conf_file_permissions_644_or_stricter; compute_scheduler_conf_file_permissions_restrictive; compute_scheduler_conf_file_permissions_secure; compute_scheduler_conf_file_permissions_compliant; compute_scheduler_conf_file_permissions_cis_benchmark,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.16,Ensure that the scheduler.conf file ownership is set to root:root,Automated,Ensure that the scheduler.conf file ownership is set to root:root.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/scheduler.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",scheduler_file_root_ownership; scheduler_conf_root_owner; scheduler_config_root_ownership; scheduler_file_owner_root; scheduler_conf_owner_root,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.17,Ensure that the controller-manager.conf file permissions are set to 644 or more restrictive,Automated,Ensure that the controller-manager.conf file has permissions of 644 or more restrictive.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/controller-manager.conf Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 644 /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_permissions_restrictive; kubernetes_controller_manager_conf_file_permissions_644_or_stricter; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_permissions_compliant; kubernetes_controller_manager_conf_file_permissions_cis_benchmark,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.18,Ensure that the controller-manager.conf file ownership is set to root:root,Automated,Ensure that the controller-manager.conf file ownership is set to root:root.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/controller-manager.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_ownership_root; kubernetes_controller_manager_conf_file_group_ownership_root; kubernetes_controller_manager_conf_file_permissions_root_only; kubernetes_controller_manager_conf_file_secure_ownership; kubernetes_controller_manager_conf_file_root_ownership_enforced,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.19,Ensure that the Kubernetes PKI directory and file ownership is set to root:root,Automated,Ensure that the Kubernetes PKI directory and file ownership is set to root:root.,Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/ Verify that the ownership of all files and directories in this hierarchy is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown -R root:root /etc/kubernetes/pki/ Default Value: By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained within it, are set to be owned by the root user. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_directory_ownership_root; kubernetes_pki_file_ownership_root; kubernetes_pki_directory_permissions_root; kubernetes_pki_file_permissions_root; kubernetes_pki_directory_and_file_ownership_root; kubernetes_pki_directory_and_file_permissions_root; kubernetes_pki_directory_and_file_ownership_and_permissions_root,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.20,Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive,Manual,Ensure that Kubernetes PKI certificate files have permissions of 600 or more restrictive.,Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/*.crt Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.crt Default Value: By default, the certificates used by Kubernetes are set to have permissions of 644 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_certificate_file_permissions_600_or_more_restrictive; kubernetes_pki_certificate_file_permissions_restrictive; kubernetes_certificate_file_permissions_secure; kubernetes_pki_file_permissions_600; kubernetes_certificate_file_permissions_restricted,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.21,Ensure that the Kubernetes PKI key file permissions are set to 600,Manual,Ensure that Kubernetes PKI key files have permissions of 600.,Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/*.key Verify that the permissions are 600.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.key Default Value: By default, the keys used by Kubernetes are set to have permissions of 600 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_key_file_permissions_600; kubernetes_pki_key_file_permissions_restricted; kubernetes_pki_key_file_permissions_secure; kubernetes_pki_key_file_permissions_strict; kubernetes_pki_key_file_permissions_protected,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.1,Ensure that the --anonymous-auth argument is set to false,Manual,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Impact: Anonymous requests will be rejected.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--anonymous-auth' | grep -i false If the exit code is '1', then the control isn't present / failed","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --anonymous-auth=false Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_auth_enabled; kubernetes_api_server_no_anonymous_access; kubernetes_api_server_secure_auth_required; kubernetes_api_server_authentication_enforced,• Level 1 - Master Node • Level 2 - Master Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests
1.2.2,Ensure that the --token-auth-file parameter is not set,Automated,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token- based authentication. Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist. Alternative Audit Method kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--token-auth-file' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the --token-auth-file=<filename> parameter. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_file_disabled; kubernetes_api_server_token_auth_file_not_set; kubernetes_api_server_token_auth_file_unused; kubernetes_api_server_token_auth_file_absent; kubernetes_api_server_token_auth_file_removed,• Level 1 - Master Node • Level 2 - Master Node,You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.,1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.3,Ensure that the DenyServiceExternalIPs admission controller is enabled,Manual,This admission controller rejects all net-new usage of the Service field externalIPs.,"Most users do not need the ability to set the externalIPs field for a Service at all, and cluster admins should consider disabling this functionality by enabling the DenyServiceExternalIPs admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage. Impact: When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the `DenyServiceExternalIPs' argument exists as a string value in --enable- admission-plugins.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and add the DenyServiceExternalIPs' to the -- enable-admission-plugins` parameter. Default Value: By default, the DenyServiceExternalIPs admission controller is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_admission_controller_deny_service_external_ips_enabled; kubernetes_service_external_ips_denied; admission_controller_deny_service_external_ips_active; kubernetes_service_external_ips_restricted; admission_controller_service_external_ips_blocked,• Level 1 - Master Node • Level 2 - Master Node,"When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.4,Ensure that the --kubelet-client-certificate and --kubelet- client-key arguments are set as appropriate,Automated,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-client-certificate' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below. --kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file> Default Value: By default, certificate-based kubelet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_client_certificate_set; kubernetes_kubelet_client_key_set; kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured,• Level 1 - Master Node • Level 2 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.5,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Automated,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-certificate-Authority' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string> Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_set; kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_validation_enabled; kubernetes_kubelet_certificate_authority_verified; kubernetes_kubelet_secure_certificate_authority; kubernetes_kubelet_certificate_authority_required; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_enforced,• Level 1 - Master Node • Level 2 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.6,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not always authorize all requests.,"The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Impact: Only authorized requests will be served.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below. --authorization-mode=RBAC Default Value: By default, AlwaysAllow is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",eks_cluster_authorization_mode_not_always_allow; kubernetes_api_authorization_mode_restricted; container_cluster_authorization_mode_secure; k8s_api_server_authorization_mode_compliant; eks_api_authorization_mode_not_always_allow,• Level 1 - Master Node • Level 2 - Master Node,Only authorized requests will be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.2.7,Ensure that the --authorization-mode argument includes Node,Automated,Restrict kubelet nodes to reading only objects associated with them.,"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include Node.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC Default Value: By default, Node authorization is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_authorization_enabled; kubernetes_kubelet_authorization_mode_node; kubernetes_kubelet_node_restricted_access; kubernetes_kubelet_node_authorization_required; kubernetes_kubelet_authorization_node_scope,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security
1.2.8,Ensure that the --authorization-mode argument includes RBAC,Automated,Turn on Role Based Access Control.,"Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Impact: When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include RBAC.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes RBAC, for example: --authorization-mode=Node,RBAC Default Value: By default, RBAC authorization is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/",kubernetes_cluster_rbac_enabled; kubernetes_cluster_authorization_mode_rbac; kubernetes_cluster_rbac_required; kubernetes_cluster_auth_mode_rbac_included; kubernetes_cluster_authorization_rbac_enabled,• Level 1 - Master Node • Level 2 - Master Node,"When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
1.2.9,Ensure that the admission control plugin EventRateLimit is set,Manual,Limit the rate at which the API server accepts requests.,"Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Note: This is an Alpha feature in the Kubernetes 1.15 release. Impact: You need to carefully tune in limits as per your environment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit.,"Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=<path/to/configuration/file> Default Value: By default, EventRateLimit is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md",kubernetes_api_server_event_rate_limit_enabled; kubernetes_admission_control_event_rate_limit_configured; kubernetes_api_request_rate_limit_enabled; kubernetes_event_rate_limit_plugin_active; kubernetes_admission_event_rate_limit_enforced,• Level 1 - Master Node • Level 2 - Master Node,You need to carefully tune in limits as per your environment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md
1.2.10,Ensure that the admission control plugin AlwaysAdmit is not set,Automated,Do not allow all requests.,Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Impact: Only requests explicitly allowed by the admissions control plugins would be served.,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --enable-admission- plugins parameter, or set it to a value that does not include AlwaysAdmit. Default Value: AlwaysAdmit is not in the list of default admission plugins. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit",kubernetes_admission_controller_always_admit_disabled; kubernetes_admission_plugin_always_admit_not_set; kubernetes_admission_policy_always_admit_restricted; kubernetes_admission_control_always_admit_denied; kubernetes_admission_rule_always_admit_blocked,• Level 1 - Master Node • Level 2 - Master Node,Only requests explicitly allowed by the admissions control plugins would be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit
1.2.11,Ensure that the admission control plugin AlwaysPullImages is set,Manual,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages",kubernetes_admission_controller_always_pull_images_enabled; kubernetes_pod_image_always_pull_enabled; kubernetes_admission_plugin_always_pull_images_set; kubernetes_container_image_always_pull_enabled; kubernetes_admission_control_always_pull_images_required,• Level 1 - Master Node • Level 2 - Master Node,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages
1.2.12,Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used,Manual,The SecurityContextDeny admission controller can be used to deny pods which make use of some SecurityContext fields which could allow for privilege escalation in the cluster. This should be used where PodSecurityPolicy is not in place within the cluster.,"SecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled. Impact: This admission controller should only be used where Pod Security Policies cannot be used on the cluster, as it can interact poorly with certain Pod Security Policies","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes SecurityContextDeny, if PodSecurityPolicy is not included.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include SecurityContextDeny, unless PodSecurityPolicy is already in place. --enable-admission-plugins=...,SecurityContextDeny,... Default Value: By default, SecurityContextDeny is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny 3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac",kubernetes_admission_controller_security_context_deny_enabled; kubernetes_pod_security_context_deny_required; kubernetes_admission_controller_security_context_deny_without_psp; kubernetes_pod_security_privilege_escalation_blocked; kubernetes_admission_controller_security_context_deny_active,• Level 1 - Master Node • Level 2 - Master Node,"This admission controller should only be used where Pod Security Policies cannot be used on the cluster, as it can interact poorly with certain Pod Security Policies",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny 3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac
1.2.13,Ensure that the admission control plugin ServiceAccount is set,Automated,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Impact: None.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and ensure that the --disable-admission-plugins parameter is set to a value that does not include ServiceAccount. Default Value: By default, ServiceAccount is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_service_account_admission_plugin_enabled; kubernetes_admission_controller_service_account_required; kubernetes_service_account_automated_management_enabled; kubernetes_admission_plugin_service_account_enforced; kubernetes_service_account_admission_control_active,• Level 1 - Master Node • Level 2 - Master Node,None.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.2.14,Ensure that the admission control plugin NamespaceLifecycle is set,Automated,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --disable-admission-plugins parameter to ensure it does not include NamespaceLifecycle. Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle",kubernetes_namespace_admission_control_namespace_lifecycle_enabled; kubernetes_namespace_admission_plugin_termination_protection_enabled; kubernetes_namespace_lifecycle_admission_control_enabled; kubernetes_admission_plugin_namespace_termination_protection_enabled; kubernetes_namespace_termination_admission_control_enabled,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle
1.2.15,Ensure that the admission control plugin NodeRestriction is set,Automated,Limit the Node and Pod objects that a kubelet could modify.,"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes NodeRestriction.,"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the -- enable-admission-plugins parameter to a value that includes NodeRestriction. --enable-admission-plugins=...,NodeRestriction,... Default Value: By default, NodeRestriction is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_noderestriction_enabled; kubernetes_admission_plugin_noderestriction_active; kubernetes_node_pod_restriction_enforced; kubernetes_kubelet_object_modification_restricted; kubernetes_admission_noderestriction_plugin_enabled,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security
1.2.16,Ensure that the --secure-port argument is not set to 0 - NoteThis recommendation is obsolete and will be deleted per the consensus process.,Manual,This recommendation is obsolete and will be deleted per the consensus process.,"The secure port is used to serve https with authentication and authorization. If you disable it, no https traffic is served and all traffic is served unencrypted. Impact: You need to set the API Server up with the right TLS certificates.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --secure-port argument is either not set or is set to an integer value between 1 and 65535.,Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --secure-port parameter or set it to a different (non-zero) desired port. Default Value: This value can no longer be changed or set to 0. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/,kubernetes_api_server_secure_port_not_disabled; kubernetes_api_server_secure_port_configured; kubernetes_api_server_secure_port_valid; kubernetes_api_server_secure_port_non_zero,• Level 1 - Master Node • Level 2 - Master Node,You need to set the API Server up with the right TLS certificates.,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.17,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; kubernetes_cluster_profiling_disabled; container_service_profiling_disabled; k8s_cluster_profiling_disabled; eks_node_group_profiling_disabled,• Level 1 - Master Node • Level 2 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.2.18,Ensure that the --audit-log-path argument is set,Automated,Enable auditing on the Kubernetes API Server and set the desired audit log path.,"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_path_set; kubernetes_api_server_audit_logging_enabled; kubernetes_api_server_audit_log_path_configured; kubernetes_api_server_audit_log_path_valid; kubernetes_api_server_audit_log_path_specified,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.19,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Automated,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxage_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_compliant; kubernetes_audit_log_retention_30d; kubernetes_api_server_audit_log_duration_valid,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.20,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Automated,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_10_or_more; kubernetes_audit_log_retention_configured; kubernetes_audit_log_maxbackup_valid; kubernetes_api_server_log_retention_compliant,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.21,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Automated,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_audit_log_rotation_size_configured; kubernetes_audit_log_maxsize_within_limit; kubernetes_api_server_audit_log_size_appropriate,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.22,Ensure that the --request-timeout argument is set as appropriate,Manual,Set global request timeout for API server requests as appropriate.,"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --request-timeout argument is either not set or set to an appropriate value.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s Default Value: By default, --request-timeout is set to 60 seconds. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415",kubernetes_api_server_request_timeout_set; kubernetes_api_server_request_timeout_configured; kubernetes_api_server_request_timeout_appropriate; kubernetes_api_server_request_timeout_valid; kubernetes_api_server_request_timeout_within_range,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415
1.2.23,Ensure that the --service-account-lookup argument is set to true,Automated,Validate service account before validating token.,"If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --service-account-lookup argument exists it is set to true.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --service-account-lookup=true Alternatively, you can delete the --service-account-lookup parameter from this file so that the default takes effect. Default Value: By default, --service-account-lookup argument is set to true. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_service_account_pre_validation_enabled; kubernetes_api_server_service_account_token_validation_enabled; kubernetes_api_server_service_account_lookup_before_validation_enabled,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use
1.2.24,Ensure that the --service-account-key-file argument is set as appropriate,Automated,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file. Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --service-account-key-file parameter to the public key file for service accounts: --service-account-key-file=<filename> Default Value: By default, --service-account-key-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_explicitly_set,• Level 1 - Master Node • Level 2 - Master Node,The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.2.25,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters. --etcd-certfile=<path/to/client-certificate-file> --etcd-keyfile=<path/to/client-key-file> Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_certfile_configured; kubernetes_etcd_keyfile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_connection_required,• Level 1 - Master Node • Level 2 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.26,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Automated,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the TLS certificate and private key file parameters. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Default Value: By default, --tls-cert-file and --tls-private-key-file are presented and created for use. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_configured; kubernetes_api_server_tls_cert_key_valid; kubernetes_api_server_tls_encryption_enabled,• Level 1 - Master Node • Level 2 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.27,Ensure that the --client-ca-file argument is set as appropriate,Automated,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the client certificate authority file. --client-ca-file=<path/to/client-ca-file> Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_set; kubernetes_api_server_tls_connection_configured; kubernetes_api_server_client_ca_file_valid; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_ca_file_secure,• Level 1 - Master Node • Level 2 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.28,Ensure that the --etcd-cafile argument is set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter. --etcd-cafile=<path/to/ca-file> Default Value: By default, --etcd-cafile is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_cafile_configured; kubernetes_etcd_client_tls_enabled; etcd_cafile_argument_set; etcd_tls_client_auth_enabled,• Level 1 - Master Node • Level 2 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.29,Ensure that the --encryption-provider-config argument is set as appropriate,Manual,Encrypt etcd key-value store.,etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Impact: None,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --encryption-provider-config argument is set to a EncryptionConfig file. Additionally, ensure that the EncryptionConfig file has all the desired resources covered especially any secrets.","Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=</path/to/EncryptionConfig/File> Default Value: By default, --encryption-provider-config is not set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92",etcd_store_encryption_enabled; etcd_store_encryption_provider_configured; etcd_store_encryption_config_valid; etcd_store_encryption_key_secure; etcd_store_encryption_tls_enabled,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92
1.2.30,Ensure that encryption providers are appropriately configured,Manual,"Where etcd encryption is used, appropriate providers should be configured.","Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms and secretbox are likely to be appropriate options. Impact: None","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Get the EncryptionConfig file set for --encryption-provider-config argument. Verify that aescbc, kms or secretbox is set as the encryption provider for all the desired resources.","Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms or secretbox as the encryption provider. Default Value: By default, no encryption provider is set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers",etcd_encryption_provider_configured; etcd_encryption_provider_secure; etcd_encryption_provider_valid; etcd_encryption_provider_approved; etcd_encryption_provider_compliant; etcd_encryption_provider_correctly_configured; etcd_encryption_provider_properly_set; etcd_encryption_provider_securely_configured,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers
1.2.31,Ensure that the API Server only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the API server is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter. --tls-cipher- suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SH A256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SH A256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SH A384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POL Y1305_SHA256,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_C BC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_S HA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 ,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TL S_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_2 56_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384. Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers References: 1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_api_server_strong_ciphers_enabled; kubernetes_api_server_weak_ciphers_disabled; kubernetes_api_server_tls_min_version_1_2; kubernetes_api_server_cipher_suite_restricted; kubernetes_api_server_insecure_ciphers_removed; kubernetes_api_server_secure_cipher_list_enforced; kubernetes_api_server_cryptographic_ciphers_compliant; kubernetes_api_server_legacy_ciphers_blocked,• Level 1 - Master Node • Level 2 - Master Node,API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.,"1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues"
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Manual,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --terminated-pod-gc- threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_pod_terminated_gc_threshold_set; kubernetes_pod_terminated_gc_threshold_configured; kubernetes_pod_gc_threshold_set; kubernetes_pod_termination_gc_enabled; kubernetes_pod_termination_gc_threshold_configured,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_api_server_profiling_disabled; kubernetes_api_server_no_profiling; kubernetes_api_server_profiling_argument_false; kubernetes_api_server_profiling_set_false; kubernetes_api_server_profiling_config_disabled,• Level 1 - Master Node • Level 2 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.3.3,Ensure that the --use-service-account-credentials argument is set to true,Automated,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service- account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node to set the below parameter. --use-service-account-credentials=true Default Value: By default, --use-service-account-credentials is set to false. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",gke_cluster_use_service_account_credentials_enabled; gke_controller_service_account_credentials_required; gke_cluster_service_account_credentials_enabled; gke_controller_individual_service_account_required; gke_cluster_service_account_credentials_true; gke_controller_service_account_credentials_true; gke_cluster_controller_service_account_credentials_enabled; gke_controller_service_account_credentials_set,• Level 1 - Master Node • Level 2 - Master Node,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.4,Ensure that the --service-account-private-key-file argument is set as appropriate,Automated,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private- key-file as appropriate. Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --service-account- private-key-file parameter to the private key file for service accounts. --service-account-private-key-file=<filename> Default Value: By default, --service-account-private-key-file it not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_specified; kubernetes_controller_manager_service_account_private_key_file_valid; kubernetes_controller_manager_service_account_private_key_file_secure,• Level 1 - Master Node • Level 2 - Master Node,You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.5,Ensure that the --root-ca-file argument is set as appropriate,Automated,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Impact: You need to setup and maintain root certificate authority file.,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`. --root-ca-file=<path/to/file> Default Value: By default, --root-ca-file is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_api_server_root_ca_file_configured; kubernetes_api_server_root_ca_file_valid; kubernetes_api_server_root_ca_file_secure; kubernetes_api_server_root_ca_file_trusted; kubernetes_api_server_root_ca_file_present,• Level 1 - Master Node • Level 2 - Master Node,You need to setup and maintain root certificate authority file.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.3.6,Ensure that the RotateKubeletServerCertificate argument is set to true,Automated,Enable kubelet server certificate rotation on controller-manager.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true. --feature-gates=RotateKubeletServerCertificate=true Default Value: By default, RotateKubeletServerCertificate is set to 'true' this recommendation verifies that it has not been disabled. References: 1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_rotate_kubelet_server_certificate_enabled; kubernetes_controller_manager_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubernetes_controller_manager_automatic_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_auto_rotation_enabled,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.7,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the Controller Manager service to non-loopback insecure addresses.,"The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address",kubernetes_controller_manager_bind_address_localhost; kubernetes_controller_manager_network_restricted; kubernetes_controller_manager_loopback_only; kubernetes_controller_manager_secure_bind_config; kubernetes_controller_manager_localhost_bind_enabled,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address
1.4.1,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml file on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",compute_cluster_profiling_disabled; compute_cluster_profiling_set_false; compute_cluster_profiling_not_enabled; compute_cluster_profiling_disabled_by_default; compute_cluster_profiling_config_false,• Level 1 - Master Node • Level 2 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.4.2,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the scheduler service to non-loopback insecure addresses.,"The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml on the Control Plane node and ensure the correct value for the --bind- address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/",kubernetes_scheduler_bind_address_localhost; kubernetes_scheduler_loopback_only; kubernetes_scheduler_insecure_bind_disabled; kubernetes_scheduler_bind_address_restricted; kubernetes_scheduler_localhost_bind_enabled,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/
2.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_tls_cert_key_valid; etcd_service_secure_communication_enabled,• Level 1 - Master Node • Level 2 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.2,Ensure that the --client-cert-auth argument is set to true,Automated,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: All clients attempting to access the etcd server will require a valid client certificate.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --client-cert-auth='true' Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_auth_enabled; etcd_client_cert_auth_required; etcd_tls_client_authentication_enabled; etcd_client_certificate_authentication_enabled; etcd_secure_client_auth_enabled,• Level 1 - Master Node • Level 2 - Master Node,All clients attempting to access the etcd server will require a valid client certificate.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
2.3,Ensure that the --auto-tls argument is not set to true,Automated,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: Clients will not be able to use self-signed certificates for TLS.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --auto-tls parameter or set it to false. --auto-tls=false Default Value: By default, --auto-tls is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_domain_self_signed_certificates_disabled; cloud_cdn_domain_tls_certificate_valid; cloud_cdn_domain_tls_managed_certificates_enabled,• Level 1 - Master Node • Level 2 - Master Node,Clients will not be able to use self-signed certificates for TLS.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
2.4,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Impact: etcd cluster peers would need to set up TLS for their communication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --peer-client-file=</path/to/peer-cert-file> --peer-key-file=</path/to/peer-key-file> Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_peer_certificate_file_set; etcd_peer_key_file_set; etcd_peer_tls_encryption_enabled; etcd_peer_certificate_file_configured; etcd_peer_key_file_configured,• Level 1 - Master Node • Level 2 - Master Node,etcd cluster peers would need to set up TLS for their communication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.5,Ensure that the --peer-client-cert-auth argument is set to true,Automated,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --peer-client-cert-auth=true Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth",etcd_peer_client_cert_auth_enabled; etcd_peer_authentication_required; etcd_client_cert_auth_enabled; etcd_peer_tls_auth_enabled; etcd_peer_cert_auth_enabled,• Level 1 - Master Node • Level 2 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth
2.6,Ensure that the --peer-auto-tls argument is not set to true,Automated,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --peer-auto-tls parameter or set it to false. --peer-auto-tls=false Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",compute_cluster_peer_auto_tls_disabled; compute_cluster_peer_tls_self_signed_disabled; compute_cluster_peer_tls_manual_cert_required; compute_cluster_peer_auto_tls_not_true; compute_cluster_peer_tls_auto_gen_disabled,• Level 1 - Master Node • Level 2 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
2.7,Ensure that a unique Certificate Authority is used for etcd,Manual,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster. Run the following command on the master node: ps -ef | grep etcd Note the file referenced by the --trusted-ca-file argument. Run the following command on the master node: ps -ef | grep apiserver Verify that the file referenced by the --client-ca-file for apiserver is different from the --trusted-ca-file used by etcd.,"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --trusted-ca-file=</path/to/ca-file> Default Value: By default, no etcd certificate is created and used. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_unique_certificate_authority; etcd_certificate_authority_separate_from_kubernetes; etcd_certificate_authority_unique; kubernetes_etcd_certificate_authority_isolated; etcd_ca_not_shared_with_kubernetes,• Level 2 - Master Node,Additional management of the certificates and keys for the dedicated certificate authority will be required.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
3.1.1,Client certificate authentication should not be used for users,Manual,"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication.,"Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. Default Value: Client certificate authentication is enabled by default. Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",kubernetes_user_no_client_cert_auth; kubernetes_user_client_cert_auth_disabled; kubernetes_auth_no_user_client_certs; kubernetes_user_auth_no_client_certificates; kubernetes_auth_client_cert_revocation_unsupported,• Level 1 - Master Node • Level 2 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.2,Service account token authentication should not be used for users,Manual,"Kubernetes provides service account tokens which are intended for use by workloads running in the Kubernetes cluster, for authentication to the API server. These tokens are not designed for use by end-users and do not provide for features such as revocation or expiry, making them insecure. A newer version of the feature (Bound service account token volumes) does introduce expiry but still does not allow for specific revocation.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Service account token authentication does not allow for this due to the use of JWT tokens as an underlying technology. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of service account token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of service account tokens. Default Value: Service account token authentication is enabled by default.,kubernetes_service_account_token_authentication_disabled; kubernetes_service_account_user_authentication_disabled; kubernetes_service_account_no_user_tokens; kubernetes_service_account_token_revocation_enabled; kubernetes_service_account_token_expiry_enabled; kubernetes_service_account_bound_token_volumes_enabled; kubernetes_service_account_no_static_tokens; kubernetes_service_account_token_rotation_enabled,• Level 1 - Master Node • Level 2 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.3,Bootstrap token authentication should not be used for users,Manual,Kubernetes provides bootstrap tokens which are intended for use by new nodes joining the cluster These tokens are not designed for use by end-users they are specifically designed for the purpose of bootstrapping new nodes and not for general authentication,Bootstrap tokens are not intended for use as a general authentication mechanism and impose constraints on user and group naming that do not facilitate good RBAC design. They also cannot be used with MFA resulting in a weak authentication mechanism being available. Impact: External mechanisms for authentication generally require additional software to be deployed.,Review user access to the cluster and ensure that users are not making use of bootstrap token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of bootstrap tokens. Default Value: Bootstrap token authentication is not enabled by default and requires an API server parameter to be set.,kubernetes_user_no_bootstrap_token_auth; kubernetes_user_bootstrap_token_disabled; kubernetes_auth_no_bootstrap_token_user; kubernetes_auth_bootstrap_token_restricted; kubernetes_user_no_node_bootstrap_token,• Level 1 - Master Node • Level 2 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.2.1,Ensure that a minimal audit policy is created,Manual,Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.,"Logging is an important detective control for all systems, to detect potential unauthorised access. Impact: Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",Run the following command on one of the cluster master nodes: ps -ef | grep kube-apiserver Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy.,"Create an audit policy file for your cluster. Default Value: Unless the --audit-policy-file flag is specified, no auditing will be carried out. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/",kubernetes_api_audit_policy_enabled; kubernetes_api_audit_policy_minimal; kubernetes_api_audit_logging_enabled; kubernetes_api_audit_policy_file_configured; kubernetes_api_audit_policy_file_exists; kubernetes_api_audit_policy_file_valid; kubernetes_api_audit_policy_file_minimal_rules; kubernetes_api_audit_policy_file_required; kubernetes_api_audit_policy_file_active; kubernetes_api_audit_policy_file_properly_set,• Level 1 - Master Node • Level 2 - Master Node,"Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
3.2.2,Ensure that the audit policy covers key security concerns,Manual,Ensure that the audit policy created for the cluster covers key security concerns.,"Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Impact: Increasing audit logging will consume resources on the nodes or other log destination.","Review the audit policy provided for the cluster and ensure that it covers at least the following areas :- • Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data. • Modification of pod and deployment objects. • Use of pods/exec, pods/portforward, pods/proxy and services/proxy. For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","Consider modification of the audit policy in use on the cluster to include these items, at a minimum. Default Value: By default Kubernetes clusters do not log audit information. References: 1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735",kubernetes_audit_policy_security_concerns_covered; kubernetes_audit_policy_key_security_events_enabled; kubernetes_audit_policy_comprehensive_logging_enabled; kubernetes_audit_policy_sensitive_actions_monitored; kubernetes_audit_policy_admin_activities_tracked; kubernetes_audit_policy_authentication_events_logged; kubernetes_audit_policy_authorization_events_logged; kubernetes_audit_policy_security_relevant_metadata_included,• Level 2 - Master Node,Increasing audit logging will consume resources on the nodes or other log destination.,1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735
4.1.1,Ensure that the kubelet service file permissions are set to 644 or more restrictive,Automated,Ensure that the kubelet service file has permissions of 644 or more restrictive.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, the kubelet service file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_permissions_644_or_restrictive; kubernetes_kubelet_service_file_permissions_restrictive; kubernetes_kubelet_file_permissions_secure; kubernetes_kubelet_config_file_permissions_restrictive; kubernetes_service_file_permissions_644_or_stricter,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.2,Ensure that the kubelet service file ownership is set to root:root,Automated,Ensure that the kubelet service file ownership is set to root:root.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, kubelet service file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_ownership_root_root; kubernetes_kubelet_service_file_root_owned; kubernetes_kubelet_file_ownership_root; kubernetes_kubelet_service_root_ownership; kubernetes_kubelet_file_root_root_owned,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.3,If proxy kubeconfig file exists ensure permissions are set to 644 or more restrictive,Manual,"If kube-proxy is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644 or more restrictive.","The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None","Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a <path><filename> Verify that a file is specified and it exists with permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 <proxy kubeconfig file> Default Value: By default, proxy file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_file_permissions_restrictive; kubernetes_proxy_kubeconfig_file_permissions_644_or_stricter; kubernetes_proxy_kubeconfig_file_permissions_secure; kubernetes_proxy_kubeconfig_file_permissions_compliant; kubernetes_proxy_kubeconfig_file_permissions_enforced,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.4,If proxy kubeconfig file exists ensure ownership is set to root:root,Manual,"If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G <path><filename> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",compute_kubeconfig_file_ownership_root; compute_kubeconfig_file_group_ownership_root; compute_kubeconfig_file_permissions_restricted; compute_kubeconfig_file_exists_secure; compute_kubeconfig_file_not_world_readable; compute_kubeconfig_file_not_world_writable; compute_kubeconfig_file_not_group_writable; compute_kubeconfig_file_not_other_writable,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.5,Ensure that the --kubeconfig kubelet.conf file permissions are set to 644 or more restrictive,Automated,Ensure that the kubelet.conf file has permissions of 644 or more restrictive.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.Verify that the permissions are 644 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_permissions_restrictive; kubernetes_kubeconfig_file_permissions_644_or_stricter; kubernetes_kubelet_conf_file_permissions_restrictive; kubernetes_kubelet_conf_file_permissions_644_or_stricter; kubernetes_kubeconfig_kubelet_conf_file_permissions_restrictive,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.6,Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root,Automated,Ensure that the kubelet.conf file ownership is set to root:root.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U %G /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_root_ownership; kubernetes_kubelet_conf_root_ownership; kubernetes_config_file_root_ownership; kubernetes_kubeconfig_file_secure_ownership; kubernetes_kubelet_conf_secure_ownership,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.7,Ensure that the certificate authorities file permissions are set to 600 or more restrictive,Manual,Ensure that the certificate authorities file has permissions of 600 or more restrictive.,The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %a <filename> Verify that the permissions are 644 or more restrictive.,Run the following command to modify the file permissions of the --client-ca-file chmod 600 <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_ssl_certificate_file_permissions_600_or_stricter; compute_certificate_authority_file_permissions_restricted; compute_ca_file_permissions_600_or_less; compute_ssl_certificate_authority_permissions_restrictive; compute_ca_file_permissions_secure,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.8,Ensure that the client certificate authorities file ownership is set to root:root,Manual,Ensure that the certificate authorities file ownership is set to root:root.,The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %U:%G <filename> Verify that the ownership is set to root:root.,Run the following command to modify the ownership of the --client-ca-file. chown root:root <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_client_certificate_authorities_file_ownership_root; compute_ca_file_ownership_root; compute_certificate_authorities_file_root_owned; compute_client_ca_file_root_ownership; compute_ca_file_root_root_ownership,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.9,If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive,Manual,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 600 or more restrictive.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml Verify that the permissions are 600 or more restrictive.","Run the following command (using the config file location identied in the Audit step) chmod 600 /var/lib/kubelet/config.yaml Default Value: By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of 600. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubelet_config_file_permissions_restrictive; kubelet_config_file_permissions_600_or_stricter; kubelet_config_file_permissions_secure; kubelet_config_file_permissions_min_600; kubelet_config_file_permissions_protected,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.1.10,If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root,Manual,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml ```Verify that the ownership is set to `root:root`.","Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by root:root. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_owner_root; kubernetes_kubelet_config_file_group_root; kubernetes_kubelet_config_file_ownership_root_root; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_access_restricted,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.2.1,Ensure that the --anonymous-auth argument is set to false,Automated,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.","If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false. This executable argument may be omitted, provided there is a corresponding entry set to false in the Kubelet config file.","If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --anonymous-auth=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_auth_enabled; kubernetes_kubelet_no_anonymous_access; kubernetes_kubelet_secure_auth_required; kubernetes_kubelet_authentication_enforced,• Level 1 - Worker Node • Level 2 - Worker Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.2,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.","Run the following command on each node: ps -ef | grep kubelet If the --authorization-mode argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by --config, and that file sets authorization: mode to something other than AlwaysAllow. It is also possible to review the running configuration of a Kubelet via the /configz endpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --authorization-mode=Webhook Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_explicit_authorization_enabled; kubernetes_api_server_secure_authorization_mode; kubernetes_api_server_restrictive_authorization_enabled; kubernetes_api_server_no_always_allow_authorization,• Level 1 - Worker Node • Level 2 - Worker Node,Unauthorized requests will be denied.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.3,Ensure that the --client-ca-file argument is set as appropriate,Automated,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. If the --client-ca-file argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets authentication: x509: clientCAFile to the location of the client certificate authority file.","If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to the location of the client CA file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --client-ca-file=<path/to/client-ca-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/",kubernetes_kubelet_client_ca_file_set; kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_ca_file_configured; kubernetes_kubelet_authentication_certificates_valid; kubernetes_kubelet_client_ca_file_present,• Level 1 - Worker Node • Level 2 - Worker Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/
4.2.4,Verify that the --read-only-port argument is set to 0,Manual,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,"Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.","If using a Kubelet config file, edit the file to set readOnlyPort to 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --read-only-port=0 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --read-only-port is set to 10255/TCP. However, if a config file is specified by --config the default value for readOnlyPort is 0. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_api_server_read_only_port_disabled; kubernetes_api_server_read_only_port_set_zero; kubernetes_api_server_read_only_port_unset; kubernetes_api_server_read_only_port_secure; kubernetes_api_server_read_only_port_restricted,• Level 1 - Worker Node • Level 2 - Worker Node,Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,1. https://kubernetes.io/docs/admin/kubelet/
4.2.5,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Manual,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.","Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0. If the argument is not present, and there is a Kubelet config file specified by --config, check that it does not set streamingConnectionIdleTimeout to 0.","If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a value other than 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --streaming-connection-idle-timeout=5m Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",kubernetes_api_server_streaming_connection_idle_timeout_not_disabled; kubernetes_api_server_streaming_connection_timeout_enabled; kubernetes_api_server_streaming_idle_timeout_configured; kubernetes_api_server_streaming_timeout_non_zero; kubernetes_api_server_connection_idle_timeout_set,• Level 1 - Worker Node • Level 2 - Worker Node,Long-lived connections could be interrupted.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
4.2.6,Ensure that the --make-iptables-util-chains argument is set to true,Automated,Allow Kubelet to manage iptables.,"Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.","Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.","If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove the --make- iptables-util-chains argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --make-iptables-util-chains argument is set to true. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_management_enabled; kubernetes_kubelet_iptables_chains_configured; kubernetes_kubelet_iptables_util_chains_set_true; kubernetes_kubelet_iptables_util_chains_configured,• Level 1 - Worker Node • Level 2 - Worker Node,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",1. https://kubernetes.io/docs/admin/kubelet/
4.2.7,Ensure that the --hostname-override argument is not set,Manual,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Impact: Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist. Note This setting is not configurable via the Kubelet config file.,"Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_default_configured; kubernetes_node_hostname_override_absent; kubernetes_node_hostname_standard_compliance,• Level 1 - Worker Node • Level 2 - Worker Node,"Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063
4.2.8,Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture,Manual,"Security relevant information should be captured. The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,"Run the following command on each node: sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10- kubeadm.conf Review the value set for the argument and determine whether this has been set to an appropriate level for the cluster. If the argument does not exist, check that there is a Kubelet config file specified by -- config and review the value in this location.","If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, eventRecordQPS argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go",kubernetes_kubelet_event_record_qps_configured; kubernetes_kubelet_event_capture_rate_limited; kubernetes_kubelet_event_record_qps_non_zero; kubernetes_kubelet_event_logging_rate_optimized; kubernetes_kubelet_event_record_qps_dos_protected,• Level 2 - Worker Node,Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go
4.2.9,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Manual,Setup TLS connection on the Kubelets.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.","Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. If these arguments are not present, check that there is a Kubelet config specified by -- config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_connection_configured; kubernetes_kubelet_tls_cert_and_key_valid; kubernetes_kubelet_tls_encryption_enabled,• Level 1 - Worker Node • Level 2 - Worker Node,,
4.2.10,Ensure that the --rotate-certificates argument is not set to false,Automated,Enable kubelet client certificate rotation.,The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None,"Run the following command on each node: ps -ef | grep kubelet Verify that the --rotate-certificates argument is not present, or is set to true. If the --rotate-certificates argument is not present, verify that if there is a Kubelet config file specified by --config, that file does not contain rotateCertificates: false.","If using a Kubelet config file, edit the file to add the line rotateCertificates: true or remove it altogether to use the default value. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove --rotate- certificates=false argument from the KUBELET_CERTIFICATE_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet client certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_rotate_certificates_not_false; kubernetes_kubelet_client_cert_rotation_enabled; kubernetes_kubelet_tls_cert_rotation_enabled; kubernetes_kubelet_auto_cert_rotation_enabled,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/
4.2.11,Verify that the RotateKubeletServerCertificate argument is set to true,Manual,Enable kubelet server certificate rotation.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate- server-certificates parameter is set on kubelet Run the following command on each node: ps -ef | grep kubelet Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable. --feature-gates=RotateKubeletServerCertificate=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet server certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",compute_kubelet_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_rotation_enabled; kubelet_server_certificate_rotation_enabled; compute_kubelet_server_certificate_rotation_enabled; kubernetes_kubelet_certificate_rotation_enabled,• Level 1 - Worker Node • Level 2 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration
4.2.12,Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the Kubelet is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.","The set of cryptographic ciphers currently considered secure is the following: • TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 • TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_128_GCM_SHA256 Run the following command on each node: ps -ef | grep kubelet If the --tls-cipher-suites argument is present, ensure it only contains values included in this set. If it is not present check that there is a Kubelet config file specified by --config, and that file sets TLSCipherSuites: to only include values from this set.","If using a Kubelet config file, edit the file to set TLSCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 ,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 ,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 ,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the --tls-cipher-suites parameter as follows, or to a subset of these values. --tls-cipher- suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_kubelet_strong_ciphers_enabled; kubernetes_kubelet_weak_ciphers_disabled; kubernetes_kubelet_tls_ciphers_restricted; kubernetes_kubelet_cipher_suite_compliant; kubernetes_kubelet_secure_ciphers_only,• Level 1 - Worker Node • Level 2 - Worker Node,Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.,
4.2.13,Ensure that a limit is set on pod PIDs,Manual,Ensure that the Kubelet sets limits on the number of PIDs that can be created by pods running on the node.,"By default pods running in a cluster can consume any number of PIDs, potentially exhausting the resources available on the node. Setting an appropriate limit reduces the risk of a denial of service attack on cluster nodes. Impact: Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.","Review the Kubelet's start-up parameters for the value of --pod-max-pids, and check the Kubelet configuration file for the PodPidsLimit . If neither of these values is set, then there is no limit in place.","Decide on an appropriate level for this parameter and set it, either via the --pod-max- pids command line parameter or the PodPidsLimit configuration file setting. Default Value: By default the number of PIDs is not limited. References: 1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 5 Policies This section contains recommendations for various Kubernetes policies which are important to the security of the environment. 5.1 RBAC and Service Accounts",kubernetes_kubelet_pid_limit_enabled; kubernetes_pod_pid_limit_configured; kubernetes_node_pid_limit_enforced; kubernetes_kubelet_pid_limit_per_pod; kubernetes_pod_pid_limit_set,• Level 1 - Worker Node • Level 2 - Worker Node,Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.,1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 5 Policies This section contains recommendations for various Kubernetes policies which are important to the security of the environment. 5.1 RBAC and Service Accounts
5.1.1,Ensure that the cluster-admin role is only used where required,Manual,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",kubernetes_role_cluster_admin_restricted; kubernetes_role_cluster_admin_minimal_usage; kubernetes_role_cluster_admin_least_privilege; kubernetes_role_cluster_admin_no_unnecessary_assignments; kubernetes_role_cluster_admin_required_only,• Level 1 - Master Node • Level 2 - Master Node,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
5.1.2,Minimize access to secrets,Manual,"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation,"Review the users who have get, list or watch access to secrets objects in the Kubernetes API.","Where possible, remove get, list and watch access to secret objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have get privileges on secret objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:expand-controller expand-controller ServiceAccount kube-system system:controller:generic-garbage-collector generic-garbage- collector ServiceAccount kube-system system:controller:namespace-controller namespace-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:kube-controller-manager system:kube-controller- manager User",kubernetes_secret_access_restricted; kubernetes_secret_minimal_access; kubernetes_secret_no_public_access; kubernetes_secret_no_anonymous_access; kubernetes_secret_no_wildcard_access; kubernetes_secret_no_default_access; kubernetes_secret_no_broad_permissions; kubernetes_secret_no_unrestricted_access; kubernetes_secret_no_excessive_permissions; kubernetes_secret_no_unauthorized_access,• Level 1 - Master Node • Level 2 - Master Node,Care should be taken not to remove access to secrets to system components which require this for their operation,
5.1.3,Minimize wildcard use in Roles and ClusterRoles,Manual,Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard '*' which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.,The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.,Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml,Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.,kubernetes_role_wildcard_minimized; kubernetes_clusterrole_wildcard_minimized; kubernetes_role_resource_restricted; kubernetes_clusterrole_resource_restricted; kubernetes_role_action_restricted; kubernetes_clusterrole_action_restricted; kubernetes_role_no_wildcard_resources; kubernetes_clusterrole_no_wildcard_resources; kubernetes_role_no_wildcard_actions; kubernetes_clusterrole_no_wildcard_actions,• Level 1 - Master Node • Level 2 - Master Node,,
5.1.4,Minimize access to create pods,Manual,"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.","The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",Review the users who have create access to pod objects in the Kubernetes API.,"Where possible, remove create access to pod objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have create privileges on pod objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:daemon-set-controller daemon-set-controller ServiceAccount kube-system system:controller:job-controller job-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:controller:replicaset-controller replicaset-controller ServiceAccount kube-system system:controller:replication-controller replication-controller ServiceAccount kube-system system:controller:statefulset-controller statefulset-controller ServiceAccount kube-system",kubernetes_namespace_pod_creation_restricted; kubernetes_role_pod_creation_minimized; kubernetes_user_pod_creation_limited; kubernetes_service_account_pod_creation_restricted; kubernetes_rbac_pod_creation_minimized; kubernetes_policy_pod_creation_controlled; kubernetes_cluster_pod_creation_restricted,• Level 1 - Master Node • Level 2 - Master Node,Care should be taken not to remove access to pods to system components which require this for their operation,
5.1.5,Ensure that default service accounts are not actively used.,Manual,The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.,"Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.","For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/,iam_service_account_default_not_used; iam_service_account_default_inactive; compute_service_account_default_disabled; compute_service_account_default_unused; service_account_default_no_active_usage,• Level 1 - Master Node • Level 2 - Master Node,All workloads which require access to the Kubernetes API will require an explicit service account to be created.,1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.6,Ensure that Service Account Tokens are only mounted where necessary,Manual,Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server,"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.","Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false","Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_pod_service_account_token_unmounted; kubernetes_pod_service_account_token_disabled; kubernetes_pod_service_account_token_restricted; kubernetes_pod_service_account_token_not_mounted; kubernetes_pod_service_account_token_minimal_access,• Level 1 - Master Node • Level 2 - Master Node,"Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.7,Avoid use of system:masters group,Manual,"The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed. When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster. Impact: Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.,Remove the system:masters group from all users in the cluster. Default Value: By default some clusters will create a 'break glass' client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations. References: 1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38,kubernetes_group_system_masters_restricted; kubernetes_group_system_masters_no_usage; kubernetes_group_system_masters_bootstrapping_only; kubernetes_group_system_masters_no_user_assignments; kubernetes_group_system_masters_no_service_account_assignments,• Level 1 - Master Node • Level 2 - Master Node,"Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38
5.1.8,"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",Manual,"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster- admin level. Impact: There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.","Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.","Where possible, remove the impersonate, bind and escalate rights from subjects. Default Value: In a default kubeadm cluster, the system:masters group and clusterrole-aggregation- controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate. References: 1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/",kubernetes_role_no_privilege_escalation; kubernetes_cluster_role_no_impersonation; kubernetes_role_no_bind_permission; kubernetes_cluster_role_no_escalate_permission; kubernetes_role_no_impersonate_permission; kubernetes_cluster_role_no_bind_permission; kubernetes_role_no_privilege_escalation_permissions; kubernetes_cluster_role_restricted_privilege_escalation,• Level 1 - Master Node • Level 2 - Master Node,"There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/
5.1.9,Minimize access to create persistent volumes,Manual,"The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of hostPath volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.","The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have create access to PersistentVolume objects in the Kubernetes API.,"Where possible, remove create access to PersistentVolume objects in the cluster. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation",kubernetes_persistent_volume_creation_restricted; kubernetes_persistent_volume_admin_access_minimized; kubernetes_persistent_volume_hostpath_creation_blocked; kubernetes_persistent_volume_privilege_escalation_prevented; kubernetes_persistent_volume_creation_scope_limited,• Level 1 - Master Node • Level 2 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation
5.1.10,Minimize access to the proxy sub-resource of nodes,Manual,"Users with access to the Proxy sub-resource of Node objects automatically have permissions to use the Kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs. The Kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the node object. The proxy sub-resource specifically allows wide ranging access to the Kubelet API. Direct access to the Kubelet API bypasses controls like audit logging (there is no audit log of Kubelet API access) and admission control.","The ability to use the proxy sub-resource of node objects opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have access to the proxy sub-resource of node objects in the Kubernetes API.,"Where possible, remove access to the proxy sub-resource of node objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization",kubernetes_node_proxy_access_restricted; kubernetes_node_proxy_no_wildcard_permissions; kubernetes_node_proxy_minimal_access; kubernetes_node_proxy_no_privilege_escalation; kubernetes_node_proxy_audit_bypass_prevented; kubernetes_node_proxy_kubelet_api_restricted; kubernetes_node_proxy_no_full_access; kubernetes_node_proxy_least_privilege_enforced,• Level 1 - Master Node • Level 2 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization
5.1.11,Minimize access to the approval sub-resource of certificatesigningrequests objects,Manual,"Users with access to the update the approval sub-resource of certificateaigningrequest objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts. This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster",The ability to update certificate signing requests should be limited.,Review the users who have access to update the approval sub-resource of certificatesigningrequest objects in the Kubernetes API.,"Where possible, remove access to the approval sub-resource of certificatesigningrequest objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing",kubernetes_certificatesigningrequest_approval_restricted; kubernetes_certificatesigningrequest_no_public_approval; kubernetes_certificatesigningrequest_approval_minimal_access; kubernetes_certificatesigningrequest_approval_privilege_escalation_prevented; kubernetes_certificatesigningrequest_approval_admin_restricted,• Level 1 - Master Node • Level 2 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing
5.1.12,Minimize access to webhook configuration objects,Manual,"Users with rights to create/modify/delete validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.",The ability to manage webhook configuration should be limited,Review the users who have access to validatingwebhookconfigurations or mutatingwebhookconfigurations objects in the Kubernetes API.,"Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks",kubernetes_validatingwebhookconfiguration_access_minimized; kubernetes_mutatingwebhookconfiguration_access_minimized; kubernetes_webhookconfiguration_admin_access_restricted; kubernetes_webhookconfiguration_write_access_restricted; kubernetes_webhookconfiguration_privileged_access_minimized,• Level 1 - Master Node • Level 2 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks
5.1.13,Minimize access to the service account token creation,Manual,"Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.",The ability to create service account tokens should be limited.,Review the users who have access to create the token sub-resource of serviceaccount objects in the Kubernetes API.,"Where possible, remove access to the token sub-resource of serviceaccount objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request",kubernetes_service_account_token_creation_restricted; kubernetes_service_account_token_creation_minimized; kubernetes_cluster_service_account_token_creation_limited; kubernetes_service_account_token_creation_admin_restricted; kubernetes_cluster_service_account_token_creation_privileged_restricted,• Level 1 - Master Node • Level 2 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request
5.2.1,Ensure that the cluster has at least one active policy control mechanism in place,Manual,"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts. Impact: Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.","Pod Security Admission is enabled by default on all clusters using Kubernetes 1.23 or higher. To assess what controls, if any, are in place using this mechanism, review the namespaces in the cluster to see if the required labels have been applied kubectl get namespaces -o yaml To confirm if any external policy control system is in use.","Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads. Default Value: By default, Pod Security Standards define three different policies available to apply. Privileged, Baseline or Restricted. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",kubernetes_cluster_policy_control_enabled; kubernetes_cluster_pod_security_admission_enabled; kubernetes_cluster_third_party_policy_control_enabled; kubernetes_cluster_active_policy_control_exists; kubernetes_cluster_policy_enforcement_mechanism_active,• Level 1 - Master Node • Level 2 - Master Node,"Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",1. https://kubernetes.io/docs/concepts/security/pod-security-admission 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.2,Minimize the admission of privileged containers,Manual,Do not generally permit containers to be run with the securityContext.privileged flag set to true.,"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. Privleged pods disable most security mechanisms and should be disallowed. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of privileged containers. This audit may also require that you review the securityContext in the Pod specification yaml.","Add security context which has user workloads to restrict the admission of privileged containers. Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privileged_disabled; compute_container_privileged_denied; compute_container_privileged_restricted; compute_container_security_context_privileged_false; compute_container_privileged_escalation_disabled,• Level 1 - Master Node • Level 2 - Master Node,"Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.3,Minimize the admission of containers wishing to share the host process ID namespace,Manual,Do not generally permit containers to be run with the hostPID flag set to true.,"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostPID containers. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",,• Level 1 - Master Node • Level 2 - Master Node,Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.4,Minimize the admission of containers wishing to share the host IPC namespace,Manual,Do not generally permit containers to be run with the hostIPC flag set to true.,"A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostIPC containers. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ipc_disabled; compute_container_host_ipc_restricted; compute_container_host_ipc_not_shared; compute_container_host_ipc_denied; compute_container_host_ipc_protected,• Level 1 - Master Node • Level 2 - Master Node,Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.5,Minimize the admission of containers wishing to share the host network namespace,Manual,Do not generally permit containers to be run with the hostNetwork flag set to true.,"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostNetwork containers. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_network_disabled; compute_container_host_network_restricted; compute_container_host_network_prohibited; compute_container_host_network_denied; compute_container_host_network_not_shared,• Level 1 - Master Node • Level 2 - Master Node,Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.6,Minimize the admission of containers with allowPrivilegeEscalation,Manual,"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with .spec.allowPrivilegeEscalationset to true. Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privilege_escalation_disabled; compute_container_allow_privilege_escalation_false; compute_container_privilege_escalation_restricted; compute_container_privilege_escalation_denied; compute_container_privilege_escalation_minimized,• Level 1 - Master Node • Level 2 - Master Node,Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.7,Minimize the admission of root containers,Manual,Do not generally permit containers to be run as the root user.,"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one admission control policy defined which does not permit root containers. If you need to run root containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run as the root user will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0. This audit may also require that you review the securityContext in the Pod specification yaml.","Create a policy for each namespace in the cluster, ensuring that either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0, is set. Default Value: By default, there are no restrictions on the use of root containers and if a User is not specified in the image, the container will run as root. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_root_user_disabled; compute_container_root_privileges_restricted; compute_container_non_root_user_required; compute_container_root_admission_minimized; compute_container_root_execution_denied,• Level 2 - Master Node,Pods with containers which run as the root user will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.8,Minimize the admission of containers with the NET_RAW capability,Manual,Do not generally permit containers with the potentially dangerous NET_RAW capability.,"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability. If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run with the NET_RAW capability will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the NET_RAW capability. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the NET_RAW capability. Default Value: By default, there are no restrictions on the creation of containers with the NET_RAW capability. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_net_raw_capability_disabled; compute_container_net_raw_capability_restricted; compute_container_net_raw_capability_minimized; compute_container_net_raw_capability_denied; compute_container_net_raw_capability_prohibited,• Level 1 - Master Node • Level 2 - Master Node,Pods with containers which run with the NET_RAW capability will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.9,Minimize the admission of containers with added capabilities,Manual,Do not generally permit containers with capabilities assigned beyond the default set.,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which require capabilities outwith the default set will not be permitted.","List the policies in use for each namespace in the cluster, ensure that policies are present which prevent allowedCapabilities to be set to anything other than an empty array. This audit may also require that you review the securityContext in the Pod specification yaml.","Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. Default Value: By default, there are no restrictions on adding capabilities to containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_minimized; compute_container_added_capabilities_restricted; compute_container_default_capabilities_only; compute_container_privilege_escalation_prevented; compute_container_capabilities_baseline_enforced,• Level 1 - Master Node • Level 2 - Master Node,Pods with containers which require capabilities outwith the default set will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.10,Minimize the admission of containers with capabilities assigned,Manual,Do not generally permit containers with capabilities,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Impact: Pods with containers require capabilities to operate will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers. This audit may also require that you review the securityContext in the Pod specification yaml.","Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities. Default Value: By default, there are no restrictions on the creation of containers with additional capabilities References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_restricted; compute_container_no_privileged_capabilities; compute_container_capabilities_minimized; compute_container_default_capabilities_denied; compute_container_privileged_capabilities_disabled; compute_container_capabilities_whitelisted; compute_container_capabilities_limited; compute_container_no_additional_capabilities,• Level 2 - Master Node,Pods with containers require capabilities to operate will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.11,Minimize the admission of Windows HostProcess Containers,Manual,Do not generally permit Windows containers to be run with the hostProcess flag set to true.,"A Windows container making use of the hostProcess flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides 'privileged access' to the Windows node. Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit hostProcess Windows containers. If you need to run Windows containers which require hostProcess, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostProcess containers. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostProcess containers. Default Value: By default, there are no restrictions on the creation of hostProcess containers. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_process_disabled; compute_windows_container_host_process_restricted; container_host_process_admission_minimized; container_windows_host_process_denied; compute_windows_container_host_process_blocked,• Level 1 - Master Node • Level 2 - Master Node,Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.12,Minimize the admission of HostPath volumes,Manual,Do not generally admit containers which make use of hostPath volumes.,"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem. There should be at least one admission control policy defined which does not permit containers to mount hostPath volumes. If you need to run containers which require hostPath volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with hostPath volumes. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPath volumes. Default Value: By default, there are no restrictions on the creation of hostPath volumes. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_hostpath_volumes_disabled; compute_container_hostpath_volumes_restricted; compute_container_hostpath_volumes_minimized; compute_container_hostpath_volumes_denied; compute_container_hostpath_volumes_blocked,• Level 1 - Master Node • Level 2 - Master Node,Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.13,Minimize the admission of containers which use HostPorts,Manual,Do not generally permit containers which require the use of HostPorts.,"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts. If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have hostPort sections. This audit may also require that you review the securityContext in the Pod specification yaml.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPort sections. Default Value: By default, there are no restrictions on the use of HostPorts. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI",compute_container_host_ports_restricted; compute_container_host_ports_disabled; compute_container_host_ports_minimized; compute_container_host_ports_denied; compute_container_host_ports_blocked,• Level 1 - Master Node • Level 2 - Master Node,"Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI
5.3.1,Ensure that the CNI in use supports Network Policies,Manual,There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.,Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None,"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",kubernetes_cni_network_policy_supported; kubernetes_network_policy_cni_compliance; kubernetes_cni_network_policy_enabled; kubernetes_network_policy_cni_validation; kubernetes_cni_network_policy_requirement,• Level 1 - Master Node • Level 2 - Master Node,None,1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.
5.3.2,Ensure that all Namespaces have Network Policies defined,Manual,Use network policies to isolate traffic in your cluster network.,"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces``` Ensure that each namespace defined in the cluster has at least one Network Policy.,"Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",kubernetes_namespace_network_policy_defined; kubernetes_namespace_traffic_isolation_enabled; kubernetes_namespace_network_policy_required; kubernetes_namespace_network_restrictions_enforced; kubernetes_namespace_policy_based_isolation_active,• Level 2 - Master Node,"Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/
5.4.1,Prefer using secrets as files over secrets as environment variables,Manual,Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.,"It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {'\n'}{end}' -A,"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",kubernetes_secret_files_preferred; kubernetes_secret_no_environment_variables; kubernetes_secret_volume_mounted; kubernetes_secret_environment_restricted; kubernetes_secret_data_volume_used,• Level 2 - Master Node,Application code which expects to read secrets in the form of environment variables would need modification,1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod
5.4.2,Consider external secret storage,Manual,"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",Review your secrets management implementation.,"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured.",secrets_manager_external_storage_required; secrets_manager_authentication_required; secrets_manager_audit_logging_enabled; secrets_manager_encryption_enabled; secrets_manager_secret_rotation_enabled; secrets_manager_complex_secret_management_required,• Level 2 - Master Node,None,
5.5.1,Configure Image Provenance using ImagePolicyWebhook admission controller,Manual,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Impact: You need to regularly maintain your provenance configuration based on container image updates.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888",kubernetes_image_provenance_enabled; kubernetes_image_policy_webhook_enabled; kubernetes_admission_controller_image_provenance; kubernetes_image_provenance_webhook_configured; kubernetes_image_policy_webhook_configured,• Level 2 - Master Node,You need to regularly maintain your provenance configuration based on container image updates.,1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888
5.7.1,Create administrative boundaries between resources using namespaces,Manual,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with 4 initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-node-lease - Namespace used for node heartbeats 4. kube-public - Namespace used for public information in a cluster References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_enabled; kubernetes_namespace_resource_separation_enabled; kubernetes_namespace_security_boundaries_enabled; kubernetes_namespace_isolation_for_admin_enabled,• Level 1 - Master Node • Level 2 - Master Node,You need to switch between namespaces for administration.,1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats
5.7.2,Ensure that the seccomp profile is set to docker/default in your pod definitions,Manual,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",Review the pod definitions in your cluster. It should create a line as below: securityContext: seccompProfile: type: RuntimeDefault,"Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_docker_default; kubernetes_pod_seccomp_profile_enabled; kubernetes_pod_security_profile_docker_default; kubernetes_pod_security_seccomp_enabled; kubernetes_pod_security_seccomp_docker_default,• Level 2 - Master Node,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/
5.7.3,Apply Security Context to Your Pods and Containers,Manual,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_configured; kubernetes_container_security_context_configured; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_privilege_escalation_disabled; kubernetes_container_privilege_escalation_disabled; kubernetes_pod_capabilities_dropped; kubernetes_container_capabilities_dropped,• Level 2 - Master Node,"If you incorrectly apply security contexts, you may have trouble running the pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
5.7.4,The default namespace should not be used,Manual,"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None","Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service","Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used",kubernetes_namespace_default_not_used; kubernetes_namespace_default_avoided; kubernetes_namespace_non_default_required; kubernetes_namespace_default_restricted; kubernetes_namespace_custom_required,• Level 2 - Master Node,None,
1.1.1,Ensure that the API server pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the API server pod specification file has permissions of 600 or more restrictive.,The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_api_server_pod_spec_file_permissions_restrictive; kubernetes_api_server_pod_spec_file_permissions_secure; kubernetes_api_server_pod_spec_file_permissions_strict; kubernetes_api_server_pod_spec_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.2,Ensure that the API server pod specification file ownership is set to root:root,Automated,Ensure that the API server pod specification file ownership is set to root:root.,The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml Default Value: By default, the kube-apiserver.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_pod_spec_file_ownership_root_root; kubernetes_api_server_pod_spec_file_owner_root; kubernetes_api_server_pod_spec_file_group_root; kubernetes_api_server_pod_spec_file_permissions_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.3,Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the controller manager pod specification file has permissions of 600 or more restrictive.,The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, the kube-controller-manager.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_controller_manager_pod_spec_file_permissions_600_or_stricter; kubernetes_controller_manager_pod_spec_file_permissions_restrictive; kubernetes_controller_manager_pod_spec_file_permissions_secure; kubernetes_controller_manager_pod_spec_file_permissions_min_600; kubernetes_controller_manager_pod_spec_file_permissions_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.4,Ensure that the controller manager pod specification file ownership is set to root:root,Automated,Ensure that the controller manager pod specification file ownership is set to root:root.,The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml Default Value: By default, kube-controller-manager.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager",kubernetes_controller_manager_file_ownership_root; kubernetes_controller_manager_pod_spec_ownership_root; kubernetes_controller_manager_file_permissions_root; kubernetes_controller_manager_spec_file_ownership_root; kubernetes_controller_manager_pod_file_ownership_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager
1.1.5,Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler pod specification file has permissions of 600 or more restrictive.,The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_scheduler_pod_spec_file_permissions_restrictive; kubernetes_scheduler_file_permissions_secure; kubernetes_pod_spec_file_permissions_600_or_stricter; kubernetes_scheduler_file_permissions_min_600,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.6,Ensure that the scheduler pod specification file ownership is set to root:root,Automated,Ensure that the scheduler pod specification file ownership is set to root:root.,The scheduler pod specification file controls various parameters that set the behavior of the kube-scheduler service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml Default Value: By default, kube-scheduler.yaml file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/",kubernetes_scheduler_pod_ownership_root_root; kubernetes_pod_spec_ownership_root_root; scheduler_pod_spec_file_ownership_root_root; kubernetes_scheduler_file_ownership_root_root; pod_spec_file_ownership_root_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-scheduler/
1.1.7,Ensure that the etcd pod specification file permissions are set to 600 or more restrictive,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file has permissions of 600 or more restrictive.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/manifests/etcd.yaml Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file has permissions of 640. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_pod_spec_file_permissions_600_or_more_restrictive; kubernetes_etcd_manifest_file_permissions_secure; kubernetes_etcd_yaml_file_permissions_restricted; kubernetes_manifests_etcd_file_permissions_600; kubernetes_etcd_pod_spec_file_permissions_compliant,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.8,Ensure that the etcd pod specification file ownership is set to root:root,Automated,Ensure that the /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root.,The etcd pod specification file /etc/kubernetes/manifests/etcd.yaml controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly- available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/manifests/etcd.yaml Default Value: By default, /etc/kubernetes/manifests/etcd.yaml file ownership is set to root:root. References: 1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/",kubernetes_etcd_manifest_file_ownership_root; kubernetes_etcd_pod_spec_file_ownership_root; kubernetes_etcd_yaml_file_ownership_root; kubernetes_manifest_file_ownership_root; etcd_pod_spec_file_ownership_root,• Level 1 - Master Node,None,1. https://coreos.com/etcd 2. https://kubernetes.io/docs/admin/etcd/
1.1.9,Ensure that the Container Network Interface file permissions are set to 600 or more restrictive,Manual,Ensure that the Container Network Interface files have permissions of 600 or more restrictive.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %a <path/to/cni/files> Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",compute_container_network_interface_file_permissions_600_or_more_restrictive; compute_cni_file_permissions_restricted; container_network_interface_file_permissions_secure; cni_file_permissions_600_or_stricter; compute_cni_config_file_permissions_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.10,Ensure that the Container Network Interface file ownership is set to root:root,Manual,Ensure that the Container Network Interface files have ownership set to root:root.,Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G <path/to/cni/files> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root <path/to/cni/files> Default Value: NA References: 1. https://kubernetes.io/docs/concepts/cluster-administration/networking/",container_network_interface_file_ownership_root_root; container_network_interface_file_ownership_set_correctly; container_network_interface_file_ownership_secure; container_network_interface_file_root_ownership; container_network_interface_file_ownership_valid,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/cluster-administration/networking/
1.1.11,Ensure that the etcd data directory permissions are set to 700 or more restrictive,Automated,Ensure that the etcd data directory has permissions of 700 or more restrictive.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %a /var/lib/etcd Verify that the permissions are 700 or more restrictive.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chmod 700 /var/lib/etcd Default Value: By default, etcd data directory has permissions of 755. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_permissions_700_or_more_restrictive; etcd_data_directory_permissions_restrictive; etcd_directory_permissions_secure; etcd_data_directory_permissions_compliant; etcd_data_directory_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.12,Ensure that the etcd data directory ownership is set to etcd:etcd,Automated,Ensure that the etcd data directory ownership is set to etcd:etcd.,etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by etcd:etcd. Impact: None,"On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, stat -c %U:%G /var/lib/etcd Verify that the ownership is set to etcd:etcd.","On the etcd server node, get the etcd data directory, passed as an argument --data- dir, from the below command: ps -ef | grep etcd Run the below command (based on the etcd data directory found above). For example, chown etcd:etcd /var/lib/etcd Default Value: By default, etcd data directory ownership is set to etcd:etcd. References: 1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/",etcd_data_directory_ownership_etcd_etcd; etcd_directory_permissions_etcd_etcd; etcd_data_directory_correct_ownership; etcd_data_directory_secure_ownership; etcd_directory_ownership_etcd_group; etcd_data_directory_etcd_user_ownership; etcd_data_directory_etcd_group_ownership; etcd_directory_ownership_restricted; etcd_data_directory_ownership_valid; etcd_directory_ownership_etcd_etcd_only,• Level 1 - Master Node,None,1. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#data-dir 2. https://kubernetes.io/docs/admin/etcd/
1.1.13,Ensure that the admin.conf file permissions are set to 600,Automated,Ensure that the admin.conf file has permissions of 600.,The admin.conf is the administrator kubeconfig file defining various settings for the administration of the cluster. This file contains private key and respective certificate allowed to fully manage the cluster. You should restrict its file permissions to maintain the integrity and confidentiality of the file. The file should be readable and writable by only the administrators on the system. Impact: None.,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/admin.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/admin.conf Default Value: By default, admin.conf has permissions of 600. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",kubernetes_admin_conf_file_permissions_600; kubernetes_admin_conf_file_restrictive_permissions; kubernetes_admin_conf_file_secure_permissions; kubernetes_admin_conf_file_owner_only_access; kubernetes_admin_conf_file_strict_permissions,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.14,Ensure that the admin.conf file ownership is set to root:root,Automated,Ensure that the admin.conf file ownership is set to root:root.,The admin.conf file contains the admin credentials for the cluster. You should set its file ownership to maintain the integrity and confidentiality of the file. The file should be owned by root:root. Impact: None.,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/admin.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/admin.conf Default Value: By default, admin.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",kubernetes_admin_conf_ownership_root; kubernetes_admin_conf_root_owner; kubernetes_admin_conf_root_group; kubernetes_admin_conf_file_ownership_root; kubernetes_admin_conf_root_ownership,• Level 1 - Master Node,None.,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.15,Ensure that the scheduler.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the scheduler.conf file has permissions of 600 or more restrictive.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/scheduler.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf has permissions of 640. References: 1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/",compute_scheduler_conf_file_permissions_600_or_restrictive; compute_scheduler_conf_file_permissions_restrictive; compute_scheduler_conf_file_permissions_secure; compute_scheduler_conf_file_permissions_strict; compute_scheduler_conf_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/
1.1.16,Ensure that the scheduler.conf file ownership is set to root:root,Automated,Ensure that the scheduler.conf file ownership is set to root:root.,The scheduler.conf file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/scheduler.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/scheduler.conf Default Value: By default, scheduler.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubeadm/",compute_scheduler_conf_root_ownership; compute_scheduler_conf_root_group_ownership; compute_scheduler_conf_file_ownership_root; scheduler_conf_file_owner_root; scheduler_conf_file_group_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubeadm/
1.1.17,Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the controller-manager.conf file has permissions of 600 or more restrictive.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Run the following command (based on the file location on your system) on the Control Plane node. For example, stat -c %a /etc/kubernetes/controller-manager.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod 600 /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_permissions_600_or_stricter; kubernetes_controller_manager_conf_file_permissions_restrictive; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_permissions_compliant; kubernetes_controller_manager_conf_file_permissions_cis_benchmark,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.18,Ensure that the controller-manager.conf file ownership is set to root:root,Automated,Ensure that the controller-manager.conf file ownership is set to root:root.,The controller-manager.conf file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c %U:%G /etc/kubernetes/controller-manager.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown root:root /etc/kubernetes/controller-manager.conf Default Value: By default, controller-manager.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_conf_file_ownership_root; kubernetes_controller_manager_conf_file_group_ownership_root; kubernetes_controller_manager_conf_file_permissions_secure; kubernetes_controller_manager_conf_file_ownership_correct; kubernetes_controller_manager_conf_file_root_owned,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.1.19,Ensure that the Kubernetes PKI directory and file ownership is set to root:root,Automated,Ensure that the Kubernetes PKI directory and file ownership is set to root:root.,Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, ls -laR /etc/kubernetes/pki/ Verify that the ownership of all files and directories in this hierarchy is set to root:root.","Run the below command (based on the file location on your system) on the Control Plane node. For example, chown -R root:root /etc/kubernetes/pki/ Default Value: By default, the /etc/kubernetes/pki/ directory and all of the files and directories contained within it, are set to be owned by the root user. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_directory_ownership_root; kubernetes_pki_file_ownership_root; kubernetes_pki_directory_permissions_root; kubernetes_pki_file_permissions_root; kubernetes_pki_directory_and_file_ownership_root; kubernetes_pki_directory_and_file_permissions_root; kubernetes_pki_directory_and_file_ownership_and_permissions_root,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.20,Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive,Manual,Ensure that Kubernetes PKI certificate files have permissions of 600 or more restrictive.,Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.crt Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.crt Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.crt Default Value: By default, the certificates used by Kubernetes are set to have permissions of 644 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_certificate_file_permissions_600_or_more_restrictive; kubernetes_pki_certificate_file_permissions_restrictive; kubernetes_certificate_file_permissions_secure; kubernetes_pki_file_permissions_600; kubernetes_certificate_file_permissions_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.1.21,Ensure that the Kubernetes PKI key file permissions are set to 600,Manual,Ensure that Kubernetes PKI key files have permissions of 600.,Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality. Impact: None,"Run the below command (based on the file location on your system) on the Control Plane node. For example, stat -c '%a' /etc/kubernetes/pki/*.key Verify that the permissions are 600 or more restrictive. or ls -l /etc/kubernetes/pki/*.key Verify -rw------","Run the below command (based on the file location on your system) on the Control Plane node. For example, chmod -R 600 /etc/kubernetes/pki/*.key Default Value: By default, the keys used by Kubernetes are set to have permissions of 600 References: 1. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_pki_key_file_permissions_600; kubernetes_pki_key_file_permissions_restricted; kubernetes_pki_key_file_permissions_secure; kubernetes_pki_key_file_permissions_strict; kubernetes_pki_key_file_permissions_protected,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.1,Ensure that the --anonymous-auth argument is set to false,Manual,Disable anonymous requests to the API server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes. Impact: Anonymous requests will be rejected.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --anonymous-auth argument is set to false. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--anonymous-auth' | grep -i false If the exit code is '1', then the control isn't present / failed","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --anonymous-auth=false Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests",kubernetes_api_server_anonymous_auth_disabled; kubernetes_api_server_auth_enabled; kubernetes_api_server_no_anonymous_access,• Level 1 - Master Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authentication/#anonymous-requests
1.2.2,Ensure that the --token-auth-file parameter is not set,Automated,Do not use token based authentication.,"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token- based authentication. Impact: You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --token-auth-file argument does not exist. Alternative Audit Method kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[ ]}{.spec.containers[ ].command} {'\n'}{end}' | grep '--token-auth-file' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the --token-auth-file=<filename> parameter. Default Value: By default, --token-auth-file argument is not set. References: 1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_api_server_token_auth_disabled; kubernetes_api_server_token_auth_file_unset; kubernetes_api_server_token_auth_not_configured; kubernetes_api_server_token_auth_file_absent,• Level 1 - Master Node,You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.,1. https://kubernetes.io/docs/admin/authentication/#static-token-file 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.3,Ensure that the DenyServiceExternalIPs is set,Manual,This admission controller rejects all net-new usage of the Service field externalIPs.,"Most users do not need the ability to set the externalIPs field for a Service at all, and cluster admins should consider disabling this functionality by enabling the DenyServiceExternalIPs admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage. Impact: When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the `DenyServiceExternalIPs' argument exist as a string value in --disable- admission-plugins.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and remove the `-- DenyServiceExternalIPs'parameter or The Kubernetes API server flag disable-admission-plugins takes a comma-delimited list of admission control plugins to be disabled, even if they are in the list of plugins enabled by default. kube-apiserver --disable-admission-plugins=DenyServiceExternalIPs,AlwaysDeny ... Default Value: By default, --disable-admission-plugins=DenyServiceExternalIP argument is not set. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/",kubernetes_service_external_ips_denied; kubernetes_service_external_ips_restricted; admission_controller_external_ips_blocked; kubernetes_service_external_ips_disabled; admission_controller_service_external_ips_denied,• Level 1 - Master Node,"When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.",1. https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 2. https://kubernetes.io/docs/admin/kube-apiserver/
1.2.4,Ensure that the --kubelet-client-certificate and --kubelet- client-key arguments are set as appropriate,Automated,Enable certificate based kubelet authentication.,"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate- based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-client-certificate and --kubelet-client-key arguments exist and they are set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-client-certificate' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below. --kubelet-client-certificate=<path/to/client-certificate-file> --kubelet-client-key=<path/to/client-key-file> Default Value: By default, certificate-based kubelet authentication is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authentication_enabled; kubernetes_kubelet_client_certificate_configured; kubernetes_kubelet_client_key_configured; kubernetes_kubelet_authentication_certificate_based; kubernetes_kubelet_tls_client_auth_enabled,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.5,Ensure that the --kubelet-certificate-authority argument is set as appropriate,Automated,Verify kubelet's certificate before establishing connection.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --kubelet-certificate-authority argument exists and is set as appropriate. Alternative Audit kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {'\n'}{end}' | grep '--kubelet-certificate-Authority' | grep -i false If the exit code is '1', then the control isn't present / failed","Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority. --kubelet-certificate-authority=<ca-string> Default Value: By default, --kubelet-certificate-authority argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet",kubernetes_kubelet_certificate_authority_set; kubernetes_kubelet_certificate_authority_valid; kubernetes_kubelet_certificate_authority_configured; kubernetes_kubelet_certificate_authority_secure; kubernetes_kubelet_certificate_authority_trusted,• Level 1 - Master Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/ 3. https://kubernetes.io/docs/concepts/cluster-administration/master-node- communication/#apiserver---kubelet
1.2.6,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not always authorize all requests.,"The API Server, can be configured to allow all requests. This mode should not be used on any production cluster. Impact: Only authorized requests will be served.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is not set to AlwaysAllow.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below. --authorization-mode=RBAC Default Value: By default, AlwaysAllow is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/",,• Level 1 - Master Node,Only authorized requests will be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/
1.2.7,Ensure that the --authorization-mode argument includes Node,Automated,Restrict kubelet nodes to reading only objects associated with them.,"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include Node.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. --authorization-mode=Node,RBAC Default Value: By default, Node authorization is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_authorization_enabled; kubernetes_kubelet_node_auth_mode_configured; kubernetes_kubelet_node_restricted_access; kubernetes_kubelet_node_authorization_required; kubernetes_kubelet_node_auth_mode_includes_node,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/authorization/node/ 3. https://github.com/kubernetes/kubernetes/pull/46076 4. https://acotten.com/post/kube17-security
1.2.8,Ensure that the --authorization-mode argument includes RBAC,Automated,Turn on Role Based Access Control.,"Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode. Impact: When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --authorization-mode argument exists and is set to a value to include RBAC.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes RBAC, for example: --authorization-mode=Node,RBAC Default Value: By default, RBAC authorization is not enabled. References: 1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/",kubernetes_cluster_authorization_mode_rbac_enabled; kubernetes_cluster_rbac_enabled; kubernetes_api_server_authorization_mode_rbac_enabled; kubernetes_api_server_rbac_enabled; kubernetes_authorization_mode_rbac_enabled,• Level 1 - Master Node,"When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.",1. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
1.2.9,Ensure that the admission control plugin EventRateLimit is set,Manual,Limit the rate at which the API server accepts requests.,"Using EventRateLimit admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept. Note: This is an Alpha feature in the Kubernetes 1.15 release. Impact: You need to carefully tune in limits as per your environment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes EventRateLimit.,"Follow the Kubernetes documentation and set the desired limits in a configuration file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameters. --enable-admission-plugins=...,EventRateLimit,... --admission-control-config-file=<path/to/configuration/file> Default Value: By default, EventRateLimit is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md",,• Level 1 - Master Node,You need to carefully tune in limits as per your environment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#eventratelimit 3. https://github.com/staebler/community/blob/9873b632f4d99b5d99c38c9b15fe2f8 b93d0a746/contributors/design- proposals/admission_control_event_rate_limit.md
1.2.10,Ensure that the admission control plugin AlwaysAdmit is not set,Automated,Do not allow all requests.,Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers. Impact: Only requests explicitly allowed by the admissions control plugins would be served.,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --enable-admission-plugins argument is set, its value does not include AlwaysAdmit.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and either remove the --enable-admission- plugins parameter, or set it to a value that does not include AlwaysAdmit. Default Value: AlwaysAdmit is not in the list of default admission plugins. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit",kubernetes_admission_controller_always_admit_disabled; kubernetes_admission_plugin_always_admit_not_set; kubernetes_admission_policy_always_admit_restricted; kubernetes_admission_control_always_admit_blocked; kubernetes_admission_rule_always_admit_prohibited,• Level 1 - Master Node,Only requests explicitly allowed by the admissions control plugins would be served.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwaysadmit
1.2.11,Ensure that the admission control plugin AlwaysPullImages is set,Manual,Always pull images.,"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. Impact: Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes AlwaysPullImages.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include AlwaysPullImages. --enable-admission-plugins=...,AlwaysPullImages,... Default Value: By default, AlwaysPullImages is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages",kubernetes_admission_controller_always_pull_images_enabled; kubernetes_pod_always_pull_images_enabled; kubernetes_image_pull_policy_always_enabled; kubernetes_admission_plugin_always_pull_images_configured; kubernetes_workload_always_pull_images_enabled,• Level 1 - Master Node,"Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed. This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#alwayspullimages
1.2.12,Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used,Manual,The SecurityContextDeny admission controller can be used to deny pods which make use of some SecurityContext fields which could allow for privilege escalation in the cluster. This should be used where PodSecurityPolicy is not in place within the cluster.,"SecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicies enabled. Impact: This admission controller should only be used where Pod Security Policies cannot be used on the cluster, as it can interact poorly with certain Pod Security Policies","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes SecurityContextDeny, if PodSecurityPolicy is not included.","Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include SecurityContextDeny, unless PodSecurityPolicy is already in place. --enable-admission-plugins=...,SecurityContextDeny,... Default Value: By default, SecurityContextDeny is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny 3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac",kubernetes_admission_controller_security_context_deny_enabled; kubernetes_pod_security_context_deny_enabled; kubernetes_admission_controller_security_context_deny_no_psp; kubernetes_pod_security_context_deny_no_psp; kubernetes_admission_controller_privilege_escalation_denied; kubernetes_pod_privilege_escalation_denied_no_psp,• Level 2 - Master Node,"This admission controller should only be used where Pod Security Policies cannot be used on the cluster, as it can interact poorly with certain Pod Security Policies",1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#securitycontextdeny 3. https://kubernetes.io/docs/user-guide/pod-security-policy/#working-with-rbac
1.2.13,Ensure that the admission control plugin ServiceAccount is set,Automated,Automate service accounts management.,"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens. Impact: None.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not includes ServiceAccount.,"Follow the documentation and create ServiceAccount objects as per your environment. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and ensure that the --disable-admission-plugins parameter is set to a value that does not include ServiceAccount. Default Value: By default, ServiceAccount is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_service_account_admission_plugin_enabled; kubernetes_service_account_automated_management_enabled; kubernetes_admission_controller_service_account_required; kubernetes_service_account_admission_control_enabled; kubernetes_admission_plugin_service_account_configured,• Level 2 - Master Node,None.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#serviceaccount 3. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
1.2.14,Ensure that the admission control plugin NamespaceLifecycle is set,Automated,Reject creating objects in a namespace that is undergoing termination.,"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --disable-admission-plugins argument is set to a value that does not include NamespaceLifecycle.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --disable-admission-plugins parameter to ensure it does not include NamespaceLifecycle. Default Value: By default, NamespaceLifecycle is set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle",kubernetes_admission_controller_namespace_lifecycle_enabled; kubernetes_admission_plugin_namespace_termination_protected; kubernetes_namespace_lifecycle_admission_control_enabled; kubernetes_namespace_termination_admission_plugin_enabled; kubernetes_admission_controller_namespace_termination_blocked,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#namespacelifecycle
1.2.15,Ensure that the admission control plugin NodeRestriction is set,Automated,Limit the Node and Pod objects that a kubelet could modify.,"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --enable-admission-plugins argument is set to a value that includes NodeRestriction.,"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the -- enable-admission-plugins parameter to a value that includes NodeRestriction. --enable-admission-plugins=...,NodeRestriction,... Default Value: By default, NodeRestriction is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security",kubernetes_kubelet_node_restriction_enabled; kubernetes_admission_control_node_restriction_enabled; kubernetes_kubelet_node_restriction_plugin_enabled; kubernetes_admission_node_restriction_enabled; kubernetes_kubelet_node_restriction_active,• Level 2 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/admin/admission-controllers/#noderestriction 3. https://kubernetes.io/docs/admin/authorization/node/ 4. https://acotten.com/post/kube17-security
1.2.16,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --profiling argument is set to false.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",eks_cluster_profiling_disabled; eks_cluster_no_profiling_enabled; eks_cluster_profiling_set_false; kubernetes_cluster_profiling_disabled; kubernetes_cluster_no_profiling_enabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.2.17,Ensure that the --audit-log-path argument is set,Automated,Enable auditing on the Kubernetes API Server and set the desired audit log path.,"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-path argument is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example: --audit-log-path=/var/log/apiserver/audit.log Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_logging_enabled; kubernetes_api_server_audit_log_path_set; kubernetes_api_server_audit_logging_configured; kubernetes_api_server_audit_log_path_valid; kubernetes_api_server_audit_logging_active,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.18,Ensure that the --audit-log-maxage argument is set to 30 or as appropriate,Automated,Retain the logs for at least 30 days or as appropriate.,Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxage argument is set to 30 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days: --audit-log-maxage=30 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxage_30d; kubernetes_api_server_audit_log_retention_configured; kubernetes_audit_log_maxage_set; kubernetes_audit_log_retention_min_30d; kubernetes_api_server_audit_log_duration_valid; kubernetes_audit_log_maxage_compliant; kubernetes_audit_log_retention_enforced; kubernetes_api_server_audit_log_age_configured,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.19,Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate,Automated,Retain 10 or an appropriate number of old log files.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxbackup argument is set to 10 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or to an appropriate value. --audit-log-maxbackup=10 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxbackup_set; kubernetes_api_server_audit_log_maxbackup_10_or_more; kubernetes_audit_log_retention_configured; kubernetes_audit_log_maxbackup_valid; kubernetes_api_server_log_retention_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.20,Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate,Automated,Rotate log files on reaching 100 MB or as appropriate.,"Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --audit-log-maxsize argument is set to 100 or as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to an appropriate size in MB. For example, to set it as 100 MB: --audit-log-maxsize=100 Default Value: By default, auditing is not enabled. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22",kubernetes_api_server_audit_log_maxsize_set; kubernetes_api_server_audit_log_maxsize_100mb; kubernetes_api_server_audit_log_rotation_enabled; kubernetes_api_server_audit_log_size_limited; kubernetes_api_server_audit_log_maxsize_configured,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://kubernetes.io/docs/concepts/cluster-administration/audit/ 3. https://github.com/kubernetes/features/issues/22
1.2.21,Ensure that the --request-timeout argument is set as appropriate,Manual,Set global request timeout for API server requests as appropriate.,"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --request-timeout argument is either not set or set to an appropriate value.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml and set the below parameter as appropriate and if needed. For example, --request-timeout=300s Default Value: By default, --request-timeout is set to 60 seconds. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415",kubernetes_api_server_request_timeout_set; kubernetes_api_server_request_timeout_configured; kubernetes_api_server_request_timeout_appropriate; kubernetes_api_server_request_timeout_valid; kubernetes_api_server_request_timeout_within_limits,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/pull/51415
1.2.22,Ensure that the --service-account-lookup argument is set to true,Automated,Validate service account before validating token.,"If --service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that if the --service-account-lookup argument exists it is set to true.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the below parameter. --service-account-lookup=true Alternatively, you can delete the --service-account-lookup parameter from this file so that the default takes effect. Default Value: By default, --service-account-lookup argument is set to true. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use",kubernetes_api_server_service_account_lookup_enabled; kubernetes_api_server_service_account_validation_enabled; kubernetes_api_server_service_account_pre_validation_enabled; kubernetes_api_server_service_account_auth_enabled; kubernetes_api_server_service_account_verify_enabled,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167 3. https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use
1.2.23,Ensure that the --service-account-key-file argument is set as appropriate,Automated,Explicitly set a service account public key file for service accounts on the apiserver.,"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file. Impact: The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --service-account-key-file argument exists and is set as appropriate.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the Control Plane node and set the --service-account-key-file parameter to the public key file for service accounts: --service-account-key-file=<filename> Default Value: By default, --service-account-key-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167",kubernetes_apiserver_service_account_key_file_set; kubernetes_apiserver_service_account_key_file_configured; kubernetes_apiserver_service_account_key_file_valid; kubernetes_apiserver_service_account_key_file_secure; kubernetes_apiserver_service_account_key_file_explicit,• Level 1 - Master Node,The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://github.com/kubernetes/kubernetes/issues/24167
1.2.24,Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-certfile and --etcd-keyfile arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters. --etcd-certfile=<path/to/client-certificate-file> --etcd-keyfile=<path/to/client-key-file> Default Value: By default, --etcd-certfile and --etcd-keyfile arguments are not set References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_tls_encryption_enabled; kubernetes_etcd_certfile_configured; kubernetes_etcd_keyfile_configured; kubernetes_etcd_client_tls_enabled; kubernetes_etcd_secure_connection_required,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.25,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Automated,Setup TLS connection on the API server.,API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the TLS certificate and private key file parameters. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Default Value: By default, --tls-cert-file and --tls-private-key-file are presented and created for use. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_tls_cert_file_set; kubernetes_api_server_tls_private_key_file_set; kubernetes_api_server_tls_cert_and_key_configured; kubernetes_api_server_tls_connection_secure; kubernetes_api_server_tls_files_valid,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.26,Ensure that the --client-ca-file argument is set as appropriate,Automated,Setup TLS connection on the API server.,"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --client-ca-file argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube- apiserver.yaml on the master node and set the client certificate authority file. --client-ca-file=<path/to/client-ca-file> Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide",kubernetes_api_server_client_ca_file_configured; kubernetes_api_server_tls_authentication_enabled; kubernetes_api_server_client_certificate_validation_enabled; kubernetes_api_server_secure_connection_required; kubernetes_api_server_client_ca_file_specified,• Level 1 - Master Node,TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide
1.2.27,Ensure that the --etcd-cafile argument is set as appropriate,Automated,etcd should be configured to make use of TLS encryption for client connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file. Impact: TLS and client certificate authentication must be configured for etcd.,Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --etcd-cafile argument exists and it is set as appropriate.,"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter. --etcd-cafile=<path/to/ca-file> Default Value: By default, --etcd-cafile is not set. References: 1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html",etcd_client_tls_encryption_enabled; etcd_cafile_argument_configured; etcd_client_connection_secure; etcd_tls_client_authentication_required; etcd_secure_client_communication_enabled,• Level 1 - Master Node,TLS and client certificate authentication must be configured for etcd.,1. https://kubernetes.io/docs/admin/kube-apiserver/ 2. https://coreos.com/etcd/docs/latest/op-guide/security.html
1.2.28,Ensure that the --encryption-provider-config argument is set as appropriate,Manual,Encrypt etcd key-value store.,etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures. Impact: None,"Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --encryption-provider-config argument is set to a EncryptionConfig file. Additionally, ensure that the EncryptionConfig file has all the desired resources covered especially any secrets.","Follow the Kubernetes documentation and configure a EncryptionConfig file. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file: --encryption-provider-config=</path/to/EncryptionConfig/File> Default Value: By default, --encryption-provider-config is not set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92",kubernetes_etcd_encryption_enabled; kubernetes_etcd_encryption_provider_configured; kubernetes_etcd_encryption_config_valid; kubernetes_etcd_encryption_key_rotation_enabled; kubernetes_etcd_encryption_tls_enabled; kubernetes_etcd_encryption_cipher_suite_secure; kubernetes_etcd_encryption_provider_key_management; kubernetes_etcd_encryption_provider_config_secure,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92
1.2.29,Ensure that encryption providers are appropriately configured,Manual,"Where etcd encryption is used, appropriate providers should be configured.","Where etcd encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the aescbc, kms and secretbox are likely to be appropriate options. Impact: None","Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Get the EncryptionConfig file set for --encryption-provider-config argument. Verify that aescbc, kms or secretbox is set as the encryption provider for all the desired resources.","Follow the Kubernetes documentation and configure a EncryptionConfig file. In this file, choose aescbc, kms or secretbox as the encryption provider. Default Value: By default, no encryption provider is set. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers",etcd_encryption_provider_configured; etcd_encryption_provider_secure; etcd_encryption_provider_valid; etcd_encryption_provider_approved; etcd_encryption_provider_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/ 2. https://acotten.com/post/kube17-security 3. https://kubernetes.io/docs/admin/kube-apiserver/ 4. https://github.com/kubernetes/features/issues/92 5. https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers
1.2.30,Ensure that the API Server only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the API server is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.",Run the following command on the Control Plane node: ps -ef | grep kube-apiserver Verify that the --tls-cipher-suites argument is set as outlined in the remediation procedure below.,"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter. --tls-cipher- suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SH A256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SH A256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SH A384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POL Y1305_SHA256,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_C BC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_S HA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 ,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TL S_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_2 56_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384. Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers References: 1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_api_server_strong_ciphers_enabled; kubernetes_api_server_weak_ciphers_disabled; kubernetes_api_server_tls_min_version_1_2; kubernetes_api_server_cipher_suite_restricted; kubernetes_api_server_secure_cipher_config; kubernetes_api_server_insecure_ciphers_removed; kubernetes_api_server_cipher_strength_high; kubernetes_api_server_cryptographic_ciphers_compliant,• Level 1 - Master Node,API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.,"1. https://github.com/ssllabs/research/wiki/SSL-and-TLS-Deployment-Best- Practices#23-use-secure-cipher-suites Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues"
1.3.1,Ensure that the --terminated-pod-gc-threshold argument is set as appropriate,Manual,"Activate garbage collector on pod termination, as appropriate.","Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection. Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --terminated-pod-gc-threshold argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --terminated-pod-gc- threshold to an appropriate threshold, for example: --terminated-pod-gc-threshold=10 Default Value: By default, --terminated-pod-gc-threshold is set to 12500. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484",kubernetes_pod_terminated_gc_threshold_set; kubernetes_pod_terminated_gc_threshold_configured; kubernetes_pod_gc_threshold_set; kubernetes_pod_gc_threshold_configured; kubernetes_pod_termination_gc_threshold_set,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/28484
1.3.2,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --profiling argument is set to false.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_cluster_profiling_disabled; kubernetes_cluster_no_profiling_enabled; kubernetes_cluster_profiling_argument_false; kubernetes_cluster_profiling_set_false; kubernetes_cluster_profiling_feature_disabled,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.3.3,Ensure that the --use-service-account-credentials argument is set to true,Automated,Use individual service account credentials for each controller.,"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-service- account-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks. Impact: Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --use-service-account-credentials argument is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node to set the below parameter. --use-service-account-credentials=true Default Value: By default, --use-service-account-credentials is set to false. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles",gke_cluster_use_service_account_credentials_enabled; gke_controller_service_account_credentials_required; gke_cluster_service_account_credentials_enforced; gke_controller_individual_service_account_credentials; gke_cluster_service_account_credentials_true,• Level 1 - Master Node,"Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the kube- system namespace automatically with default roles and rolebindings that are auto- reconciled on startup. If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the controller-roles.yaml and controller- role-bindings.yaml files for the RBAC roles.",1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://kubernetes.io/docs/admin/service-accounts-admin/ 3. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-roles.yaml 4. https://github.com/kubernetes/kubernetes/blob/release- 1.6/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/controller-role- bindings.yaml 5. https://kubernetes.io/docs/admin/authorization/rbac/#controller-roles
1.3.4,Ensure that the --service-account-private-key-file argument is set as appropriate,Automated,Explicitly set a service account private key file for service accounts on the controller manager.,"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private- key-file as appropriate. Impact: You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --service-account-private-key-file argument is set as appropriate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --service-account- private-key-file parameter to the private key file for service accounts. --service-account-private-key-file=<filename> Default Value: By default, --service-account-private-key-file it not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/",kubernetes_controller_manager_service_account_private_key_file_set; kubernetes_controller_manager_service_account_private_key_file_configured; kubernetes_controller_manager_service_account_private_key_file_valid; kubernetes_controller_manager_service_account_private_key_file_secure; kubernetes_controller_manager_service_account_private_key_file_protected,• Level 1 - Master Node,You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.,1. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.5,Ensure that the --root-ca-file argument is set as appropriate,Automated,Allow pods to verify the API server's serving certificate before establishing connections.,Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks. Providing the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server. Impact: You need to setup and maintain root certificate authority file.,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --root-ca-file argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`. --root-ca-file=<path/to/file> Default Value: By default, --root-ca-file is not set. References: 1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000",kubernetes_api_server_root_ca_file_configured; kubernetes_api_server_certificate_verification_enabled; kubernetes_pod_api_server_certificate_validated; kubernetes_api_server_tls_trusted_ca_configured; kubernetes_api_server_certificate_authentication_enabled,• Level 1 - Master Node,You need to setup and maintain root certificate authority file.,1. https://kubernetes.io/docs/admin/kube-controller-manager/ 2. https://github.com/kubernetes/kubernetes/issues/11000
1.3.6,Ensure that the RotateKubeletServerCertificate argument is set to true,Automated,Enable kubelet server certificate rotation on controller-manager.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true. --feature-gates=RotateKubeletServerCertificate=true Default Value: By default, RotateKubeletServerCertificate is set to 'true' this recommendation verifies that it has not been disabled. References: 1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/",,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#approval-controller 2. https://github.com/kubernetes/features/issues/267 3. https://github.com/kubernetes/kubernetes/pull/45059 4. https://kubernetes.io/docs/admin/kube-controller-manager/
1.3.7,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the Controller Manager service to non-loopback insecure addresses.,"The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-controller-manager Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube- controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address",kubernetes_controller_manager_bind_address_localhost; kubernetes_controller_manager_loopback_only; kubernetes_controller_manager_insecure_bind_disabled; kubernetes_controller_manager_localhost_bind_enabled; kubernetes_controller_manager_restrict_bind_address,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- controller-manager/ Additional Information: Although the current Kubernetes documentation site says that --address is deprecated in favour of --bind-address Kubeadm 1.11 still makes use of --address
1.4.1,Ensure that the --profiling argument is set to false,Automated,"Disable profiling, if not needed.","Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. Impact: Profiling information would not be available.",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --profiling argument is set to false.,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml file on the Control Plane node and set the below parameter. --profiling=false Default Value: By default, profiling is enabled. References: 1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md",kubernetes_kubelet_profiling_disabled; kubernetes_kubelet_profiling_set_false; kubernetes_kubelet_no_profiling_enabled; kubernetes_kubelet_profiling_config_disabled; kubernetes_kubelet_profiling_argument_false,• Level 1 - Master Node,Profiling information would not be available.,1. https://kubernetes.io/docs/admin/kube-scheduler/ 2. https://github.com/kubernetes/community/blob/master/contributors/devel/profiling. md
1.4.2,Ensure that the --bind-address argument is set to 127.0.0.1,Automated,Do not bind the scheduler service to non-loopback insecure addresses.,"The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface Impact: None",Run the following command on the Control Plane node: ps -ef | grep kube-scheduler Verify that the --bind-address argument is set to 127.0.0.1,"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube- scheduler.yaml on the Control Plane node and ensure the correct value for the --bind- address parameter Default Value: By default, the --bind-address parameter is set to 0.0.0.0 References: 1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/",kubernetes_scheduler_bind_address_localhost; kubernetes_scheduler_loopback_only; kubernetes_scheduler_insecure_bind_disabled; kubernetes_scheduler_bind_address_secure; kubernetes_scheduler_localhost_restricted,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/reference/command-line-tools-reference/kube- scheduler/
2.1,Ensure that the --cert-file and --key-file arguments are set as appropriate,Automated,Configure TLS encryption for the etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit. Impact: Client connections only over TLS would be served.,Run the following command on the etcd server node ps -ef | grep etcd Verify that the --cert-file and the --key-file arguments are set as appropriate.,"Follow the etcd service documentation and configure TLS encryption. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --cert-file=</path/to/ca-file> --key-file=</path/to/key-file> Default Value: By default, TLS encryption is not set. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_service_tls_encryption_enabled; etcd_service_cert_file_configured; etcd_service_key_file_configured; etcd_service_cert_key_files_valid; etcd_service_tls_certificates_present,• Level 1 - Master Node,Client connections only over TLS would be served.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.2,Ensure that the --client-cert-auth argument is set to true,Automated,Enable client authentication on etcd service.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: All clients attempting to access the etcd server will require a valid client certificate.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --client-cert-auth argument is set to true.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --client-cert-auth='true' Default Value: By default, the etcd service can be queried by unauthenticated clients. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth",etcd_service_client_auth_enabled; etcd_client_cert_auth_required; etcd_client_authentication_enabled; etcd_service_client_cert_auth_enabled; etcd_client_auth_tls_required,• Level 1 - Master Node,All clients attempting to access the etcd server will require a valid client certificate.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#client-cert-auth
2.3,Ensure that the --auto-tls argument is not set to true,Automated,Do not use self-signed certificates for TLS.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service. Impact: Clients will not be able to use self-signed certificates for TLS.,"Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --auto-tls argument exists, it is not set to true.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --auto-tls parameter or set it to false. --auto-tls=false Default Value: By default, --auto-tls is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls",cloud_cdn_domain_auto_tls_disabled; cloud_cdn_certificate_self_signed_disallowed; cloud_cdn_tls_managed_certificate_required; cloud_cdn_domain_tls_custom_certificate_required,• Level 1 - Master Node,Clients will not be able to use self-signed certificates for TLS.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#auto-tls
2.4,Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate,Automated,etcd should be configured to make use of TLS encryption for peer connections.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters. Impact: etcd cluster peers would need to set up TLS for their communication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-cert-file and --peer-key-file arguments are set as appropriate. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters. --peer-client-file=</path/to/peer-cert-file> --peer-key-file=</path/to/peer-key-file> Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, peer communication over TLS is not configured. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/",etcd_cluster_peer_tls_enabled; etcd_cluster_peer_cert_file_configured; etcd_cluster_peer_key_file_configured; etcd_cluster_peer_tls_encryption_enabled; etcd_cluster_peer_auth_enabled,• Level 1 - Master Node,etcd cluster peers would need to set up TLS for their communication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/
2.5,Ensure that the --peer-client-cert-auth argument is set to true,Automated,etcd should be configured for peer authentication.,etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,Run the following command on the etcd server node: ps -ef | grep etcd Verify that the --peer-client-cert-auth argument is set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.,"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --peer-client-cert-auth=true Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-client-cert-auth argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth",etcd_peer_client_cert_auth_enabled; etcd_peer_authentication_required; etcd_peer_cert_authentication_enabled; etcd_peer_tls_authentication_enabled; etcd_peer_client_cert_auth_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-client-cert- auth
2.6,Ensure that the --peer-auto-tls argument is not set to true,Automated,Do not use automatically generated self-signed certificates for TLS connections between peers.,"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication. Impact: All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.","Run the following command on the etcd server node: ps -ef | grep etcd Verify that if the --peer-auto-tls argument exists, it is not set to true. Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and either remove the --peer-auto-tls parameter or set it to false. --peer-auto-tls=false Default Value: Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable. By default, --peer-auto-tls argument is set to false. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls",compute_cluster_peer_auto_tls_disabled; compute_cluster_peer_tls_manual_certificates_required; compute_cluster_peer_tls_self_signed_disabled; compute_cluster_peer_tls_auto_generated_disabled; compute_cluster_peer_tls_custom_certificates_required,• Level 1 - Master Node,All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html 2. https://kubernetes.io/docs/admin/etcd/ 3. https://coreos.com/etcd/docs/latest/op-guide/configuration.html#peer-auto-tls
2.7,Ensure that a unique Certificate Authority is used for etcd,Manual,Use a different certificate authority for etcd from the one used for Kubernetes.,"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. Its access should be restricted to specifically designated clients and peers only. Authentication to etcd is based on whether the certificate presented was issued by a trusted certificate authority. There is no checking of certificate attributes such as common name or subject alternative name. As such, if any attackers were able to gain access to any certificate issued by the trusted certificate authority, they would be able to gain full access to the etcd database. Impact: Additional management of the certificates and keys for the dedicated certificate authority will be required.",Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster. Run the following command on the master node: ps -ef | grep etcd Note the file referenced by the --trusted-ca-file argument. Run the following command on the master node: ps -ef | grep apiserver Verify that the file referenced by the --client-ca-file for apiserver is different from the --trusted-ca-file used by etcd.,"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service. Then, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameter. --trusted-ca-file=</path/to/ca-file> Default Value: By default, no etcd certificate is created and used. References: 1. https://coreos.com/etcd/docs/latest/op-guide/security.html",kubernetes_etcd_unique_certificate_authority; etcd_certificate_authority_separate_from_kubernetes; compute_etcd_distinct_certificate_authority; etcd_certificate_authority_unique_per_cluster; kubernetes_etcd_certificate_authority_isolated,• Level 2 - Master Node,Additional management of the certificates and keys for the dedicated certificate authority will be required.,1. https://coreos.com/etcd/docs/latest/op-guide/security.html
3.1.1,Client certificate authentication should not be used for users,Manual,"Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication.,"Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. Default Value: Client certificate authentication is enabled by default. Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",kubernetes_user_no_client_cert_auth; kubernetes_user_client_cert_auth_disabled; kubernetes_auth_no_user_client_certs; kubernetes_auth_user_client_cert_revocation; kubernetes_user_auth_no_client_certificates,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.2,Service account token authentication should not be used for users,Manual,"Kubernetes provides service account tokens which are intended for use by workloads running in the Kubernetes cluster, for authentication to the API server. These tokens are not designed for use by end-users and do not provide for features such as revocation or expiry, making them insecure. A newer version of the feature (Bound service account token volumes) does introduce expiry but still does not allow for specific revocation.","With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Service account token authentication does not allow for this due to the use of JWT tokens as an underlying technology. Impact: External mechanisms for authentication generally require additional software to be deployed.",Review user access to the cluster and ensure that users are not making use of service account token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of service account tokens. Default Value: Service account token authentication is enabled by default.,kubernetes_service_account_token_authentication_disabled; kubernetes_service_account_user_authentication_disabled; kubernetes_service_account_no_user_tokens; kubernetes_service_account_token_revocation_enabled; kubernetes_service_account_token_expiry_enabled; kubernetes_service_account_bound_token_volumes_used; kubernetes_service_account_no_static_tokens; kubernetes_service_account_token_rotation_enabled,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.1.3,Bootstrap token authentication should not be used for users,Manual,Kubernetes provides bootstrap tokens which are intended for use by new nodes joining the cluster These tokens are not designed for use by end-users they are specifically designed for the purpose of bootstrapping new nodes and not for general authentication,Bootstrap tokens are not intended for use as a general authentication mechanism and impose constraints on user and group naming that do not facilitate good RBAC design. They also cannot be used with MFA resulting in a weak authentication mechanism being available. Impact: External mechanisms for authentication generally require additional software to be deployed.,Review user access to the cluster and ensure that users are not making use of bootstrap token authentication.,Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of bootstrap tokens. Default Value: Bootstrap token authentication is not enabled by default and requires an API server parameter to be set.,kubernetes_user_no_bootstrap_token_auth; kubernetes_user_bootstrap_token_disabled; kubernetes_auth_no_bootstrap_token; kubernetes_auth_bootstrap_token_restricted; kubernetes_user_auth_no_bootstrap_usage,• Level 1 - Master Node,External mechanisms for authentication generally require additional software to be deployed.,
3.2.1,Ensure that a minimal audit policy is created,Manual,Kubernetes can audit the details of requests made to the API server. The --audit- policy-file flag must be set for this logging to be enabled.,"Logging is an important detective control for all systems, to detect potential unauthorised access. Impact: Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",Run the following command on one of the cluster master nodes: ps -ef | grep kube-apiserver Verify that the --audit-policy-file is set. Review the contents of the file specified and ensure that it contains a valid audit policy.,"Create an audit policy file for your cluster. Default Value: Unless the --audit-policy-file flag is specified, no auditing will be carried out. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/",kubernetes_api_audit_policy_created; kubernetes_api_audit_policy_minimal; kubernetes_api_audit_logging_enabled; kubernetes_api_audit_policy_file_set; kubernetes_api_audit_configuration_valid,• Level 1 - Master Node,"Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.",1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
3.2.2,Ensure that the audit policy covers key security concerns,Manual,Ensure that the audit policy created for the cluster covers key security concerns.,"Security audit logs should cover access and modification of key resources in the cluster, to enable them to form an effective part of a security environment. Impact: Increasing audit logging will consume resources on the nodes or other log destination.","Review the audit policy provided for the cluster and ensure that it covers at least the following areas :- • Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data. • Modification of pod and deployment objects. • Use of pods/exec, pods/portforward, pods/proxy and services/proxy. For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","Consider modification of the audit policy in use on the cluster to include these items, at a minimum. Default Value: By default Kubernetes clusters do not log audit information. References: 1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735",kubernetes_audit_policy_key_security_concerns_covered; kubernetes_audit_policy_security_events_logged; kubernetes_audit_policy_admin_activities_monitored; kubernetes_audit_policy_authentication_events_tracked; kubernetes_audit_policy_authorization_events_recorded; kubernetes_audit_policy_request_response_logging_enabled; kubernetes_audit_policy_metadata_included; kubernetes_audit_policy_sensitive_operations_audited; kubernetes_audit_policy_compliance_events_captured; kubernetes_audit_policy_security_relevant_actions_logged,• Level 2 - Master Node,Increasing audit logging will consume resources on the nodes or other log destination.,1. https://github.com/k8scop/k8s-security- dashboard/blob/master/configs/kubernetes/adv-audit.yaml 2. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy 3. https://github.com/falcosecurity/falco/blob/master/examples/k8s_audit_config/aud it-policy.yaml 4. https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure- helper.sh#L735
4.1.1,Ensure that the kubelet service file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet service file has permissions of 600 or more restrictive.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, the kubelet service file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_permissions_600_or_more_restrictive; kubernetes_kubelet_service_file_permissions_restrictive; kubernetes_kubelet_file_permissions_secure; kubernetes_kubelet_file_permissions_restricted; kubernetes_kubelet_service_file_permissions_compliant,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.2,Ensure that the kubelet service file ownership is set to root:root,Automated,Ensure that the kubelet service file ownership is set to root:root.,The kubelet service file controls various parameters that set the behavior of the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file. Please set $kubelet_service_config=<PATH> based on the file location on your system for example: export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf Default Value: By default, kubelet service file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in",kubernetes_kubelet_service_file_ownership_root; kubernetes_kubelet_service_file_owner_root; kubernetes_kubelet_file_ownership_root_root; kubernetes_kubelet_config_file_ownership_root; kubernetes_service_file_ownership_root_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#44- joining-your-nodes 3. https://kubernetes.io/docs/admin/kubeadm/#kubelet-drop-in
4.1.3,If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive,Manual,"If kube-proxy is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of 600 or more restrictive.","The kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kube-proxy with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. Impact: None","Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a <path><filename> Verify that a file is specified and it exists with permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 <proxy kubeconfig file> Default Value: By default, proxy file has permissions of 640. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",kubernetes_proxy_kubeconfig_file_permissions_600; kubernetes_proxy_kubeconfig_file_permissions_restrictive; kubernetes_proxy_kubeconfig_file_permissions_secure; kubernetes_proxy_kubeconfig_file_permissions_min_600; kubernetes_proxy_kubeconfig_file_permissions_strict,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.4,If proxy kubeconfig file exists ensure ownership is set to root:root,Manual,"If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,"Find the kubeconfig file being used by kube-proxy by running the following command: ps -ef | grep kube-proxy If kube-proxy is running, get the kubeconfig file location from the --kubeconfig parameter. To perform the audit: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G <path><filename> Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <proxy kubeconfig file> Default Value: By default, proxy file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kube-proxy/",compute_kubeconfig_file_ownership_root; compute_kubeconfig_file_ownership_root_root; compute_kubeconfig_proxy_ownership_root; compute_kubeconfig_proxy_ownership_root_root; compute_proxy_kubeconfig_ownership_root; compute_proxy_kubeconfig_ownership_root_root; kubernetes_kubeconfig_file_ownership_root; kubernetes_kubeconfig_file_ownership_root_root; kubernetes_kubeconfig_proxy_ownership_root; kubernetes_kubeconfig_proxy_ownership_root_root; kubernetes_proxy_kubeconfig_ownership_root; kubernetes_proxy_kubeconfig_ownership_root_root,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kube-proxy/
4.1.5,Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive,Automated,Ensure that the kubelet.conf file has permissions of 600 or more restrictive.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.Verify that the permissions are 600 or more restrictive.","Run the below command (based on the file location on your system) on the each worker node. For example, chmod 600 /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file has permissions of 600. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_conf_file_permissions_600_or_more_restrictive; kubernetes_kubeconfig_file_permissions_restrictive; kubelet_conf_file_permissions_secure; kubernetes_kubelet_conf_file_permissions_compliant; kubeconfig_file_permissions_restricted_access,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.6,Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root,Automated,Ensure that the kubelet.conf file ownership is set to root:root.,"The kubelet.conf file is the kubeconfig file for the node, and controls various parameters that set the behavior and identity of the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config file. Please set $kubelet_config=<PATH> based on the file location on your system for example: export kubelet_config=/etc/kubernetes/kubelet.conf To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %U:%G /etc/kubernetes/kubelet.conf Verify that the ownership is set to root:root.","Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, kubelet.conf file ownership is set to root:root. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubeconfig_file_ownership_root; kubernetes_kubelet_conf_file_ownership_root; kubernetes_kubeconfig_root_ownership; kubernetes_kubelet_conf_root_ownership; kubernetes_kubeconfig_file_root_owner; kubernetes_kubelet_conf_file_root_owner,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/kubelet/
4.1.7,Ensure that the certificate authorities file permissions are set to 600 or more restrictive,Manual,Ensure that the certificate authorities file has permissions of 600 or more restrictive.,The certificate authorities file controls the authorities used to validate API requests. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %a <filename> Verify that the permissions are 644 or more restrictive.,Run the following command to modify the file permissions of the --client-ca-file chmod 600 <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_certificate_authorities_file_permissions_600_or_stricter; compute_ca_file_permissions_restrictive; compute_ssl_certificate_authorities_file_permissions_secure; compute_trusted_ca_file_permissions_600_or_less; compute_ca_file_permissions_not_world_readable,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.8,Ensure that the client certificate authorities file ownership is set to root:root,Manual,Ensure that the certificate authorities file ownership is set to root:root.,The certificate authorities file controls the authorities used to validate API requests. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None,Run the following command: ps -ef | grep kubelet Find the file specified by the --client-ca-file argument. Run the following command: stat -c %U:%G <filename> Verify that the ownership is set to root:root.,Run the following command to modify the ownership of the --client-ca-file. chown root:root <filename> Default Value: By default no --client-ca-file is specified. References: 1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs,compute_client_certificate_authorities_file_ownership_root; compute_client_certificate_authorities_file_owner_root; compute_client_certificate_authorities_file_group_root; compute_client_certificate_authorities_file_root_owned; compute_client_certificate_authorities_file_root_root_owned,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/admin/authentication/#x509-client-certs
4.1.9,If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 600 or more restrictive.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %a /var/lib/kubelet/config.yaml Verify that the permissions are 600 or more restrictive.","Run the following command (using the config file location identied in the Audit step) chmod 600 /var/lib/kubelet/config.yaml Default Value: By default, the /var/lib/kubelet/config.yaml file as set up by kubeadm has permissions of 600. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubelet_config_file_permissions_600_or_stricter; kubelet_config_file_restrictive_permissions; kubelet_config_file_permissions_compliant; kubelet_config_file_secure_permissions; kubelet_config_file_permissions_enforced,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.1.10,If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root,Automated,"Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.","The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root. Impact: None","Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet config yaml file. Please set $kubelet_config_yaml=<PATH> based on the file location on your system for example: export kubelet_config_yaml=/var/lib/kubelet/config.yaml To perform the audit manually: Run the below command (based on the file location on your system) on the each worker node. For example, stat -c %aU %G /var/lib/kubelet/config.yaml ```Verify that the ownership is set to `root:root`.","Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: By default, /var/lib/kubelet/config.yaml file as set up by kubeadm is owned by root:root. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/",kubernetes_kubelet_config_file_owner_root; kubernetes_kubelet_config_file_group_root; kubernetes_kubelet_config_file_permissions_secure; kubernetes_kubelet_config_file_ownership_correct; kubernetes_kubelet_config_file_root_owned,• Level 1 - Worker Node,None,1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
4.2.1,Ensure that the --anonymous-auth argument is set to false,Automated,Disable anonymous requests to the Kubelet server.,"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.","If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. Run the following command on each node: ps -ef | grep kubelet Verify that the --anonymous-auth argument is set to false. This executable argument may be omitted, provided there is a corresponding entry set to false in the Kubelet config file.","If using a Kubelet config file, edit the file to set authentication: anonymous: enabled to false. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --anonymous-auth=false Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, anonymous access is enabled. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_kubelet_anonymous_auth_disabled; kubernetes_kubelet_anonymous_auth_set_false; kubernetes_kubelet_auth_anonymous_disabled; kubernetes_kubelet_secure_auth_enabled; kubernetes_kubelet_no_anonymous_auth,• Level 1 - Worker Node,Anonymous requests will be rejected.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.2,Ensure that the --authorization-mode argument is not set to AlwaysAllow,Automated,Do not allow all requests. Enable explicit authorization.,"Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.","Run the following command on each node: ps -ef | grep kubelet If the --authorization-mode argument is present check that it is not set to AlwaysAllow. If it is not present check that there is a Kubelet config file specified by --config, and that file sets authorization: mode to something other than AlwaysAllow. It is also possible to review the running configuration of a Kubelet via the /configz endpoint on the Kubelet API port (typically 10250/TCP). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","If using a Kubelet config file, edit the file to set authorization: mode to Webhook. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --authorization-mode=Webhook Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --authorization-mode argument is set to AlwaysAllow. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication",kubernetes_api_server_authorization_mode_not_always_allow; kubernetes_api_server_authorization_mode_explicit; kubernetes_api_server_authorization_restricted; kubernetes_api_server_no_always_allow_auth; kubernetes_api_server_explicit_auth_required,• Level 1 - Worker Node,Unauthorized requests will be denied.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication
4.2.3,Ensure that the --client-ca-file argument is set as appropriate,Automated,Enable Kubelet authentication using certificates.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.","Run the following command on each node: ps -ef | grep kubelet Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. If the --client-ca-file argument is not present, check that there is a Kubelet config file specified by --config, and that the file sets authentication: x509: clientCAFile to the location of the client certificate authority file.","If using a Kubelet config file, edit the file to set authentication: x509: clientCAFile to the location of the client CA file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_AUTHZ_ARGS variable. --client-ca-file=<path/to/client-ca-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --client-ca-file argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/",kubernetes_kubelet_client_ca_file_set; kubernetes_kubelet_authentication_certificate_enabled; kubernetes_kubelet_client_ca_file_configured; kubernetes_kubelet_certificate_authentication_required; kubernetes_kubelet_client_ca_file_valid,• Level 1 - Worker Node,You require TLS to be configured on apiserver as well as kubelets.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/
4.2.4,Verify that the --read-only-port argument is set to 0,Manual,Disable the read-only port.,The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,"Run the following command on each node: ps -ef | grep kubelet Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.","If using a Kubelet config file, edit the file to set readOnlyPort to 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --read-only-port=0 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --read-only-port is set to 10255/TCP. However, if a config file is specified by --config the default value for readOnlyPort is 0. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_api_server_read_only_port_disabled; kubernetes_api_server_read_only_port_set_zero; kubernetes_api_server_read_only_port_secure; kubernetes_api_server_read_only_port_unset; kubernetes_api_server_read_only_port_zero,• Level 1 - Worker Node,Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.,1. https://kubernetes.io/docs/admin/kubelet/
4.2.5,Ensure that the --streaming-connection-idle-timeout argument is not set to 0,Manual,Do not disable timeouts on streaming connections.,"Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.","Run the following command on each node: ps -ef | grep kubelet Verify that the --streaming-connection-idle-timeout argument is not set to 0. If the argument is not present, and there is a Kubelet config file specified by --config, check that it does not set streamingConnectionIdleTimeout to 0.","If using a Kubelet config file, edit the file to set streamingConnectionIdleTimeout to a value other than 0. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. --streaming-connection-idle-timeout=5m Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --streaming-connection-idle-timeout is set to 4 hours. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552",kubernetes_api_server_streaming_connection_idle_timeout_not_disabled; kubernetes_api_server_streaming_connection_timeout_configured; kubernetes_api_server_streaming_idle_timeout_non_zero; kubernetes_api_server_connection_idle_timeout_enabled; kubernetes_api_server_streaming_timeout_valid,• Level 1 - Worker Node,Long-lived connections could be interrupted.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552
4.2.6,Ensure that the --make-iptables-util-chains argument is set to true,Automated,Allow Kubelet to manage iptables.,"Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.","Run the following command on each node: ps -ef | grep kubelet Verify that if the --make-iptables-util-chains argument exists then it is set to true. If the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.","If using a Kubelet config file, edit the file to set makeIPTablesUtilChains: true. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove the --make- iptables-util-chains argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --make-iptables-util-chains argument is set to true. References: 1. https://kubernetes.io/docs/admin/kubelet/",kubernetes_kubelet_iptables_util_chains_enabled; kubernetes_kubelet_iptables_management_enabled; kubernetes_kubelet_iptables_chains_configured; kubernetes_kubelet_iptables_util_chains_set_true; kubernetes_kubelet_iptables_util_chains_configured,• Level 1 - Worker Node,"Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",1. https://kubernetes.io/docs/admin/kubelet/
4.2.7,Ensure that the --hostname-override argument is not set,Manual,Do not override node hostnames.,"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Impact: Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",Run the following command on each node: ps -ef | grep kubelet Verify that --hostname-override argument does not exist. Note This setting is not configurable via the Kubelet config file.,"Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and remove the --hostname-override argument from the KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, --hostname-override argument is not set. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063",kubernetes_node_hostname_override_disabled; kubernetes_node_hostname_default; kubernetes_node_hostname_unchanged; kubernetes_node_hostname_override_not_set; kubernetes_node_hostname_preserved,• Level 1 - Worker Node,"Some cloud providers may require this flag to ensure that hostname matches names issued by the cloud provider. In these environments, this recommendation should not apply.",1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/issues/22063
4.2.8,Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture,Manual,"Security relevant information should be captured. The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,"Run the following command on each node: sudo grep 'eventRecordQPS' /etc/systemd/system/kubelet.service.d/10- kubeadm.conf Review the value set for the argument and determine whether this has been set to an appropriate level for the cluster. If the argument does not exist, check that there is a Kubelet config file specified by -- config and review the value in this location.","If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level. If using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, eventRecordQPS argument is set to 5. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go",kubernetes_kubelet_event_record_qps_set; kubernetes_kubelet_event_record_qps_appropriate_level; kubernetes_kubelet_event_record_qps_not_unlimited; kubernetes_kubelet_event_record_qps_rate_limited; kubernetes_kubelet_event_record_qps_configured; kubernetes_kubelet_event_record_qps_secure_value; kubernetes_kubelet_event_record_qps_non_zero; kubernetes_kubelet_event_record_qps_dos_protected,• Level 2 - Worker Node,Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.,1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go
4.2.9,Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate,Manual,Setup TLS connection on the Kubelets.,"The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.","Run the following command on each node: ps -ef | grep kubelet Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate. If these arguments are not present, check that there is a Kubelet config specified by -- config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET_CERTIFICATE_ARGS variable. --tls-cert-file=<path/to/tls-certificate-file> --tls-private-key-file=<path/to/tls-key-file> Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service",kubernetes_kubelet_tls_cert_file_set; kubernetes_kubelet_tls_private_key_file_set; kubernetes_kubelet_tls_configured; kubernetes_kubelet_tls_files_valid; kubernetes_kubelet_tls_connection_secure,• Level 1 - Worker Node,,
4.2.10,Ensure that the --rotate-certificates argument is not set to false,Automated,Enable kubelet client certificate rotation.,The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None,"Run the following command on each node: ps -ef | grep kubelet Verify that the --rotate-certificates argument is not present, or is set to true. If the --rotate-certificates argument is not present, verify that if there is a Kubelet config file specified by --config, that file does not contain rotateCertificates: false.","If using a Kubelet config file, edit the file to add the line rotateCertificates: true or remove it altogether to use the default value. If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and remove --rotate- certificates=false argument from the KUBELET_CERTIFICATE_ARGS variable. Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet client certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_rotate_certificates_not_disabled; kubernetes_kubelet_client_cert_rotation_required; kubernetes_kubelet_rotate_certificates_true; kubernetes_kubelet_cert_rotation_not_false,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/
4.2.11,Verify that the RotateKubeletServerCertificate argument is set to true,Manual,Enable kubelet server certificate rotation.,RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None,Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate- server-certificates parameter is set on kubelet Run the following command on each node: ps -ef | grep kubelet Verify that RotateKubeletServerCertificate argument exists and is set to true.,"Edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable. --feature-gates=RotateKubeletServerCertificate=true Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default, kubelet server certificate rotation is enabled. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration",kubernetes_kubelet_certificate_rotation_enabled; kubernetes_kubelet_server_certificate_auto_rotated; kubernetes_kubelet_tls_certificate_rotation_active; kubernetes_kubelet_rotate_server_certificate_set_true; kubernetes_kubelet_certificate_rotation_configured,• Level 1 - Worker Node,None,1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration
4.2.12,Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers,Manual,Ensure that the Kubelet is configured to only use strong cryptographic ciphers.,"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided. Impact: Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.","The set of cryptographic ciphers currently considered secure is the following: • TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 • TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 • TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 • TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_256_GCM_SHA384 • TLS_RSA_WITH_AES_128_GCM_SHA256 Run the following command on each node: ps -ef | grep kubelet If the --tls-cipher-suites argument is present, ensure it only contains values included in this set. If it is not present check that there is a Kubelet config file specified by --config, and that file sets TLSCipherSuites: to only include values from this set.","If using a Kubelet config file, edit the file to set TLSCipherSuites: to TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 ,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 ,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 ,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 or to a subset of these values. If using executable arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the --tls-cipher-suites parameter as follows, or to a subset of these values. --tls-cipher- suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM _SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM _SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM _SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 Based on your system, restart the kubelet service. For example: systemctl daemon-reload systemctl restart kubelet.service Default Value: By default the Kubernetes API server supports a wide range of TLS ciphers Additional Information: The list chosen above should be fine for modern clients. It's essentially the list from the Mozilla 'Modern cipher' option with the ciphersuites supporting CBC mode removed, as CBC has traditionally had a lot of issues",kubernetes_kubelet_strong_ciphers_enabled; kubernetes_kubelet_weak_ciphers_disabled; kubernetes_kubelet_tls_ciphers_restricted; kubernetes_kubelet_cipher_suite_compliant; kubernetes_kubelet_secure_ciphers_only,• Level 1 - Worker Node,Kubelet clients that cannot support modern cryptographic ciphers will not be able to make connections to the Kubelet API.,
4.2.13,Ensure that a limit is set on pod PIDs,Manual,Ensure that the Kubelet sets limits on the number of PIDs that can be created by pods running on the node.,"By default pods running in a cluster can consume any number of PIDs, potentially exhausting the resources available on the node. Setting an appropriate limit reduces the risk of a denial of service attack on cluster nodes. Impact: Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.","Review the Kubelet's start-up parameters for the value of --pod-max-pids, and check the Kubelet configuration file for the PodPidsLimit . If neither of these values is set, then there is no limit in place.","Decide on an appropriate level for this parameter and set it, either via the --pod-max- pids command line parameter or the PodPidsLimit configuration file setting. Default Value: By default the number of PIDs is not limited. References: 1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 5 Policies This section contains recommendations for various Kubernetes policies which are important to the security of the environment. 5.1 RBAC and Service Accounts",kubernetes_kubelet_pid_limit_enabled; kubernetes_pod_pid_limit_configured; kubernetes_node_pid_limit_enforced; kubernetes_kubelet_pid_limit_set; kubernetes_pod_pid_limit_restricted,• Level 1 - Worker Node,Setting this value will restrict the number of processes per pod. If this limit is lower than the number of PIDs required by a pod it will not operate.,1. https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits 5 Policies This section contains recommendations for various Kubernetes policies which are important to the security of the environment. 5.1 RBAC and Service Accounts
5.1.1,Ensure that the cluster-admin role is only used where required,Manual,The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.,"Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.","Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.","Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles",kubernetes_role_no_cluster_admin; kubernetes_role_cluster_admin_restricted; kubernetes_role_cluster_admin_minimal_usage; kubernetes_role_cluster_admin_least_privilege; kubernetes_role_cluster_admin_required_only,• Level 1 - Master Node,"Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles
5.1.2,Minimize access to secrets,Manual,"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation,"Review the users who have get, list or watch access to secrets objects in the Kubernetes API.","Where possible, remove get, list and watch access to secret objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have get privileges on secret objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:expand-controller expand-controller ServiceAccount kube-system system:controller:generic-garbage-collector generic-garbage- collector ServiceAccount kube-system system:controller:namespace-controller namespace-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:kube-controller-manager system:kube-controller- manager User",kubernetes_secret_access_restricted; kubernetes_secret_minimal_access; kubernetes_secret_no_public_access; kubernetes_secret_no_anonymous_access; kubernetes_secret_no_wildcard_access; kubernetes_secret_no_default_service_account; kubernetes_secret_no_broad_role_binding; kubernetes_secret_no_excessive_permissions; kubernetes_secret_no_unrestricted_access; kubernetes_secret_no_cluster_admin_binding,• Level 1 - Master Node,Care should be taken not to remove access to secrets to system components which require this for their operation,
5.1.3,Minimize wildcard use in Roles and ClusterRoles,Manual,Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard '*' which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.,The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.,Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml,Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.,kubernetes_role_no_wildcard_resources; kubernetes_clusterrole_no_wildcard_resources; kubernetes_role_no_wildcard_actions; kubernetes_clusterrole_no_wildcard_actions; kubernetes_role_no_wildcard_all; kubernetes_clusterrole_no_wildcard_all,• Level 1 - Worker Node,,
5.1.4,Minimize access to create pods,Manual,"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.","The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",Review the users who have create access to pod objects in the Kubernetes API.,"Where possible, remove create access to pod objects in the cluster. Default Value: By default in a kubeadm cluster the following list of principals have create privileges on pod objects CLUSTERROLEBINDING SUBJECT TYPE SA-NAMESPACE cluster-admin system:masters Group system:controller:clusterrole-aggregation-controller clusterrole- aggregation-controller ServiceAccount kube-system system:controller:daemon-set-controller daemon-set-controller ServiceAccount kube-system system:controller:job-controller job-controller ServiceAccount kube-system system:controller:persistent-volume-binder persistent-volume- binder ServiceAccount kube-system system:controller:replicaset-controller replicaset-controller ServiceAccount kube-system system:controller:replication-controller replication-controller ServiceAccount kube-system system:controller:statefulset-controller statefulset-controller ServiceAccount kube-system",kubernetes_namespace_pod_creation_restricted; kubernetes_role_pod_creation_minimized; kubernetes_user_pod_creation_limited; kubernetes_service_account_pod_creation_restricted; kubernetes_rbac_pod_creation_minimized; kubernetes_policy_pod_creation_limited; kubernetes_cluster_pod_creation_restricted; kubernetes_admission_pod_creation_controlled,• Level 1 - Master Node,Care should be taken not to remove access to pods to system components which require this for their operation,
5.1.5,Ensure that default service accounts are not actively used.,Manual,The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.,"Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.","For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/,iam_service_account_default_not_used; iam_service_account_default_inactive; compute_service_account_default_disabled; compute_service_account_default_unused; service_account_default_no_active_usage,• Level 1 - Master Node,All workloads which require access to the Kubernetes API will require an explicit service account to be created.,1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.6,Ensure that Service Account Tokens are only mounted where necessary,Manual,Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server,"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.","Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false","Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",kubernetes_pod_service_account_token_unmounted; kubernetes_pod_service_account_token_mounted_only_when_required; kubernetes_pod_service_account_token_restricted; kubernetes_pod_service_account_token_minimal_access; kubernetes_pod_service_account_token_disabled_by_default,• Level 1 - Master Node,"Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/
5.1.7,Avoid use of system:masters group,Manual,"The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed. When combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster. Impact: Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.,Remove the system:masters group from all users in the cluster. Default Value: By default some clusters will create a 'break glass' client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations. References: 1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38,iam_group_system_masters_restricted; iam_group_system_masters_no_usage; iam_group_system_masters_minimal_usage; iam_group_system_masters_bootstrap_only; iam_group_system_masters_no_grants; iam_group_system_masters_no_user_assignments; iam_group_system_masters_no_service_account_assignments; iam_group_system_masters_no_unnecessary_usage,• Level 1 - Master Node,"Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",1. https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/rbac/escalatio n_check.go#L38
5.1.8,"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",Manual,"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster- admin level. Impact: There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.","Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.","Where possible, remove the impersonate, bind and escalate rights from subjects. Default Value: In a default kubeadm cluster, the system:masters group and clusterrole-aggregation- controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate. References: 1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/",kubernetes_role_no_privilege_escalation; kubernetes_cluster_role_no_impersonate_permission; kubernetes_role_no_bind_permission; kubernetes_cluster_role_no_escalate_permission; kubernetes_role_no_privilege_escalation_permissions; kubernetes_cluster_role_restrict_high_risk_permissions,• Level 1 - Master Node,"There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",1. https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls 2. https://raesene.github.io/blog/2020/12/12/Escalating_Away/ 3. https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/
5.1.9,Minimize access to create persistent volumes,Manual,"The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of hostPath volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.","The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have create access to PersistentVolume objects in the Kubernetes API.,"Where possible, remove create access to PersistentVolume objects in the cluster. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation",kubernetes_persistent_volume_creation_restricted; kubernetes_persistent_volume_hostpath_disabled; kubernetes_persistent_volume_admin_access_minimized; kubernetes_persistent_volume_privilege_escalation_prevented; kubernetes_persistent_volume_creation_scope_limited; kubernetes_persistent_volume_sensitive_host_access_blocked; kubernetes_persistent_volume_creation_roles_restricted; kubernetes_persistent_volume_creation_permissions_minimized,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#persistent- volume-creation
5.1.10,Minimize access to the proxy sub-resource of nodes,Manual,"Users with access to the Proxy sub-resource of Node objects automatically have permissions to use the Kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs. The Kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the node object. The proxy sub-resource specifically allows wide ranging access to the Kubelet API. Direct access to the Kubelet API bypasses controls like audit logging (there is no audit log of Kubelet API access) and admission control.","The ability to use the proxy sub-resource of node objects opens up possibilities for privilege escalation and should be restricted, where possible.",Review the users who have access to the proxy sub-resource of node objects in the Kubernetes API.,"Where possible, remove access to the proxy sub-resource of node objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization",kubernetes_node_proxy_access_restricted; kubernetes_node_proxy_no_wildcard_permissions; kubernetes_node_proxy_minimal_roles; kubernetes_node_proxy_no_admin_access; kubernetes_node_proxy_api_disabled; kubernetes_node_proxy_subresource_protected; kubernetes_node_proxy_no_anonymous_access; kubernetes_node_proxy_no_public_access; kubernetes_node_proxy_no_privilege_escalation; kubernetes_node_proxy_audit_logging_enabled,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#access-to- proxy-subresource-of-nodes 2. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn- authz/#kubelet-authorization
5.1.11,Minimize access to the approval sub-resource of certificatesigningrequests objects,Manual,"Users with access to the update the approval sub-resource of certificateaigningrequest objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts. This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster",The ability to update certificate signing requests should be limited.,Review the users who have access to update the approval sub-resource of certificatesigningrequest objects in the Kubernetes API.,"Where possible, remove access to the approval sub-resource of certificatesigningrequest objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing",kubernetes_certificatesigningrequest_approval_restricted; kubernetes_certificatesigningrequest_no_public_approval; kubernetes_certificatesigningrequest_approval_minimized; kubernetes_certificatesigningrequest_approval_high_privilege_denied; kubernetes_certificatesigningrequest_approval_admin_restricted,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#csrs-and- certificate-issuing
5.1.12,Minimize access to webhook configuration objects,Manual,"Users with rights to create/modify/delete validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.",The ability to manage webhook configuration should be limited,Review the users who have access to validatingwebhookconfigurations or mutatingwebhookconfigurations objects in the Kubernetes API.,"Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks",kubernetes_validatingwebhookconfiguration_minimize_access; kubernetes_mutatingwebhookconfiguration_minimize_access; kubernetes_webhookconfiguration_restrict_modify_access; kubernetes_webhookconfiguration_restrict_delete_access; kubernetes_webhookconfiguration_restrict_create_access; kubernetes_webhookconfiguration_admin_access_restricted; kubernetes_webhookconfiguration_privilege_escalation_prevented,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#control- admission-webhooks
5.1.13,Minimize access to the service account token creation,Manual,"Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.",The ability to create service account tokens should be limited.,Review the users who have access to create the token sub-resource of serviceaccount objects in the Kubernetes API.,"Where possible, remove access to the token sub-resource of serviceaccount objects. References: 1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request",kubernetes_service_account_token_creation_restricted; kubernetes_service_account_token_creation_minimized; kubernetes_cluster_service_account_token_creation_limited; kubernetes_service_account_token_creation_admin_restricted; kubernetes_cluster_service_account_token_creation_disabled,• Level 1 - Master Node,,1. https://kubernetes.io/docs/concepts/security/rbac-good-practices/#token-request
5.2.1,Ensure that the cluster has at least one active policy control mechanism in place,Manual,"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts. Impact: Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.","Run the following command: get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}' It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection. calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node- c4xv4: {} {'privileged':true} {'privileged':true} {'privileged':true} {'privileged':true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {'seccompProfile':{'type':'RuntimeDefault'}} {'allowPrivilegeEscalation':false,'readOnlyRootFilesystem':true,'runAsGroup':2001,'ru nAsUser':1001}","Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads. Default Value: By default, Pod Security Admission is enabled but no policies are in place. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission",kubernetes_cluster_policy_control_enabled; kubernetes_cluster_pod_security_admission_enabled; kubernetes_cluster_third_party_policy_control_enabled; kubernetes_cluster_active_policy_control_mechanism_exists; kubernetes_cluster_policy_enforcement_mechanism_active,• Level 1 - Master Node,"Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.",1. https://kubernetes.io/docs/concepts/security/pod-security-admission
5.2.2,Minimize the admission of privileged containers,Manual,Do not generally permit containers to be run with the securityContext.privileged flag set to true.,"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.","Run the following command: get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}' It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection. calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node- c4xv4: {} {'privileged':true} {'privileged':true} {'privileged':true} {'privileged':true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {'seccompProfile':{'type':'RuntimeDefault'}} {'allowPrivilegeEscalation':false,'readOnlyRootFilesystem':true,'runAsGroup':2001,'ru nAsUser':1001}","Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privileged_disabled; compute_container_privileged_escalation_disabled; compute_container_security_context_restricted; compute_container_privileged_flag_false; compute_container_privileged_minimized; compute_container_privileged_denied; compute_container_privileged_mode_disabled; compute_container_privileged_access_restricted,• Level 1 - Master Node,"Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.3,Minimize the admission of containers wishing to share the host process ID namespace,Automated,Do not generally permit containers to be run with the hostPID flag set to true.,"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostPID containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_pid_disabled; compute_container_host_pid_restricted; compute_container_host_pid_not_shared; compute_container_host_pid_isolated; compute_container_host_pid_protected,• Level 1 - Master Node,Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.4,Minimize the admission of containers wishing to share the host IPC namespace,Automated,Do not generally permit containers to be run with the hostIPC flag set to true.,"A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostIPC containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_ipc_disabled; compute_container_host_ipc_restricted; compute_container_host_ipc_not_shared; compute_container_host_ipc_denied; compute_container_host_ipc_protected,• Level 1 - Master Node,Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.5,Minimize the admission of containers wishing to share the host network namespace,Automated,Do not generally permit containers to be run with the hostNetwork flag set to true.,"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostNetwork containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_network_disabled; compute_container_host_network_restricted; compute_container_host_network_denied; compute_container_host_network_protected; compute_container_host_network_isolated,• Level 1 - Master Node,Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.6,Minimize the admission of containers with allowPrivilegeEscalation,Automated,"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with .spec.allowPrivilegeEscalationset to true. Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_privilege_escalation_disabled; compute_container_allow_privilege_escalation_false; compute_container_privilege_escalation_restricted; compute_container_privilege_escalation_minimized; compute_container_privilege_escalation_denied,• Level 1 - Master Node,Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.7,Minimize the admission of root containers,Automated,Do not generally permit containers to be run as the root user.,"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user. There should be at least one admission control policy defined which does not permit root containers. If you need to run root containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run as the root user will not be permitted.","List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0.","Create a policy for each namespace in the cluster, ensuring that either MustRunAsNonRoot or MustRunAs with the range of UIDs not including 0, is set. Default Value: By default, there are no restrictions on the use of root containers and if a User is not specified in the image, the container will run as root. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_root_user_disabled; compute_container_root_privileges_restricted; compute_container_non_root_user_required; compute_container_root_admission_minimized; compute_container_root_execution_prohibited,• Level 2 - Master Node,Pods with containers which run as the root user will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.8,Minimize the admission of containers with the NET_RAW capability,Automated,Do not generally permit containers with the potentially dangerous NET_RAW capability.,"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability. There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability. If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which run with the NET_RAW capability will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the NET_RAW capability.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the NET_RAW capability. Default Value: By default, there are no restrictions on the creation of containers with the NET_RAW capability. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_net_raw_capability_disabled; compute_container_net_raw_capability_restricted; compute_container_net_raw_capability_minimized; compute_container_net_raw_capability_denied; compute_container_net_raw_capability_prohibited,• Level 1 - Master Node,Pods with containers which run with the NET_RAW capability will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.9,Minimize the admission of containers with added capabilities,Automated,Do not generally permit containers with capabilities assigned beyond the default set.,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks. There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching. If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods with containers which require capabilities outwith the default set will not be permitted.","List the policies in use for each namespace in the cluster, ensure that policies are present which prevent allowedCapabilities to be set to anything other than an empty array.","Ensure that allowedCapabilities is not present in policies for the cluster unless it is set to an empty array. Default Value: By default, there are no restrictions on adding capabilities to containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_minimized; compute_container_default_capabilities_only; compute_container_added_capabilities_restricted; compute_container_capabilities_whitelisted; compute_container_privileged_capabilities_disabled,• Level 1 - Master Node,Pods with containers which require capabilities outwith the default set will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.10,Minimize the admission of containers with capabilities assigned,Manual,Do not generally permit containers with capabilities,"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized. Impact: Pods with containers require capabilities to operate will not be permitted.","List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers.","Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities. Default Value: By default, there are no restrictions on the creation of containers with additional capabilities References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/",compute_container_capabilities_restricted; compute_container_capabilities_minimized; compute_container_no_privileged_capabilities; compute_container_capabilities_disabled; compute_container_capabilities_limited; compute_container_no_additional_capabilities; compute_container_capabilities_denied; compute_container_privileged_capabilities_blocked,• Level 2 - Master Node,Pods with containers require capabilities to operate will not be permitted.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 2. https://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged- linux-containers/
5.2.11,Minimize the admission of Windows HostProcess Containers,Manual,Do not generally permit Windows containers to be run with the hostProcess flag set to true.,"A Windows container making use of the hostProcess flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides 'privileged access' to the Windows node. Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit hostProcess Windows containers. If you need to run Windows containers which require hostProcess, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostProcess containers","Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostProcess containers. Default Value: By default, there are no restrictions on the creation of hostProcess containers. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_host_process_disabled; compute_windows_container_host_process_restricted; container_windows_host_process_minimized; container_host_process_flag_disabled; compute_windows_container_host_process_blocked,• Level 1 - Master Node,Pods defined with securityContext.windowsOptions.hostProcess: true will not be permitted unless they are run under a specific policy.,1. https://kubernetes.io/docs/tasks/configure-pod-container/create-hostprocess-pod/ 2. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.12,Minimize the admission of HostPath volumes,Manual,Do not generally admit containers which make use of hostPath volumes.,"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem. There should be at least one admission control policy defined which does not permit containers to mount hostPath volumes. If you need to run containers which require hostPath volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with hostPath volumes.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPath volumes. Default Value: By default, there are no restrictions on the creation of hostPath volumes. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/",compute_container_hostpath_volumes_disabled; compute_container_hostpath_volumes_restricted; compute_container_hostpath_volumes_minimized; compute_container_hostpath_volumes_denied; compute_container_hostpath_volumes_blocked,• Level 1 - Master Node,Pods defined which make use of hostPath volumes will not be permitted unless they are run under a spefific policy.,1. https://kubernetes.io/docs/concepts/security/pod-security-standards/
5.2.13,Minimize the admission of containers which use HostPorts,Manual,Do not generally permit containers which require the use of HostPorts.,"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts. If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.","List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have hostPort sections.","Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use hostPort sections. Default Value: By default, there are no restrictions on the use of HostPorts. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI",compute_container_host_ports_restricted; compute_container_host_ports_disabled; compute_container_host_ports_minimized; compute_container_host_ports_denied; compute_container_host_ports_blocked,• Level 1 - Master Node,"Pods defined with hostPort settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.",1. https://kubernetes.io/docs/concepts/security/pod-security-standards/ 5.3 Network Policies and CNI
5.3.1,Ensure that the CNI in use supports Network Policies,Manual,There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.,Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None,"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",kubernetes_cni_network_policy_support; kubernetes_cni_network_policy_enabled; kubernetes_network_policy_cni_compliance; kubernetes_cni_network_policy_required; kubernetes_network_policy_cni_support_check,• Level 1 - Master Node,None,1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.
5.3.2,Ensure that all Namespaces have Network Policies defined,Manual,Use network policies to isolate traffic in your cluster network.,"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces``` Ensure that each namespace defined in the cluster has at least one Network Policy.,"Follow the documentation and create NetworkPolicy objects as you need them. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",,• Level 2 - Master Node,"Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/
5.4.1,Prefer using secrets as files over secrets as environment variables,Manual,Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.,"It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {'\n'}{end}' -A,"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",kubernetes_secret_files_preferred; kubernetes_secret_no_environment_variables; kubernetes_secret_volume_mounted; kubernetes_secret_environment_avoided; kubernetes_secret_file_based_usage,• Level 2 - Master Node,Application code which expects to read secrets in the form of environment variables would need modification,1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod
5.4.2,Consider external secret storage,Manual,"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",Review your secrets management implementation.,"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. Default Value: By default, no external secret management is configured.",kubernetes_secret_external_storage_required; kubernetes_secret_authentication_required; kubernetes_secret_access_auditing_enabled; kubernetes_secret_encryption_enabled; kubernetes_secret_rotation_enabled,• Level 2 - Master Node,None,
5.5.1,Configure Image Provenance using ImagePolicyWebhook admission controller,Manual,Configure Image Provenance for your deployment.,Kubernetes supports plugging in provenance rules to accept or reject the images in your deployments. You could configure such rules to ensure that only approved images are deployed in the cluster. Impact: You need to regularly maintain your provenance configuration based on container image updates.,Review the pod definitions in your cluster and verify that image provenance is configured as appropriate.,"Follow the Kubernetes documentation and setup image provenance. Default Value: By default, image provenance is not set. References: 1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888",kubernetes_image_provenance_enabled; kubernetes_image_policy_webhook_enabled; kubernetes_admission_controller_image_provenance; kubernetes_image_provenance_webhook_configured; kubernetes_image_policy_webhook_active; kubernetes_image_provenance_validation_enabled; kubernetes_admission_controller_image_policy_webhook; kubernetes_image_provenance_webhook_required,• Level 2 - Master Node,You need to regularly maintain your provenance configuration based on container image updates.,1. https://kubernetes.io/docs/admin/admission-controllers/#imagepolicywebhook 2. https://github.com/kubernetes/community/blob/master/contributors/design- proposals/image-provenance.md 3. https://hub.docker.com/r/dnurmi/anchore-toolbox/ 4. https://github.com/kubernetes/kubernetes/issues/22888
5.7.1,Create administrative boundaries between resources using namespaces,Manual,Use namespaces to isolate your Kubernetes objects.,"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.,"Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with 4 initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-node-lease - Namespace used for node heartbeats 4. kube-public - Namespace used for public information in a cluster References: 1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats",kubernetes_namespace_isolation_enabled; kubernetes_namespace_admin_boundaries_enforced; kubernetes_namespace_resource_separation_required; kubernetes_namespace_default_deny_policy; kubernetes_namespace_network_policies_restricted; kubernetes_namespace_rbac_scoped; kubernetes_namespace_quota_limits_defined; kubernetes_namespace_label_selector_required; kubernetes_namespace_pod_security_policies_enforced; kubernetes_namespace_service_account_restrictions_applied,• Level 1 - Master Node,You need to switch between namespaces for administration.,1. https://kubernetes.io/docs/concepts/overview/working-with- objects/namespaces/#viewing-namespaces 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html 3. https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/589- efficient-node-heartbeats
5.7.2,Ensure that the seccomp profile is set to docker/default in your pod definitions,Manual,Enable docker/default seccomp profile in your pod definitions.,"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container. Impact: If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",Review the pod definitions in your cluster. It should create a line as below: securityContext: seccompProfile: type: RuntimeDefault,"Use security context to enable the docker/default seccomp profile in your pod definitions. An example is as below: securityContext: seccompProfile: type: RuntimeDefault Default Value: By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled. References: 1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/",kubernetes_pod_seccomp_profile_enabled; kubernetes_pod_seccomp_profile_default; kubernetes_pod_seccomp_profile_docker_default; pod_security_seccomp_profile_default; pod_security_seccomp_profile_docker_default,• Level 2 - Master Node,"If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.",1. https://kubernetes.io/docs/tutorials/clusters/seccomp/ 2. https://docs.docker.com/engine/security/seccomp/
5.7.3,Apply Security Context to Your Pods and Containers,Manual,Apply Security Context to Your Pods and Containers,"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.,"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks",kubernetes_pod_security_context_configured; kubernetes_container_security_context_configured; kubernetes_pod_privileged_mode_disabled; kubernetes_container_privileged_mode_disabled; kubernetes_pod_read_only_root_filesystem_enabled; kubernetes_container_read_only_root_filesystem_enabled; kubernetes_pod_run_as_non_root_enabled; kubernetes_container_run_as_non_root_enabled; kubernetes_pod_capabilities_dropped; kubernetes_container_capabilities_dropped,• Level 2 - Master Node,"If you incorrectly apply security contexts, you may have trouble running the pods.",1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks
5.7.4,The default namespace should not be used,Manual,"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None","Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service","Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used",,• Level 2 - Master Node,None,
