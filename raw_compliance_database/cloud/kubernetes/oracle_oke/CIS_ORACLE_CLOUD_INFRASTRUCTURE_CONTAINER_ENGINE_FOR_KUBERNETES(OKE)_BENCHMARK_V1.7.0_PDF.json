[
  {
    "id": "2.1.1",
    "title": "Client certificate authentication should not be used for users",
    "assessment": "Manual",
    "description": "Kubernetes provides the option to use client certificates for user authentication. However as there is no way to revoke these certificates when a user leaves an organization or loses their credential, they are not suitable for this purpose. It is not possible to fully disable client certificate use within a cluster as it is used for component to component authentication.",
    "rationale": "With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. Impact: External mechanisms for authentication generally require additional software to be deployed.",
    "audit": "Review user access to the cluster and ensure that users are not making use of Kubernetes client certificate authentication. You can verify the availability of client certificates in your OKE cluster. Run the following command to verify the availability of client certificates in your OKE cluster: kubectl get secrets --namespace kube-system This command lists all the secrets in the kube-system namespace, which includes the client certificates used for authentication. Look for secrets with names starting with oci- or oke-. These secrets contain the client certificates. If the command returns secrets with such names, it indicates that client certificates are available in your OKE cluster.",
    "remediation": "Alternative mechanisms provided by Kubernetes such as the use of OIDC should be implemented in place of client certificates. You can remediate the availability of client certificates in your OKE cluster. Default Value: See the OKE documentation for the default value. References: 1. https://cloud.google.com/kubernetes-engine/docs/concepts/cis-benchmarks Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",
    "profile_applicability": "•  Level 1",
    "impact": "External mechanisms for authentication generally require additional software to be deployed.",
    "references": "1. https://cloud.google.com/kubernetes-engine/docs/concepts/cis-benchmarks Additional Information: The lack of certificate revocation was flagged up as a high risk issue in the recent Kubernetes security audit. Without this feature, client certificate authentication is not suitable for end users.",
    "function_names": [
      "kubernetes_user_no_client_cert_auth",
      "kubernetes_user_client_cert_auth_disabled",
      "kubernetes_auth_no_user_client_certificates",
      "kubernetes_user_auth_no_client_certificates",
      "kubernetes_auth_client_cert_revocation_check"
    ]
  },
  {
    "id": "2.2.1",
    "title": "Ensure access to OCI Audit service Log for OKE",
    "assessment": "Manual",
    "description": "The audit logs are part of the OKE managed Kubernetes control plane logs managed by OKE. OKE integrates with Oracle Cloud Infrastructure Audit Service. All operations performed by the Kubernetes API server are visible as log events on the Oracle Cloud Infrastructure Audit service.",
    "rationale": "Logging is a crucial detective control for all systems to detect potential unauthorized access. Impact: The Control plane audit logs are managed by OKE. OKE Control plane logs are written to the Oracle Cloud Infrastructure Audit Service. The Oracle Cloud Infrastructure Audit service automatically records calls to all supported Oracle Cloud Infrastructure public application programming interface (API) endpoints as log events.",
    "audit": "Using Oracle Cloud Infrastructure Console To monitor and manage operations performed by Container Engine for Kubernetes on a particular cluster: 1. In the Console, open the navigation menu. Under Solutions and Platform , go to Developer Services and click Kubernetes Clusters . 2. Choose a Compartment you have permission to work in. 3. On the Cluster List page, click the cluster's name for which you want to monitor and manage operations. 4. The Cluster page shows information about the cluster. 5. Display the Work Requests tab, showing the recent operations performed on the cluster. To view operations performed by Container Engine for Kubernetes and the Kubernetes API server as log events in the Oracle Cloud Infrastructure Audit service: 1. In the Console, open the navigation menu. Under Governance and Administration , go to Governance and click Audit . 2. Choose a Compartment you have permission to work in. 3. Search and filter to show the operations you're interested in: • To view operations performed by Container Engine for Kubernetes, enter ClustersAPI in the Keywords field and click Search . • To view operations performed by the Kubernetes API server, enter OKE API Server Admin Access in the Keywords field and click Search .",
    "remediation": "No remediation is necessary for this control. Default Value: By default, Kubernetes API server logs and Container Engine for Kubernetes audit events are sent to the Oracle Cloud Infrastructure Audit service. By default, the Audit Log retention period is 90 days. References: 1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Tasks/contengmonitoringoke.htm 3. https://docs.cloud.oracle.com/en- us/iaas/Content/Audit/Tasks/viewinglogevents.htm#Viewing_Audit_Log_Events 4. https://docs.cloud.oracle.com/en- us/iaas/Content/Audit/Tasks/settingretentionperiod.htm",
    "profile_applicability": "•  Level 1",
    "impact": "The Control plane audit logs are managed by OKE. OKE Control plane logs are written to the Oracle Cloud Infrastructure Audit Service. The Oracle Cloud Infrastructure Audit service automatically records calls to all supported Oracle Cloud Infrastructure public application programming interface (API) endpoints as log events.",
    "references": "1. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Tasks/contengmonitoringoke.htm 3. https://docs.cloud.oracle.com/en- us/iaas/Content/Audit/Tasks/viewinglogevents.htm#Viewing_Audit_Log_Events 4. https://docs.cloud.oracle.com/en- us/iaas/Content/Audit/Tasks/settingretentionperiod.htm",
    "function_names": [
      "audit_service_log_access_enabled",
      "oke_audit_log_integration_enabled",
      "audit_service_oke_logging_enabled",
      "oke_control_plane_audit_logs_enabled",
      "audit_service_oke_operations_logged",
      "oke_audit_log_visibility_configured",
      "audit_service_oke_api_events_logged",
      "oke_audit_log_integration_active"
    ]
  },
  {
    "id": "3.1.1",
    "title": "Ensure that the kubelet-config.json file permissions are set to 644 or more restrictive",
    "assessment": "Automated",
    "description": "If kubelet is running, and if it is using a file-based kubelet-config.json file, ensure that the proxy kubelet-config.json file has permissions of 644 or more restrictive.",
    "rationale": "The kubelet kubelet-config.json file controls various parameters of the kubelet service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. It is possible to run kubelet with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubelet- config.json file. Impact: None.",
    "audit": "Method 1 SSH to the worker nodes To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate kubelet-config.json file: ps -ef | grep kubelet The output of the above command should return something similar to --kubeconfig /etc/kubernetes/kubelet-config.json which is the location of the kubelet- config.json file. Run this command to obtain the kubelet-config.json file permissions: stat -c %a /etc/kubernetes/kubelet-config.json The output of the above command gives you the kubelet-config.json file's permissions. Verify that if a file is specified and it exists, the permissions are 644 or more restrictive. Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file permissions on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the permission level of the file: ls -l /host/etc/kubernetes/kubelet-config.json Verify that if a file is specified and it exists, the permissions are 644 or more restrictive.",
    "remediation": "Run the below command (based on the file location on your system) on the each worker node. For example, chmod 644 <kubelet-config.json file> Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_config_file_permissions_restrictive",
      "kubernetes_kubelet_config_json_permissions_644_or_stricter",
      "kubernetes_kubelet_config_file_permissions_secure",
      "kubernetes_kubelet_config_json_file_permissions_compliant",
      "kubernetes_kubelet_config_file_permissions_cis_benchmark"
    ]
  },
  {
    "id": "3.1.2",
    "title": "Ensure that the kubelet-config.json file ownership is set to root:root",
    "assessment": "Automated",
    "description": "If kubelet is running, ensure that the file ownership of its kubelet-config.json file is set to root:root.",
    "rationale": "The kubelet-config.json file for kubelet controls various parameters for the kubelet service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root. Impact: None",
    "audit": "Method 1 SSH to the worker nodes To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate kubelet-config.json file: ps -ef | grep kubelet The output of the above command should return something similar to --kubeconfig /etc/kubernetes/kubelet-config.json which is the location of the kubelet- config.json file. Run this command to obtain the kubelet-config.json file ownership: stat -c %U:%G /etc/kubernetes/kubelet-config.json The output of the above command gives you the kubelet-config.json file's ownership. Verify that the ownership is set to root:root. Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file ownership on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the ownership of the file: ls -l /host/etc/kubernetes/kubelet-config.json The output of the above command gives you the kubelet-config.json file's ownership. Verify that the ownership is set to root:root.",
    "remediation": "Run the below command (based on the file location on your system) on the each worker node. For example, chown root:root <kubelet-config.json file> Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://kubernetes.io/docs/admin/kube-proxy/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_config_file_ownership_root",
      "kubernetes_kubelet_config_file_owner_root",
      "kubernetes_kubelet_config_file_group_root",
      "kubernetes_kubelet_config_file_permissions_root_only",
      "kubernetes_kubelet_config_file_secure_ownership"
    ]
  },
  {
    "id": "3.1.3",
    "title": "Ensure that the kubelet configuration file has permissions set to 644 or more restrictive",
    "assessment": "Automated",
    "description": "Ensure that if the kubelet refers to a configuration file with the --config argument, that file has permissions of 644 or more restrictive.",
    "rationale": "The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None.",
    "audit": "Method 1 First, SSH to the relevant worker node: To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet.conf which is the location of the Kubelet config file. Run the following command: stat -c %a /etc/kubernetes/kubelet.conf The output of the above command is the Kubelet config file's permissions. Verify that the permissions are 644 or more restrictive. Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file permissions on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the permission level of the file: ls -l /host/etc/kubernetes/kubelet.conf Verify that if a file is specified and it exists, the permissions are 644 or more restrictive.",
    "remediation": "Run the following command (using the config file location identied in the Audit step) chmod 644 etc/kubernetes/kubelet.conf Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_config_file_permissions_644_or_stricter",
      "kubernetes_kubelet_config_file_permissions_restrictive",
      "kubernetes_kubelet_config_file_permissions_secure",
      "kubernetes_kubelet_config_file_permissions_compliant",
      "kubernetes_kubelet_config_file_permissions_cis_benchmark"
    ]
  },
  {
    "id": "3.1.4",
    "title": "Ensure that the kubelet configuration file ownership is set to root:root",
    "assessment": "Automated",
    "description": "Ensure that if the kubelet refers to a configuration file with the --config argument, that file is owned by root:root.",
    "rationale": "The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. Impact: None.",
    "audit": "Method 1 First, SSH to the relevant worker node: To check to see if the Kubelet Service is running: sudo systemctl status kubelet The output should return Active: active (running) since.. Run the following command on each node to find the appropriate Kubelet config file: ps -ef | grep kubelet The output of the above command should return something similar to --config /etc/kubernetes/kubelet.conf which is the location of the Kubelet config file. Run the following command: stat -c %U:%G /etc/kubernetes/kubelet.conf The output of the above command is the Kubelet config file's ownership. Verify that the ownership is set to root:root Method 2 Create and Run a Privileged Pod. You will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod. Here's an example of a simple pod definition that mounts the root of the host to /host within the pod: apiVersion: v1 kind: Pod metadata: name: file-check spec: volumes: - name: host-root hostPath: path: / type: Directory containers: - name: nsenter image: busybox command: [\"sleep\", \"3600\"] volumeMounts: - name: host-root mountPath: /host securityContext: privileged: true Save this to a file (e.g., file-check-pod.yaml) and create the pod: kubectl apply -f file-check-pod.yaml Once the pod is running, you can exec into it to check file ownership on the node: kubectl exec -it file-check -- sh Now you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the ownership of the file: ls -l /etc/kubernetes/kubelet.conf The output of the above command gives you the azure.json file's ownership. Verify that the ownership is set to root:root.",
    "remediation": "Run the following command (using the config file location identied in the Audit step) chown root:root /etc/kubernetes/kubelet.conf Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubelet_config_file_owner_root",
      "kubelet_config_file_group_root",
      "kubelet_config_file_ownership_root_root",
      "kubelet_config_file_permissions_root",
      "kubelet_config_file_secure_ownership"
    ]
  },
  {
    "id": "3.2.1",
    "title": "Ensure that the --anonymous-auth argument is set to false",
    "assessment": "Automated",
    "description": "Disable anonymous requests to the Kubelet server.",
    "rationale": "When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests. Impact: Anonymous requests will be rejected.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to false. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --anonymous-auth=false. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication... \"anonymous\":{\"enabled\":false} by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --anonymous-auth=false Remediation Method 2: If using the api configz endpoint consider searching for the status of \"authentication.*anonymous\":{\"enabled\":false}\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Anonymous requests will be rejected.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_anonymous_auth_disabled",
      "kubernetes_kubelet_auth_enabled",
      "kubernetes_kubelet_anonymous_access_blocked",
      "kubernetes_kubelet_secure_auth_required",
      "kubernetes_kubelet_auth_configured_properly"
    ]
  },
  {
    "id": "3.2.2",
    "title": "Ensure that the --authorization-mode argument is not set to AlwaysAllow",
    "assessment": "Automated",
    "description": "Do not allow all requests. Enable explicit authorization.",
    "rationale": "Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests. Impact: Unauthorized requests will be denied.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --authentication- mode. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --authentication-mode=Webhook. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication... \"webhook\":{\"enabled\":true} by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --authorization-mode=Webhook Remediation Method 2: If using the api configz endpoint consider searching for the status of \"authentication.*webhook\":{\"enabled\":true}\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Unauthorized requests will be denied.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet- authentication 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_api_server_authorization_mode_not_always_allow",
      "kubernetes_api_server_explicit_authorization_enabled",
      "kubernetes_api_server_authorization_restricted",
      "kubernetes_api_server_no_always_allow_auth",
      "kubernetes_api_server_authorization_mode_secure"
    ]
  },
  {
    "id": "3.2.3",
    "title": "Ensure that the --client-ca-file argument is set as appropriate",
    "assessment": "Automated",
    "description": "Enable Kubelet authentication using certificates.",
    "rationale": "The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests. Impact: You require TLS to be configured on apiserver as well as kubelets.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --client-ca-file set to the location of the client certificate authority file. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --client-ca-file argument exists and is set to the location of the client certificate authority file. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication.. x509\":(\"clientCAFile\":\"/etc/kubernetes/ca.crt by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --client-ca-file=/etc/kubernetes/ca.crt \\ Remediation Method 2: If using the api configz endpoint consider searching for the status of \"authentication.*x509\":(\"clientCAFile\":\"/etc/kubernetes/pki/ca.crt\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/ 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "You require TLS to be configured on apiserver as well as kubelets.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet- authentication-authorization/ 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_client_ca_file_set",
      "kubernetes_kubelet_certificate_authentication_enabled",
      "kubernetes_kubelet_client_ca_file_configured",
      "kubernetes_kubelet_authentication_certificate_valid",
      "kubernetes_kubelet_client_ca_file_present"
    ]
  },
  {
    "id": "3.2.4",
    "title": "Ensure that the --read-only-port argument is set to 0",
    "assessment": "Automated",
    "description": "Disable the read-only port.",
    "rationale": "The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster. Impact: Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.",
    "audit": "If using a Kubelet configuration file, check that there is an entry for --read-only- port=0. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet service config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --read-only-port argument exists and is set to 0. If the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.",
    "remediation": "If modifying the Kubelet config file, edit the kubelet.service file /etc/sytemd/system/kubelet.service and set the below parameter --read-only-port=0 For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/",
    "profile_applicability": "•  Level 1",
    "impact": "Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/",
    "function_names": [
      "kubernetes_api_server_read_only_port_disabled",
      "kubernetes_api_server_read_only_port_zero",
      "kubernetes_api_server_read_only_port_unset",
      "kubernetes_api_server_read_only_port_secure",
      "kubernetes_api_server_read_only_port_restricted"
    ]
  },
  {
    "id": "3.2.5",
    "title": "Ensure that the --streaming-connection-idle-timeout argument is not set to 0",
    "assessment": "Automated",
    "description": "Do not disable timeouts on streaming connections.",
    "rationale": "Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports. Note: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases. Impact: Long-lived connections could be interrupted.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --streaming- connection-idle-timeout is not set to 0. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --streaming-connection-idle-timeout argument is not set to 0. Audit Method 2: If using the api configz endpoint consider searching for the status of \"streamingConnectionIdleTimeout\":\"4h0m0s\" by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --streaming-connection-idle-timeout Remediation Method 2: If using the api configz endpoint consider searching for the status of \"streamingConnectionIdleTimeout\": by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Long-lived connections could be interrupted.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/pull/18552 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_api_server_streaming_connection_idle_timeout_not_disabled",
      "kubernetes_api_server_streaming_connection_timeout_configured",
      "kubernetes_api_server_streaming_idle_timeout_non_zero",
      "kubernetes_api_server_connection_idle_timeout_enabled",
      "kubernetes_api_server_streaming_timeout_valid"
    ]
  },
  {
    "id": "3.2.6",
    "title": "Ensure that the --make-iptables-util-chains argument is set to true",
    "assessment": "Automated",
    "description": "Allow Kubelet to manage iptables.",
    "rationale": "Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open. Impact: Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --make-iptables- util-chains set to true. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --make-iptables-util-chains=true. Audit Method 2: If using the api configz endpoint consider searching for the status of authentication... \"makeIPTablesUtilChains\":true by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --make-iptables-util-chains:true Remediation Method 2: If using the api configz endpoint consider searching for the status of \"makeIPTablesUtilChains\": true by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_iptables_util_chains_enabled",
      "kubernetes_kubelet_iptables_managed",
      "kubernetes_kubelet_iptables_util_chains_true",
      "kubernetes_kubelet_iptables_config_valid",
      "kubernetes_kubelet_iptables_chains_configured"
    ]
  },
  {
    "id": "3.2.7",
    "title": "Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture",
    "assessment": "Automated",
    "description": "Security relevant information should be captured. The --event-qps flag on the Kubelet can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",
    "rationale": "It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data. Impact: Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --event-qps set to 0 or a value equal to or greater than 0. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --event-qps=0. Audit Method 2: If using the api configz endpoint consider searching for the status of \"eventRecordQPS\": 0 by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --event-qps=0 Remediation Method 2: If using the api configz endpoint consider searching for the status of \"eventRecordQPS\" by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Setting this parameter to 0 could result in a denial of service condition due to excessive events being created. The cluster's event processing and storage systems should be scaled to handle expected event loads.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletco nfig/v1beta1/types.go 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_event_qps_disabled",
      "kubernetes_kubelet_event_qps_within_safe_limit",
      "kubernetes_kubelet_event_qps_appropriate_rate",
      "kubernetes_kubelet_event_qps_no_denial_of_service",
      "kubernetes_kubelet_event_qps_logging_optimized"
    ]
  },
  {
    "id": "3.2.8",
    "title": "Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate",
    "assessment": "Automated",
    "description": "Setup TLS connection on the Kubelets.",
    "rationale": "Kubelet communication contains sensitive parameters that should remain encrypted in transit. Configure the Kubelets to serve only HTTPS traffic. Impact: TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for tls-cert-file set to correct pem file and tls-private-key-file is set to correct key file First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the tls-cert-file=/var/lib/kubelet/pki/tls.pem. Verify that the tls-private-key-file=/var/lib/kubelet/pki/tls.key. Audit Method 2: If using the api configz endpoint consider searching for the status of tlsCertFile and tlsPrivateKeyFile are set by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter Verify that the `tls-cert-file=/var/lib/kubelet/pki/tls.pem`. Verify that the `tls-private-key-file=/var/lib/kubelet/pki/tls.key`. Remediation Method 2: If using the api configz endpoint consider searching for the status of tlsCertFile and tlsPrivateKeyFile are set by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://kubernetes.io/docs/admin/kubelet/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide 4. https://jvns.ca/blog/2017/08/05/how-kubernetes-certificates-work/ 5. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.",
    "references": "1. https://kubernetes.io/docs/admin/kubelet/ 2. http://rootsquash.com/2016/05/10/securing-the-kubernetes-api/ 3. https://github.com/kelseyhightower/docker-kubernetes-tls-guide 4. https://jvns.ca/blog/2017/08/05/how-kubernetes-certificates-work/ 5. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_tls_cert_file_set",
      "kubernetes_kubelet_tls_private_key_file_set",
      "kubernetes_kubelet_tls_connection_configured",
      "kubernetes_kubelet_tls_cert_and_key_valid",
      "kubernetes_kubelet_tls_encryption_enabled"
    ]
  },
  {
    "id": "3.2.9",
    "title": "Ensure that the --rotate-certificates argument is not set to false",
    "assessment": "Automated",
    "description": "Enable kubelet client certificate rotation.",
    "rationale": "The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Note: This feature also require the RotateKubeletClientCertificate feature gate to be enabled (which is the default since Kubernetes v1.7) Impact: None",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --rotate- certificates set to true. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --rotate-certificates is present. Audit Method 2: If using the api configz endpoint consider searching for the status of rotateCertificates by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter Verify that the `--rotate-certificates` is present. Remediation Method 2: If using the api configz endpoint consider searching for the status of rotateCertificates by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ 5. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://github.com/kubernetes/kubernetes/pull/41912 2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls- bootstrapping/#kubelet-configuration 3. https://kubernetes.io/docs/imported/release/notes/ 4. https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/ 5. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_kubelet_certificate_rotation_enabled",
      "kubernetes_kubelet_rotate_certificates_not_disabled",
      "kubernetes_kubelet_client_cert_rotation_required",
      "kubernetes_kubelet_no_false_certificate_rotation",
      "kubernetes_kubelet_cert_rotation_argument_valid"
    ]
  },
  {
    "id": "3.2.10",
    "title": "Ensure that the --rotate-server-certificates argument is set to true",
    "assessment": "Automated",
    "description": "Enable kubelet server certificate rotation.",
    "rationale": "--rotate-server-certificates causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad. Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself. Impact: None",
    "audit": "Audit Method 1: If using a Kubelet configuration file, check that there is an entry for --rotate-server- certificates is set to true. First, SSH to the relevant node: Run the following command on each node to find the appropriate Kubelet config file: find / -name kubelet.service The output of the above command should return the file and location /etc/systemd/system/kublet.service which is the location of the Kubelet service config file. Open the Kubelet service config file: sudo more etc/systemd/system/kublet.service Verify that the --rotate-server-certificates=true. Audit Method 2: If using the api configz endpoint consider searching for the status of --rotate-server- certificates by extracting the live configuration from the nodes running kubelet. Set the local proxy port and the following variables and provide proxy port number and node name; HOSTNAME_PORT=\"localhost-and-port-number\" NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\" kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation": "Remediation Method 1: If modifying the Kubelet service config file, edit the kubelet.service file /etc/systemd/system/kubelet.service and set the below parameter --rotate-server-certificates=true Remediation Method 2: If using the api configz endpoint consider searching for the status of --rotate-server- certificates by extracting the live configuration from the nodes running kubelet. **See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes kubectl proxy --port=8001 & export HOSTNAME_PORT=localhost:8001 (example host and port number) export NODE_NAME=10.0.10.4 (example node name from \"kubectl get nodes\") curl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\" For all remediations: Based on your system, restart the kubelet service and check status systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet -l Default Value: See the OKE documentation for the default value. References: 1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm 4. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls- bootstrapping/#certificate-rotation",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://github.com/kubernetes/kubernetes/pull/45059 2. https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration 3. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm 4. https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls- bootstrapping/#certificate-rotation",
    "function_names": [
      "kubernetes_kubelet_server_certificate_rotation_enabled",
      "kubernetes_kubelet_rotate_server_certificates_enabled",
      "kubernetes_kubelet_certificate_rotation_enabled",
      "kubernetes_kubelet_tls_certificate_rotation_enabled",
      "kubernetes_kubelet_server_cert_rotation_enabled"
    ]
  },
  {
    "id": "4.1.1",
    "title": "Ensure that the cluster-admin role is only used where required",
    "assessment": "Automated",
    "description": "The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.",
    "rationale": "Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself. Impact: Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",
    "audit": "Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster- admin role. kubectl get clusterrolebindings -o=custom- columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name Review each principal listed and ensure that cluster-admin privilege is required for it.",
    "remediation": "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges. Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name] Default Value: By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal. References: 1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",
    "references": "1. https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_role_no_cluster_admin",
      "kubernetes_role_cluster_admin_restricted",
      "kubernetes_role_cluster_admin_minimal_usage",
      "kubernetes_role_cluster_admin_least_privilege",
      "kubernetes_role_cluster_admin_required_only"
    ]
  },
  {
    "id": "4.1.2",
    "title": "Minimize access to secrets",
    "assessment": "Automated",
    "description": "The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",
    "rationale": "Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets. Impact: Care should be taken not to remove access to secrets to system components which require this for their operation",
    "audit": "Review the users who have get, list or watch access to secrets objects in the Kubernetes API. Run the following command to list all the users who have access to secrets objects in the Kubernetes API: kubectl auth can-i get,list,watch secrets --all-namespaces -- as=system:authenticated This command checks if the system:authenticated group (which includes all authenticated users) has the get, list, or watch permissions for secrets objects in all namespaces. The output will display either yes or no for each user. Note: If you want to check for specific namespaces, replace --all-namespaces with the desired namespace(s) separated by commas.",
    "remediation": "Where possible, remove get, list and watch access to secret objects in the cluster. Default Value: By default, the following list of principals have get privileges on secret objects CLUSTERROLEBINDING                                    SUBJECT TYPE            SA-NAMESPACE cluster-admin                                         system:masters Group system:controller:clusterrole-aggregation-controller  clusterrole- aggregation-controller  ServiceAccount  kube-system system:controller:expand-controller                   expand-controller ServiceAccount  kube-system system:controller:generic-garbage-collector           generic-garbage- collector           ServiceAccount  kube-system system:controller:namespace-controller                namespace-controller ServiceAccount  kube-system system:controller:persistent-volume-binder            persistent-volume- binder            ServiceAccount  kube-system system:kube-controller-manager                        system:kube-controller- manager      User References: 1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken not to remove access to secrets to system components which require this for their operation",
    "references": "1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_secret_access_restricted",
      "kubernetes_secret_minimal_access",
      "kubernetes_secret_no_public_access",
      "kubernetes_secret_no_wildcard_access",
      "kubernetes_secret_no_default_access",
      "kubernetes_secret_no_anonymous_access",
      "kubernetes_secret_no_broad_permissions",
      "kubernetes_secret_no_unrestricted_access",
      "kubernetes_secret_no_privileged_access",
      "kubernetes_secret_no_excessive_permissions"
    ]
  },
  {
    "id": "4.1.3",
    "title": "Minimize wildcard use in Roles and ClusterRoles",
    "assessment": "Automated",
    "description": "Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard \"*\" which matches all items. Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.",
    "rationale": "The principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.",
    "audit": "Retrieve the roles defined across each namespaces in the cluster and review for wildcards kubectl get roles --all-namespaces -o yaml Retrieve the cluster roles defined in the cluster and review for wildcards kubectl get clusterroles -o yaml",
    "remediation": "Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.",
    "profile_applicability": "•  Level 1",
    "function_names": [
      "kubernetes_role_no_wildcard_resources",
      "kubernetes_clusterrole_no_wildcard_resources",
      "kubernetes_role_no_wildcard_actions",
      "kubernetes_clusterrole_no_wildcard_actions",
      "kubernetes_role_no_wildcard_resources_or_actions",
      "kubernetes_clusterrole_no_wildcard_resources_or_actions",
      "kubernetes_role_minimize_wildcard_usage",
      "kubernetes_clusterrole_minimize_wildcard_usage"
    ]
  },
  {
    "id": "4.1.4",
    "title": "Minimize access to create pods",
    "assessment": "Automated",
    "description": "The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access) As such, access to create new pods should be restricted to the smallest possible group of users.",
    "rationale": "The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible. Impact: Care should be taken not to remove access to pods to system components which require this for their operation",
    "audit": "Review the users who have create access to pod objects in the Kubernetes API. Run the following command to list all the users who have access to create pod objects in the Kubernetes API: kubectl auth can-i create pods --all-namespaces --as=system:authenticated This command checks if the system:authenticated group (which includes all authenticated users) has the create permission for pod objects in all namespaces. The output will display either yes or no for each user. Note: If you want to check for specific namespaces, replace --all-namespaces with the desired namespace(s) separated by commas.",
    "remediation": "Where possible, remove create access to pod objects in the cluster. Default Value: By default, the following list of principals have create privileges on pod objects CLUSTERROLEBINDING                                    SUBJECT TYPE            SA-NAMESPACE cluster-admin                                         system:masters Group system:controller:clusterrole-aggregation-controller  clusterrole- aggregation-controller  ServiceAccount  kube-system system:controller:daemon-set-controller               daemon-set-controller ServiceAccount  kube-system system:controller:job-controller                      job-controller ServiceAccount  kube-system system:controller:persistent-volume-binder            persistent-volume- binder            ServiceAccount  kube-system system:controller:replicaset-controller               replicaset-controller ServiceAccount  kube-system system:controller:replication-controller              replication-controller ServiceAccount  kube-system system:controller:statefulset-controller              statefulset-controller ServiceAccount  kube-system",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken not to remove access to pods to system components which require this for their operation",
    "function_names": [
      "kubernetes_namespace_pod_creation_restricted",
      "kubernetes_role_pod_creation_minimized",
      "kubernetes_cluster_pod_creation_limited",
      "kubernetes_service_account_pod_creation_restricted",
      "kubernetes_rbac_pod_creation_minimized",
      "kubernetes_policy_pod_creation_limited",
      "kubernetes_user_pod_creation_restricted",
      "kubernetes_group_pod_creation_minimized"
    ]
  },
  {
    "id": "4.1.5",
    "title": "Ensure that default service accounts are not actively used.",
    "assessment": "Automated",
    "description": "The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.",
    "rationale": "Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments. Impact: All workloads which require access to the Kubernetes API will require an explicit service account to be created.",
    "audit": "For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account. Check for automountServiceAccountToken using: export SERVICE_ACCOUNT=<service account name> kubectl get serviceaccounts/$SERVICE_ACCOUNT -o yaml",
    "remediation": "Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server. Modify the configuration of each default service account to include this value automountServiceAccountToken: false Automatic remediation for the default account: kubectl patch serviceaccount default -p $'automountServiceAccountToken: false' Default Value: By default the default service account allows for its service account token to be mounted in pods in its namespace. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "All workloads which require access to the Kubernetes API will require an explicit service account to be created.",
    "references": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "iam_service_account_default_not_used",
      "iam_service_account_default_inactive",
      "compute_service_account_default_disabled",
      "compute_service_account_default_no_usage",
      "service_account_default_no_active_usage",
      "service_account_default_no_privileges_used",
      "service_account_default_no_application_usage"
    ]
  },
  {
    "id": "4.1.6",
    "title": "Ensure that Service Account Tokens are only mounted where necessary",
    "assessment": "Automated",
    "description": "Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server",
    "rationale": "Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue. Impact: Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",
    "audit": "Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access. automountServiceAccountToken: false Check for automountServiceAccountToken using: export SERVICE_ACCOUNT=<service account name> kubectl get serviceaccounts/$SERVICE_ACCOUNT -o yaml export POD=<pod name> kubectl get pods/$POD -o yaml",
    "remediation": "Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it. Default Value: By default, all pods get a service account token mounted in them. References: 1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",
    "profile_applicability": "•  Level 1",
    "impact": "Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",
    "references": "1. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service- account/",
    "function_names": [
      "kubernetes_pod_service_account_token_not_mounted",
      "kubernetes_pod_service_account_token_mounted_only_when_required",
      "kubernetes_pod_service_account_token_unnecessary_mount_disabled",
      "kubernetes_pod_service_account_token_mount_restricted",
      "kubernetes_pod_service_account_token_mount_minimized"
    ]
  },
  {
    "id": "4.2.1",
    "title": "Minimize the admission of privileged containers",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the securityContext.privileged flag set to true.",
    "rationale": "Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices. There should be at least one admission control policy defined which does not permit privileged containers. If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of privileged containers. Since manually searching through each pod's configuration might be tedious, especially in environments with many pods, you can use a more automated approach with grep or other command-line tools. Here's an example of how you might approach this with a combination of kubectl, grep, and shell scripting for a more automated solution: kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[].securityContext.privileged == true) | .metadata.name' OR kubectl get pods --all-namespaces -o json | jq '.items[] | select(.metadata.namespace != \"kube-system\" and .spec.containers[]?.securityContext?.privileged == true) | {pod: .metadata.name, namespace: .metadata.namespace, container: .spec.containers[].name}' When creating a Pod Security Policy, [\"kube-system\"] namespaces are excluded by default. This command uses jq, a command-line JSON processor, to parse the JSON output from kubectl get pods and filter out pods where any container has the securityContext.privileged flag set to true. Please note that you might need to adjust the command depending on your specific requirements and the structure of your pod specifications.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers. To enable PSA for a namespace in your cluster, set the pod- security.kubernetes.io/enforce label with the policy value you want to enforce. kubectl label --overwrite ns NAMESPACE pod- security.kubernetes.io/enforce=restricted The above command enforces the restricted policy for the NAMESPACE namespace. You can also enable Pod Security Admission for all your namespaces. For example: kubectl label --overwrite ns --all pod- security.kubernetes.io/warn=baseline Default Value: By default, there are no restrictions on the creation of privileged containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission/",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.containers[].securityContext.privileged: true, spec.initContainers[].securityContext.privileged: true and spec.ephemeralContainers[].securityContext.privileged: true will not be permitted.",
    "references": "1. https://kubernetes.io/docs/concepts/security/pod-security-admission/",
    "function_names": [
      "compute_container_privileged_disabled",
      "compute_container_privileged_escalation_disabled",
      "compute_container_security_context_privileged_disabled",
      "compute_container_privileged_mode_disabled",
      "compute_container_privileged_flag_disabled"
    ]
  },
  {
    "id": "4.2.2",
    "title": "Minimize the admission of containers wishing to share the host process ID namespace",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the hostPID flag set to true.",
    "rationale": "A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container. There should be at least one admission control policy defined which does not permit containers to share the host PID namespace. If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostPID containers Search for the hostPID Flag: In the YAML output, look for the hostPID setting under the spec section to check if it is set to true. kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostPID == true) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq '.items[] | select(.metadata.namespace != \"kube-system\" and .spec.hostPID == true) | {pod: .metadata.name, namespace: .metadata.namespace, container: .spec.containers[].name}' When creating a Pod Security Policy, [\"kube-system\"] namespaces are excluded by default. This command retrieves all pods across all namespaces in JSON format, then uses jq to filter out those with the hostPID flag set to true, and finally formats the output to show the namespace and name of each matching pod.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostPID containers. Default Value: By default, there are no restrictions on the creation of hostPID containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.hostPID: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "compute_container_host_pid_disabled",
      "compute_container_host_pid_restricted",
      "compute_container_host_pid_not_shared",
      "compute_container_host_pid_isolated",
      "compute_container_host_pid_protected"
    ]
  },
  {
    "id": "4.2.3",
    "title": "Minimize the admission of containers wishing to share the host IPC namespace",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the hostIPC flag set to true.",
    "rationale": "A container running in the host's IPC namespace can use IPC to interact with processes outside the container. There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace. If you need to run containers which require hostIPC, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostIPC containers Search for the hostIPC Flag: In the YAML output, look for the hostIPC setting under the spec section to check if it is set to true. kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostIPC == true) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq '.items[] | select(.metadata.namespace != \"kube-system\" and .spec.hostIPC == true) | {pod: .metadata.name, namespace: .metadata.namespace, container: .spec.containers[].name}' When creating a Pod Security Policy, [\"kube-system\"] namespaces are excluded by default. This command retrieves all pods across all namespaces in JSON format, then uses jq to filter out those with the hostIPC flag set to true, and finally formats the output to show the namespace and name of each matching pod.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers. Default Value: By default, there are no restrictions on the creation of hostIPC containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.hostIPC: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "compute_container_host_ipc_disabled",
      "compute_container_host_ipc_not_shared",
      "compute_container_host_ipc_restricted",
      "compute_container_host_ipc_denied",
      "compute_container_host_ipc_protected"
    ]
  },
  {
    "id": "4.2.4",
    "title": "Minimize the admission of containers wishing to share the host network namespace",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the hostNetwork flag set to true.",
    "rationale": "A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods. There should be at least one admission control policy defined which does not permit containers to share the host network namespace. If you need to run containers which require access to the host's network namespaces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of hostNetwork containers Given that manually checking each pod can be time-consuming, especially in large environments, you can use a more automated approach to filter out pods where hostNetwork is set to true. Here’s a command using kubectl and jq: kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.hostNetwork == true) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq '.items[] | select(.metadata.namespace != \"kube-system\" and .spec.hostNetwork == true) | {pod: .metadata.name, namespace: .metadata.namespace, container: .spec.containers[].name}' When creating a Pod Security Policy, [\"kube-system\"] namespaces are excluded by default. This command retrieves all pods across all namespaces in JSON format, then uses jq to filter out those with the hostNetwork flag set to true, and finally formats the output to show the namespace and name of each matching pod.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers. Default Value: By default, there are no restrictions on the creation of hostNetwork containers. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.hostNetwork: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "compute_container_host_network_disabled",
      "compute_pod_host_network_disabled",
      "kubernetes_pod_host_network_disabled",
      "kubernetes_container_host_network_disabled",
      "container_host_network_sharing_disabled",
      "pod_host_network_sharing_disabled",
      "container_host_network_namespace_isolated",
      "pod_host_network_namespace_isolated"
    ]
  },
  {
    "id": "4.2.5",
    "title": "Minimize the admission of containers with allowPrivilegeEscalation",
    "assessment": "Automated",
    "description": "Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.",
    "rationale": "A container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent. There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run. If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy. Impact: Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.",
    "audit": "List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation. This command gets all pods across all namespaces, outputs their details in JSON format, and uses jq to parse and filter the output for containers with allowPrivilegeEscalation set to true. kubectl get pods --all-namespaces -o json | jq -r '.items[] | select(any(.spec.containers[]; .securityContext.allowPrivilegeEscalation == true)) | \"\\(.metadata.namespace)/\\(.metadata.name)\"' OR kubectl get pods --all-namespaces -o json | jq '.items[] | select(.metadata.namespace != \"kube-system\" and .spec.containers[]; .securityContext.allowPrivilegeEscalation == true) | {pod: .metadata.name, namespace: .metadata.namespace, container: .spec.containers[].name}' When creating a Pod Security Policy, [\"kube-system\"] namespaces are excluded by default. This command uses jq, a command-line JSON processor, to parse the JSON output from kubectl get pods and filter out pods where any container has the securityContext.privileged flag set to true. Please note that you might need to adjust the command depending on your specific requirements and the structure of your pod specifications.",
    "remediation": "Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with .spec.allowPrivilegeEscalation set to true. Default Value: By default, there are no restrictions on contained process ability to escalate privileges, within the context of the container. References: 1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Pods defined with spec.allowPrivilegeEscalation: true will not be permitted unless they are run under a specific policy.",
    "references": "1. https://kubernetes.io/docs/concepts/security/pod-security-admission/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "compute_container_privilege_escalation_disabled",
      "compute_container_allow_privilege_escalation_false",
      "compute_container_privilege_escalation_restricted",
      "compute_container_privilege_escalation_minimized",
      "compute_container_privilege_escalation_denied"
    ]
  },
  {
    "id": "4.3.1",
    "title": "Ensure latest CNI version is used",
    "assessment": "Automated",
    "description": "There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.",
    "rationale": "Kubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies. Impact: None.",
    "audit": "Review the documentation of CNI plugin in use by the cluster, and confirm that it supports network policies. Check the DaemonSets in the kube-system namespace: Many CNI plugins operate as DaemonSets within the kube-system namespace. To see what's running: kubectl get daemonset -n kube-system Look for known CNI providers like Calico, Flannel, Cilium, etc. You can further inspect the configuration of these DaemonSets to understand more about the CNI setup: kubectl describe daemonset <daemonset-name> -n kube-system Check the CNI Configuration Files: If you have access to the nodes (via SSH), you can check the CNI configuration directly in /etc/cni/net.d/. This often requires node-level access, which might not be available depending on your permissions and the security setup of your environment.",
    "remediation": "As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico. Default Value: This will depend on the CNI plugin in use. References: 1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage- net/network-plugins/ 2. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm Additional Information: One example here is Flannel (https://github.com/coreos/flannel) which does not support Network policy unless Calico is also in use.",
    "function_names": [
      "kubernetes_cni_network_policy_supported",
      "kubernetes_cni_latest_version_used",
      "kubernetes_cni_network_policy_enabled",
      "kubernetes_cni_plugin_up_to_date",
      "kubernetes_cni_plugin_supports_network_policies"
    ]
  },
  {
    "id": "4.3.2",
    "title": "Ensure that all Namespaces have Network Policies defined",
    "assessment": "Automated",
    "description": "Use network policies to isolate traffic in your cluster network.",
    "rationale": "Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints. Network Policies are namespace scoped. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace. Impact: Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",
    "audit": "Run the below command and review the NetworkPolicy objects created in the cluster. kubectl get networkpolicy --all-namespaces Ensure that each namespace defined in the cluster has at least one Network Policy.",
    "remediation": "Follow the documentation and create NetworkPolicy objects as you need them. Clusters you create with Container Engine for Kubernetes have flannel installed as the default CNI network provider. flannel is a simple overlay virtual network that satisfies the requirements of the Kubernetes networking model by attaching IP addresses to containers. Although flannel satisfies the requirements of the Kubernetes networking model, it does not support NetworkPolicy resources. If you want to enhance the security of clusters you create with Container Engine for Kubernetes by implementing network policies, you have to install and configure a network provider that does support NetworkPolicy resources. One such provider is Calico (refer to the Kubernetes documentation for a list of other network providers). Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. Use the Calico open-source software in conjunction with flannel. The Calico Enterprise does not support flannel. Default Value: By default, network policies are not created. References: 1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",
    "profile_applicability": "•  Level 1",
    "impact": "Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied. As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",
    "references": "1. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/ 2. https://octetz.com/posts/k8s-network-policy-apis 3. https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/",
    "function_names": [
      "kubernetes_namespace_network_policy_defined",
      "kubernetes_namespace_network_policy_required",
      "kubernetes_namespace_traffic_isolation_enabled",
      "kubernetes_network_policy_namespace_coverage",
      "kubernetes_namespace_network_restrictions_enforced"
    ]
  },
  {
    "id": "4.4.1",
    "title": "Prefer using secrets as files over secrets as environment variables",
    "assessment": "Automated",
    "description": "Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.",
    "rationale": "It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Impact: Application code which expects to read secrets in the form of environment variables would need modification",
    "audit": "Run the following command to find references to objects which use environment variables defined from secrets. kubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A",
    "remediation": "If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables. Default Value: By default, secrets are not defined References: 1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",
    "profile_applicability": "•  Level 1",
    "impact": "Application code which expects to read secrets in the form of environment variables would need modification",
    "references": "1. https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets Additional Information: Mounting secrets as volumes has the additional benefit that secret values can be updated without restarting the pod",
    "function_names": [
      "kubernetes_secret_files_preferred",
      "kubernetes_secret_environment_variables_avoided",
      "kubernetes_secret_volume_mount_required",
      "kubernetes_secret_non_env_var_usage",
      "kubernetes_secret_data_volume_enabled"
    ]
  },
  {
    "id": "4.4.2",
    "title": "Consider external secret storage",
    "assessment": "Manual",
    "description": "Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.",
    "rationale": "Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments. Impact: None",
    "audit": "Review your secrets management implementation.",
    "remediation": "Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution. The master nodes in a Kubernetes cluster store sensitive configuration data (such as authentication tokens, passwords, and SSH keys) as Kubernetes secret objects in etcd. Etcd is an open source distributed key-value store that Kubernetes uses for cluster coordination and state management. In the Kubernetes clusters created by Container Engine for Kubernetes, etcd writes and reads data to and from block storage volumes in the Oracle Cloud Infrastructure Block Volume service. Although the data in block storage volumes is encrypted, Kubernetes secrets at rest in etcd itself are not encrypted by default. For additional security, when you create a new cluster you can specify that Kubernetes secrets at rest in etcd are to be encrypted using the Oracle Cloud Infrastructure Vault service. Default Value: By default, no external secret management is configured. References: 1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_secrets_external_storage_required",
      "kubernetes_secrets_authentication_required",
      "kubernetes_secrets_audit_logging_enabled",
      "kubernetes_secrets_encryption_enabled",
      "kubernetes_secrets_rotation_enabled"
    ]
  },
  {
    "id": "4.5.1",
    "title": "Create administrative boundaries between resources using namespaces",
    "assessment": "Manual",
    "description": "Use namespaces to isolate your Kubernetes objects.",
    "rationale": "Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users. Impact: You need to switch between namespaces for administration.",
    "audit": "Run the below command and review the namespaces created in the cluster. kubectl get namespaces Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.",
    "remediation": "Follow the documentation and create namespaces for objects in your deployment as you need them. Default Value: By default, Kubernetes starts with two initial namespaces: 1. default - The default namespace for objects with no other namespace 2. kube-system - The namespace for objects created by the Kubernetes system 3. kube-public - The namespace for public-readable ConfigMap 4. kube-node-lease - The namespace for associated lease object for each node References: 1. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html",
    "profile_applicability": "•  Level 1",
    "impact": "You need to switch between namespaces for administration.",
    "references": "1. https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/ 2. http://blog.kubernetes.io/2016/08/security-best-practices-kubernetes- deployment.html",
    "function_names": [
      "kubernetes_namespace_isolation_enabled",
      "kubernetes_namespace_admin_boundaries_configured",
      "kubernetes_namespace_resource_separation_enforced",
      "kubernetes_namespace_security_boundaries_defined",
      "kubernetes_namespace_isolation_policy_applied"
    ]
  },
  {
    "id": "4.5.2",
    "title": "Apply Security Context to Your Pods and Containers",
    "assessment": "Manual",
    "description": "Apply Security Context to Your Pods and Containers",
    "rationale": "A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context. Impact: If you incorrectly apply security contexts, you may have trouble running the pods.",
    "audit": "Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.",
    "remediation": "As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we recommend implementing a more restrictive policy such as this: apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' spec: privileged: false # Required to prevent escalations to root. allowPrivilegeEscalation: false # This is redundant with non-root + disallow privilege escalation, # but we can provide it for defense in depth. requiredDropCapabilities: - ALL # Allow core volume types. volumes: - 'configMap' - 'emptyDir' - 'projected' - 'secret' - 'downwardAPI' # Assume that persistentVolumes set up by the cluster admin are safe to use. - 'persistentVolumeClaim' hostNetwork: false hostIPC: false hostPID: false runAsUser: # Require the container to run without root privileges. rule: 'MustRunAsNonRoot' seLinux: # This policy assumes the nodes are using AppArmor rather than SELinux. rule: 'RunAsAny' supplementalGroups: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 fsGroup: rule: 'MustRunAs' ranges: # Forbid adding the root group. - min: 1 max: 65535 readOnlyRootFilesystem: false This policy prevents pods from running as privileged or escalating privileges. It also restricts the types of volumes that can be mounted and the root supplemental groups that can be added. Another, albeit similar, approach is to start with policy that locks everything down and incrementally add exceptions for applications that need looser restrictions such as logging agents which need the ability to mount a host path. Default Value: By default, no security contexts are automatically applied to pods. References: 1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks 3. https://aws.github.io/aws-eks-best-practices/pods/#restrict-the-containers-that- can-run-as-privileged 4. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "If you incorrectly apply security contexts, you may have trouble running the pods.",
    "references": "1. https://kubernetes.io/docs/concepts/policy/security-context/ 2. https://learn.cisecurity.org/benchmarks 3. https://aws.github.io/aws-eks-best-practices/pods/#restrict-the-containers-that- can-run-as-privileged 4. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_pod_security_context_configured",
      "kubernetes_container_security_context_configured",
      "kubernetes_pod_privileged_mode_disabled",
      "kubernetes_container_privileged_mode_disabled",
      "kubernetes_pod_read_only_root_filesystem_enabled",
      "kubernetes_container_read_only_root_filesystem_enabled",
      "kubernetes_pod_run_as_non_root_enabled",
      "kubernetes_container_run_as_non_root_enabled",
      "kubernetes_pod_capabilities_dropped",
      "kubernetes_container_capabilities_dropped"
    ]
  },
  {
    "id": "4.5.3",
    "title": "The default namespace should not be used",
    "assessment": "Automated",
    "description": "Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.",
    "rationale": "Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources. Impact: None",
    "audit": "Run this command to list objects in default namespace kubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default The only entries there should be system managed resources such as the kubernetes service OR kubectl get all -n default",
    "remediation": "Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace. Default Value: Unless a namespace is specific on object creation, the default namespace will be used References: 1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None",
    "references": "1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengoverview.htm",
    "function_names": [
      "kubernetes_namespace_not_default",
      "kubernetes_namespace_default_denied",
      "kubernetes_namespace_non_default_used",
      "kubernetes_namespace_default_unused",
      "kubernetes_namespace_default_restricted"
    ]
  },
  {
    "id": "5.1.1",
    "title": "Oracle Cloud Security Penetration and Vulnerability Testing",
    "assessment": "Manual",
    "description": "Oracle regularly performs penetration and vulnerability testing and security assessments against the Oracle Cloud infrastructure, platforms, and applications. These tests are intended to validate and improve the overall security of Oracle Cloud services.",
    "rationale": "Vulnerabilities in software packages can be exploited by hackers or malicious users to obtain unauthorized access to local cloud resources. Oracle Cloud Container Analysis and other third party products allow images stored in Oracle Cloud to be scanned for known vulnerabilities. Impact: None.",
    "audit": "As a service administrator, you can run tests for some Oracle Cloud services. Before running the tests, you must first review the Oracle Cloud Testing Policies section. Follow the steps below to notify Oracle of a penetration and vulnerability test.",
    "remediation": "As a service administrator, you can run tests for some Oracle Cloud services. Before running the tests, you must first review the Oracle Cloud Testing Policies section. Note: You must have an Oracle Account with the necessary privileges to file service maintenance requests, and you must be signed in to the environment that will be the subject of the penetration and vulnerability testing. Submitting a Cloud Security Testing Notification References: 1. https://docs.cloud.oracle.com/en- us/iaas/Content/Security/Concepts/security_testing-policy.htm",
    "profile_applicability": "•  Level 1",
    "impact": "None.",
    "references": "1. https://docs.cloud.oracle.com/en- us/iaas/Content/Security/Concepts/security_testing-policy.htm",
    "function_names": [
      "cloud_infrastructure_penetration_testing_performed",
      "cloud_platform_vulnerability_testing_enabled",
      "cloud_application_security_assessment_performed",
      "cloud_infrastructure_security_validation_completed",
      "cloud_platform_penetration_testing_enabled",
      "cloud_application_vulnerability_testing_performed",
      "cloud_infrastructure_security_assessment_completed",
      "cloud_platform_security_validation_performed",
      "cloud_application_penetration_testing_enabled",
      "cloud_infrastructure_vulnerability_testing_completed"
    ]
  },
  {
    "id": "5.1.2",
    "title": "Minimize user access control to Container Engine for Kubernetes",
    "assessment": "Manual",
    "description": "Restrict user access to OKE, limiting interaction with build images to only authorized personnel and service accounts.",
    "rationale": "Weak access control to OKE may allow malicious users to replace built images with vulnerable or backdoored containers. Impact: Care should be taken not to remove access to Oracle Cloud Infrastructure Registry (OCR) for accounts that require this for their operation. Any account granted the Storage Object Viewer role at the project level can view all objects stored in OCS for the project.",
    "audit": "For most operations on Kubernetes clusters created and managed by Container Engine for Kubernetes, Oracle Cloud Infrastructure Identity and Access Management (IAM) provides access control. A user's permissions to access clusters comes from the groups to which they belong. The permissions for a group are defined by policies. Policies define what actions members of a group can perform, and in which compartments. Users can then access clusters and perform operations based on the policies set for the groups they are members of. IAM provides control over: • whether a user can create or delete clusters • whether a user can add, remove, or modify node pools • which Kubernetes object create/delete/view operations a user can perform on all clusters within a compartment or tenancy See Policy Configuration for Cluster Creation and Deployment. In addition to IAM, the Kubernetes RBAC Authorizer can enforce additional fine-grained access control for users on specific clusters via Kubernetes RBAC roles and clusterroles. A Kubernetes RBAC role is a collection of permissions. For example, a role might include read permission on pods and list permission for pods. A Kubernetes RBAC clusterrole is just like a role, but can be used anywhere in the cluster. A Kubernetes RBAC rolebinding maps a role to a user or set of users, granting that role's permissions to those users for resources in that namespace. Similarly, a Kubernetes RBAC clusterrolebinding maps a clusterrole to a user or set of users, granting that clusterrole's permissions to those users across the entire cluster.",
    "remediation": "By default, users are not assigned any Kubernetes RBAC roles (or clusterroles) by default. So before attempting to create a new role (or clusterrole), you must be assigned an appropriately privileged role (or clusterrole). A number of such roles and clusterroles are always created by default, including the cluster-admin clusterrole (for a full list, see Default Roles and Role Bindings in the Kubernetes documentation). The cluster-admin clusterrole essentially confers super-user privileges. A user granted the cluster-admin clusterrole can perform any operation across all namespaces in a given cluster. Note that Oracle Cloud Infrastructure tenancy administrators already have sufficient privileges, and do not require the cluster-admin clusterrole. See: Granting the Kubernetes RBAC cluster-admin clusterrole",
    "profile_applicability": "•  Level 1",
    "impact": "Care should be taken not to remove access to Oracle Cloud Infrastructure Registry (OCR) for accounts that require this for their operation. Any account granted the Storage Object Viewer role at the project level can view all objects stored in OCS for the project.",
    "function_names": [
      "oke_cluster_user_access_restricted",
      "oke_cluster_build_image_access_authorized",
      "oke_cluster_service_account_access_limited",
      "oke_cluster_user_privileges_minimized",
      "oke_cluster_image_interaction_restricted",
      "oke_cluster_access_control_enforced",
      "oke_cluster_user_permissions_least_privilege",
      "oke_cluster_build_access_authorized_only"
    ]
  },
  {
    "id": "5.1.3",
    "title": "Minimize cluster access to read-only",
    "assessment": "Manual",
    "description": "Configure the Cluster Service Account to only allow read-only access to OKE.",
    "rationale": "The Cluster Service Account does not require administrative access to OCR, only requiring pull access to containers to deploy onto OKE. Restricting permissions follows the principles of least privilege and prevents credentials from being abused beyond the required role. Impact: A separate dedicated service account may be required for use by build servers and other robot users pushing or managing container images.",
    "audit": "Review Oracle OCS worker node IAM role IAM Policy Permissions to verify that they are set and the minimum required level. If utilizing a 3rd party tool to scan images utilize the minimum required permission level required to interact with the cluster - generally this should be read-only.",
    "remediation": "To access a cluster using kubectl, you have to set up a Kubernetes configuration file (commonly known as a 'kubeconfig' file) for the cluster. The kubeconfig file (by default named config and stored in the $HOME/.kube directory) provides the necessary details to access the cluster. Having set up the kubeconfig file, you can start using kubectl to manage the cluster. The steps to follow when setting up the kubeconfig file depend on how you want to access the cluster: • To access the cluster using kubectl in Cloud Shell, run an Oracle Cloud Infrastructure CLI command in the Cloud Shell window to set up the kubeconfig file. • To access the cluster using a local installation of kubectl: 1. Generate an API signing key pair (if you don't already have one). 2. Upload the public key of the API signing key pair. 3. Install and configure the Oracle Cloud Infrastructure CLI. 4. Set up the kubeconfig file. See Setting Up Local Access to Clusters Default Value: The default permissions for the cluster Service account is dependent on the initial configuration and IAM policy.",
    "profile_applicability": "•  Level 1",
    "impact": "A separate dedicated service account may be required for use by build servers and other robot users pushing or managing container images.",
    "function_names": [
      "oke_cluster_service_account_read_only_access",
      "oke_cluster_read_only_permissions",
      "oke_service_account_minimal_access",
      "oke_cluster_access_restricted_read_only",
      "oke_cluster_service_account_least_privilege"
    ]
  },
  {
    "id": "5.1.4",
    "title": "Minimize Container Registries to only those approved",
    "assessment": "Manual",
    "description": "Use approved container registries.",
    "rationale": "Allowing unrestricted access to external container registries provides the opportunity for malicious or unapproved containers to be deployed into the cluster. Allow listing only approved container registries reduces this risk. Impact: All container images to be deployed to the cluster must be hosted within an approved container image registry.",
    "audit": "",
    "remediation": null,
    "profile_applicability": "•  Level 1",
    "impact": "All container images to be deployed to the cluster must be hosted within an approved container image registry.",
    "function_names": [
      "container_registry_approved_only",
      "container_registry_unapproved_blocked",
      "container_registry_allowlist_enabled",
      "container_registry_denylist_enabled",
      "container_registry_restricted_to_approved",
      "container_registry_approved_sources_only",
      "container_registry_approved_registries_only",
      "container_registry_approved_repositories_only"
    ]
  },
  {
    "id": "5.2.1",
    "title": "Prefer using dedicated Service Accounts",
    "assessment": "Automated",
    "description": "Kubernetes workloads should not use cluster node service accounts to authenticate to Oracle Cloud APIs. Each Kubernetes Workload that needs to authenticate to other Oracle services using Cloud IAM should be provisioned a dedicated Service account.",
    "rationale": "Manual approaches for authenticating Kubernetes workloads running on OKE against Oracle Cloud APIs are: storing service account keys as a Kubernetes secret (which introduces manual key rotation and potential for key compromise); or use of the underlying nodes' IAM Service account, which violates the principle of least privilege on a multitenanted node, when one pod needs to have access to a service, but every other pod on the node that uses the Service account does not.",
    "audit": "For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults. Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",
    "remediation": "When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. If you get the raw json or yaml for a pod you have created (for example, kubectl get pods/<podname> -o yaml), you can see the spec.serviceAccountName field has been automatically set. See Configure Service Accounts for Pods",
    "profile_applicability": "•  Level 1"
  },
  {
    "id": "5.3.1",
    "title": "Encrypting Kubernetes Secrets at Rest in Etcd",
    "assessment": "Manual",
    "description": "Encrypt Kubernetes secrets, stored in etcd, at the application-layer using a customer- managed key.",
    "rationale": "The master nodes in a Kubernetes cluster store sensitive configuration data (such as authentication tokens, passwords, and SSH keys) as Kubernetes secret objects in etcd. Etcd is an open source distributed key-value store that Kubernetes uses for cluster coordination and state management. In the Kubernetes clusters created by Container Engine for Kubernetes, etcd writes and reads data to and from block storage volumes in the Oracle Cloud Infrastructure Block Volume service. Although the data in block storage volumes is encrypted, Kubernetes secrets at rest in etcd itself are not encrypted by default.",
    "audit": "Before you can create a cluster where Kubernetes secrets are encrypted in the etcd key-value store, you have to: • know the name and OCID of a suitable master encryption key in Vault • create a dynamic group that includes all clusters in the compartment in which you are going to create the new cluster • create a policy authorizing the dynamic group to use the master encryption key",
    "remediation": "You can create a cluster in one tenancy that uses a master encryption key in a different tenancy. In this case, you have to write cross-tenancy policies to enable the cluster in its tenancy to access the master encryption key in the Vault service's tenancy. Note that if you want to create a cluster and specify a master encryption key that's in a different tenancy, you cannot use the Console to create the cluster. For example, assume the cluster is in the ClusterTenancy, and the master encryption key is in the KeyTenancy. Users belonging to a group (OKEAdminGroup) in the ClusterTenancy have permissions to create clusters. A dynamic group (OKEAdminDynGroup) has been created in the cluster, with the rule ALL {resource.type = 'cluster', resource.compartment.id = 'ocid1.compartment.oc1..<unique_ID>'}, so all clusters created in the ClusterTenancy belong to the dynamic group. In the root compartment of the KeyTenancy, the following policies: • use the ClusterTenancy's OCID to map ClusterTenancy to the alias OKE_Tenancy • use the OCIDs of OKEAdminGroup and OKEAdminDynGroup to map them to the aliases RemoteOKEAdminGroup and RemoteOKEClusterDynGroup respectively • give RemoteOKEAdminGroup and RemoteOKEClusterDynGroup the ability to list, view, and perform cryptographic operations with a particular master key in the KeyTenancy Define tenancy OKE_Tenancy as ocid1.tenancy.oc1..<unique_ID> Define dynamic-group RemoteOKEClusterDynGroup as ocid1.dynamicgroup.oc1..<unique_ID> Define group RemoteOKEAdminGroup as ocid1.group.oc1..<unique_ID> Admit dynamic-group RemoteOKEClusterDynGroup of tenancy ClusterTenancy to use keys in tenancy where target.key.id = 'ocid1.key.oc1..<unique_ID>' Admit group RemoteOKEAdminGroup of tenancy ClusterTenancy to use keys in tenancy where target.key.id = 'ocid1.key.oc1..<unique_ID>' In the root compartment of the ClusterTenancy, the following policies: • use the KeyTenancy's OCID to map KeyTenancy to the alias KMS_Tenancy • give OKEAdminGroup and OKEAdminDynGroup the ability to use master keys in the KeyTenancy • allow OKEAdminDynGroup to use a specific master key obtained from the KeyTenancy in the ClusterTenancy Define tenancy KMS_Tenancy as ocid1.tenancy.oc1..<unique_ID> Endorse group OKEAdminGroup to use keys in tenancy KMS_Tenancy Endorse dynamic-group OKEAdminDynGroup to use keys in tenancy KMS_Tenancy Allow dynamic-group OKEAdminDynGroup to use keys in tenancy where target.key.id = 'ocid1.key.oc1..<unique_ID>' See Accessing Object Storage Resources Across Tenancies for more examples of writing cross-tenancy policies. Having entered the policies, you can now run a command similar to the following to create a cluster in the ClusterTenancy that uses the master key obtained from the KeyTenancy: oci ce cluster create --name oke-with-cross-kms --kubernetes-version v1.16.8 --vcn-id ocid1.vcn.oc1.iad.<unique_ID> --service-lb-subnet-ids '[\"ocid1.subnet.oc1.iad.<unique_ID>\"]' --compartment-id ocid1.compartment.oc1..<unique_ID> --kms-key-id ocid1.key.oc1.iad.<unique_ID> References: 1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Tasks/contengencryptingdata.htm",
    "profile_applicability": "•  Level 1",
    "references": "1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Tasks/contengencryptingdata.htm",
    "function_names": [
      "kubernetes_etcd_secrets_encryption_enabled",
      "kubernetes_etcd_secrets_customer_key_managed",
      "kubernetes_etcd_secrets_application_layer_encrypted",
      "kubernetes_etcd_secrets_at_rest_encrypted",
      "kubernetes_etcd_secrets_encryption_key_configured"
    ]
  },
  {
    "id": "5.4.1",
    "title": "Restrict Access to the Control Plane Endpoint",
    "assessment": "Automated",
    "description": "Enable Master Authorized Networks to restrict access to the cluster's control plane (master endpoint) to only an allowlist (whitelist) of authorized IPs.",
    "rationale": "Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your cluster's control plane. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster's control plane from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network. Restricting access to an authorized network can provide additional security benefits for your container cluster, including: • Better protection from outsider attacks: Authorized networks provide an additional layer of security by limiting external, non-OCP access to a specific set of addresses you designate, such as those that originate from your premises. This helps protect access to your cluster in the case of a vulnerability in the cluster's authentication or authorization mechanism. • Better protection from insider attacks: Authorized networks help protect your cluster from accidental leaks of master certificates from your company's premises. Leaked certificates used from outside OCP and outside the authorized IP ranges (for example, from addresses outside your company) are still denied access. Impact: When implementing Master Authorized Networks, be careful to ensure all desired networks are on the allowlist (whitelist) to prevent inadvertently blocking external access to your cluster's control plane.",
    "audit": "Check for the following: export OKE_CLUSTERID=<oke cluster id> oci ce cluster get --cluster-id $OKE_CLUSTERID Check for endpoint URL of the Kubernetes API Server in output",
    "remediation": "Default Value: By default, Master Authorized Networks is disabled.",
    "profile_applicability": "•  Level 1",
    "impact": "When implementing Master Authorized Networks, be careful to ensure all desired networks are on the allowlist (whitelist) to prevent inadvertently blocking external access to your cluster's control plane.",
    "function_names": [
      "gke_cluster_master_authorized_networks_enabled",
      "gke_cluster_control_plane_access_restricted",
      "gke_cluster_master_endpoint_whitelisted",
      "gke_cluster_master_ip_allowlist_enabled",
      "gke_cluster_control_plane_ip_restricted"
    ]
  },
  {
    "id": "5.4.2",
    "title": "Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled",
    "assessment": "Automated",
    "description": "Disable access to the Kubernetes API from outside the node network if it is not required.",
    "rationale": "In a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's VPC network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's VPC network. Although Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's VPC network to perform any attack on the Kubernetes API. Impact: This topic gives an overview of the options for enabling private access to services within Oracle Cloud Infrastructure. Private access means that traffic does not go over the internet. Access can be from hosts within your virtual cloud network (VCN) or your on- premises network. • You can enable private access to certain services within Oracle Cloud Infrastructure from your VCN or on-premises network by using either a private endpoint or a service gateway. See the sections that follow. • For each private access option, these services or resource types are available: o With a private endpoint: Autonomous Database (shared Exadata infrastructure) o With a service gateway: Available services • With either private access option, the traffic stays within the Oracle Cloud Infrastructure network and does not traverse the internet. However, if you use a service gateway, requests to the service use a public endpoint for the service. • If you do not want to access a given Oracle service through a public endpoint, Oracle recommends using a private endpoint in your VCN (assuming the service supports private endpoints). A private endpoint is represented as a private IP address within a subnet in your VCN. See About Private Endpoints",
    "audit": "Check for the following: export OKE_CLUSTERID=<oke cluster id> oci ce cluster get --cluster-id $OKE_CLUSTERID",
    "remediation": "Default Value: By default, the Public Endpoint is disabled.",
    "profile_applicability": "•  Level 1",
    "impact": "This topic gives an overview of the options for enabling private access to services within Oracle Cloud Infrastructure. Private access means that traffic does not go over the internet. Access can be from hosts within your virtual cloud network (VCN) or your on- premises network. • You can enable private access to certain services within Oracle Cloud Infrastructure from your VCN or on-premises network by using either a private endpoint or a service gateway. See the sections that follow. • For each private access option, these services or resource types are available: o With a private endpoint: Autonomous Database (shared Exadata infrastructure) o With a service gateway: Available services • With either private access option, the traffic stays within the Oracle Cloud Infrastructure network and does not traverse the internet. However, if you use a service gateway, requests to the service use a public endpoint for the service. • If you do not want to access a given Oracle service through a public endpoint, Oracle recommends using a private endpoint in your VCN (assuming the service supports private endpoints). A private endpoint is represented as a private IP address within a subnet in your VCN. See About Private Endpoints",
    "function_names": [
      "kubernetes_cluster_private_endpoint_enabled",
      "kubernetes_cluster_public_access_disabled",
      "kubernetes_cluster_network_access_restricted",
      "kubernetes_api_external_access_disabled",
      "kubernetes_control_plane_private_only",
      "kubernetes_cluster_endpoint_no_public_ip",
      "kubernetes_api_server_private_network",
      "kubernetes_cluster_public_access_blocked"
    ]
  },
  {
    "id": "5.4.3",
    "title": "Ensure clusters are created with Private Nodes",
    "assessment": "Automated",
    "description": "Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with no public IP addresses.",
    "rationale": "Disabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts. Impact: To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.",
    "audit": "Check for the following: export OKE_NODEPOOLID=<node pool id> export OKE_NODEID=<node id> oci ce node-pool get --node-pool-id ${OKE_NODEPOOLID=} --node-id ${OKE_NODEID=} [--query <query>] [--auth <auth-token>] [--profile <profile- name>] [--region <region>]",
    "remediation": "Default Value: By default, Private Nodes are disabled.",
    "profile_applicability": "•  Level 1",
    "impact": "To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled. Private Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.",
    "function_names": [
      "gke_cluster_private_nodes_enabled",
      "gke_node_public_ip_disabled",
      "gke_cluster_node_private_networking_enabled",
      "gke_node_external_ip_disabled",
      "gke_cluster_private_ip_only_enabled"
    ]
  },
  {
    "id": "5.4.4",
    "title": "Ensure Network Policy is Enabled and set as appropriate",
    "assessment": "Automated",
    "description": "Use Network Policy to restrict pod to pod traffic within a cluster and segregate workloads.",
    "rationale": "By default, all pod to pod traffic within a cluster is allowed. Network Policy creates a pod-level firewall that can be used to restrict traffic between sources. Pod traffic is restricted by having a Network Policy that selects it (through the use of labels). Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic. Network Policies are managed via the Kubernetes Network Policy API and enforced by a network plugin, simply creating the resource without a compatible network plugin to implement it will have no effect. OKE supports Network Policy enforcement through the use of Calico. Impact: Network Policy requires the Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an existing cluster, needs to be added prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block other operations on the cluster (including delete) until it has run to completion. If Network Policy is used, a cluster must have at least 2 nodes of type n1-standard-1 or higher. The recommended minimum size cluster to run Network Policy enforcement is 3 n1-standard-1 instances. Enabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory footprint of the kube-system process by approximately 128MB, and requires approximately 300 millicores of CPU.",
    "audit": "Check for the following: export OKE_CLUSTERID=<oke cluster id> oci ce cluster get --cluster-id $OKE_CLUSTERID Check settings for Network Policy is enabled set correctly for the cluster \"kubernetesNetworkConfig\": { \"podsCidr\": \"10.244.0.0/16\", \"servicesCidr\": \"10.96.0.0/12\", \"networkPolicyConfig\": { \"isEnabled\": true } }",
    "remediation": "Configure Network Policy for the Cluster Default Value: By default, Network Policy is disabled.",
    "profile_applicability": "•  Level 1",
    "impact": "Network Policy requires the Network Policy add-on. This add-on is included automatically when a cluster with Network Policy is created, but for an existing cluster, needs to be added prior to enabling Network Policy. Enabling/Disabling Network Policy causes a rolling update of all cluster nodes, similar to performing a cluster upgrade. This operation is long-running and will block other operations on the cluster (including delete) until it has run to completion. If Network Policy is used, a cluster must have at least 2 nodes of type n1-standard-1 or higher. The recommended minimum size cluster to run Network Policy enforcement is 3 n1-standard-1 instances. Enabling Network Policy enforcement consumes additional resources in nodes. Specifically, it increases the memory footprint of the kube-system process by approximately 128MB, and requires approximately 300 millicores of CPU.",
    "function_names": [
      "kubernetes_network_policy_enabled",
      "kubernetes_network_policy_restrict_pod_traffic",
      "kubernetes_network_policy_workload_segregation",
      "kubernetes_network_policy_appropriate_settings",
      "kubernetes_network_policy_pod_traffic_restricted",
      "kubernetes_network_policy_workloads_isolated"
    ]
  },
  {
    "id": "5.4.5",
    "title": "Encrypt traffic to HTTPS load balancers with TLS certificates",
    "assessment": "Manual",
    "description": "Encrypt traffic to HTTPS load balancers using TLS certificates.",
    "rationale": "Encrypting traffic between users and your Kubernetes workload is fundamental to protecting data sent over the web.",
    "audit": "Your load balancer vendor can provide details on auditing the certificates and policies required to utilize TLS.",
    "remediation": "Your load balancer vendor can provide details on configuring HTTPS with TLS.",
    "profile_applicability": "•  Level 1",
    "function_names": [
      "cloud_cdn_load_balancer_tls_encryption_enabled",
      "compute_load_balancer_https_tls_certificate_attached",
      "cloud_cdn_load_balancer_min_tls_1_2",
      "compute_load_balancer_ssl_certificate_valid",
      "cloud_cdn_load_balancer_https_redirect_enabled"
    ]
  },
  {
    "id": "5.5.1",
    "title": "Access Control and Container Engine for Kubernetes",
    "assessment": "Manual",
    "description": "Cluster Administrators should leverage Oracle Groups and Cloud IAM to assign Kubernetes user roles to a collection of users, instead of to individual emails using only Cloud IAM.",
    "rationale": "For most operations on Kubernetes clusters created and managed by Container Engine for Kubernetes, Oracle Cloud Infrastructure Identity and Access Management (IAM) provides access control. A user's permissions to access clusters comes from the groups to which they belong. The permissions for a group are defined by policies. Policies define what actions members of a group can perform, and in which compartments. Users can then access clusters and perform operations based on the policies set for the groups they are members of. IAM provides control over: • whether a user can create or delete clusters • whether a user can add, remove, or modify node pools • which Kubernetes object create/delete/view operations a user can perform on all clusters within a compartment or tenancy See Policy Configuration for Cluster Creation and Deployment Impact: Users must now be assigned to the IAM group created to use this namespace and deploy applications. If they are not they will not be able to access the namespace or deploy.",
    "audit": "By default, users are not assigned any Kubernetes RBAC roles (or clusterroles) by default. So before attempting to create a new role (or clusterrole), you must be assigned an appropriately privileged role (or clusterrole). A number of such roles and clusterroles are always created by default, including the cluster-admin clusterrole (for a full list, see Default Roles and Role Bindings in the Kubernetes documentation). The cluster-admin clusterrole essentially confers super-user privileges. A user granted the cluster-admin clusterrole can perform any operation across all namespaces in a given cluster.",
    "remediation": "Example: Granting the Kubernetes RBAC cluster-admin clusterrole Follow these steps to grant a user who is not a tenancy administrator the Kubernetes RBAC cluster-admin clusterrole on a cluster deployed on Oracle Cloud Infrastructure: 1. If you haven't already done so, follow the steps to set up the cluster's kubeconfig configuration file and (if necessary) set the KUBECONFIG environment variable to point to the file. Note that you must set up your own kubeconfig file. You cannot access a cluster using a kubeconfig file that a different user set up. See Setting Up Cluster Access. 2. In a terminal window, grant the Kubernetes RBAC cluster-admin clusterrole to the user by entering: $ kubectl create clusterrolebinding <my-cluster-admin-binding> -- clusterrole=cluster-admin --user=<user_OCID> where: • is a string of your choice to be used as the name for the binding between the user and the Kubernetes RBAC cluster-admin clusterrole. For example, jdoe_clst_adm • <user_OCID> is the user's OCID (obtained from the Console ). For example, ocid1.user.oc1..aaaaa...zutq (abbreviated for readability). For example: $ kubectl create clusterrolebinding jdoe_clst_adm --clusterrole=cluster-admin --user=ocid1.user.oc1..aaaaa...zutq References: 1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengaboutaccesscontrol.htm",
    "profile_applicability": "•  Level 1",
    "impact": "Users must now be assigned to the IAM group created to use this namespace and deploy applications. If they are not they will not be able to access the namespace or deploy.",
    "references": "1. https://docs.cloud.oracle.com/en- us/iaas/Content/ContEng/Concepts/contengaboutaccesscontrol.htm",
    "function_names": [
      "iam_group_kubernetes_roles_assigned",
      "iam_user_no_direct_kubernetes_roles",
      "kubernetes_cluster_admin_iam_groups_only",
      "kubernetes_role_iam_group_based",
      "container_engine_iam_group_roles_enabled",
      "kubernetes_user_roles_no_individual_emails",
      "iam_group_kubernetes_admin_privileges",
      "kubernetes_cluster_iam_group_access_control"
    ]
  }
]